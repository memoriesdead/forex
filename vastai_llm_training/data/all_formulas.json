[
  {
    "name": "__init__",
    "category": "data_processing",
    "formula": "k = ~8 hours)",
    "explanation": "Args:\n    max_size: Max ticks to store (default 50k = ~8 hours)\n    horizons: Outcome horizons in ticks (default [10, 50])",
    "python_code": "def __init__(self, max_size: int = 50000, horizons: List[int] = None):\n        \"\"\"\n        Args:\n            max_size: Max ticks to store (default 50k = ~8 hours)\n            horizons: Outcome horizons in ticks (default [10, 50])\n        \"\"\"\n        self.max_size = max_size\n        self.horizons = horizons or [10, 50]\n        self.max_horizon = max(self.horizons)\n\n        # Main buffer - stores TickRecords\n        self._buffer: deque = deque(maxlen=max_size)\n\n        # Pending outcomes - ticks waiting for outcome calculation\n        self._pending: deque = deque(maxlen=max_size)\n\n        # Lock for thread safety\n        self._lock = threading.RLock()\n\n        # Stats\n        self.total_ticks = 0\n        self.labeled_ticks = 0",
    "source_file": "core\\data\\buffer.py",
    "academic_reference": null,
    "class_name": "LiveTickBuffer"
  },
  {
    "name": "add_tick",
    "category": "data_processing",
    "formula": "",
    "explanation": "Add a new tick to the buffer.\n\nArgs:\n    symbol: Currency pair\n    bid: Bid price\n    ask: Ask price\n    features: Feature dict from HFTFeatureEngine\n    timestamp: Unix timestamp (default: now)",
    "python_code": "def add_tick(self, symbol: str, bid: float, ask: float,\n                 features: Dict[str, float], timestamp: float = None):\n        \"\"\"\n        Add a new tick to the buffer.\n\n        Args:\n            symbol: Currency pair\n            bid: Bid price\n            ask: Ask price\n            features: Feature dict from HFTFeatureEngine\n            timestamp: Unix timestamp (default: now)\n        \"\"\"\n        if timestamp is None:\n            timestamp = time.time()\n\n        mid = (bid + ask) / 2\n\n        record = TickRecord(\n            timestamp=timestamp,\n            symbol=symbol,\n            bid=bid,\n            ask=ask,\n            mid=mid,\n            features=features.copy()\n        )\n\n        with self._lock:\n            self._buffer.append(record)\n            self._pending.append(record)\n            self.total_ticks += 1\n\n            # Calculate outcomes for old ticks\n            self._calculate_outcomes(mid)",
    "source_file": "core\\data\\buffer.py",
    "academic_reference": null,
    "class_name": "LiveTickBuffer"
  },
  {
    "name": "_calculate_outcomes",
    "category": "data_processing",
    "formula": "",
    "explanation": "Calculate outcomes for ticks that are old enough.",
    "python_code": "def _calculate_outcomes(self, current_mid: float):\n        \"\"\"Calculate outcomes for ticks that are old enough.\"\"\"\n        with self._lock:\n            while len(self._pending) > self.max_horizon:\n                old_tick = self._pending.popleft()\n\n                # Calculate direction outcomes\n                if 10 in self.horizons:\n                    future_mid = self._get_mid_at_offset(10)\n                    if future_mid is not None:\n                        old_tick.outcome_direction_10 = 1 if future_mid > old_tick.mid else 0\n                        old_tick.outcome_return_10 = (future_mid - old_tick.mid) / old_tick.mid * 10000  # bps\n\n                if 50 in self.horizons:\n                    future_mid = self._get_mid_at_offset(50)\n                    if future_mid is not None:\n                        old_tick.outcome_direction_50 = 1 if future_mid > old_tick.mid else 0\n                        old_tick.outcome_return_50 = (future_mid - old_tick.mid) / old_tick.mid * 10000  # bps\n\n                self.labeled_ticks += 1",
    "source_file": "core\\data\\buffer.py",
    "academic_reference": null,
    "class_name": "LiveTickBuffer"
  },
  {
    "name": "_get_mid_at_offset",
    "category": "data_processing",
    "formula": "",
    "explanation": "Get mid price at offset from pending head.",
    "python_code": "def _get_mid_at_offset(self, offset: int) -> Optional[float]:\n        \"\"\"Get mid price at offset from pending head.\"\"\"\n        with self._lock:\n            if len(self._buffer) > offset:\n                # Get the tick at offset from the oldest pending tick\n                idx = len(self._buffer) - len(self._pending) + offset\n                if 0 <= idx < len(self._buffer):\n                    return self._buffer[idx].mid\n        return None",
    "source_file": "core\\data\\buffer.py",
    "academic_reference": null,
    "class_name": "LiveTickBuffer"
  },
  {
    "name": "get_training_data",
    "category": "technical",
    "formula": "None = all) | symbol: Filter by symbol (None = all) | X, y, feature_names",
    "explanation": "Get labeled training data for retraining.\n\nArgs:\n    n_samples: Number of samples to return\n    symbol: Filter by symbol (None = all)\n\nReturns:\n    X: Feature matrix (n_samples, n_features)\n    y: Labels (n_samples,)\n    feature_names: List of feature names",
    "python_code": "def get_training_data(self, n_samples: int = 5000,\n                          symbol: str = None) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n        \"\"\"\n        Get labeled training data for retraining.\n\n        Args:\n            n_samples: Number of samples to return\n            symbol: Filter by symbol (None = all)\n\n        Returns:\n            X: Feature matrix (n_samples, n_features)\n            y: Labels (n_samples,)\n            feature_names: List of feature names\n        \"\"\"\n        with self._lock:\n            # Get labeled ticks\n            labeled = [t for t in self._buffer\n                      if t.outcome_direction_10 is not None\n                      and (symbol is None or t.symbol == symbol)]\n\n            if len(labeled) < 100:\n                return None, None, None\n\n            # Take most recent n_samples\n            samples = labeled[-n_samples:] if len(labeled) > n_samples else labeled\n\n            # Extract features and labels\n            feature_names = list(samples[0].features.keys())\n            X = np.array([[t.features[f] for f in feature_names] for t in samples])\n            y = np.array([t.outcome_direction_10 for t in samples])\n\n            return X, y, feature_names",
    "source_file": "core\\data\\buffer.py",
    "academic_reference": null,
    "class_name": "LiveTickBuffer"
  },
  {
    "name": "get_stats",
    "category": "data_processing",
    "formula": "{",
    "explanation": "Get buffer statistics.",
    "python_code": "def get_stats(self) -> Dict:\n        \"\"\"Get buffer statistics.\"\"\"\n        with self._lock:\n            return {\n                'total_ticks': self.total_ticks,\n                'buffer_size': len(self._buffer),\n                'pending_labels': len(self._pending),\n                'labeled_ticks': self.labeled_ticks,\n                'buffer_capacity': self.max_size,\n                'fill_pct': len(self._buffer) / self.max_size * 100\n            }",
    "source_file": "core\\data\\buffer.py",
    "academic_reference": null,
    "class_name": "LiveTickBuffer"
  },
  {
    "name": "clear",
    "category": "data_processing",
    "formula": "",
    "explanation": "Clear all data.",
    "python_code": "def clear(self):\n        \"\"\"Clear all data.\"\"\"\n        with self._lock:\n            self._buffer.clear()\n            self._pending.clear()\n            self.total_ticks = 0\n            self.labeled_ticks = 0",
    "source_file": "core\\data\\buffer.py",
    "academic_reference": null,
    "class_name": "LiveTickBuffer"
  },
  {
    "name": "get_tick_buffer",
    "category": "data_processing",
    "formula": "_global_buffer",
    "explanation": "Get or create the global tick buffer.",
    "python_code": "def get_tick_buffer(max_size: int = 50000) -> LiveTickBuffer:\n    \"\"\"Get or create the global tick buffer.\"\"\"\n    global _global_buffer\n    with _buffer_lock:\n        if _global_buffer is None:\n            _global_buffer = LiveTickBuffer(max_size=max_size)\n        return _global_buffer",
    "source_file": "core\\data\\buffer.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "data_processing",
    "formula": "",
    "explanation": "Initialize loader.",
    "python_code": "def __init__(self, data_dir: Path = None):\n        \"\"\"Initialize loader.\"\"\"\n        self.data_dir = data_dir or Path(\"data/truefx\")\n        self.data_dir.mkdir(parents=True, exist_ok=True)",
    "source_file": "core\\data\\loader.py",
    "academic_reference": null,
    "class_name": "TrueFXHistoricalLoader"
  },
  {
    "name": "load_month",
    "category": "data_processing",
    "formula": "pd.read_parquet(cache_file) | pd.DataFrame() | df",
    "explanation": "Load one month of tick data.\n\nArgs:\n    symbol: Currency pair\n    year: Year (e.g., 2024)\n    month: Month (1-12)\n\nReturns:\n    DataFrame with tick data",
    "python_code": "def load_month(self, symbol: str, year: int, month: int) -> pd.DataFrame:\n        \"\"\"\n        Load one month of tick data.\n\n        Args:\n            symbol: Currency pair\n            year: Year (e.g., 2024)\n            month: Month (1-12)\n\n        Returns:\n            DataFrame with tick data\n        \"\"\"\n        # Check local cache first\n        cache_file = self.data_dir / f\"{symbol}_{year}_{month:02d}.parquet\"\n\n        if cache_file.exists():\n            logger.info(f\"Loading from cache: {cache_file}\")\n            return pd.read_parquet(cache_file)\n\n        # Download from TrueFX\n        url = f\"{self.BASE_URL}/{year}/{month:02d}/{symbol}-{year}-{month:02d}.zip\"\n\n        try:\n            import requests\n            import zipfile\n\n            response = requests.get(url, timeout=60)\n            if response.status_code != 200:\n                logger.warning(f\"Failed to download: {url}\")\n                return pd.DataFrame()\n\n            # Extract and parse\n            with zipfile.ZipFile(io.BytesIO(response.content)) as zf:\n                for name in zf.namelist():\n                    if name.endswith('.csv'):\n                        with zf.open(name) as f:\n                            df = pd.read_csv(\n                                f,\n                                header=None,\n                                names=['symbol', 'timestamp', 'bid', 'ask'],\n                                parse_dates=['timestamp']\n                            )\n                            break\n\n            # Cache locally\n            df.to_parquet(cache_file)\n            logger.info(f\"Cached {len(df)} ticks to {cache_file}\")\n\n            return df\n\n        except Exception as e:\n            logger.error(f\"Error loading TrueFX data: {e}\")\n            return pd.DataFrame()",
    "source_file": "core\\data\\loader.py",
    "academic_reference": null,
    "class_name": "TrueFXHistoricalLoader"
  },
  {
    "name": "load_range",
    "category": "data_processing",
    "formula": "pd.DataFrame() | pd.concat(frames, ignore_index=True).sort_values('timestamp')",
    "explanation": "Load tick data for date range.",
    "python_code": "def load_range(self, symbol: str, start_date: datetime,\n                   end_date: datetime) -> pd.DataFrame:\n        \"\"\"Load tick data for date range.\"\"\"\n        frames = []\n\n        current = start_date.replace(day=1)\n        while current <= end_date:\n            df = self.load_month(symbol, current.year, current.month)\n            if not df.empty:\n                # Filter to date range\n                df = df[(df['timestamp'] >= start_date) &\n                        (df['timestamp'] <= end_date)]\n                frames.append(df)\n\n            # Next month\n            if current.month == 12:\n                current = current.replace(year=current.year + 1, month=1)\n            else:\n                current = current.replace(month=current.month + 1)\n\n        if not frames:\n            return pd.DataFrame()\n\n        return pd.concat(frames, ignore_index=True).sort_values('timestamp')",
    "source_file": "core\\data\\loader.py",
    "academic_reference": null,
    "class_name": "TrueFXHistoricalLoader"
  },
  {
    "name": "stop",
    "category": "data_processing",
    "formula": "",
    "explanation": "Stop streaming.",
    "python_code": "def stop(self):\n        \"\"\"Stop streaming.\"\"\"\n        self.running = False",
    "source_file": "core\\data\\loader.py",
    "academic_reference": null,
    "class_name": "TrueFXLiveLoader"
  },
  {
    "name": "add_callback",
    "category": "data_processing",
    "formula": "",
    "explanation": "Add tick callback.",
    "python_code": "def add_callback(self, callback: Callable[[TickData], None]):\n        \"\"\"Add tick callback.\"\"\"\n        self.callbacks.append(callback)",
    "source_file": "core\\data\\loader.py",
    "academic_reference": null,
    "class_name": "TrueFXLiveLoader"
  },
  {
    "name": "list_available",
    "category": "data_processing",
    "formula": "organized | {}",
    "explanation": "List available data on Oracle Cloud.",
    "python_code": "def list_available(self) -> Dict[str, List[str]]:\n        \"\"\"List available data on Oracle Cloud.\"\"\"\n        import subprocess\n\n        try:\n            cmd = f'ssh -i \"{self.ssh_key}\" ubuntu@{self.host} \"find {self.remote_path} -name \\'*.csv\\' -o -name \\'*.parquet\\' | head -100\"'\n            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)\n\n            files = result.stdout.strip().split('\\n')\n\n            # Organize by type\n            organized = {\n                'forex': [],\n                'hft_1sec': [],\n                'historical': []\n            }\n\n            for f in files:\n                if 'forex' in f.lower():\n                    organized['forex'].append(f)\n                elif 'hft_1sec' in f.lower():\n                    organized['hft_1sec'].append(f)\n                else:\n                    organized['historical'].append(f)\n\n            return organized\n\n        except Exception as e:\n            logger.error(f\"Error listing Oracle Cloud data: {e}\")\n            return {}",
    "source_file": "core\\data\\loader.py",
    "academic_reference": null,
    "class_name": "OracleCloudLoader"
  },
  {
    "name": "download_file",
    "category": "data_processing",
    "formula": "result.returncode == 0 | False",
    "explanation": "Download file from Oracle Cloud.",
    "python_code": "def download_file(self, remote_file: str, local_path: Path) -> bool:\n        \"\"\"Download file from Oracle Cloud.\"\"\"\n        import subprocess\n\n        try:\n            cmd = f'scp -i \"{self.ssh_key}\" ubuntu@{self.host}:{remote_file} \"{local_path}\"'\n            result = subprocess.run(cmd, shell=True, capture_output=True, timeout=300)\n            return result.returncode == 0\n\n        except Exception as e:\n            logger.error(f\"Download error: {e}\")\n            return False",
    "source_file": "core\\data\\loader.py",
    "academic_reference": null,
    "class_name": "OracleCloudLoader"
  },
  {
    "name": "load_forex_ticks",
    "category": "data_processing",
    "formula": "pd.DataFrame() | df | pd.DataFrame()",
    "explanation": "Load forex tick data from Oracle Cloud.",
    "python_code": "def load_forex_ticks(self, symbol: str, date: datetime = None) -> pd.DataFrame:\n        \"\"\"Load forex tick data from Oracle Cloud.\"\"\"\n        import subprocess\n\n        if date is None:\n            date = datetime.now()\n\n        date_str = date.strftime('%Y-%m-%d')\n        remote_file = f\"{self.remote_path}/forex/{symbol}_{date_str}.csv\"\n\n        try:\n            cmd = f'ssh -i \"{self.ssh_key}\" ubuntu@{self.host} \"cat {remote_file} 2>/dev/null\"'\n            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=60)\n\n            if result.returncode != 0 or not result.stdout.strip():\n                return pd.DataFrame()\n\n            df = pd.read_csv(io.StringIO(result.stdout))\n\n            # Standardize columns\n            if 'timestamp' in df.columns:\n                df['timestamp'] = pd.to_datetime(df['timestamp'])\n\n            return df\n\n        except Exception as e:\n            logger.error(f\"Error loading from Oracle Cloud: {e}\")\n            return pd.DataFrame()",
    "source_file": "core\\data\\loader.py",
    "academic_reference": null,
    "class_name": "OracleCloudLoader"
  },
  {
    "name": "load_csv",
    "category": "data_processing",
    "formula": "pd.DataFrame() | df",
    "explanation": "Load CSV file.",
    "python_code": "def load_csv(self, filepath: Path) -> pd.DataFrame:\n        \"\"\"Load CSV file.\"\"\"\n        if not filepath.exists():\n            logger.warning(f\"File not found: {filepath}\")\n            return pd.DataFrame()\n\n        df = pd.read_csv(filepath)\n\n        # Auto-detect and parse timestamp\n        for col in ['timestamp', 'time', 'datetime', 'date']:\n            if col in df.columns:\n                df[col] = pd.to_datetime(df[col], utc=True)\n                # Convert to timezone-naive for consistency\n                if df[col].dt.tz is not None:\n                    df[col] = df[col].dt.tz_localize(None)\n                if col != 'timestamp':\n                    df = df.rename(columns={col: 'timestamp'})\n                break\n\n        return df",
    "source_file": "core\\data\\loader.py",
    "academic_reference": null,
    "class_name": "LocalDataLoader"
  },
  {
    "name": "load_parquet",
    "category": "data_processing",
    "formula": "pd.DataFrame() | pd.read_parquet(filepath)",
    "explanation": "Load Parquet file.",
    "python_code": "def load_parquet(self, filepath: Path) -> pd.DataFrame:\n        \"\"\"Load Parquet file.\"\"\"\n        if not filepath.exists():\n            logger.warning(f\"File not found: {filepath}\")\n            return pd.DataFrame()\n\n        return pd.read_parquet(filepath)",
    "source_file": "core\\data\\loader.py",
    "academic_reference": null,
    "class_name": "LocalDataLoader"
  },
  {
    "name": "find_tick_files",
    "category": "data_processing",
    "formula": "sorted(set(files))",
    "explanation": "Find tick data files for symbol.",
    "python_code": "def find_tick_files(self, symbol: str) -> List[Path]:\n        \"\"\"Find tick data files for symbol.\"\"\"\n        patterns = [\n            f\"{symbol}*.csv\",\n            f\"{symbol}*.parquet\",\n            f\"*{symbol}*.csv\",\n            f\"ticks/{symbol}*.csv\"\n        ]\n\n        files = []\n        # Search main data directory\n        for pattern in patterns:\n            files.extend(self.data_dir.glob(pattern))\n            files.extend(self.data_dir.glob(f\"**/{pattern}\"))\n\n        # Search extra directories (Dukascopy, HFT, etc.)\n        for extra_dir in self.extra_dirs:\n            if extra_dir.exists():\n                for pattern in patterns:\n                    files.extend(extra_dir.glob(pattern))\n\n        return sorted(set(files))",
    "source_file": "core\\data\\loader.py",
    "academic_reference": null,
    "class_name": "LocalDataLoader"
  },
  {
    "name": "load_historical",
    "category": "data_processing",
    "formula": "df",
    "explanation": "Load historical tick data.\n\nTries sources in order:\n1. Local cache\n2. Oracle Cloud\n3. TrueFX download\n\nReturns:\n    DataFrame with standardized tick data",
    "python_code": "def load_historical(self, symbol: str, start_date: datetime,\n                        end_date: datetime, source: DataSource = None) -> pd.DataFrame:\n        \"\"\"\n        Load historical tick data.\n\n        Tries sources in order:\n        1. Local cache\n        2. Oracle Cloud\n        3. TrueFX download\n\n        Returns:\n            DataFrame with standardized tick data\n        \"\"\"\n        cache_key = f\"{symbol}_{start_date.date()}_{end_date.date()}\"\n\n        if cache_key in self.cache:\n            return self.cache[cache_key]\n\n        # Ensure start/end dates are timezone-naive\n        if hasattr(start_date, 'tzinfo') and start_date.tzinfo is not None:\n            start_date = start_date.replace(tzinfo=None)\n        if hasattr(end_date, 'tzinfo') and end_date.tzinfo is not None:\n            end_date = end_date.replace(tzinfo=None)\n\n        df = pd.DataFrame()\n\n        # Try local first\n        local_files = self.local.find_tick_files(symbol)\n        if local_files:\n            frames = []\n            for f in local_files:\n                if f.suffix == '.parquet':\n                    frame = self.local.load_parquet(f)\n                else:\n                    frame = self.local.load_csv(f)\n\n                if not frame.empty and 'timestamp' in frame.columns:\n                    # Ensure timestamp is timezone-naive\n                    if frame['timestamp'].dt.tz is not None:\n                        frame['timestamp'] = frame['timestamp'].dt.tz_localize(None)\n                    frame = frame[(frame['timestamp'] >= start_date) &\n                                  (frame['timestamp'] <= end_date)]\n                    if not frame.empty:\n                        frames.append(frame)\n\n            if frames:\n                df = pd.concat(frames, ignore_index=True)\n\n        # Try Oracle Cloud if local empty\n        if df.empty:\n            current = start_date\n            frames = []\n            while current <= end_date:\n                frame = self.oracle.load_forex_ticks(symbol, current)\n                if not frame.empty:\n                    frames.append(frame)\n                current += timedelta(days=1)\n\n            if frames:\n                df = pd.concat(frames, ignore_index=True)\n\n        # Try TrueFX download\n        if df.empty:\n            df = self.truefx_historical.load_range(symbol, start_date, end_date)\n\n        # Normalize\n        if not df.empty:\n            df = self._normalize_dataframe(df, symbol)\n            self.cache[cache_key] = df\n\n        return df",
    "source_file": "core\\data\\loader.py",
    "academic_reference": null,
    "class_name": "UnifiedDataLoader"
  },
  {
    "name": "_normalize_dataframe",
    "category": "statistical",
    "formula": "result",
    "explanation": "Normalize DataFrame to standard format.",
    "python_code": "def _normalize_dataframe(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:\n        \"\"\"Normalize DataFrame to standard format.\"\"\"\n        result = df.copy()\n\n        # Ensure required columns\n        if 'symbol' not in result.columns:\n            result['symbol'] = symbol\n\n        # Rename common columns\n        rename_map = {\n            'time': 'timestamp',\n            'datetime': 'timestamp',\n            'Bid': 'bid',\n            'Ask': 'ask',\n            'BidSize': 'bid_size',\n            'AskSize': 'ask_size',\n            'Last': 'last_price',\n            'LastSize': 'last_size',\n            'Volume': 'last_size'\n        }\n\n        for old, new in rename_map.items():\n            if old in result.columns and new not in result.columns:\n                result = result.rename(columns={old: new})\n\n        # Add missing columns with defaults\n        if 'bid_size' not in result.columns:\n            result['bid_size'] = 1.0\n        if 'ask_size' not in result.columns:\n            result['ask_size'] = 1.0\n\n        # Sort by timestamp\n        if 'timestamp' in result.columns:\n            result = result.sort_values('timestamp').reset_index(drop=True)\n\n        return result",
    "source_file": "core\\data\\loader.py",
    "academic_reference": null,
    "class_name": "UnifiedDataLoader"
  },
  {
    "name": "historical_to_live_generator",
    "category": "data_processing",
    "formula": "",
    "explanation": "Generator that transitions from historical to live.\n\nFirst yields historical data up to historical_end,\nthen switches to live stream.\n\nArgs:\n    symbol: Symbol to load\n    historical_end: When to switch to live\n\nYields:\n    TickData objects",
    "python_code": "def historical_to_live_generator(self, symbol: str,\n                                      historical_end: datetime) -> Generator[TickData, None, None]:\n        \"\"\"\n        Generator that transitions from historical to live.\n\n        First yields historical data up to historical_end,\n        then switches to live stream.\n\n        Args:\n            symbol: Symbol to load\n            historical_end: When to switch to live\n\n        Yields:\n            TickData objects\n        \"\"\"\n        # Historical phase\n        historical_start = historical_end - timedelta(days=30)\n        df = self.load_historical(symbol, historical_start, historical_end)\n\n        for _, row in df.iterrows():\n            yield TickData(\n                timestamp=row['timestamp'],\n                symbol=symbol,\n                bid=row['bid'],\n                ask=row['ask'],\n                bid_size=row.get('bid_size', 1.0),\n                ask_size=row.get('ask_size', 1.0),\n                source=DataSource.TRUEFX_HISTORICAL\n            )\n\n        logger.info(f\"Switching to live data for {symbol}\")",
    "source_file": "core\\data\\loader.py",
    "academic_reference": null,
    "class_name": "UnifiedDataLoader"
  },
  {
    "name": "get_data_summary",
    "category": "data_processing",
    "formula": "{",
    "explanation": "Get summary of available data.",
    "python_code": "def get_data_summary(self) -> Dict:\n        \"\"\"Get summary of available data.\"\"\"\n        return {\n            'local_files': len(self.local.find_tick_files('*')),\n            'cache_keys': list(self.cache.keys()),\n            'truefx_symbols': TrueFXHistoricalLoader.SYMBOLS,\n            'oracle_cloud': self.oracle.list_available()\n        }",
    "source_file": "core\\data\\loader.py",
    "academic_reference": null,
    "class_name": "UnifiedDataLoader"
  },
  {
    "name": "is_valid",
    "category": "data_processing",
    "formula": "",
    "explanation": "",
    "python_code": "def is_valid(self) -> bool:\n        return self.bid > 0 and self.ask > 0 and self.ask >= self.bid",
    "source_file": "core\\data\\multi_source_feed.py",
    "academic_reference": null,
    "class_name": "LiveTick"
  },
  {
    "name": "__init__",
    "category": "data_processing",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, username: str = 'demo', password: str = 'demo'):\n        self.username = username\n        self.password = password\n        self.session_id = None\n        self._session = None\n        self._running = False",
    "source_file": "core\\data\\multi_source_feed.py",
    "academic_reference": null,
    "class_name": "TrueFXFeed"
  },
  {
    "name": "_parse_line",
    "category": "data_processing",
    "formula": "LiveTick(",
    "explanation": "Parse TrueFX CSV line.",
    "python_code": "def _parse_line(self, line: str) -> Optional[LiveTick]:\n        \"\"\"Parse TrueFX CSV line.\"\"\"\n        try:\n            parts = line.strip().split(',')\n            if len(parts) < 6:\n                return None\n\n            # Format: EUR/USD,1705432100000,1.0850,5,1.0851,5\n            symbol = parts[0].replace('/', '')\n            timestamp_ms = int(parts[1])\n            bid = float(parts[2]) + float(parts[3]) / 100000\n            ask = float(parts[4]) + float(parts[5]) / 100000\n\n            pip_value = self.PIP_VALUES.get(symbol, 0.0001)\n            spread = ask - bid\n            spread_pips = spread / pip_value\n\n            return LiveTick(\n                symbol=symbol,\n                bid=bid,\n                ask=ask,\n                mid=(bid + ask) / 2,\n                spread=spread,\n                spread_pips=spread_pips,\n                timestamp=datetime.fromtimestamp(timestamp_ms / 1000),\n                source=DataSource.TRUEFX\n            )\n\n        except Exception:\n            return None",
    "source_file": "core\\data\\multi_source_feed.py",
    "academic_reference": null,
    "class_name": "TrueFXFeed"
  },
  {
    "name": "get_symbols",
    "category": "data_processing",
    "formula": "",
    "explanation": "",
    "python_code": "def get_symbols(self) -> List[str]:\n        return self.SYMBOLS.copy()",
    "source_file": "core\\data\\multi_source_feed.py",
    "academic_reference": null,
    "class_name": "TrueFXFeed"
  },
  {
    "name": "_subscribe_sync",
    "category": "data_processing",
    "formula": "from ib_insync import Forex",
    "explanation": "Subscribe to market data (runs in thread).",
    "python_code": "def _subscribe_sync(self, symbols: List[str]):\n        \"\"\"Subscribe to market data (runs in thread).\"\"\"\n        if not self._ib or not self._connected:\n            return\n\n        from ib_insync import Forex\n\n        for symbol in symbols:\n            if symbol in self._subscriptions:\n                continue\n\n            try:\n                # Create forex contract\n                contract = Forex(symbol[:3] + symbol[3:])\n                self._ib.qualifyContracts(contract)\n\n                # Subscribe to market data\n                ticker = self._ib.reqMktData(contract, '', False, False)\n                self._subscriptions[symbol] = contract\n\n                # Set up callback\n                ticker.updateEvent += lambda t, sym=symbol: self._on_tick_sync(sym, t)\n\n                logger.debug(f\"IB subscribed to {symbol}\")\n\n            except Exception as e:\n                logger.error(f\"IB subscribe error for {symbol}: {e}\")",
    "source_file": "core\\data\\multi_source_feed.py",
    "academic_reference": null,
    "class_name": "IBMarketDataFeed"
  },
  {
    "name": "_on_tick_sync",
    "category": "data_processing",
    "formula": "if ticker.bid <= 0 or ticker.ask <= 0: | pip_value = self.PIP_VALUES.get(symbol, 0.0001 if 'JPY' not in symbol else 0.01)",
    "explanation": "Handle tick update from IB (runs in IB thread).",
    "python_code": "def _on_tick_sync(self, symbol: str, ticker):\n        \"\"\"Handle tick update from IB (runs in IB thread).\"\"\"\n        try:\n            if not hasattr(ticker, 'bid') or not hasattr(ticker, 'ask'):\n                return\n            if ticker.bid <= 0 or ticker.ask <= 0:\n                return\n\n            pip_value = self.PIP_VALUES.get(symbol, 0.0001 if 'JPY' not in symbol else 0.01)\n            spread = ticker.ask - ticker.bid\n\n            tick = LiveTick(\n                symbol=symbol,\n                bid=ticker.bid,\n                ask=ticker.ask,\n                mid=(ticker.bid + ticker.ask) / 2,\n                spread=spread,\n                spread_pips=spread / pip_value,\n                timestamp=datetime.now(),\n                source=DataSource.IB,\n                volume=ticker.volume if hasattr(ticker, 'volume') and ticker.volume else 0\n            )\n\n            # Queue tick for async processing\n            with self._queue_lock:\n                self._tick_queue.append(tick)\n\n        except Exception as e:\n            logger.error(f\"IB tick error for {symbol}: {e}\")",
    "source_file": "core\\data\\multi_source_feed.py",
    "academic_reference": null,
    "class_name": "IBMarketDataFeed"
  },
  {
    "name": "_run_ib_loop",
    "category": "data_processing",
    "formula": "",
    "explanation": "Run IB event loop in separate thread.",
    "python_code": "def _run_ib_loop(self, callback: Callable[[LiveTick], None]):\n        \"\"\"Run IB event loop in separate thread.\"\"\"\n        try:\n            # Subscribe to all symbols\n            self._subscribe_sync(self.SYMBOLS)\n            logger.info(f\"IB subscribed to {len(self._subscriptions)} symbols\")\n\n            # Run IB event loop\n            while self._running and self._ib:\n                try:\n                    self._ib.sleep(0.05)  # Process IB events\n\n                    # Process queued ticks\n                    with self._queue_lock:\n                        ticks = self._tick_queue.copy()\n                        self._tick_queue.clear()\n\n                    for tick in ticks:\n                        try:\n                            callback(tick)\n                        except Exception as e:\n                            logger.error(f\"IB callback error: {e}\")\n\n                except Exception as e:\n                    if self._running:\n                        logger.error(f\"IB loop error: {e}\")\n                    break\n\n        except Exception as e:\n            logger.error(f\"IB thread error: {e}\")\n        finally:\n            logger.info(\"IB thread stopped\")",
    "source_file": "core\\data\\multi_source_feed.py",
    "academic_reference": null,
    "class_name": "IBMarketDataFeed"
  },
  {
    "name": "_set_primary_sources",
    "category": "data_processing",
    "formula": "",
    "explanation": "Set primary data source for each symbol.",
    "python_code": "def _set_primary_sources(self):\n        \"\"\"Set primary data source for each symbol.\"\"\"\n        # Priority: TrueFX > OANDA > IB (for latency)\n        priority = [DataSource.TRUEFX, DataSource.OANDA, DataSource.IB]\n\n        for symbol, coverage in self._coverage.items():\n            for source in priority:\n                if source in coverage.sources:\n                    coverage.primary_source = source\n                    break",
    "source_file": "core\\data\\multi_source_feed.py",
    "academic_reference": null,
    "class_name": "MultiSourceFeed"
  },
  {
    "name": "register_callback",
    "category": "data_processing",
    "formula": "",
    "explanation": "Register callback for tick updates.",
    "python_code": "def register_callback(self, callback: Callable[[LiveTick], None]):\n        \"\"\"Register callback for tick updates.\"\"\"\n        self._callbacks.append(callback)",
    "source_file": "core\\data\\multi_source_feed.py",
    "academic_reference": null,
    "class_name": "MultiSourceFeed"
  },
  {
    "name": "_on_tick",
    "category": "data_processing",
    "formula": "",
    "explanation": "Handle tick from any source.",
    "python_code": "def _on_tick(self, tick: LiveTick):\n        \"\"\"Handle tick from any source.\"\"\"\n        with self._lock:\n            # Store tick\n            self._latest_ticks[tick.symbol][tick.source] = tick\n            self._tick_counts[tick.source] += 1\n            self._last_tick_time[tick.source] = datetime.now()\n\n            # Update coverage\n            if tick.symbol in self._coverage:\n                self._coverage[tick.symbol].tick_count += 1\n                self._coverage[tick.symbol].last_tick = tick\n\n            # Determine best quote\n            best = self._get_best_quote(tick.symbol)\n            if best:\n                self._best_quotes[tick.symbol] = best\n\n                # Notify callbacks with best quote\n                for callback in self._callbacks:\n                    try:\n                        callback(best)\n                    except Exception as e:\n                        logger.error(f\"Callback error: {e}\")",
    "source_file": "core\\data\\multi_source_feed.py",
    "academic_reference": null,
    "class_name": "MultiSourceFeed"
  },
  {
    "name": "_get_best_quote",
    "category": "microstructure",
    "formula": "min(valid_ticks, key=lambda t: t.spread_pips)",
    "explanation": "Get best quote (lowest spread) for symbol.",
    "python_code": "def _get_best_quote(self, symbol: str) -> Optional[LiveTick]:\n        \"\"\"Get best quote (lowest spread) for symbol.\"\"\"\n        ticks = self._latest_ticks.get(symbol, {})\n        if not ticks:\n            return None\n\n        valid_ticks = [t for t in ticks.values() if t.is_valid]\n        if not valid_ticks:\n            return None\n\n        # Sort by spread\n        return min(valid_ticks, key=lambda t: t.spread_pips)",
    "source_file": "core\\data\\multi_source_feed.py",
    "academic_reference": null,
    "class_name": "MultiSourceFeed"
  },
  {
    "name": "get_quote",
    "category": "data_processing",
    "formula": "",
    "explanation": "Get latest best quote for symbol.",
    "python_code": "def get_quote(self, symbol: str) -> Optional[LiveTick]:\n        \"\"\"Get latest best quote for symbol.\"\"\"\n        return self._best_quotes.get(symbol)",
    "source_file": "core\\data\\multi_source_feed.py",
    "academic_reference": null,
    "class_name": "MultiSourceFeed"
  },
  {
    "name": "get_all_quotes",
    "category": "data_processing",
    "formula": "dict(self._latest_ticks.get(symbol, {}))",
    "explanation": "Get quotes from all sources for symbol.",
    "python_code": "def get_all_quotes(self, symbol: str) -> Dict[DataSource, LiveTick]:\n        \"\"\"Get quotes from all sources for symbol.\"\"\"\n        return dict(self._latest_ticks.get(symbol, {}))",
    "source_file": "core\\data\\multi_source_feed.py",
    "academic_reference": null,
    "class_name": "MultiSourceFeed"
  },
  {
    "name": "get_coverage",
    "category": "data_processing",
    "formula": "dict(self._coverage)",
    "explanation": "Get symbol coverage information.",
    "python_code": "def get_coverage(self) -> Dict[str, SymbolCoverage]:\n        \"\"\"Get symbol coverage information.\"\"\"\n        return dict(self._coverage)",
    "source_file": "core\\data\\multi_source_feed.py",
    "academic_reference": null,
    "class_name": "MultiSourceFeed"
  },
  {
    "name": "get_all_symbols",
    "category": "data_processing",
    "formula": "list(self._coverage.keys())",
    "explanation": "Get all covered symbols.",
    "python_code": "def get_all_symbols(self) -> List[str]:\n        \"\"\"Get all covered symbols.\"\"\"\n        return list(self._coverage.keys())",
    "source_file": "core\\data\\multi_source_feed.py",
    "academic_reference": null,
    "class_name": "MultiSourceFeed"
  },
  {
    "name": "get_stats",
    "category": "data_processing",
    "formula": "{",
    "explanation": "Get feed statistics.",
    "python_code": "def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get feed statistics.\"\"\"\n        return {\n            'feeds_active': len([f for f in self.feeds.values()]),\n            'total_symbols': len(self._coverage),\n            'tick_counts': dict(self._tick_counts),\n            'last_tick_times': {\n                k.value: v.isoformat()\n                for k, v in self._last_tick_time.items()\n            },\n            'coverage_by_source': {\n                source.value: sum(\n                    1 for c in self._coverage.values()\n                    if source in c.sources\n                )\n                for source in DataSource\n            }\n        }",
    "source_file": "core\\data\\multi_source_feed.py",
    "academic_reference": null,
    "class_name": "MultiSourceFeed"
  },
  {
    "name": "print_coverage",
    "category": "data_processing",
    "formula": "",
    "explanation": "Print coverage summary.",
    "python_code": "def print_coverage(self):\n        \"\"\"Print coverage summary.\"\"\"\n        print(\"\\n\" + \"=\" * 70)\n        print(\"MULTI-SOURCE DATA FEED COVERAGE\")\n        print(\"=\" * 70)\n\n        # By source\n        print(\"\\nSOURCES:\")\n        for source in [DataSource.TRUEFX, DataSource.IB, DataSource.OANDA]:\n            if source in self.feeds:\n                count = sum(1 for c in self._coverage.values() if source in c.sources)\n                ticks = self._tick_counts.get(source, 0)\n                status = \"ACTIVE\" if ticks > 0 else \"CONNECTED\"\n                print(f\"  {source.value:12} : {count:3} symbols ({status}, {ticks} ticks)\")\n\n        # Total coverage\n        print(f\"\\nTOTAL SYMBOLS: {len(self._coverage)}\")\n\n        # List symbols by coverage\n        multi_source = [s for s, c in self._coverage.items() if len(c.sources) > 1]\n        single_source = [s for s, c in self._coverage.items() if len(c.sources) == 1]\n\n        print(f\"\\nMulti-source coverage ({len(multi_source)}): {', '.join(sorted(multi_source)[:10])}...\")\n        print(f\"Single-source coverage ({len(single_source)}): {', '.join(sorted(single_source)[:10])}...\")\n\n        print(\"=\" * 70 + \"\\n\")",
    "source_file": "core\\data\\multi_source_feed.py",
    "academic_reference": null,
    "class_name": "MultiSourceFeed"
  },
  {
    "name": "create_multi_source_feed",
    "category": "reinforcement_learning",
    "formula": "MultiSourceFeed(",
    "explanation": "Factory function to create multi-source feed from environment.\n\nReads credentials from environment variables:\n- TRUEFX_USERNAME, TRUEFX_PASSWORD\n- IB_HOST, IB_PORT\n- OANDA_API_KEY, OANDA_ACCOUNT_ID, OANDA_PAPER",
    "python_code": "def create_multi_source_feed(\n    enable_truefx: bool = True,\n    enable_ib: bool = True,\n    enable_oanda: bool = True,\n) -> MultiSourceFeed:\n    \"\"\"\n    Factory function to create multi-source feed from environment.\n\n    Reads credentials from environment variables:\n    - TRUEFX_USERNAME, TRUEFX_PASSWORD\n    - IB_HOST, IB_PORT\n    - OANDA_API_KEY, OANDA_ACCOUNT_ID, OANDA_PAPER\n    \"\"\"\n    import os\n    from dotenv import load_dotenv\n\n    # Load .env file\n    load_dotenv()\n\n    # Check OANDA paper mode\n    oanda_paper = os.getenv('OANDA_PAPER', 'true').lower() in ('true', '1', 'yes')\n\n    return MultiSourceFeed(\n        truefx_username=os.getenv('TRUEFX_USERNAME', 'demo'),\n        truefx_password=os.getenv('TRUEFX_PASSWORD', 'demo'),\n        ib_host=os.getenv('IB_HOST', 'localhost'),\n        ib_port=int(os.getenv('IB_PORT', 4004)),\n        oanda_api_key=os.getenv('OANDA_API_KEY', ''),\n        oanda_account_id=os.getenv('OANDA_ACCOUNT_ID', ''),\n        oanda_paper=oanda_paper,\n        enable_truefx=enable_truefx,\n        enable_ib=enable_ib,\n        enable_oanda=enable_oanda,\n    )",
    "source_file": "core\\data\\multi_source_feed.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "_connect_sync",
    "category": "data_processing",
    "formula": "True | False",
    "explanation": "",
    "python_code": "def _connect_sync():\n                    try:\n                        # Create new IB instance for thread\n                        import asyncio\n                        loop = asyncio.new_event_loop()\n                        asyncio.set_event_loop(loop)\n\n                        self._ib = IB()\n                        self._ib.connect(\n                            self.host,\n                            self.port,\n                            clientId=self.client_id,\n                            readonly=True\n                        )\n                        self._connected = True\n                        logger.info(f\"IB Market Data connected to {self.host}:{self.port}\")\n                        return True\n                    except Exception as e2:\n                        logger.error(f\"IB sync connect error: {e2}\")\n                        return False",
    "source_file": "core\\data\\multi_source_feed.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "data_processing",
    "formula": "",
    "explanation": "Args:\n    output_dir: Directory for tick files\n    buffer_size: Ticks to buffer before write\n    flush_interval: Max seconds between flushes\n    compress: Use gzip compression\n    per_symbol: Save separate files per symbol",
    "python_code": "def __init__(\n        self,\n        output_dir: str = \"data/live\",\n        buffer_size: int = 1000,\n        flush_interval: float = 5.0,\n        compress: bool = False,\n        per_symbol: bool = True,\n    ):\n        \"\"\"\n        Args:\n            output_dir: Directory for tick files\n            buffer_size: Ticks to buffer before write\n            flush_interval: Max seconds between flushes\n            compress: Use gzip compression\n            per_symbol: Save separate files per symbol\n        \"\"\"\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n        self.buffer_size = buffer_size\n        self.flush_interval = flush_interval\n        self.compress = compress\n        self.per_symbol = per_symbol\n\n        # Write queue (thread-safe)\n        self._queue: queue.Queue = queue.Queue()\n\n        # Active file handles\n        self._files: Dict[str, Any] = {}\n        self._writers: Dict[str, csv.writer] = {}\n        self._current_date: date = None\n\n        # Background writer thread\n        self._running = False\n        self._thread: Optional[threading.Thread] = None\n\n        # Stats\n        self.ticks_saved = 0\n        self.ticks_queued = 0\n        self.files_created = 0\n        self._lock = threading.Lock()\n\n        # Start background writer\n        self.start()",
    "source_file": "core\\data\\tick_saver.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "LiveTickSaver"
  },
  {
    "name": "start",
    "category": "data_processing",
    "formula": "",
    "explanation": "Start background writer thread.",
    "python_code": "def start(self):\n        \"\"\"Start background writer thread.\"\"\"\n        if self._running:\n            return\n\n        self._running = True\n        self._thread = threading.Thread(\n            target=self._writer_loop,\n            daemon=True,\n            name=\"TickSaver\"\n        )\n        self._thread.start()\n        logger.info(f\"LiveTickSaver started: {self.output_dir}\")",
    "source_file": "core\\data\\tick_saver.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "LiveTickSaver"
  },
  {
    "name": "stop",
    "category": "technical",
    "formula": "",
    "explanation": "Stop background writer and flush remaining data.",
    "python_code": "def stop(self):\n        \"\"\"Stop background writer and flush remaining data.\"\"\"\n        self._running = False\n\n        # Signal thread to stop\n        self._queue.put(None)\n\n        if self._thread:\n            self._thread.join(timeout=5.0)\n\n        # Close all files\n        self._close_files()\n        logger.info(f\"LiveTickSaver stopped. Saved {self.ticks_saved} ticks\")",
    "source_file": "core\\data\\tick_saver.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "LiveTickSaver"
  },
  {
    "name": "save_tick",
    "category": "data_processing",
    "formula": "",
    "explanation": "Queue a tick for saving.\n\nNon-blocking - tick is added to queue and written in background.",
    "python_code": "def save_tick(\n        self,\n        symbol: str,\n        bid: float,\n        ask: float,\n        source: str = \"unknown\",\n        volume: float = 0.0,\n        latency_ms: float = 0.0,\n        timestamp: datetime = None,\n    ):\n        \"\"\"\n        Queue a tick for saving.\n\n        Non-blocking - tick is added to queue and written in background.\n        \"\"\"\n        if timestamp is None:\n            timestamp = datetime.utcnow()\n\n        mid = (bid + ask) / 2\n        spread = ask - bid\n\n        # Calculate pip value\n        if 'JPY' in symbol:\n            pip_value = 0.01\n        else:\n            pip_value = 0.0001\n        spread_pips = spread / pip_value\n\n        tick = SavedTick(\n            timestamp=timestamp.isoformat(),\n            symbol=symbol,\n            bid=bid,\n            ask=ask,\n            mid=mid,\n            spread=spread,\n            spread_pips=spread_pips,\n            source=source,\n            volume=volume,\n            latency_ms=latency_ms,\n        )\n\n        self._queue.put(tick)\n        with self._lock:\n            self.ticks_queued += 1",
    "source_file": "core\\data\\tick_saver.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "LiveTickSaver"
  },
  {
    "name": "_writer_loop",
    "category": "data_processing",
    "formula": "",
    "explanation": "Background thread that writes ticks to disk.",
    "python_code": "def _writer_loop(self):\n        \"\"\"Background thread that writes ticks to disk.\"\"\"\n        buffer: List[SavedTick] = []\n        last_flush = datetime.utcnow()\n\n        while self._running or not self._queue.empty():\n            try:\n                # Get tick with timeout\n                try:\n                    tick = self._queue.get(timeout=1.0)\n                except queue.Empty:\n                    tick = None\n\n                # None signals shutdown\n                if tick is None and not self._running:\n                    break\n\n                if tick:\n                    buffer.append(tick)\n\n                # Flush if buffer full or interval elapsed\n                elapsed = (datetime.utcnow() - last_flush).total_seconds()\n                should_flush = (\n                    len(buffer) >= self.buffer_size or\n                    elapsed >= self.flush_interval\n                )\n\n                if buffer and should_flush:\n                    self._flush_buffer(buffer)\n                    buffer = []\n                    last_flush = datetime.utcnow()\n\n            except Exception as e:\n                logger.error(f\"TickSaver write error: {e}\")\n\n        # Final flush\n        if buffer:\n            self._flush_buffer(buffer)",
    "source_file": "core\\data\\tick_saver.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "LiveTickSaver"
  },
  {
    "name": "_flush_buffer",
    "category": "data_processing",
    "formula": "# Check for date rollover",
    "explanation": "Write buffered ticks to disk.",
    "python_code": "def _flush_buffer(self, ticks: List[SavedTick]):\n        \"\"\"Write buffered ticks to disk.\"\"\"\n        if not ticks:\n            return\n\n        # Check for date rollover\n        today = date.today()\n        if self._current_date != today:\n            self._rotate_files()\n            self._current_date = today\n\n        # Group ticks by symbol (if per_symbol enabled)\n        if self.per_symbol:\n            by_symbol: Dict[str, List[SavedTick]] = {}\n            for tick in ticks:\n                if tick.symbol not in by_symbol:\n                    by_symbol[tick.symbol] = []\n                by_symbol[tick.symbol].append(tick)\n\n            for symbol, symbol_ticks in by_symbol.items():\n                self._write_ticks(symbol_ticks, symbol)\n        else:\n            self._write_ticks(ticks, \"all\")\n\n        with self._lock:\n            self.ticks_saved += len(ticks)",
    "source_file": "core\\data\\tick_saver.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "LiveTickSaver"
  },
  {
    "name": "_get_file_path",
    "category": "data_processing",
    "formula": "symbol_dir / filename",
    "explanation": "Get file path for today's data.",
    "python_code": "def _get_file_path(self, key: str) -> Path:\n        \"\"\"Get file path for today's data.\"\"\"\n        date_str = date.today().strftime(\"%Y%m%d\")\n\n        if self.per_symbol:\n            symbol_dir = self.output_dir / key\n            symbol_dir.mkdir(exist_ok=True)\n            filename = f\"ticks_{date_str}.csv\"\n            if self.compress:\n                filename += \".gz\"\n            return symbol_dir / filename\n        else:\n            filename = f\"live_{date_str}.csv\"\n            if self.compress:\n                filename += \".gz\"\n            return self.output_dir / filename",
    "source_file": "core\\data\\tick_saver.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "LiveTickSaver"
  },
  {
    "name": "_get_writer",
    "category": "data_processing",
    "formula": "",
    "explanation": "Get or create CSV writer for key.",
    "python_code": "def _get_writer(self, key: str) -> csv.writer:\n        \"\"\"Get or create CSV writer for key.\"\"\"\n        if key not in self._writers:\n            filepath = self._get_file_path(key)\n\n            # Check if file exists (to skip header)\n            file_exists = filepath.exists()\n\n            # Open file\n            if self.compress:\n                f = gzip.open(filepath, 'at', newline='', encoding='utf-8')\n            else:\n                f = open(filepath, 'a', newline='', encoding='utf-8')\n\n            self._files[key] = f\n            writer = csv.writer(f)\n\n            # Write header if new file\n            if not file_exists:\n                writer.writerow([\n                    'timestamp', 'symbol', 'bid', 'ask', 'mid',\n                    'spread', 'spread_pips', 'source', 'volume', 'latency_ms'\n                ])\n                self.files_created += 1\n                logger.info(f\"Created tick file: {filepath}\")\n\n            self._writers[key] = writer\n\n        return self._writers[key]",
    "source_file": "core\\data\\tick_saver.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "LiveTickSaver"
  },
  {
    "name": "_write_ticks",
    "category": "data_processing",
    "formula": "",
    "explanation": "Write ticks to CSV file.",
    "python_code": "def _write_ticks(self, ticks: List[SavedTick], key: str):\n        \"\"\"Write ticks to CSV file.\"\"\"\n        writer = self._get_writer(key)\n\n        for tick in ticks:\n            writer.writerow([\n                tick.timestamp,\n                tick.symbol,\n                f\"{tick.bid:.6f}\",\n                f\"{tick.ask:.6f}\",\n                f\"{tick.mid:.6f}\",\n                f\"{tick.spread:.6f}\",\n                f\"{tick.spread_pips:.2f}\",\n                tick.source,\n                tick.volume,\n                f\"{tick.latency_ms:.1f}\",\n            ])\n\n        # Flush to disk\n        if key in self._files:\n            self._files[key].flush()",
    "source_file": "core\\data\\tick_saver.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "LiveTickSaver"
  },
  {
    "name": "_rotate_files",
    "category": "data_processing",
    "formula": "",
    "explanation": "Close old files on date change.",
    "python_code": "def _rotate_files(self):\n        \"\"\"Close old files on date change.\"\"\"\n        logger.info(\"Rotating tick files for new day\")\n        self._close_files()",
    "source_file": "core\\data\\tick_saver.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "LiveTickSaver"
  },
  {
    "name": "_close_files",
    "category": "data_processing",
    "formula": "",
    "explanation": "Close all open file handles.",
    "python_code": "def _close_files(self):\n        \"\"\"Close all open file handles.\"\"\"\n        for key, f in self._files.items():\n            try:\n                f.close()\n            except:\n                pass\n\n        self._files.clear()\n        self._writers.clear()",
    "source_file": "core\\data\\tick_saver.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "LiveTickSaver"
  },
  {
    "name": "get_stats",
    "category": "data_processing",
    "formula": "{",
    "explanation": "Get saver statistics.",
    "python_code": "def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get saver statistics.\"\"\"\n        with self._lock:\n            return {\n                \"ticks_saved\": self.ticks_saved,\n                \"ticks_queued\": self.ticks_queued,\n                \"queue_size\": self._queue.qsize(),\n                \"files_created\": self.files_created,\n                \"output_dir\": str(self.output_dir),\n            }",
    "source_file": "core\\data\\tick_saver.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "LiveTickSaver"
  },
  {
    "name": "get_tick_saver",
    "category": "data_processing",
    "formula": "_tick_saver",
    "explanation": "Get or create singleton tick saver.",
    "python_code": "def get_tick_saver(output_dir: str = \"data/live\", **kwargs) -> LiveTickSaver:\n    \"\"\"Get or create singleton tick saver.\"\"\"\n    global _tick_saver\n    if _tick_saver is None:\n        _tick_saver = LiveTickSaver(output_dir=output_dir, **kwargs)\n    return _tick_saver",
    "source_file": "core\\data\\tick_saver.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": null
  },
  {
    "name": "on_tick",
    "category": "execution",
    "formula": "",
    "explanation": "Called on each tick.",
    "python_code": "def on_tick(self, tick: TickData, engine: 'TickBacktestEngine') -> None:\n        \"\"\"Called on each tick.\"\"\"\n        pass",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "Strategy"
  },
  {
    "name": "on_fill",
    "category": "execution",
    "formula": "",
    "explanation": "Called when order is filled.",
    "python_code": "def on_fill(self, fill: BacktestFill, engine: 'TickBacktestEngine') -> None:\n        \"\"\"Called when order is filled.\"\"\"\n        pass",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "Strategy"
  },
  {
    "name": "on_start",
    "category": "execution",
    "formula": "",
    "explanation": "Called at backtest start.",
    "python_code": "def on_start(self, engine: 'TickBacktestEngine') -> None:\n        \"\"\"Called at backtest start.\"\"\"\n        pass",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "Strategy"
  },
  {
    "name": "on_end",
    "category": "execution",
    "formula": "",
    "explanation": "Called at backtest end.",
    "python_code": "def on_end(self, engine: 'TickBacktestEngine') -> None:\n        \"\"\"Called at backtest end.\"\"\"\n        pass",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "Strategy"
  },
  {
    "name": "__init__",
    "category": "execution",
    "formula": "",
    "explanation": "Initialize backtest engine.\n\nArgs:\n    config: Backtesting configuration",
    "python_code": "def __init__(self, config: BacktestConfig = None):\n        \"\"\"\n        Initialize backtest engine.\n\n        Args:\n            config: Backtesting configuration\n        \"\"\"\n        self.config = config or BacktestConfig()\n\n        # Market data\n        self.symbols: List[str] = []\n        self.tick_data: Dict[str, pd.DataFrame] = {}\n        self.current_tick: Dict[str, TickData] = {}\n\n        # Order book simulation\n        self.order_books: Dict[str, OrderBookL3] = {}\n        self.queue_trackers: Dict[str, QueuePositionTracker] = {}\n        self.fill_engine = FillProbabilityEngine()\n\n        # Orders and fills\n        self.orders: Dict[str, BacktestOrder] = {}\n        self.pending_orders: List[str] = []\n        self.fills: List[BacktestFill] = []\n\n        # Position tracking\n        self.positions: Dict[str, float] = defaultdict(float)  # symbol -> quantity\n        self.avg_entry_prices: Dict[str, float] = defaultdict(float)\n        self.trades: List[BacktestTrade] = []\n\n        # Account\n        self.cash = self.config.initial_capital\n        self.equity_curve: List[Tuple[datetime, float]] = []\n\n        # State\n        self.current_time: Optional[datetime] = None\n        self._order_counter = 0\n        self._trade_counter = 0",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "TickBacktestEngine"
  },
  {
    "name": "load_data",
    "category": "execution",
    "formula": "",
    "explanation": "Load tick data for symbol.\n\nExpects DataFrame with columns:\n- timestamp: datetime\n- bid: float\n- ask: float\n- bid_size: float (optional)\n- ask_size: float (optional)\n- last_price: float (optional, for trades)\n- last_size: float (optional, for trades)",
    "python_code": "def load_data(self, symbol: str, data: pd.DataFrame) -> None:\n        \"\"\"\n        Load tick data for symbol.\n\n        Expects DataFrame with columns:\n        - timestamp: datetime\n        - bid: float\n        - ask: float\n        - bid_size: float (optional)\n        - ask_size: float (optional)\n        - last_price: float (optional, for trades)\n        - last_size: float (optional, for trades)\n        \"\"\"\n        if 'timestamp' not in data.columns:\n            raise ValueError(\"Data must have 'timestamp' column\")\n\n        if 'bid' not in data.columns or 'ask' not in data.columns:\n            raise ValueError(\"Data must have 'bid' and 'ask' columns\")\n\n        self.symbols.append(symbol)\n        self.tick_data[symbol] = data.sort_values('timestamp').reset_index(drop=True)\n\n        # Initialize order book\n        tick_size = 0.0001 if 'JPY' not in symbol else 0.01\n        self.order_books[symbol] = OrderBookL3(tick_size=tick_size)\n        self.queue_trackers[symbol] = QueuePositionTracker(model='probabilistic')\n\n        logger.info(f\"Loaded {len(data)} ticks for {symbol}\")",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "TickBacktestEngine"
  },
  {
    "name": "run",
    "category": "execution",
    "formula": "",
    "explanation": "Run backtest with strategy.\n\nArgs:\n    strategy: Trading strategy to test",
    "python_code": "def run(self, strategy: Strategy) -> None:\n        \"\"\"\n        Run backtest with strategy.\n\n        Args:\n            strategy: Trading strategy to test\n        \"\"\"\n        if not self.symbols:\n            raise ValueError(\"No data loaded\")\n\n        logger.info(f\"Starting backtest with {len(self.symbols)} symbols\")\n\n        # Merge all tick data into single timeline\n        all_ticks = self._merge_tick_data()\n\n        strategy.on_start(self)\n\n        # Main backtest loop\n        for _, row in all_ticks.iterrows():\n            symbol = row['symbol']\n            self.current_time = row['timestamp']\n\n            # Create tick\n            tick = TickData(\n                timestamp=row['timestamp'],\n                bid=row['bid'],\n                ask=row['ask'],\n                bid_size=row.get('bid_size', 1.0),\n                ask_size=row.get('ask_size', 1.0),\n                last_price=row.get('last_price'),\n                last_size=row.get('last_size'),\n                is_trade='last_price' in row and pd.notna(row.get('last_price'))\n            )\n\n            self.current_tick[symbol] = tick\n\n            # Update order book\n            self._update_order_book(symbol, tick)\n\n            # Process pending orders\n            self._process_orders(symbol, tick)\n\n            # Call strategy\n            strategy.on_tick(tick, self)\n\n            # Update equity curve\n            self._update_equity()\n\n        strategy.on_end(self)\n\n        logger.info(f\"Backtest complete: {len(self.fills)} fills, {len(self.trades)} trades\")",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "TickBacktestEngine"
  },
  {
    "name": "_merge_tick_data",
    "category": "execution",
    "formula": "merged.sort_values('timestamp').reset_index(drop=True)",
    "explanation": "Merge tick data from all symbols into timeline.",
    "python_code": "def _merge_tick_data(self) -> pd.DataFrame:\n        \"\"\"Merge tick data from all symbols into timeline.\"\"\"\n        frames = []\n        for symbol, df in self.tick_data.items():\n            df = df.copy()\n            df['symbol'] = symbol\n            frames.append(df)\n\n        merged = pd.concat(frames, ignore_index=True)\n        return merged.sort_values('timestamp').reset_index(drop=True)",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "TickBacktestEngine"
  },
  {
    "name": "_update_order_book",
    "category": "execution",
    "formula": "",
    "explanation": "Update order book from tick.",
    "python_code": "def _update_order_book(self, symbol: str, tick: TickData) -> None:\n        \"\"\"Update order book from tick.\"\"\"\n        book = self.order_books[symbol]\n\n        # Update L2 quotes\n        book._update_level2_bid(tick.timestamp, tick.bid, tick.bid_size)\n        book._update_level2_ask(tick.timestamp, tick.ask, tick.ask_size)\n\n        # Process trade if present\n        if tick.is_trade and tick.last_price and tick.last_size:\n            # Determine trade side (at ask = buy, at bid = sell)\n            if abs(tick.last_price - tick.ask) < abs(tick.last_price - tick.bid):\n                trade_side = BookSide.BUY\n            else:\n                trade_side = BookSide.SELL\n\n            update = BookUpdate(\n                timestamp=tick.timestamp,\n                update_type=OrderType.TRADE,\n                side=trade_side,\n                price=tick.last_price,\n                quantity=tick.last_size\n            )\n            book.process_update(update)\n\n            # Update fill probability engine\n            self.fill_engine.record_trade(tick.timestamp, tick.last_size)\n\n            # Advance queues\n            tracker = self.queue_trackers[symbol]\n            side = Side.BUY if trade_side == BookSide.BUY else Side.SELL\n            tracker.on_trade(tick.last_price, tick.last_size, side)",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "TickBacktestEngine"
  },
  {
    "name": "_process_orders",
    "category": "execution",
    "formula": "",
    "explanation": "Process pending orders against current tick.",
    "python_code": "def _process_orders(self, symbol: str, tick: TickData) -> None:\n        \"\"\"Process pending orders against current tick.\"\"\"\n        orders_to_remove = []\n\n        for order_id in self.pending_orders:\n            order = self.orders.get(order_id)\n            if not order or order.symbol != symbol:\n                continue\n\n            filled = False\n\n            if order.order_type == OrderType.MARKET:\n                # Market orders fill immediately\n                filled = self._fill_market_order(order, tick)\n\n            elif order.order_type == OrderType.LIMIT:\n                # Limit orders check price and queue\n                filled = self._check_limit_fill(order, tick)\n\n            if filled:\n                orders_to_remove.append(order_id)\n\n        for order_id in orders_to_remove:\n            if order_id in self.pending_orders:\n                self.pending_orders.remove(order_id)",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "TickBacktestEngine"
  },
  {
    "name": "_fill_market_order",
    "category": "execution",
    "formula": "True",
    "explanation": "Fill market order.",
    "python_code": "def _fill_market_order(self, order: BacktestOrder, tick: TickData) -> bool:\n        \"\"\"Fill market order.\"\"\"\n        if order.side == OrderSide.BUY:\n            fill_price = tick.ask\n        else:\n            fill_price = tick.bid\n\n        # Apply slippage\n        slippage = self._calculate_slippage(order, tick)\n        if order.side == OrderSide.BUY:\n            fill_price *= (1 + slippage / 10000)\n        else:\n            fill_price *= (1 - slippage / 10000)\n\n        # Create fill\n        self._create_fill(order, fill_price, order.quantity, tick.timestamp, slippage, is_maker=False)\n\n        return True",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "TickBacktestEngine"
  },
  {
    "name": "_check_limit_fill",
    "category": "execution",
    "formula": "False | False | False",
    "explanation": "Check if limit order fills.",
    "python_code": "def _check_limit_fill(self, order: BacktestOrder, tick: TickData) -> bool:\n        \"\"\"Check if limit order fills.\"\"\"\n        if order.limit_price is None:\n            return False\n\n        # Check price crossing\n        if order.side == OrderSide.BUY:\n            # Buy limit: fills if ask <= limit price\n            if tick.ask > order.limit_price:\n                return False\n            fill_price = order.limit_price\n        else:\n            # Sell limit: fills if bid >= limit price\n            if tick.bid < order.limit_price:\n                return False\n            fill_price = order.limit_price\n\n        # Queue-based fill model\n        if self.config.fill_model == \"queue\":\n            tracker = self.queue_trackers[order.symbol]\n            fill_prob = tracker.get_fill_probability(order.order_id)\n\n            # Only fill if probability high enough\n            if fill_prob < 0.5 and np.random.random() > fill_prob:\n                return False\n\n        # Create fill\n        self._create_fill(order, fill_price, order.quantity, tick.timestamp, 0.0, is_maker=True)\n\n        return True",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "TickBacktestEngine"
  },
  {
    "name": "_calculate_slippage",
    "category": "execution",
    "formula": "0.0 | 0.5 + spread * 0.2 + size_impact",
    "explanation": "Calculate slippage in basis points.",
    "python_code": "def _calculate_slippage(self, order: BacktestOrder, tick: TickData) -> float:\n        \"\"\"Calculate slippage in basis points.\"\"\"\n        if self.config.slippage_model == \"none\":\n            return 0.0\n\n        if self.config.slippage_model == \"fixed\":\n            return self.config.fixed_slippage_bps\n\n        # Realistic slippage based on order size and spread\n        spread = (tick.ask - tick.bid) / ((tick.ask + tick.bid) / 2) * 10000\n        size_impact = 0.1 * order.quantity / 100  # Scale by lots\n\n        return 0.5 + spread * 0.2 + size_impact",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "TickBacktestEngine"
  },
  {
    "name": "_create_fill",
    "category": "execution",
    "formula": "",
    "explanation": "Create fill and update position.",
    "python_code": "def _create_fill(self, order: BacktestOrder, fill_price: float, quantity: float,\n                     timestamp: datetime, slippage: float, is_maker: bool) -> None:\n        \"\"\"Create fill and update position.\"\"\"\n        # Calculate commission\n        commission = self.config.commission_per_lot * quantity\n\n        # Apply maker rebate / taker fee\n        if is_maker:\n            commission *= 0.5  # Maker rebate\n\n        fill = BacktestFill(\n            order_id=order.order_id,\n            symbol=order.symbol,\n            side=order.side,\n            quantity=quantity,\n            price=fill_price,\n            timestamp=timestamp,\n            slippage_bps=slippage,\n            commission=commission,\n            is_maker=is_maker\n        )\n\n        self.fills.append(fill)\n\n        # Update order status\n        order.filled_quantity = quantity\n        order.filled_price = fill_price\n        order.fill_time = timestamp\n        order.slippage_bps = slippage\n        order.commission = commission\n        order.status = OrderStatus.FILLED\n\n        # Update position\n        self._update_position(fill)\n\n        logger.debug(f\"Fill: {order.side.value} {quantity} {order.symbol} @ {fill_price:.5f}\")",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "TickBacktestEngine"
  },
  {
    "name": "_update_position",
    "category": "execution",
    "formula": "",
    "explanation": "Update position from fill.",
    "python_code": "def _update_position(self, fill: BacktestFill) -> None:\n        \"\"\"Update position from fill.\"\"\"\n        symbol = fill.symbol\n        prev_pos = self.positions[symbol]\n        prev_avg = self.avg_entry_prices[symbol]\n\n        if fill.side == OrderSide.BUY:\n            new_pos = prev_pos + fill.quantity\n            if new_pos > 0:\n                # Calculate new average entry\n                total_cost = prev_pos * prev_avg + fill.quantity * fill.price\n                self.avg_entry_prices[symbol] = total_cost / new_pos if new_pos != 0 else 0\n        else:\n            new_pos = prev_pos - fill.quantity\n            if new_pos < 0:\n                # Short position, update average\n                total_cost = abs(prev_pos) * prev_avg + fill.quantity * fill.price\n                self.avg_entry_prices[symbol] = total_cost / abs(new_pos) if new_pos != 0 else 0\n\n        # Check for position close (PnL realization)\n        if prev_pos != 0 and np.sign(new_pos) != np.sign(prev_pos):\n            # Position flipped or closed\n            closed_qty = min(abs(prev_pos), fill.quantity)\n            self._record_trade_close(symbol, fill.side, closed_qty, fill.price, fill.timestamp)\n\n        elif prev_pos != 0 and abs(new_pos) < abs(prev_pos):\n            # Partial close\n            closed_qty = abs(prev_pos) - abs(new_pos)\n            self._record_trade_close(symbol, fill.side, closed_qty, fill.price, fill.timestamp)\n\n        self.positions[symbol] = new_pos\n\n        # Deduct commission\n        self.cash -= fill.commission",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "TickBacktestEngine"
  },
  {
    "name": "_record_trade_close",
    "category": "execution",
    "formula": "",
    "explanation": "Record closed trade with PnL.",
    "python_code": "def _record_trade_close(self, symbol: str, side: OrderSide, quantity: float,\n                            exit_price: float, exit_time: datetime) -> None:\n        \"\"\"Record closed trade with PnL.\"\"\"\n        entry_price = self.avg_entry_prices[symbol]\n\n        if side == OrderSide.SELL:\n            # Was long, now selling\n            pnl = (exit_price - entry_price) * quantity * self.config.lot_size\n        else:\n            # Was short, now buying\n            pnl = (entry_price - exit_price) * quantity * self.config.lot_size\n\n        pnl_bps = ((exit_price / entry_price) - 1) * 10000 if entry_price != 0 else 0\n\n        self._trade_counter += 1\n        trade = BacktestTrade(\n            trade_id=f\"trade_{self._trade_counter}\",\n            symbol=symbol,\n            side=OrderSide.SELL if side == OrderSide.SELL else OrderSide.BUY,\n            quantity=quantity,\n            entry_price=entry_price,\n            entry_time=exit_time,  # Approximate\n            exit_price=exit_price,\n            exit_time=exit_time,\n            pnl=pnl,\n            pnl_bps=pnl_bps\n        )\n\n        self.trades.append(trade)\n        self.cash += pnl",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "TickBacktestEngine"
  },
  {
    "name": "_update_equity",
    "category": "execution",
    "formula": "",
    "explanation": "Update equity curve.",
    "python_code": "def _update_equity(self) -> None:\n        \"\"\"Update equity curve.\"\"\"\n        # Mark-to-market PnL\n        mtm_pnl = 0.0\n\n        for symbol, position in self.positions.items():\n            if position == 0:\n                continue\n\n            tick = self.current_tick.get(symbol)\n            if tick:\n                mid = (tick.bid + tick.ask) / 2\n                entry = self.avg_entry_prices[symbol]\n                if position > 0:\n                    mtm_pnl += (mid - entry) * abs(position) * self.config.lot_size\n                else:\n                    mtm_pnl += (entry - mid) * abs(position) * self.config.lot_size\n\n        equity = self.cash + mtm_pnl\n        self.equity_curve.append((self.current_time, equity))",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "TickBacktestEngine"
  },
  {
    "name": "submit_order",
    "category": "execution",
    "formula": "order_id",
    "explanation": "Submit order to backtest engine.\n\nArgs:\n    symbol: Trading symbol\n    side: BUY or SELL\n    quantity: Order quantity (in lots)\n    order_type: MARKET or LIMIT\n    limit_price: Limit price (required for LIMIT orders)\n\nReturns:\n    Order ID",
    "python_code": "def submit_order(self, symbol: str, side: OrderSide, quantity: float,\n                     order_type: OrderType = OrderType.MARKET,\n                     limit_price: Optional[float] = None) -> str:\n        \"\"\"\n        Submit order to backtest engine.\n\n        Args:\n            symbol: Trading symbol\n            side: BUY or SELL\n            quantity: Order quantity (in lots)\n            order_type: MARKET or LIMIT\n            limit_price: Limit price (required for LIMIT orders)\n\n        Returns:\n            Order ID\n        \"\"\"\n        # Validate\n        if symbol not in self.symbols:\n            raise ValueError(f\"Unknown symbol: {symbol}\")\n\n        if order_type == OrderType.LIMIT and limit_price is None:\n            raise ValueError(\"Limit price required for LIMIT orders\")\n\n        # Check position limits\n        current_pos = self.positions[symbol]\n        if side == OrderSide.BUY:\n            new_pos = current_pos + quantity\n        else:\n            new_pos = current_pos - quantity\n\n        if abs(new_pos) > self.config.max_position:\n            logger.warning(f\"Order would exceed max position, rejecting\")\n            return None\n\n        # Create order\n        self._order_counter += 1\n        order_id = f\"order_{self._order_counter}\"\n\n        order = BacktestOrder(\n            order_id=order_id,\n            symbol=symbol,\n            side=side,\n            order_type=order_type,\n            quantity=quantity,\n            limit_price=limit_price,\n            timestamp=self.current_time,\n            status=OrderStatus.SUBMITTED\n        )\n\n        # For limit orders, track queue position\n        if order_type == OrderType.LIMIT:\n            book = self.order_books[symbol]\n            queue_side = Side.BUY if side == OrderSide.BUY else Side.SELL\n            order.queue_position = book.estimate_queue_ahead(limit_price, queue_side)\n            order.initial_queue_position = order.queue_position\n\n            # Add to queue tracker\n            tracker = self.queue_trackers[symbol]\n            tracker.add_order(order_id, limit_price, quantity, queue_side, order.queue_position)\n\n        self.orders[order_id] = order\n        self.pending_orders.append(order_id)\n\n        return order_id",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "TickBacktestEngine"
  },
  {
    "name": "cancel_order",
    "category": "execution",
    "formula": "False | False | True",
    "explanation": "Cancel pending order.",
    "python_code": "def cancel_order(self, order_id: str) -> bool:\n        \"\"\"Cancel pending order.\"\"\"\n        if order_id not in self.orders:\n            return False\n\n        order = self.orders[order_id]\n        if order.status != OrderStatus.SUBMITTED:\n            return False\n\n        order.status = OrderStatus.CANCELLED\n\n        if order_id in self.pending_orders:\n            self.pending_orders.remove(order_id)\n\n        # Remove from queue tracker\n        tracker = self.queue_trackers.get(order.symbol)\n        if tracker:\n            tracker.remove_order(order_id)\n\n        return True",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "TickBacktestEngine"
  },
  {
    "name": "get_position",
    "category": "execution",
    "formula": "",
    "explanation": "Get current position for symbol.",
    "python_code": "def get_position(self, symbol: str) -> float:\n        \"\"\"Get current position for symbol.\"\"\"\n        return self.positions.get(symbol, 0.0)",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "TickBacktestEngine"
  },
  {
    "name": "get_mid_price",
    "category": "execution",
    "formula": "(tick.bid + tick.ask) / 2",
    "explanation": "Get current mid price.",
    "python_code": "def get_mid_price(self, symbol: str) -> Optional[float]:\n        \"\"\"Get current mid price.\"\"\"\n        tick = self.current_tick.get(symbol)\n        if tick:\n            return (tick.bid + tick.ask) / 2\n        return None",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "TickBacktestEngine"
  },
  {
    "name": "get_spread",
    "category": "microstructure",
    "formula": "tick.ask - tick.bid",
    "explanation": "Get current spread.",
    "python_code": "def get_spread(self, symbol: str) -> Optional[float]:\n        \"\"\"Get current spread.\"\"\"\n        tick = self.current_tick.get(symbol)\n        if tick:\n            return tick.ask - tick.bid\n        return None",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "TickBacktestEngine"
  },
  {
    "name": "get_book_imbalance",
    "category": "execution",
    "formula": "book.get_imbalance() | 0.0",
    "explanation": "Get order book imbalance.",
    "python_code": "def get_book_imbalance(self, symbol: str) -> float:\n        \"\"\"Get order book imbalance.\"\"\"\n        book = self.order_books.get(symbol)\n        if book:\n            return book.get_imbalance()\n        return 0.0",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "TickBacktestEngine"
  },
  {
    "name": "get_results",
    "category": "execution",
    "formula": "{} | = (final / initial - 1) * 100 | {",
    "explanation": "Get backtest results summary.",
    "python_code": "def get_results(self) -> Dict[str, Any]:\n        \"\"\"Get backtest results summary.\"\"\"\n        if not self.equity_curve:\n            return {}\n\n        equity_df = pd.DataFrame(self.equity_curve, columns=['timestamp', 'equity'])\n\n        # Calculate metrics\n        initial = self.config.initial_capital\n        final = equity_df['equity'].iloc[-1]\n        returns = equity_df['equity'].pct_change().dropna()\n\n        total_return = (final / initial - 1) * 100\n        sharpe = returns.mean() / returns.std() * np.sqrt(252 * 24 * 60) if len(returns) > 1 else 0\n\n        # Win rate\n        winning_trades = [t for t in self.trades if t.pnl > 0]\n        losing_trades = [t for t in self.trades if t.pnl <= 0]\n        win_rate = len(winning_trades) / len(self.trades) * 100 if self.trades else 0\n\n        # Max drawdown\n        peak = equity_df['equity'].cummax()\n        drawdown = (equity_df['equity'] - peak) / peak\n        max_dd = drawdown.min() * 100\n\n        # Average trade metrics\n        avg_pnl = np.mean([t.pnl for t in self.trades]) if self.trades else 0\n        avg_pnl_bps = np.mean([t.pnl_bps for t in self.trades]) if self.trades else 0\n\n        return {\n            'initial_capital': initial,\n            'final_equity': final,\n            'total_return_pct': total_return,\n            'sharpe_ratio': sharpe,\n            'max_drawdown_pct': max_dd,\n            'total_trades': len(self.trades),\n            'total_fills': len(self.fills),\n            'win_rate_pct': win_rate,\n            'avg_pnl_per_trade': avg_pnl,\n            'avg_pnl_bps': avg_pnl_bps,\n            'total_commission': sum(f.commission for f in self.fills),\n            'equity_curve': equity_df,\n            'trades': self.trades,\n            'fills': self.fills\n        }",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "TickBacktestEngine"
  },
  {
    "name": "get_trade_log",
    "category": "execution",
    "formula": "pd.DataFrame(data)",
    "explanation": "Get trade log as DataFrame.",
    "python_code": "def get_trade_log(self) -> pd.DataFrame:\n        \"\"\"Get trade log as DataFrame.\"\"\"\n        data = []\n        for trade in self.trades:\n            data.append({\n                'trade_id': trade.trade_id,\n                'symbol': trade.symbol,\n                'side': trade.side.value,\n                'quantity': trade.quantity,\n                'entry_price': trade.entry_price,\n                'exit_price': trade.exit_price,\n                'pnl': trade.pnl,\n                'pnl_bps': trade.pnl_bps,\n                'exit_time': trade.exit_time\n            })\n        return pd.DataFrame(data)",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "TickBacktestEngine"
  },
  {
    "name": "on_tick",
    "category": "execution",
    "formula": "# Calculate MAs",
    "explanation": "",
    "python_code": "def on_tick(self, tick: TickData, engine: TickBacktestEngine) -> None:\n        # Get symbol from current tick\n        for symbol, t in engine.current_tick.items():\n            if t == tick:\n                self.prices[symbol].append((t.bid + t.ask) / 2)\n\n                # Need enough history\n                if len(self.prices[symbol]) < self.slow_period:\n                    return\n\n                # Calculate MAs\n                fast_ma = np.mean(self.prices[symbol][-self.fast_period:])\n                slow_ma = np.mean(self.prices[symbol][-self.slow_period:])\n\n                position = engine.get_position(symbol)\n\n                # Simple crossover logic\n                if fast_ma > slow_ma and position <= 0:\n                    # Buy signal\n                    if position < 0:\n                        engine.submit_order(symbol, OrderSide.BUY, abs(position))\n                    engine.submit_order(symbol, OrderSide.BUY, 0.1)\n\n                elif fast_ma < slow_ma and position >= 0:\n                    # Sell signal\n                    if position > 0:\n                        engine.submit_order(symbol, OrderSide.SELL, position)\n                    engine.submit_order(symbol, OrderSide.SELL, 0.1)",
    "source_file": "core\\execution\\backtest.py",
    "academic_reference": null,
    "class_name": "SimpleMovingAverageStrategy"
  },
  {
    "name": "__init__",
    "category": "execution",
    "formula": "",
    "explanation": "Initialize model.\n\nArgs:\n    calibration_window: Number of trades to use for rate estimation",
    "python_code": "def __init__(self, calibration_window: int = 1000):\n        \"\"\"\n        Initialize model.\n\n        Args:\n            calibration_window: Number of trades to use for rate estimation\n        \"\"\"\n        self.calibration_window = calibration_window\n        self.trade_times: List[datetime] = []\n        self.trade_volumes: List[float] = []\n\n        # Calibrated parameters\n        self.arrival_rate: float = 1.0  # Trades per second\n        self.avg_trade_size: float = 100.0",
    "source_file": "core\\execution\\fill_probability.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "PoissonFillModel"
  },
  {
    "name": "record_trade",
    "category": "execution",
    "formula": "",
    "explanation": "Record a trade for rate estimation.",
    "python_code": "def record_trade(self, timestamp: datetime, volume: float) -> None:\n        \"\"\"Record a trade for rate estimation.\"\"\"\n        self.trade_times.append(timestamp)\n        self.trade_volumes.append(volume)\n\n        if len(self.trade_times) > self.calibration_window:\n            self.trade_times.pop(0)\n            self.trade_volumes.pop(0)\n\n        self._recalibrate()",
    "source_file": "core\\execution\\fill_probability.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "PoissonFillModel"
  },
  {
    "name": "_recalibrate",
    "category": "execution",
    "formula": "# Time span | # Arrival rate (trades per second)",
    "explanation": "Recalibrate arrival rate from trade history.",
    "python_code": "def _recalibrate(self) -> None:\n        \"\"\"Recalibrate arrival rate from trade history.\"\"\"\n        if len(self.trade_times) < 10:\n            return\n\n        # Time span\n        time_span = (self.trade_times[-1] - self.trade_times[0]).total_seconds()\n        if time_span <= 0:\n            return\n\n        # Arrival rate (trades per second)\n        self.arrival_rate = len(self.trade_times) / time_span\n\n        # Average trade size\n        self.avg_trade_size = np.mean(self.trade_volumes)",
    "source_file": "core\\execution\\fill_probability.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "PoissonFillModel"
  },
  {
    "name": "fill_probability",
    "category": "execution",
    "formula": "1.0 | 0.0 | 1 - stats.norm.cdf(z)",
    "explanation": "Calculate probability of fill within time horizon.\n\nArgs:\n    queue_position: Quantity ahead in queue\n    time_horizon: Time window in seconds\n\nReturns:\n    Probability of fill [0, 1]",
    "python_code": "def fill_probability(self, queue_position: float, time_horizon: float = 60.0) -> float:\n        \"\"\"\n        Calculate probability of fill within time horizon.\n\n        Args:\n            queue_position: Quantity ahead in queue\n            time_horizon: Time window in seconds\n\n        Returns:\n            Probability of fill [0, 1]\n        \"\"\"\n        if queue_position <= 0:\n            return 1.0\n\n        # Expected volume to trade\n        expected_volume = self.arrival_rate * self.avg_trade_size * time_horizon\n\n        if expected_volume <= 0:\n            return 0.0\n\n        # Number of \"average trades\" needed to fill position\n        trades_needed = queue_position / self.avg_trade_size\n\n        # Poisson probability P(X >= trades_needed) where X ~ Poisson(rate * T)\n        lambda_param = self.arrival_rate * time_horizon\n\n        if lambda_param > 50:\n            # Normal approximation for large lambda\n            z = (trades_needed - lambda_param) / np.sqrt(lambda_param)\n            return 1 - stats.norm.cdf(z)\n        else:\n            # Exact Poisson\n            return 1 - stats.poisson.cdf(int(trades_needed) - 1, lambda_param)",
    "source_file": "core\\execution\\fill_probability.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "PoissonFillModel"
  },
  {
    "name": "expected_time_to_fill",
    "category": "execution",
    "formula": "0.0 | float('inf') | queue_position / volume_rate",
    "explanation": "Estimate expected time to fill.\n\nArgs:\n    queue_position: Quantity ahead in queue\n\nReturns:\n    Expected time in seconds",
    "python_code": "def expected_time_to_fill(self, queue_position: float) -> float:\n        \"\"\"\n        Estimate expected time to fill.\n\n        Args:\n            queue_position: Quantity ahead in queue\n\n        Returns:\n            Expected time in seconds\n        \"\"\"\n        if queue_position <= 0:\n            return 0.0\n\n        if self.arrival_rate <= 0 or self.avg_trade_size <= 0:\n            return float('inf')\n\n        # Expected time = queue_position / (arrival_rate * avg_trade_size)\n        volume_rate = self.arrival_rate * self.avg_trade_size\n        return queue_position / volume_rate",
    "source_file": "core\\execution\\fill_probability.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "PoissonFillModel"
  },
  {
    "name": "temporary_impact",
    "category": "microstructure",
    "formula": "impact",
    "explanation": "Calculate temporary market impact in basis points.\n\nArgs:\n    order_size: Order size\n    execution_time: Execution time in seconds\n\nReturns:\n    Temporary impact in basis points",
    "python_code": "def temporary_impact(self, order_size: float, execution_time: float) -> float:\n        \"\"\"\n        Calculate temporary market impact in basis points.\n\n        Args:\n            order_size: Order size\n            execution_time: Execution time in seconds\n\n        Returns:\n            Temporary impact in basis points\n        \"\"\"\n        # Participation rate\n        volume_per_second = self.daily_volume / (6.5 * 3600)  # Assuming 6.5 hour trading day\n        participation = order_size / (volume_per_second * execution_time) if execution_time > 0 else 1.0\n\n        # Square-root impact model\n        impact = self.temp_coef * self.daily_volatility * np.sqrt(participation) * 10000\n\n        return impact",
    "source_file": "core\\execution\\fill_probability.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "MarketImpactModel"
  },
  {
    "name": "permanent_impact",
    "category": "microstructure",
    "formula": "impact",
    "explanation": "Calculate permanent market impact in basis points.\n\nArgs:\n    order_size: Order size\n\nReturns:\n    Permanent impact in basis points",
    "python_code": "def permanent_impact(self, order_size: float) -> float:\n        \"\"\"\n        Calculate permanent market impact in basis points.\n\n        Args:\n            order_size: Order size\n\n        Returns:\n            Permanent impact in basis points\n        \"\"\"\n        # Linear impact model\n        volume_fraction = order_size / self.daily_volume\n        impact = self.perm_coef * self.daily_volatility * volume_fraction * 10000\n\n        return impact",
    "source_file": "core\\execution\\fill_probability.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "MarketImpactModel"
  },
  {
    "name": "total_impact",
    "category": "microstructure",
    "formula": "",
    "explanation": "Calculate total market impact.\n\nArgs:\n    order_size: Order size\n    execution_time: Expected execution time in seconds\n\nReturns:\n    Total impact in basis points",
    "python_code": "def total_impact(self, order_size: float, execution_time: float = 60.0) -> float:\n        \"\"\"\n        Calculate total market impact.\n\n        Args:\n            order_size: Order size\n            execution_time: Expected execution time in seconds\n\n        Returns:\n            Total impact in basis points\n        \"\"\"\n        return self.temporary_impact(order_size, execution_time) + self.permanent_impact(order_size)",
    "source_file": "core\\execution\\fill_probability.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "MarketImpactModel"
  },
  {
    "name": "optimal_execution_time",
    "category": "microstructure",
    "formula": "1.0  # Execute immediately | base_time * (1 - urgency)",
    "explanation": "Calculate optimal execution time.\n\nArgs:\n    order_size: Order size\n    urgency: Urgency parameter (0 = patient, 1 = aggressive)\n\nReturns:\n    Optimal execution time in seconds",
    "python_code": "def optimal_execution_time(self, order_size: float, urgency: float = 0.5) -> float:\n        \"\"\"\n        Calculate optimal execution time.\n\n        Args:\n            order_size: Order size\n            urgency: Urgency parameter (0 = patient, 1 = aggressive)\n\n        Returns:\n            Optimal execution time in seconds\n        \"\"\"\n        # Almgren-Chriss optimal trading rate\n        # Balance between temporary impact (faster = worse) and timing risk (slower = worse)\n\n        if urgency >= 1.0:\n            return 1.0  # Execute immediately\n\n        # Simple heuristic: larger orders need more time\n        volume_fraction = order_size / self.daily_volume\n        base_time = 60 * (1 + 100 * volume_fraction)  # Base time in seconds\n\n        # Adjust for urgency\n        return base_time * (1 - urgency)",
    "source_file": "core\\execution\\fill_probability.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "MarketImpactModel"
  },
  {
    "name": "record_execution",
    "category": "execution",
    "formula": "# Calculate actual slippage",
    "explanation": "Record execution for calibration.",
    "python_code": "def record_execution(self, result: ExecutionResult) -> None:\n        \"\"\"Record execution for calibration.\"\"\"\n        if not result.filled or result.fill_price is None:\n            return\n\n        # Calculate actual slippage\n        if result.side == Side.BUY:\n            slippage_bps = (result.fill_price - result.limit_price) / result.limit_price * 10000\n        else:\n            slippage_bps = (result.limit_price - result.fill_price) / result.limit_price * 10000\n\n        self.historical_slippages.append({\n            'slippage_bps': slippage_bps,\n            'quantity': result.quantity,\n            'queue_position': result.queue_position_at_submit,\n            'time_in_queue': result.time_in_queue\n        })\n\n        if len(self.historical_slippages) > self.max_history:\n            self.historical_slippages.pop(0)\n\n        self._recalibrate()",
    "source_file": "core\\execution\\fill_probability.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "SlippageEstimator"
  },
  {
    "name": "_recalibrate",
    "category": "execution",
    "formula": "slippages = [h['slippage_bps'] for h in self.historical_slippages]",
    "explanation": "Recalibrate from historical data.",
    "python_code": "def _recalibrate(self) -> None:\n        \"\"\"Recalibrate from historical data.\"\"\"\n        if len(self.historical_slippages) < 20:\n            return\n\n        slippages = [h['slippage_bps'] for h in self.historical_slippages]\n        self.base_slippage_bps = np.median(slippages)",
    "source_file": "core\\execution\\fill_probability.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "SlippageEstimator"
  },
  {
    "name": "estimate_slippage",
    "category": "microstructure",
    "formula": "slippage",
    "explanation": "Estimate expected slippage.\n\nArgs:\n    order_size: Order quantity\n    spread_bps: Current spread in basis points\n    book_depth: Available liquidity at best level\n    side: Order side\n\nReturns:\n    Expected slippage in basis points",
    "python_code": "def estimate_slippage(self,\n                          order_size: float,\n                          spread_bps: float,\n                          book_depth: float,\n                          side: Side) -> float:\n        \"\"\"\n        Estimate expected slippage.\n\n        Args:\n            order_size: Order quantity\n            spread_bps: Current spread in basis points\n            book_depth: Available liquidity at best level\n            side: Order side\n\n        Returns:\n            Expected slippage in basis points\n        \"\"\"\n        # Base slippage\n        slippage = self.base_slippage_bps\n\n        # Spread component (crossing = half spread)\n        slippage += spread_bps * self.spread_coef\n\n        # Size component (larger orders = more slippage)\n        if book_depth > 0:\n            size_ratio = order_size / book_depth\n            slippage += self.size_impact_coef * size_ratio * 10  # Scale factor\n\n        return slippage",
    "source_file": "core\\execution\\fill_probability.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "SlippageEstimator"
  },
  {
    "name": "estimate_execution_cost",
    "category": "execution",
    "formula": "{",
    "explanation": "Estimate total execution cost breakdown.\n\nReturns:\n    Dict with cost components in basis points",
    "python_code": "def estimate_execution_cost(self,\n                                order_size: float,\n                                mid_price: float,\n                                spread_bps: float,\n                                book_depth: float,\n                                side: Side) -> Dict[str, float]:\n        \"\"\"\n        Estimate total execution cost breakdown.\n\n        Returns:\n            Dict with cost components in basis points\n        \"\"\"\n        # Slippage\n        slippage = self.estimate_slippage(order_size, spread_bps, book_depth, side)\n\n        # Commission (assume 0.1 bps for institutional)\n        commission = 0.1\n\n        # Spread cost (half-spread for market orders)\n        spread_cost = spread_bps / 2\n\n        # Total\n        total = slippage + commission + spread_cost\n\n        return {\n            'slippage_bps': slippage,\n            'commission_bps': commission,\n            'spread_cost_bps': spread_cost,\n            'total_cost_bps': total,\n            'total_cost_dollars': total / 10000 * mid_price * order_size\n        }",
    "source_file": "core\\execution\\fill_probability.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "SlippageEstimator"
  },
  {
    "name": "estimate_fill",
    "category": "microstructure",
    "formula": "FillEstimate(",
    "explanation": "Generate comprehensive fill estimate.\n\nArgs:\n    side: Order side\n    limit_price: Limit order price\n    quantity: Order quantity\n    queue_position: Current queue position\n    mid_price: Current mid price\n    spread_bps: Current spread in bps\n    book_depth: Depth at limit price level\n    time_horizon: Time window in seconds\n\nReturns:\n    FillEstimate with all components",
    "python_code": "def estimate_fill(self,\n                      side: Side,\n                      limit_price: float,\n                      quantity: float,\n                      queue_position: float,\n                      mid_price: float,\n                      spread_bps: float,\n                      book_depth: float,\n                      time_horizon: float = 60.0) -> FillEstimate:\n        \"\"\"\n        Generate comprehensive fill estimate.\n\n        Args:\n            side: Order side\n            limit_price: Limit order price\n            quantity: Order quantity\n            queue_position: Current queue position\n            mid_price: Current mid price\n            spread_bps: Current spread in bps\n            book_depth: Depth at limit price level\n            time_horizon: Time window in seconds\n\n        Returns:\n            FillEstimate with all components\n        \"\"\"\n        # Fill probability\n        fill_prob = self.poisson_model.fill_probability(queue_position, time_horizon)\n\n        # Time to fill\n        time_to_fill = self.poisson_model.expected_time_to_fill(queue_position)\n\n        # Slippage (for limit orders that fill, minimal)\n        slippage = self.slippage_estimator.estimate_slippage(\n            quantity, spread_bps, book_depth, side\n        )\n\n        # Market impact (if we were to use market order)\n        impact = self.impact_model.total_impact(quantity, time_to_fill)\n\n        # Confidence based on data quality\n        confidence = min(1.0, len(self.poisson_model.trade_times) / 100)\n\n        return FillEstimate(\n            fill_probability=fill_prob,\n            expected_time_to_fill=time_to_fill,\n            expected_slippage_bps=slippage,\n            market_impact_bps=impact,\n            adverse_selection_bps=self.adverse_selection_bps,\n            confidence=confidence\n        )",
    "source_file": "core\\execution\\fill_probability.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "FillProbabilityEngine"
  },
  {
    "name": "should_use_market_order",
    "category": "execution",
    "formula": "True, \"Low fill probability, market order cheaper\" | True, \"High urgency, long expected wait\" | True, \"Market order significantly cheaper\"",
    "explanation": "Decide whether to use market order instead of limit.\n\nArgs:\n    limit_fill_estimate: Fill estimate for limit order\n    urgency: How urgent is the fill (0-1)\n\nReturns:\n    (use_market, reason)",
    "python_code": "def should_use_market_order(self,\n                                limit_fill_estimate: FillEstimate,\n                                urgency: float = 0.5) -> Tuple[bool, str]:\n        \"\"\"\n        Decide whether to use market order instead of limit.\n\n        Args:\n            limit_fill_estimate: Fill estimate for limit order\n            urgency: How urgent is the fill (0-1)\n\n        Returns:\n            (use_market, reason)\n        \"\"\"\n        # Cost of waiting (opportunity cost)\n        wait_cost = limit_fill_estimate.expected_time_to_fill * urgency * 0.01  # bps per second\n\n        # Cost of limit order\n        limit_cost = limit_fill_estimate.expected_slippage_bps * limit_fill_estimate.fill_probability\n\n        # Cost of market order\n        market_cost = limit_fill_estimate.market_impact_bps + limit_fill_estimate.adverse_selection_bps\n\n        # Probability-adjusted comparison\n        if limit_fill_estimate.fill_probability < 0.5:\n            # Low fill prob, consider market\n            if market_cost < limit_cost + wait_cost:\n                return True, \"Low fill probability, market order cheaper\"\n\n        if urgency > 0.8 and limit_fill_estimate.expected_time_to_fill > 30:\n            return True, \"High urgency, long expected wait\"\n\n        if market_cost < limit_cost * 0.5:\n            return True, \"Market order significantly cheaper\"\n\n        return False, \"Limit order preferred\"",
    "source_file": "core\\execution\\fill_probability.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "FillProbabilityEngine"
  },
  {
    "name": "record_execution",
    "category": "execution",
    "formula": "",
    "explanation": "Record execution for model calibration.",
    "python_code": "def record_execution(self, result: ExecutionResult) -> None:\n        \"\"\"Record execution for model calibration.\"\"\"\n        self.slippage_estimator.record_execution(result)",
    "source_file": "core\\execution\\fill_probability.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "FillProbabilityEngine"
  },
  {
    "name": "optimal_aggressiveness",
    "category": "execution",
    "formula": "{",
    "explanation": "Calculate optimal order aggressiveness.\n\nReturns:\n    Dict with recommendation and analysis",
    "python_code": "def optimal_aggressiveness(self,\n                               fill_estimate: FillEstimate,\n                               spread_bps: float,\n                               urgency: float = 0.5) -> Dict[str, any]:\n        \"\"\"\n        Calculate optimal order aggressiveness.\n\n        Returns:\n            Dict with recommendation and analysis\n        \"\"\"\n        # Cost of different strategies\n        strategies = {}\n\n        # Passive maker (at bid/ask)\n        maker_cost = (\n            fill_estimate.expected_slippage_bps\n            - self.maker_rebate\n            + fill_estimate.adverse_selection_bps * fill_estimate.fill_probability\n        )\n        maker_expected = maker_cost * fill_estimate.fill_probability\n\n        # Aggressive taker (cross spread)\n        taker_cost = spread_bps / 2 + self.taker_fee + fill_estimate.market_impact_bps\n        taker_expected = taker_cost  # 100% fill\n\n        # Mid-price pegging\n        mid_fill_prob = fill_estimate.fill_probability * 0.7  # Lower fill rate at mid\n        mid_cost = spread_bps / 4 - self.maker_rebate / 2\n        mid_expected = mid_cost * mid_fill_prob\n\n        strategies = {\n            'maker': {'cost': maker_expected, 'fill_prob': fill_estimate.fill_probability},\n            'taker': {'cost': taker_expected, 'fill_prob': 1.0},\n            'mid': {'cost': mid_expected, 'fill_prob': mid_fill_prob}\n        }\n\n        # Adjust for urgency\n        for strat, data in strategies.items():\n            wait_penalty = (1 - data['fill_prob']) * urgency * 10\n            data['adjusted_cost'] = data['cost'] + wait_penalty\n\n        # Find optimal\n        optimal = min(strategies.items(), key=lambda x: x[1]['adjusted_cost'])\n\n        return {\n            'recommendation': optimal[0],\n            'expected_cost_bps': optimal[1]['adjusted_cost'],\n            'strategies': strategies,\n            'urgency': urgency,\n            'spread_bps': spread_bps\n        }",
    "source_file": "core\\execution\\fill_probability.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "MakerTakerDecision"
  },
  {
    "name": "__init__",
    "category": "execution",
    "formula": "",
    "explanation": "Initialize latency simulator.\n\nArgs:\n    venue: Venue name from VENUE_CONFIGS\n    config: Custom LatencyConfig (overrides venue)",
    "python_code": "def __init__(self, venue: str = 'retail_forex', config: LatencyConfig = None):\n        \"\"\"\n        Initialize latency simulator.\n\n        Args:\n            venue: Venue name from VENUE_CONFIGS\n            config: Custom LatencyConfig (overrides venue)\n        \"\"\"\n        if config:\n            self.config = config\n        elif venue in VENUE_CONFIGS:\n            self.config = VENUE_CONFIGS[venue]\n        else:\n            self.config = LatencyConfig()\n\n        # For reproducibility\n        self.rng = np.random.default_rng(42)\n\n        # Track latency statistics\n        self.latency_history: Dict[LatencyType, List[float]] = {\n            lt: [] for lt in LatencyType\n        }",
    "source_file": "core\\execution\\latency.py",
    "academic_reference": null,
    "class_name": "LatencySimulator"
  },
  {
    "name": "_sample_latency",
    "category": "execution",
    "formula": "0.0 | mean | max(0, latency)",
    "explanation": "Sample latency from distribution.\n\nUses log-normal for realistic latency distribution.",
    "python_code": "def _sample_latency(self, mean: float, std: float) -> float:\n        \"\"\"\n        Sample latency from distribution.\n\n        Uses log-normal for realistic latency distribution.\n        \"\"\"\n        # Log-normal parameters from mean and std\n        if mean <= 0:\n            return 0.0\n\n        # Check for spike\n        if self.rng.random() < self.config.spike_probability:\n            mean *= self.config.spike_multiplier\n            std *= self.config.spike_multiplier\n\n        # Log-normal sampling (always positive, right-skewed)\n        if std <= 0:\n            return mean\n\n        mu = np.log(mean ** 2 / np.sqrt(std ** 2 + mean ** 2))\n        sigma = np.sqrt(np.log(1 + (std ** 2) / (mean ** 2)))\n\n        latency = self.rng.lognormal(mu, sigma)\n\n        return max(0, latency)",
    "source_file": "core\\execution\\latency.py",
    "academic_reference": null,
    "class_name": "LatencySimulator"
  },
  {
    "name": "apply_feed_latency",
    "category": "execution",
    "formula": "timestamp + timedelta(milliseconds=latency_ms)",
    "explanation": "Apply feed latency to market data timestamp.\n\nArgs:\n    timestamp: Original tick timestamp\n\nReturns:\n    Delayed timestamp (when we receive the tick)",
    "python_code": "def apply_feed_latency(self, timestamp: datetime) -> datetime:\n        \"\"\"\n        Apply feed latency to market data timestamp.\n\n        Args:\n            timestamp: Original tick timestamp\n\n        Returns:\n            Delayed timestamp (when we receive the tick)\n        \"\"\"\n        latency_ms = self._sample_latency(\n            self.config.feed_latency_ms,\n            self.config.feed_jitter_ms\n        )\n\n        self.latency_history[LatencyType.FEED].append(latency_ms)\n\n        return timestamp + timedelta(milliseconds=latency_ms)",
    "source_file": "core\\execution\\latency.py",
    "academic_reference": null,
    "class_name": "LatencySimulator"
  },
  {
    "name": "apply_order_latency",
    "category": "execution",
    "formula": "submit_time + timedelta(milliseconds=latency_ms)",
    "explanation": "Apply order submission latency.\n\nArgs:\n    submit_time: Time order was submitted\n\nReturns:\n    Time order is received by exchange",
    "python_code": "def apply_order_latency(self, submit_time: datetime) -> datetime:\n        \"\"\"\n        Apply order submission latency.\n\n        Args:\n            submit_time: Time order was submitted\n\n        Returns:\n            Time order is received by exchange\n        \"\"\"\n        latency_ms = self._sample_latency(\n            self.config.order_latency_ms,\n            self.config.order_jitter_ms\n        )\n\n        self.latency_history[LatencyType.ORDER].append(latency_ms)\n\n        return submit_time + timedelta(milliseconds=latency_ms)",
    "source_file": "core\\execution\\latency.py",
    "academic_reference": null,
    "class_name": "LatencySimulator"
  },
  {
    "name": "apply_fill_latency",
    "category": "execution",
    "formula": "match_time + timedelta(milliseconds=latency_ms)",
    "explanation": "Apply fill confirmation latency.\n\nArgs:\n    match_time: Time order was matched\n\nReturns:\n    Time we receive fill confirmation",
    "python_code": "def apply_fill_latency(self, match_time: datetime) -> datetime:\n        \"\"\"\n        Apply fill confirmation latency.\n\n        Args:\n            match_time: Time order was matched\n\n        Returns:\n            Time we receive fill confirmation\n        \"\"\"\n        latency_ms = self._sample_latency(\n            self.config.fill_latency_ms,\n            self.config.fill_jitter_ms\n        )\n\n        self.latency_history[LatencyType.FILL].append(latency_ms)\n\n        return match_time + timedelta(milliseconds=latency_ms)",
    "source_file": "core\\execution\\latency.py",
    "academic_reference": null,
    "class_name": "LatencySimulator"
  },
  {
    "name": "apply_cancel_latency",
    "category": "execution",
    "formula": "cancel_time + timedelta(milliseconds=latency_ms)",
    "explanation": "Apply cancel request latency.\n\nArgs:\n    cancel_time: Time cancel was requested\n\nReturns:\n    Time cancel is processed",
    "python_code": "def apply_cancel_latency(self, cancel_time: datetime) -> datetime:\n        \"\"\"\n        Apply cancel request latency.\n\n        Args:\n            cancel_time: Time cancel was requested\n\n        Returns:\n            Time cancel is processed\n        \"\"\"\n        latency_ms = self._sample_latency(\n            self.config.cancel_latency_ms,\n            self.config.cancel_jitter_ms\n        )\n\n        self.latency_history[LatencyType.CANCEL].append(latency_ms)\n\n        return cancel_time + timedelta(milliseconds=latency_ms)",
    "source_file": "core\\execution\\latency.py",
    "academic_reference": null,
    "class_name": "LatencySimulator"
  },
  {
    "name": "get_total_roundtrip",
    "category": "execution",
    "formula": "(",
    "explanation": "Get total expected roundtrip latency (ms).\n\nOrder submit -> fill confirmation received",
    "python_code": "def get_total_roundtrip(self) -> float:\n        \"\"\"\n        Get total expected roundtrip latency (ms).\n\n        Order submit -> fill confirmation received\n        \"\"\"\n        return (\n            self.config.order_latency_ms +\n            self.config.fill_latency_ms\n        )",
    "source_file": "core\\execution\\latency.py",
    "academic_reference": null,
    "class_name": "LatencySimulator"
  },
  {
    "name": "get_statistics",
    "category": "execution",
    "formula": "stats",
    "explanation": "Get latency statistics.\n\nReturns:\n    Dict with statistics per latency type",
    "python_code": "def get_statistics(self) -> Dict[str, Dict[str, float]]:\n        \"\"\"\n        Get latency statistics.\n\n        Returns:\n            Dict with statistics per latency type\n        \"\"\"\n        stats = {}\n\n        for lt, history in self.latency_history.items():\n            if history:\n                stats[lt.value] = {\n                    'mean_ms': np.mean(history),\n                    'std_ms': np.std(history),\n                    'min_ms': np.min(history),\n                    'max_ms': np.max(history),\n                    'p50_ms': np.percentile(history, 50),\n                    'p99_ms': np.percentile(history, 99),\n                    'count': len(history)\n                }\n            else:\n                stats[lt.value] = {\n                    'mean_ms': 0, 'std_ms': 0, 'min_ms': 0,\n                    'max_ms': 0, 'p50_ms': 0, 'p99_ms': 0, 'count': 0\n                }\n\n        return stats",
    "source_file": "core\\execution\\latency.py",
    "academic_reference": null,
    "class_name": "LatencySimulator"
  },
  {
    "name": "reset_statistics",
    "category": "execution",
    "formula": "",
    "explanation": "Reset latency statistics.",
    "python_code": "def reset_statistics(self) -> None:\n        \"\"\"Reset latency statistics.\"\"\"\n        for lt in LatencyType:\n            self.latency_history[lt] = []",
    "source_file": "core\\execution\\latency.py",
    "academic_reference": null,
    "class_name": "LatencySimulator"
  },
  {
    "name": "delay_tick",
    "category": "execution",
    "formula": "delayed, tick_data",
    "explanation": "Delay tick data by feed latency.\n\nArgs:\n    tick_time: Original tick timestamp\n    tick_data: Tick data dict\n\nReturns:\n    (delayed_time, tick_data)",
    "python_code": "def delay_tick(self, tick_time: datetime, tick_data: dict) -> Tuple[datetime, dict]:\n        \"\"\"\n        Delay tick data by feed latency.\n\n        Args:\n            tick_time: Original tick timestamp\n            tick_data: Tick data dict\n\n        Returns:\n            (delayed_time, tick_data)\n        \"\"\"\n        delayed = self.simulator.apply_feed_latency(tick_time)\n        return delayed, tick_data",
    "source_file": "core\\execution\\latency.py",
    "academic_reference": null,
    "class_name": "LatencyAwareBacktest"
  },
  {
    "name": "delay_order",
    "category": "execution",
    "formula": "received, order",
    "explanation": "Delay order submission.\n\nArgs:\n    submit_time: Order submission time\n    order: Order data dict\n\nReturns:\n    (exchange_received_time, order)",
    "python_code": "def delay_order(self, submit_time: datetime, order: dict) -> Tuple[datetime, dict]:\n        \"\"\"\n        Delay order submission.\n\n        Args:\n            submit_time: Order submission time\n            order: Order data dict\n\n        Returns:\n            (exchange_received_time, order)\n        \"\"\"\n        received = self.simulator.apply_order_latency(submit_time)\n        return received, order",
    "source_file": "core\\execution\\latency.py",
    "academic_reference": null,
    "class_name": "LatencyAwareBacktest"
  },
  {
    "name": "process_events",
    "category": "execution",
    "formula": "arrived",
    "explanation": "Process events that have arrived by current time.\n\nArgs:\n    current_time: Current simulation time\n\nReturns:\n    List of (event_type, event_data) that have arrived",
    "python_code": "def process_events(self, current_time: datetime) -> List[Tuple[str, dict]]:\n        \"\"\"\n        Process events that have arrived by current time.\n\n        Args:\n            current_time: Current simulation time\n\n        Returns:\n            List of (event_type, event_data) that have arrived\n        \"\"\"\n        arrived = []\n        remaining = []\n\n        for arrival_time, event_type, event_data in self.pending_events:\n            if arrival_time <= current_time:\n                arrived.append((event_type, event_data))\n            else:\n                remaining.append((arrival_time, event_type, event_data))\n\n        self.pending_events = remaining\n        return arrived",
    "source_file": "core\\execution\\latency.py",
    "academic_reference": null,
    "class_name": "LatencyAwareBacktest"
  },
  {
    "name": "add_pending_event",
    "category": "execution",
    "formula": "",
    "explanation": "Add event to pending queue.",
    "python_code": "def add_pending_event(self, arrival_time: datetime,\n                          event_type: str, event_data: dict) -> None:\n        \"\"\"Add event to pending queue.\"\"\"\n        self.pending_events.append((arrival_time, event_type, event_data))\n        self.pending_events.sort(key=lambda x: x[0])",
    "source_file": "core\\execution\\latency.py",
    "academic_reference": null,
    "class_name": "LatencyAwareBacktest"
  },
  {
    "name": "record_trade_with_latency",
    "category": "microstructure",
    "formula": "",
    "explanation": "Record trade with latency impact.",
    "python_code": "def record_trade_with_latency(self,\n                                  intended_price: float,\n                                  actual_price: float,\n                                  latency_ms: float,\n                                  side: str) -> None:\n        \"\"\"Record trade with latency impact.\"\"\"\n        if side == 'buy':\n            slippage = actual_price - intended_price\n        else:\n            slippage = intended_price - actual_price\n\n        self.trades_with_latency.append({\n            'intended_price': intended_price,\n            'actual_price': actual_price,\n            'latency_ms': latency_ms,\n            'slippage': slippage,\n            'slippage_bps': slippage / intended_price * 10000\n        })",
    "source_file": "core\\execution\\latency.py",
    "academic_reference": null,
    "class_name": "LatencyImpactAnalyzer"
  },
  {
    "name": "record_trade_without_latency",
    "category": "microstructure",
    "formula": "",
    "explanation": "Record trade without latency (ideal case).",
    "python_code": "def record_trade_without_latency(self,\n                                     intended_price: float,\n                                     actual_price: float,\n                                     side: str) -> None:\n        \"\"\"Record trade without latency (ideal case).\"\"\"\n        if side == 'buy':\n            slippage = actual_price - intended_price\n        else:\n            slippage = intended_price - actual_price\n\n        self.trades_without_latency.append({\n            'intended_price': intended_price,\n            'actual_price': actual_price,\n            'slippage': slippage,\n            'slippage_bps': slippage / intended_price * 10000\n        })",
    "source_file": "core\\execution\\latency.py",
    "academic_reference": null,
    "class_name": "LatencyImpactAnalyzer"
  },
  {
    "name": "get_latency_impact",
    "category": "microstructure",
    "formula": "{} | impact",
    "explanation": "Calculate latency impact on trading.\n\nReturns:\n    Dict with impact metrics",
    "python_code": "def get_latency_impact(self) -> Dict[str, float]:\n        \"\"\"\n        Calculate latency impact on trading.\n\n        Returns:\n            Dict with impact metrics\n        \"\"\"\n        if not self.trades_with_latency:\n            return {}\n\n        with_latency = pd.DataFrame(self.trades_with_latency)\n        without_latency = pd.DataFrame(self.trades_without_latency) if self.trades_without_latency else None\n\n        impact = {\n            'avg_slippage_with_latency_bps': with_latency['slippage_bps'].mean(),\n            'total_slippage_with_latency': with_latency['slippage'].sum(),\n            'avg_latency_ms': with_latency['latency_ms'].mean(),\n            'latency_correlation': with_latency['latency_ms'].corr(with_latency['slippage_bps'])\n        }\n\n        if without_latency is not None and len(without_latency) > 0:\n            impact['avg_slippage_without_latency_bps'] = without_latency['slippage_bps'].mean()\n            impact['latency_cost_bps'] = (\n                impact['avg_slippage_with_latency_bps'] -\n                impact['avg_slippage_without_latency_bps']\n            )\n        else:\n            impact['latency_cost_bps'] = impact['avg_slippage_with_latency_bps']\n\n        return impact",
    "source_file": "core\\execution\\latency.py",
    "academic_reference": null,
    "class_name": "LatencyImpactAnalyzer"
  },
  {
    "name": "total_quantity",
    "category": "execution",
    "formula": "sum(o.quantity for o in self.orders)",
    "explanation": "",
    "python_code": "def total_quantity(self) -> float:\n        return sum(o.quantity for o in self.orders)",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "PriceLevel"
  },
  {
    "name": "order_count",
    "category": "execution",
    "formula": "len(self.orders)",
    "explanation": "",
    "python_code": "def order_count(self) -> int:\n        return len(self.orders)",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "PriceLevel"
  },
  {
    "name": "add_order",
    "category": "execution",
    "formula": "queue position.\"\"\" | len(self.orders) - 1",
    "explanation": "Add order, return queue position.",
    "python_code": "def add_order(self, order: Order) -> int:\n        \"\"\"Add order, return queue position.\"\"\"\n        self.orders.append(order)\n        return len(self.orders) - 1",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "PriceLevel"
  },
  {
    "name": "remove_order",
    "category": "execution",
    "formula": "",
    "explanation": "Remove order by ID.",
    "python_code": "def remove_order(self, order_id: str) -> Optional[Order]:\n        \"\"\"Remove order by ID.\"\"\"\n        for i, order in enumerate(self.orders):\n            if order.order_id == order_id:\n                return self.orders.pop(i)\n        return None",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "PriceLevel"
  },
  {
    "name": "get_queue_position",
    "category": "execution",
    "formula": "i | -1",
    "explanation": "Get queue position (0 = front).",
    "python_code": "def get_queue_position(self, order_id: str) -> int:\n        \"\"\"Get queue position (0 = front).\"\"\"\n        for i, order in enumerate(self.orders):\n            if order.order_id == order_id:\n                return i\n        return -1",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "PriceLevel"
  },
  {
    "name": "__init__",
    "category": "execution",
    "formula": "forex = 1 pip)",
    "explanation": "Initialize order book.\n\nArgs:\n    tick_size: Minimum price increment (0.0001 for forex = 1 pip)\n    max_levels: Maximum depth levels to track",
    "python_code": "def __init__(self, tick_size: float = 0.0001, max_levels: int = 20):\n        \"\"\"\n        Initialize order book.\n\n        Args:\n            tick_size: Minimum price increment (0.0001 for forex = 1 pip)\n            max_levels: Maximum depth levels to track\n        \"\"\"\n        self.tick_size = tick_size\n        self.max_levels = max_levels\n\n        # Order book: price -> PriceLevel\n        self.bids: Dict[float, PriceLevel] = {}\n        self.asks: Dict[float, PriceLevel] = {}\n\n        # Order index for fast lookup\n        self.order_index: Dict[str, Tuple[Side, float]] = {}\n\n        # Trade history for queue advancement\n        self.recent_trades: List[BookUpdate] = []\n        self.max_trade_history = 1000\n\n        # Statistics\n        self.update_count = 0\n        self.last_update_time: Optional[datetime] = None",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "process_update",
    "category": "execution",
    "formula": "",
    "explanation": "Process a single order book update.",
    "python_code": "def process_update(self, update: BookUpdate) -> None:\n        \"\"\"Process a single order book update.\"\"\"\n        self.update_count += 1\n        self.last_update_time = update.timestamp\n\n        if update.update_type == OrderType.ADD:\n            self._add_order(update)\n        elif update.update_type == OrderType.MODIFY:\n            self._modify_order(update)\n        elif update.update_type == OrderType.DELETE:\n            self._delete_order(update)\n        elif update.update_type == OrderType.TRADE:\n            self._process_trade(update)",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "_add_order",
    "category": "execution",
    "formula": "",
    "explanation": "Add new order to book.",
    "python_code": "def _add_order(self, update: BookUpdate) -> None:\n        \"\"\"Add new order to book.\"\"\"\n        book = self.bids if update.side == Side.BUY else self.asks\n        price = self._round_price(update.price)\n\n        if price not in book:\n            book[price] = PriceLevel(price=price)\n\n        order = Order(\n            order_id=update.order_id or f\"order_{self.update_count}\",\n            price=price,\n            quantity=update.quantity,\n            side=update.side,\n            timestamp=update.timestamp\n        )\n\n        book[price].add_order(order)\n        self.order_index[order.order_id] = (update.side, price)",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "_modify_order",
    "category": "execution",
    "formula": "side, price = self.order_index[update.order_id]",
    "explanation": "Modify existing order quantity.",
    "python_code": "def _modify_order(self, update: BookUpdate) -> None:\n        \"\"\"Modify existing order quantity.\"\"\"\n        if update.order_id not in self.order_index:\n            return\n\n        side, price = self.order_index[update.order_id]\n        book = self.bids if side == Side.BUY else self.asks\n\n        if price in book:\n            for order in book[price].orders:\n                if order.order_id == update.order_id:\n                    order.quantity = update.quantity\n                    break",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "_delete_order",
    "category": "execution",
    "formula": "side, price = self.order_index[update.order_id]",
    "explanation": "Remove order from book.",
    "python_code": "def _delete_order(self, update: BookUpdate) -> None:\n        \"\"\"Remove order from book.\"\"\"\n        if update.order_id not in self.order_index:\n            return\n\n        side, price = self.order_index[update.order_id]\n        book = self.bids if side == Side.BUY else self.asks\n\n        if price in book:\n            book[price].remove_order(update.order_id)\n            if book[price].order_count == 0:\n                del book[price]\n\n        del self.order_index[update.order_id]",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "_process_trade",
    "category": "execution",
    "formula": "",
    "explanation": "Process trade (removes liquidity from book).",
    "python_code": "def _process_trade(self, update: BookUpdate) -> None:\n        \"\"\"Process trade (removes liquidity from book).\"\"\"\n        # Trade at ask = buy aggressor, remove from asks\n        # Trade at bid = sell aggressor, remove from bids\n        book = self.asks if update.side == Side.BUY else self.bids\n        price = self._round_price(update.price)\n\n        self.recent_trades.append(update)\n        if len(self.recent_trades) > self.max_trade_history:\n            self.recent_trades.pop(0)\n\n        if price in book:\n            remaining = update.quantity\n            level = book[price]\n\n            # Remove orders FIFO\n            while remaining > 0 and level.orders:\n                front_order = level.orders[0]\n                if front_order.quantity <= remaining:\n                    remaining -= front_order.quantity\n                    level.orders.pop(0)\n                    if front_order.order_id in self.order_index:\n                        del self.order_index[front_order.order_id]\n                else:\n                    front_order.quantity -= remaining\n                    remaining = 0\n\n            if level.order_count == 0:\n                del book[price]",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "_round_price",
    "category": "execution",
    "formula": "round(price / self.tick_size) * self.tick_size",
    "explanation": "Round price to tick size.",
    "python_code": "def _round_price(self, price: float) -> float:\n        \"\"\"Round price to tick size.\"\"\"\n        return round(price / self.tick_size) * self.tick_size",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "get_best_bid",
    "category": "execution",
    "formula": "max(self.bids.keys())",
    "explanation": "Get best (highest) bid price.",
    "python_code": "def get_best_bid(self) -> Optional[float]:\n        \"\"\"Get best (highest) bid price.\"\"\"\n        if not self.bids:\n            return None\n        return max(self.bids.keys())",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "get_best_ask",
    "category": "execution",
    "formula": "min(self.asks.keys())",
    "explanation": "Get best (lowest) ask price.",
    "python_code": "def get_best_ask(self) -> Optional[float]:\n        \"\"\"Get best (lowest) ask price.\"\"\"\n        if not self.asks:\n            return None\n        return min(self.asks.keys())",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "get_mid_price",
    "category": "execution",
    "formula": "(bid + ask) / 2",
    "explanation": "Get mid price.",
    "python_code": "def get_mid_price(self) -> Optional[float]:\n        \"\"\"Get mid price.\"\"\"\n        bid = self.get_best_bid()\n        ask = self.get_best_ask()\n        if bid is None or ask is None:\n            return None\n        return (bid + ask) / 2",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "get_spread",
    "category": "microstructure",
    "formula": "ask - bid",
    "explanation": "Get bid-ask spread.",
    "python_code": "def get_spread(self) -> Optional[float]:\n        \"\"\"Get bid-ask spread.\"\"\"\n        bid = self.get_best_bid()\n        ask = self.get_best_ask()\n        if bid is None or ask is None:\n            return None\n        return ask - bid",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "get_spread_bps",
    "category": "microstructure",
    "formula": "(spread / mid) * 10000",
    "explanation": "Get spread in basis points.",
    "python_code": "def get_spread_bps(self) -> Optional[float]:\n        \"\"\"Get spread in basis points.\"\"\"\n        spread = self.get_spread()\n        mid = self.get_mid_price()\n        if spread is None or mid is None or mid == 0:\n            return None\n        return (spread / mid) * 10000",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "get_imbalance",
    "category": "execution",
    "formula": "Positive = buying pressure, Negative = selling pressure | 0.0 | (bid_qty - ask_qty) / total",
    "explanation": "Calculate order book imbalance.\n\nReturns:\n    Imbalance ratio: (bid_qty - ask_qty) / (bid_qty + ask_qty)\n    Range: -1 (all asks) to +1 (all bids)\n    Positive = buying pressure, Negative = selling pressure",
    "python_code": "def get_imbalance(self, levels: int = 5) -> float:\n        \"\"\"\n        Calculate order book imbalance.\n\n        Returns:\n            Imbalance ratio: (bid_qty - ask_qty) / (bid_qty + ask_qty)\n            Range: -1 (all asks) to +1 (all bids)\n            Positive = buying pressure, Negative = selling pressure\n        \"\"\"\n        bid_qty = self._get_total_quantity(self.bids, levels)\n        ask_qty = self._get_total_quantity(self.asks, levels)\n\n        total = bid_qty + ask_qty\n        if total == 0:\n            return 0.0\n\n        return (bid_qty - ask_qty) / total",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "get_depth_ratio",
    "category": "execution",
    "formula": "float('inf') if bid_qty > 0 else 1.0 | bid_qty / ask_qty",
    "explanation": "Calculate depth ratio.\n\nReturns:\n    Ratio: bid_depth / ask_depth\n    >1 = more buy interest, <1 = more sell interest",
    "python_code": "def get_depth_ratio(self, levels: int = 10) -> float:\n        \"\"\"\n        Calculate depth ratio.\n\n        Returns:\n            Ratio: bid_depth / ask_depth\n            >1 = more buy interest, <1 = more sell interest\n        \"\"\"\n        bid_qty = self._get_total_quantity(self.bids, levels)\n        ask_qty = self._get_total_quantity(self.asks, levels)\n\n        if ask_qty == 0:\n            return float('inf') if bid_qty > 0 else 1.0\n\n        return bid_qty / ask_qty",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "get_weighted_mid_price",
    "category": "execution",
    "formula": "(bid_prices[0] + ask_prices[0]) / 2 | wmid",
    "explanation": "Calculate volume-weighted mid price.\n\nMore accurate than simple mid when book is imbalanced.",
    "python_code": "def get_weighted_mid_price(self, levels: int = 5) -> Optional[float]:\n        \"\"\"\n        Calculate volume-weighted mid price.\n\n        More accurate than simple mid when book is imbalanced.\n        \"\"\"\n        if not self.bids or not self.asks:\n            return None\n\n        bid_prices = sorted(self.bids.keys(), reverse=True)[:levels]\n        ask_prices = sorted(self.asks.keys())[:levels]\n\n        if not bid_prices or not ask_prices:\n            return None\n\n        bid_qty = sum(self.bids[p].total_quantity for p in bid_prices)\n        ask_qty = sum(self.asks[p].total_quantity for p in ask_prices)\n\n        if bid_qty + ask_qty == 0:\n            return (bid_prices[0] + ask_prices[0]) / 2\n\n        # Weight by opposing side (more ask volume = price closer to bid)\n        wmid = (bid_prices[0] * ask_qty + ask_prices[0] * bid_qty) / (bid_qty + ask_qty)\n        return wmid",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "get_microprice",
    "category": "execution",
    "formula": "(bid + ask) / 2 | (bid * ask_qty + ask * bid_qty) / total",
    "explanation": "Calculate microprice (imbalance-adjusted mid).\n\nBetter predictor of next trade direction than mid price.\nSource: Gatheral & Oomen (2010)",
    "python_code": "def get_microprice(self) -> Optional[float]:\n        \"\"\"\n        Calculate microprice (imbalance-adjusted mid).\n\n        Better predictor of next trade direction than mid price.\n        Source: Gatheral & Oomen (2010)\n        \"\"\"\n        bid = self.get_best_bid()\n        ask = self.get_best_ask()\n\n        if bid is None or ask is None:\n            return None\n\n        bid_qty = self.bids[bid].total_quantity if bid in self.bids else 0\n        ask_qty = self.asks[ask].total_quantity if ask in self.asks else 0\n\n        total = bid_qty + ask_qty\n        if total == 0:\n            return (bid + ask) / 2\n\n        # Microprice: weighted by opposite side quantity\n        return (bid * ask_qty + ask * bid_qty) / total",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "_get_total_quantity",
    "category": "execution",
    "formula": "0.0 | sum(book[p].total_quantity for p in prices)",
    "explanation": "Get total quantity across N levels.",
    "python_code": "def _get_total_quantity(self, book: Dict[float, PriceLevel], levels: int) -> float:\n        \"\"\"Get total quantity across N levels.\"\"\"\n        if not book:\n            return 0.0\n\n        if book is self.bids:\n            prices = sorted(book.keys(), reverse=True)[:levels]\n        else:\n            prices = sorted(book.keys())[:levels]\n\n        return sum(book[p].total_quantity for p in prices)",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "estimate_queue_position",
    "category": "execution",
    "formula": "0  # Would be first at this level | book[price].order_count",
    "explanation": "Estimate queue position if order placed at price.\n\nArgs:\n    price: Limit order price\n    side: BUY or SELL\n\nReturns:\n    Estimated queue position (0 = front of queue)",
    "python_code": "def estimate_queue_position(self, price: float, side: Side) -> int:\n        \"\"\"\n        Estimate queue position if order placed at price.\n\n        Args:\n            price: Limit order price\n            side: BUY or SELL\n\n        Returns:\n            Estimated queue position (0 = front of queue)\n        \"\"\"\n        price = self._round_price(price)\n        book = self.bids if side == Side.BUY else self.asks\n\n        if price not in book:\n            return 0  # Would be first at this level\n\n        return book[price].order_count",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "estimate_queue_ahead",
    "category": "execution",
    "formula": "0.0 | book[price].total_quantity",
    "explanation": "Estimate total quantity ahead in queue.\n\nReturns:\n    Total quantity that must trade before our order fills",
    "python_code": "def estimate_queue_ahead(self, price: float, side: Side) -> float:\n        \"\"\"\n        Estimate total quantity ahead in queue.\n\n        Returns:\n            Total quantity that must trade before our order fills\n        \"\"\"\n        price = self._round_price(price)\n        book = self.bids if side == Side.BUY else self.asks\n\n        if price not in book:\n            return 0.0\n\n        return book[price].total_quantity",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "get_own_queue_position",
    "category": "execution",
    "formula": "-1 | -1 | book[price].get_queue_position(order_id)",
    "explanation": "Get queue position of our own order.",
    "python_code": "def get_own_queue_position(self, order_id: str) -> int:\n        \"\"\"Get queue position of our own order.\"\"\"\n        if order_id not in self.order_index:\n            return -1\n\n        side, price = self.order_index[order_id]\n        book = self.bids if side == Side.BUY else self.asks\n\n        if price not in book:\n            return -1\n\n        return book[price].get_queue_position(order_id)",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "get_snapshot",
    "category": "execution",
    "formula": "{",
    "explanation": "Get order book snapshot.\n\nReturns:\n    Dict with bids, asks, and summary statistics",
    "python_code": "def get_snapshot(self, levels: int = 10) -> Dict:\n        \"\"\"\n        Get order book snapshot.\n\n        Returns:\n            Dict with bids, asks, and summary statistics\n        \"\"\"\n        bid_prices = sorted(self.bids.keys(), reverse=True)[:levels]\n        ask_prices = sorted(self.asks.keys())[:levels]\n\n        return {\n            'timestamp': self.last_update_time,\n            'bids': [\n                {'price': p, 'quantity': self.bids[p].total_quantity, 'orders': self.bids[p].order_count}\n                for p in bid_prices\n            ],\n            'asks': [\n                {'price': p, 'quantity': self.asks[p].total_quantity, 'orders': self.asks[p].order_count}\n                for p in ask_prices\n            ],\n            'best_bid': self.get_best_bid(),\n            'best_ask': self.get_best_ask(),\n            'mid': self.get_mid_price(),\n            'spread': self.get_spread(),\n            'imbalance': self.get_imbalance(levels),\n            'depth_ratio': self.get_depth_ratio(levels),\n            'microprice': self.get_microprice(),\n        }",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "to_dataframe",
    "category": "execution",
    "formula": "pd.DataFrame([data])",
    "explanation": "Convert order book to DataFrame.",
    "python_code": "def to_dataframe(self, levels: int = 10) -> pd.DataFrame:\n        \"\"\"Convert order book to DataFrame.\"\"\"\n        snapshot = self.get_snapshot(levels)\n\n        # Create bid/ask columns\n        data = {'level': list(range(levels))}\n\n        for i in range(levels):\n            if i < len(snapshot['bids']):\n                data[f'bid_price_{i}'] = snapshot['bids'][i]['price']\n                data[f'bid_qty_{i}'] = snapshot['bids'][i]['quantity']\n            if i < len(snapshot['asks']):\n                data[f'ask_price_{i}'] = snapshot['asks'][i]['price']\n                data[f'ask_qty_{i}'] = snapshot['asks'][i]['quantity']\n\n        return pd.DataFrame([data])",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "from_ticks",
    "category": "execution",
    "formula": "book",
    "explanation": "Reconstruct order book from tick data.\n\nExpects DataFrame with columns:\n- timestamp: datetime\n- type: 'trade' or 'quote'\n- side: 'buy' or 'sell' (for trades)\n- price: float\n- bid: float (for quotes)\n- ask: float (for quotes)\n- bid_size: float (for quotes)\n- ask_size: float (for quotes)\n- size: float (for trades)",
    "python_code": "def from_ticks(cls, ticks: pd.DataFrame, tick_size: float = 0.0001) -> 'OrderBookL3':\n        \"\"\"\n        Reconstruct order book from tick data.\n\n        Expects DataFrame with columns:\n        - timestamp: datetime\n        - type: 'trade' or 'quote'\n        - side: 'buy' or 'sell' (for trades)\n        - price: float\n        - bid: float (for quotes)\n        - ask: float (for quotes)\n        - bid_size: float (for quotes)\n        - ask_size: float (for quotes)\n        - size: float (for trades)\n        \"\"\"\n        book = cls(tick_size=tick_size)\n\n        for _, row in ticks.iterrows():\n            if row.get('type') == 'quote' or 'bid' in row:\n                # Process quote update\n                if pd.notna(row.get('bid')):\n                    book._update_level2_bid(row['timestamp'], row['bid'], row.get('bid_size', 1.0))\n                if pd.notna(row.get('ask')):\n                    book._update_level2_ask(row['timestamp'], row['ask'], row.get('ask_size', 1.0))\n\n            elif row.get('type') == 'trade' or 'size' in row:\n                # Process trade\n                side = Side.BUY if row.get('side', 'buy').lower() == 'buy' else Side.SELL\n                update = BookUpdate(\n                    timestamp=row['timestamp'],\n                    update_type=OrderType.TRADE,\n                    side=side,\n                    price=row['price'],\n                    quantity=row.get('size', row.get('volume', 1.0))\n                )\n                book.process_update(update)\n\n        return book",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "_update_level2_bid",
    "category": "execution",
    "formula": "",
    "explanation": "Update Level 2 bid (aggregate at price level).",
    "python_code": "def _update_level2_bid(self, timestamp: datetime, price: float, quantity: float) -> None:\n        \"\"\"Update Level 2 bid (aggregate at price level).\"\"\"\n        price = self._round_price(price)\n\n        # Clear old best bid if price changed\n        old_best = self.get_best_bid()\n        if old_best and old_best != price:\n            if old_best in self.bids:\n                for order in self.bids[old_best].orders:\n                    if order.order_id in self.order_index:\n                        del self.order_index[order.order_id]\n                del self.bids[old_best]\n\n        # Update new level\n        if quantity > 0:\n            if price not in self.bids:\n                self.bids[price] = PriceLevel(price=price)\n\n            # Replace with single aggregate order\n            self.bids[price].orders = [Order(\n                order_id=f\"l2_bid_{price}\",\n                price=price,\n                quantity=quantity,\n                side=Side.BUY,\n                timestamp=timestamp\n            )]\n        elif price in self.bids:\n            del self.bids[price]",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "_update_level2_ask",
    "category": "execution",
    "formula": "",
    "explanation": "Update Level 2 ask (aggregate at price level).",
    "python_code": "def _update_level2_ask(self, timestamp: datetime, price: float, quantity: float) -> None:\n        \"\"\"Update Level 2 ask (aggregate at price level).\"\"\"\n        price = self._round_price(price)\n\n        # Clear old best ask if price changed\n        old_best = self.get_best_ask()\n        if old_best and old_best != price:\n            if old_best in self.asks:\n                for order in self.asks[old_best].orders:\n                    if order.order_id in self.order_index:\n                        del self.order_index[order.order_id]\n                del self.asks[old_best]\n\n        # Update new level\n        if quantity > 0:\n            if price not in self.asks:\n                self.asks[price] = PriceLevel(price=price)\n\n            # Replace with single aggregate order\n            self.asks[price].orders = [Order(\n                order_id=f\"l2_ask_{price}\",\n                price=price,\n                quantity=quantity,\n                side=Side.SELL,\n                timestamp=timestamp\n            )]\n        elif price in self.asks:\n            del self.asks[price]",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookL3"
  },
  {
    "name": "__init__",
    "category": "execution",
    "formula": "",
    "explanation": "Initialize signal generator.\n\nArgs:\n    lookback: Number of snapshots for rolling calculations",
    "python_code": "def __init__(self, lookback: int = 100):\n        \"\"\"\n        Initialize signal generator.\n\n        Args:\n            lookback: Number of snapshots for rolling calculations\n        \"\"\"\n        self.lookback = lookback\n        self.snapshots: List[Dict] = []\n        self.trade_history: List[BookUpdate] = []",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookSignals"
  },
  {
    "name": "update",
    "category": "execution",
    "formula": "",
    "explanation": "Update with new book state and generate signals.\n\nReturns:\n    Dict of signal values",
    "python_code": "def update(self, book: OrderBookL3) -> Dict[str, float]:\n        \"\"\"\n        Update with new book state and generate signals.\n\n        Returns:\n            Dict of signal values\n        \"\"\"\n        snapshot = book.get_snapshot()\n        self.snapshots.append(snapshot)\n\n        if len(self.snapshots) > self.lookback:\n            self.snapshots.pop(0)\n\n        # Store recent trades\n        for trade in book.recent_trades[-10:]:\n            if trade not in self.trade_history:\n                self.trade_history.append(trade)\n\n        if len(self.trade_history) > self.lookback * 10:\n            self.trade_history = self.trade_history[-self.lookback * 5:]\n\n        return self.generate_signals()",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookSignals"
  },
  {
    "name": "generate_signals",
    "category": "execution",
    "formula": "{} | signals",
    "explanation": "Generate all order book signals.",
    "python_code": "def generate_signals(self) -> Dict[str, float]:\n        \"\"\"Generate all order book signals.\"\"\"\n        if len(self.snapshots) < 2:\n            return {}\n\n        current = self.snapshots[-1]\n\n        signals = {\n            # Current state signals\n            'obi': current.get('imbalance', 0),\n            'depth_ratio': current.get('depth_ratio', 1),\n            'spread_bps': (current.get('spread', 0) / current.get('mid', 1)) * 10000 if current.get('mid') else 0,\n\n            # Microprice vs mid\n            'microprice_bias': self._microprice_bias(current),\n\n            # Momentum signals\n            'obi_momentum': self._obi_momentum(),\n            'microprice_momentum': self._microprice_momentum(),\n\n            # Trade flow\n            'trade_flow_imbalance': self._trade_flow_imbalance(),\n\n            # Spread regime\n            'spread_percentile': self._spread_percentile(),\n\n            # Combined signal\n            'book_signal': self._combined_signal(),\n        }\n\n        return signals",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookSignals"
  },
  {
    "name": "_microprice_bias",
    "category": "execution",
    "formula": "0.0 | (microprice - mid) / mid * 10000",
    "explanation": "Calculate microprice bias from mid.",
    "python_code": "def _microprice_bias(self, snapshot: Dict) -> float:\n        \"\"\"Calculate microprice bias from mid.\"\"\"\n        mid = snapshot.get('mid')\n        microprice = snapshot.get('microprice')\n\n        if mid is None or microprice is None or mid == 0:\n            return 0.0\n\n        return (microprice - mid) / mid * 10000",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookSignals"
  },
  {
    "name": "_obi_momentum",
    "category": "execution",
    "formula": "0.0 | np.mean(recent) - np.mean(older)",
    "explanation": "Calculate OBI momentum (change in imbalance).",
    "python_code": "def _obi_momentum(self) -> float:\n        \"\"\"Calculate OBI momentum (change in imbalance).\"\"\"\n        if len(self.snapshots) < 5:\n            return 0.0\n\n        recent = [s.get('imbalance', 0) for s in self.snapshots[-5:]]\n        older = [s.get('imbalance', 0) for s in self.snapshots[-10:-5]] if len(self.snapshots) >= 10 else recent\n\n        return np.mean(recent) - np.mean(older)",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookSignals"
  },
  {
    "name": "_microprice_momentum",
    "category": "execution",
    "formula": "0.0 | 0.0 | (microprices[-1] / microprices[0] - 1) * 10000",
    "explanation": "Calculate microprice momentum.",
    "python_code": "def _microprice_momentum(self) -> float:\n        \"\"\"Calculate microprice momentum.\"\"\"\n        if len(self.snapshots) < 5:\n            return 0.0\n\n        microprices = [s.get('microprice', s.get('mid', 0)) for s in self.snapshots[-10:]]\n        microprices = [m for m in microprices if m is not None and m > 0]\n\n        if len(microprices) < 2:\n            return 0.0\n\n        # Return change in basis points\n        return (microprices[-1] / microprices[0] - 1) * 10000",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookSignals"
  },
  {
    "name": "_trade_flow_imbalance",
    "category": "execution",
    "formula": "0.0 | 0.0 | (buy_volume - sell_volume) / total",
    "explanation": "Calculate trade flow imbalance.",
    "python_code": "def _trade_flow_imbalance(self) -> float:\n        \"\"\"Calculate trade flow imbalance.\"\"\"\n        if len(self.trade_history) < 10:\n            return 0.0\n\n        recent = self.trade_history[-20:]\n\n        buy_volume = sum(t.quantity for t in recent if t.side == Side.BUY)\n        sell_volume = sum(t.quantity for t in recent if t.side == Side.SELL)\n\n        total = buy_volume + sell_volume\n        if total == 0:\n            return 0.0\n\n        return (buy_volume - sell_volume) / total",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookSignals"
  },
  {
    "name": "_spread_percentile",
    "category": "microstructure",
    "formula": "50.0 | 50.0 | percentile",
    "explanation": "Calculate current spread percentile vs history.",
    "python_code": "def _spread_percentile(self) -> float:\n        \"\"\"Calculate current spread percentile vs history.\"\"\"\n        if len(self.snapshots) < 10:\n            return 50.0\n\n        spreads = [s.get('spread', 0) for s in self.snapshots]\n        current = spreads[-1]\n\n        if current is None or current == 0:\n            return 50.0\n\n        percentile = sum(1 for s in spreads[:-1] if s < current) / len(spreads[:-1]) * 100\n        return percentile",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookSignals"
  },
  {
    "name": "_combined_signal",
    "category": "execution",
    "formula": "0.0 | max(-1, min(1, signal))",
    "explanation": "Generate combined book signal.\n\nRange: -1 (strong sell) to +1 (strong buy)",
    "python_code": "def _combined_signal(self) -> float:\n        \"\"\"\n        Generate combined book signal.\n\n        Range: -1 (strong sell) to +1 (strong buy)\n        \"\"\"\n        if len(self.snapshots) < 5:\n            return 0.0\n\n        current = self.snapshots[-1]\n\n        # Weights for different components\n        obi = current.get('imbalance', 0) * 0.3\n        microprice_bias = self._microprice_bias(current) / 10 * 0.2  # Normalize\n        obi_mom = self._obi_momentum() * 0.2\n        trade_flow = self._trade_flow_imbalance() * 0.3\n\n        signal = obi + microprice_bias + obi_mom + trade_flow\n\n        # Clip to [-1, 1]\n        return max(-1, min(1, signal))",
    "source_file": "core\\execution\\order_book.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "OrderBookSignals"
  },
  {
    "name": "estimate_position",
    "category": "execution",
    "formula": "",
    "explanation": "Estimate current queue position.",
    "python_code": "def estimate_position(self, order: QueueOrder, book_state: Dict) -> float:\n        \"\"\"Estimate current queue position.\"\"\"\n        pass",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": null,
    "class_name": "QueueModel"
  },
  {
    "name": "advance_queue",
    "category": "execution",
    "formula": "",
    "explanation": "Advance queue position based on trade.",
    "python_code": "def advance_queue(self, order: QueueOrder, trade_volume: float, trade_price: float) -> QueueAdvancement:\n        \"\"\"Advance queue position based on trade.\"\"\"\n        pass",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": null,
    "class_name": "QueueModel"
  },
  {
    "name": "fill_probability",
    "category": "execution",
    "formula": "",
    "explanation": "Estimate probability of fill.",
    "python_code": "def fill_probability(self, order: QueueOrder, book_state: Dict) -> float:\n        \"\"\"Estimate probability of fill.\"\"\"\n        pass",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": null,
    "class_name": "QueueModel"
  },
  {
    "name": "__init__",
    "category": "execution",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self):\n        self.name = \"RiskAverse\"",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": null,
    "class_name": "RiskAverseQueueModel"
  },
  {
    "name": "estimate_position",
    "category": "execution",
    "formula": "Position = initial_position - trades_at_level | max(0, order.current_queue_position)",
    "explanation": "Estimate queue position.\n\nPosition = initial_position - trades_at_level",
    "python_code": "def estimate_position(self, order: QueueOrder, book_state: Dict) -> float:\n        \"\"\"\n        Estimate queue position.\n\n        Position = initial_position - trades_at_level\n        \"\"\"\n        return max(0, order.current_queue_position)",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": null,
    "class_name": "RiskAverseQueueModel"
  },
  {
    "name": "advance_queue",
    "category": "execution",
    "formula": "QueueAdvancement(",
    "explanation": "Advance queue only if trade at same price level.\n\nArgs:\n    order: Order to advance\n    trade_volume: Volume of trade\n    trade_price: Price of trade\n\nReturns:\n    QueueAdvancement with new position",
    "python_code": "def advance_queue(self, order: QueueOrder, trade_volume: float, trade_price: float) -> QueueAdvancement:\n        \"\"\"\n        Advance queue only if trade at same price level.\n\n        Args:\n            order: Order to advance\n            trade_volume: Volume of trade\n            trade_price: Price of trade\n\n        Returns:\n            QueueAdvancement with new position\n        \"\"\"\n        previous = order.current_queue_position\n\n        # Only advance if trade at our price level\n        if abs(trade_price - order.price) < 1e-8:\n            advancement = min(trade_volume, order.current_queue_position)\n            order.current_queue_position = max(0, order.current_queue_position - advancement)\n        else:\n            advancement = 0\n\n        # Estimate fill probability\n        if order.current_queue_position <= 0:\n            fill_prob = 1.0\n        else:\n            fill_prob = order.quantity / (order.current_queue_position + order.quantity)\n\n        return QueueAdvancement(\n            order_id=order.order_id,\n            previous_position=previous,\n            new_position=order.current_queue_position,\n            advancement=advancement,\n            fill_probability=fill_prob\n        )",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": null,
    "class_name": "RiskAverseQueueModel"
  },
  {
    "name": "fill_probability",
    "category": "execution",
    "formula": "1.0 | order.quantity / (order.current_queue_position + order.quantity)",
    "explanation": "Estimate fill probability based on queue position.",
    "python_code": "def fill_probability(self, order: QueueOrder, book_state: Dict) -> float:\n        \"\"\"Estimate fill probability based on queue position.\"\"\"\n        if order.current_queue_position <= 0:\n            return 1.0\n\n        # Simple model: P(fill) = order_size / (position + order_size)\n        return order.quantity / (order.current_queue_position + order.quantity)",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": null,
    "class_name": "RiskAverseQueueModel"
  },
  {
    "name": "estimate_position",
    "category": "execution",
    "formula": "max(0, raw_position - expected_cancels)",
    "explanation": "Probabilistic position estimate.\n\nAccounts for expected cancellations ahead.",
    "python_code": "def estimate_position(self, order: QueueOrder, book_state: Dict) -> float:\n        \"\"\"\n        Probabilistic position estimate.\n\n        Accounts for expected cancellations ahead.\n        \"\"\"\n        raw_position = order.current_queue_position\n\n        # Estimate cancellations (exponential decay)\n        time_in_queue = (datetime.now() - order.timestamp).total_seconds()\n        expected_cancels = raw_position * (1 - np.exp(-self.cancel_rate * time_in_queue / 60))\n\n        return max(0, raw_position - expected_cancels)",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": null,
    "class_name": "ProbabilisticQueueModel"
  },
  {
    "name": "_calculate_fill_probability",
    "category": "execution",
    "formula": "1.0 | order.quantity / (order.current_queue_position + order.quantity) | 0.5",
    "explanation": "Calculate fill probability using Poisson arrival model.\n\nP(fill) = P(trades >= queue_position within time horizon)",
    "python_code": "def _calculate_fill_probability(self, order: QueueOrder) -> float:\n        \"\"\"\n        Calculate fill probability using Poisson arrival model.\n\n        P(fill) = P(trades >= queue_position within time horizon)\n        \"\"\"\n        if order.current_queue_position <= 0:\n            return 1.0\n\n        # Estimate trade arrival rate from history\n        if len(self.trade_arrivals) < 10:\n            # Default to simple model\n            return order.quantity / (order.current_queue_position + order.quantity)\n\n        # Calculate average trade rate\n        time_span = (self.trade_arrivals[-1][0] - self.trade_arrivals[0][0]).total_seconds()\n        if time_span <= 0:\n            return 0.5\n\n        total_volume = sum(t[1] for t in self.trade_arrivals)\n        rate = total_volume / time_span  # Volume per second\n\n        # Poisson probability of filling within 60 seconds\n        expected_trades = rate * 60\n        position = order.current_queue_position\n\n        # P(X >= position) where X ~ Poisson(rate * time)\n        if expected_trades <= 0:\n            return 0.0\n\n        # Approximate using normal for large lambda\n        if expected_trades > 20:\n            z = (position - expected_trades) / np.sqrt(expected_trades)\n            return 1 - 0.5 * (1 + np.tanh(z * 0.7))\n\n        # Exact Poisson for small lambda\n        prob = 0.0\n        factorial = 1\n        for k in range(int(position)):\n            if k > 0:\n                factorial *= k\n            prob += (expected_trades ** k * np.exp(-expected_trades)) / factorial\n\n        return 1 - prob",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": null,
    "class_name": "ProbabilisticQueueModel"
  },
  {
    "name": "calibrate",
    "category": "execution",
    "formula": "# Calculate fill rate",
    "explanation": "Calibrate model from historical trades.\n\nExpects DataFrame with: timestamp, price, volume",
    "python_code": "def calibrate(self, trade_history: pd.DataFrame) -> None:\n        \"\"\"\n        Calibrate model from historical trades.\n\n        Expects DataFrame with: timestamp, price, volume\n        \"\"\"\n        if len(trade_history) < 10:\n            return\n\n        # Calculate fill rate\n        time_span = (trade_history['timestamp'].max() - trade_history['timestamp'].min()).total_seconds()\n        if time_span > 0:\n            self.fill_rate = len(trade_history) / time_span * 60",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": null,
    "class_name": "ProbabilisticQueueModel"
  },
  {
    "name": "update_book",
    "category": "execution",
    "formula": "",
    "explanation": "Update internal queue state from L3 feed.\n\nArgs:\n    price: Price level\n    order_id: Order identifier\n    quantity: Order quantity\n    action: 'add', 'modify', 'delete', 'fill'",
    "python_code": "def update_book(self, price: float, order_id: str, quantity: float, action: str) -> None:\n        \"\"\"\n        Update internal queue state from L3 feed.\n\n        Args:\n            price: Price level\n            order_id: Order identifier\n            quantity: Order quantity\n            action: 'add', 'modify', 'delete', 'fill'\n        \"\"\"\n        if price not in self.queue_state:\n            self.queue_state[price] = []\n\n        if action == 'add':\n            self.queue_state[price].append(order_id)\n            self.order_quantities[order_id] = quantity\n\n        elif action == 'modify':\n            if order_id in self.order_quantities:\n                self.order_quantities[order_id] = quantity\n\n        elif action in ('delete', 'fill'):\n            if order_id in self.queue_state[price]:\n                self.queue_state[price].remove(order_id)\n            if order_id in self.order_quantities:\n                del self.order_quantities[order_id]",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": null,
    "class_name": "L3FIFOQueueModel"
  },
  {
    "name": "estimate_position",
    "category": "execution",
    "formula": "0.0 | order.current_queue_position | quantity_ahead",
    "explanation": "Get exact queue position from L3 state.",
    "python_code": "def estimate_position(self, order: QueueOrder, book_state: Dict) -> float:\n        \"\"\"\n        Get exact queue position from L3 state.\n        \"\"\"\n        price = order.price\n\n        if price not in self.queue_state:\n            return 0.0\n\n        queue = self.queue_state[price]\n\n        # Find our position\n        try:\n            idx = queue.index(order.order_id)\n        except ValueError:\n            return order.current_queue_position\n\n        # Sum quantity ahead of us\n        quantity_ahead = 0.0\n        for i in range(idx):\n            oid = queue[i]\n            quantity_ahead += self.order_quantities.get(oid, 0)\n\n        return quantity_ahead",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": null,
    "class_name": "L3FIFOQueueModel"
  },
  {
    "name": "fill_probability",
    "category": "execution",
    "formula": "1.0 if position <= 0 else 0.0",
    "explanation": "Binary fill probability based on position.",
    "python_code": "def fill_probability(self, order: QueueOrder, book_state: Dict) -> float:\n        \"\"\"Binary fill probability based on position.\"\"\"\n        position = self.estimate_position(order, book_state)\n        return 1.0 if position <= 0 else 0.0",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": null,
    "class_name": "L3FIFOQueueModel"
  },
  {
    "name": "add_order",
    "category": "execution",
    "formula": "order",
    "explanation": "Add new order to track.\n\nArgs:\n    order_id: Unique order identifier\n    price: Limit order price\n    quantity: Order quantity\n    side: BUY or SELL\n    queue_ahead: Quantity ahead in queue when placed\n\nReturns:\n    QueueOrder object",
    "python_code": "def add_order(self, order_id: str, price: float, quantity: float,\n                  side: Side, queue_ahead: float) -> QueueOrder:\n        \"\"\"\n        Add new order to track.\n\n        Args:\n            order_id: Unique order identifier\n            price: Limit order price\n            quantity: Order quantity\n            side: BUY or SELL\n            queue_ahead: Quantity ahead in queue when placed\n\n        Returns:\n            QueueOrder object\n        \"\"\"\n        order = QueueOrder(\n            order_id=order_id,\n            price=price,\n            quantity=quantity,\n            side=side,\n            timestamp=datetime.now(),\n            initial_queue_position=queue_ahead,\n            current_queue_position=queue_ahead\n        )\n\n        self.orders[order_id] = order\n\n        # Update L3 model if used\n        if isinstance(self.model, L3FIFOQueueModel):\n            self.model.update_book(price, order_id, quantity, 'add')\n\n        return order",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": null,
    "class_name": "QueuePositionTracker"
  },
  {
    "name": "remove_order",
    "category": "execution",
    "formula": "order",
    "explanation": "Remove order from tracking.",
    "python_code": "def remove_order(self, order_id: str) -> Optional[QueueOrder]:\n        \"\"\"Remove order from tracking.\"\"\"\n        if order_id not in self.orders:\n            return None\n\n        order = self.orders.pop(order_id)\n\n        if isinstance(self.model, L3FIFOQueueModel):\n            self.model.update_book(order.price, order_id, 0, 'delete')\n\n        return order",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": null,
    "class_name": "QueuePositionTracker"
  },
  {
    "name": "on_trade",
    "category": "execution",
    "formula": "BUY = took from asks, SELL = took from bids) | advancements",
    "explanation": "Process trade event, advance queues.\n\nArgs:\n    price: Trade price\n    volume: Trade volume\n    side: Aggressor side (BUY = took from asks, SELL = took from bids)\n\nReturns:\n    List of queue advancements for affected orders",
    "python_code": "def on_trade(self, price: float, volume: float, side: Side) -> List[QueueAdvancement]:\n        \"\"\"\n        Process trade event, advance queues.\n\n        Args:\n            price: Trade price\n            volume: Trade volume\n            side: Aggressor side (BUY = took from asks, SELL = took from bids)\n\n        Returns:\n            List of queue advancements for affected orders\n        \"\"\"\n        advancements = []\n\n        # Determine which orders are affected\n        # Buy aggressor hits asks, sell aggressor hits bids\n        affected_side = Side.SELL if side == Side.BUY else Side.BUY\n\n        for order in self.orders.values():\n            if order.side == affected_side and abs(order.price - price) < 1e-8:\n                advancement = self.model.advance_queue(order, volume, price)\n                advancements.append(advancement)\n\n        return advancements",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": null,
    "class_name": "QueuePositionTracker"
  },
  {
    "name": "get_position",
    "category": "execution",
    "formula": "-1",
    "explanation": "Get current queue position for order.",
    "python_code": "def get_position(self, order_id: str) -> float:\n        \"\"\"Get current queue position for order.\"\"\"\n        if order_id not in self.orders:\n            return -1\n\n        return self.model.estimate_position(self.orders[order_id], self.book_state)",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": null,
    "class_name": "QueuePositionTracker"
  },
  {
    "name": "get_fill_probability",
    "category": "execution",
    "formula": "0.0",
    "explanation": "Get fill probability for order.",
    "python_code": "def get_fill_probability(self, order_id: str) -> float:\n        \"\"\"Get fill probability for order.\"\"\"\n        if order_id not in self.orders:\n            return 0.0\n\n        return self.model.fill_probability(self.orders[order_id], self.book_state)",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": null,
    "class_name": "QueuePositionTracker"
  },
  {
    "name": "get_all_probabilities",
    "category": "execution",
    "formula": "{",
    "explanation": "Get fill probabilities for all tracked orders.",
    "python_code": "def get_all_probabilities(self) -> Dict[str, float]:\n        \"\"\"Get fill probabilities for all tracked orders.\"\"\"\n        return {\n            oid: self.model.fill_probability(order, self.book_state)\n            for oid, order in self.orders.items()\n        }",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": null,
    "class_name": "QueuePositionTracker"
  },
  {
    "name": "update_book_state",
    "category": "execution",
    "formula": "",
    "explanation": "Update book state for probability calculations.",
    "python_code": "def update_book_state(self, book_state: Dict) -> None:\n        \"\"\"Update book state for probability calculations.\"\"\"\n        self.book_state = book_state",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": null,
    "class_name": "QueuePositionTracker"
  },
  {
    "name": "get_summary",
    "category": "execution",
    "formula": "pd.DataFrame(data)",
    "explanation": "Get summary DataFrame of all tracked orders.",
    "python_code": "def get_summary(self) -> pd.DataFrame:\n        \"\"\"Get summary DataFrame of all tracked orders.\"\"\"\n        data = []\n        for oid, order in self.orders.items():\n            data.append({\n                'order_id': oid,\n                'side': order.side.name,\n                'price': order.price,\n                'quantity': order.quantity,\n                'initial_position': order.initial_queue_position,\n                'current_position': order.current_queue_position,\n                'position_improvement': order.initial_queue_position - order.current_queue_position,\n                'fill_probability': self.get_fill_probability(oid),\n                'age_seconds': (datetime.now() - order.timestamp).total_seconds()\n            })\n\n        return pd.DataFrame(data)",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": null,
    "class_name": "QueuePositionTracker"
  },
  {
    "name": "record_fill",
    "category": "execution",
    "formula": "",
    "explanation": "Record a fill with subsequent price moves.\n\nUsed to calibrate adverse selection model.",
    "python_code": "def record_fill(self, fill_price: float, fill_side: Side,\n                    price_after_1s: float, price_after_5s: float) -> None:\n        \"\"\"\n        Record a fill with subsequent price moves.\n\n        Used to calibrate adverse selection model.\n        \"\"\"\n        if fill_side == Side.BUY:\n            # We bought, adverse move is price going down\n            adverse_1s = fill_price > price_after_1s\n            adverse_5s = fill_price > price_after_5s\n            move_1s = (price_after_1s - fill_price) / fill_price * 10000\n            move_5s = (price_after_5s - fill_price) / fill_price * 10000\n        else:\n            # We sold, adverse move is price going up\n            adverse_1s = fill_price < price_after_1s\n            adverse_5s = fill_price < price_after_5s\n            move_1s = (fill_price - price_after_1s) / fill_price * 10000\n            move_5s = (fill_price - price_after_5s) / fill_price * 10000\n\n        self.fills.append({\n            'side': fill_side,\n            'price': fill_price,\n            'adverse_1s': adverse_1s,\n            'adverse_5s': adverse_5s,\n            'move_1s_bps': move_1s,\n            'move_5s_bps': move_5s\n        })\n\n        if len(self.fills) > self.max_history:\n            self.fills.pop(0)",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "AdverseSelectionModel"
  },
  {
    "name": "adverse_selection_probability",
    "category": "execution",
    "formula": "adverse_count / len(self.fills)",
    "explanation": "Calculate probability of adverse selection.\n\nReturns probability that a fill will be followed by adverse move.",
    "python_code": "def adverse_selection_probability(self) -> float:\n        \"\"\"\n        Calculate probability of adverse selection.\n\n        Returns probability that a fill will be followed by adverse move.\n        \"\"\"\n        if len(self.fills) < 10:\n            return self.informed_fraction\n\n        adverse_count = sum(1 for f in self.fills if f['adverse_5s'])\n        return adverse_count / len(self.fills)",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "AdverseSelectionModel"
  },
  {
    "name": "expected_adverse_move_bps",
    "category": "execution",
    "formula": "1.0  # Default 1 bps | 0.0 | np.mean(moves)",
    "explanation": "Calculate expected adverse move in basis points.",
    "python_code": "def expected_adverse_move_bps(self) -> float:\n        \"\"\"Calculate expected adverse move in basis points.\"\"\"\n        if len(self.fills) < 10:\n            return 1.0  # Default 1 bps\n\n        moves = [f['move_5s_bps'] for f in self.fills if f['adverse_5s']]\n        if not moves:\n            return 0.0\n\n        return np.mean(moves)",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "AdverseSelectionModel"
  },
  {
    "name": "toxicity_score",
    "category": "microstructure",
    "formula": "Higher = more informed trading, worse fills expected. | 0.5 | toxicity",
    "explanation": "Calculate VPIN-style toxicity score.\n\nHigher = more informed trading, worse fills expected.\nRange: 0 (no toxicity) to 1 (all informed)",
    "python_code": "def toxicity_score(self) -> float:\n        \"\"\"\n        Calculate VPIN-style toxicity score.\n\n        Higher = more informed trading, worse fills expected.\n        Range: 0 (no toxicity) to 1 (all informed)\n        \"\"\"\n        if len(self.fills) < 20:\n            return 0.5\n\n        # Rolling window toxicity\n        recent = self.fills[-20:]\n\n        # Measure: How often do we get adversely selected?\n        adverse_rate = sum(1 for f in recent if f['adverse_1s']) / len(recent)\n\n        # Measure: How large are the adverse moves?\n        adverse_moves = [f['move_1s_bps'] for f in recent if f['adverse_1s']]\n        avg_move = np.mean(adverse_moves) if adverse_moves else 0\n\n        # Combine into toxicity score\n        toxicity = 0.5 * adverse_rate + 0.5 * min(1, avg_move / 5)\n\n        return toxicity",
    "source_file": "core\\execution\\queue_position.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "AdverseSelectionModel"
  },
  {
    "name": "tau",
    "category": "execution",
    "formula": "",
    "explanation": "Length of each trading period.",
    "python_code": "def tau(self) -> float:\n        \"\"\"Length of each trading period.\"\"\"\n        return self.T / self.N",
    "source_file": "core\\execution\\optimization\\almgren_chriss.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "ACParameters"
  },
  {
    "name": "kappa",
    "category": "execution",
    "formula": "kappa = sqrt(lambda * sigma^2 / eta) | 1.0 | np.sqrt(self.lambda_ * self.sigma ** 2 / self.eta)",
    "explanation": "Optimal trading rate parameter.\nkappa = sqrt(lambda * sigma^2 / eta)",
    "python_code": "def kappa(self) -> float:\n        \"\"\"\n        Optimal trading rate parameter.\n        kappa = sqrt(lambda * sigma^2 / eta)\n        \"\"\"\n        if self.eta <= 0:\n            return 1.0\n        return np.sqrt(self.lambda_ * self.sigma ** 2 / self.eta)",
    "source_file": "core\\execution\\optimization\\almgren_chriss.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "ACParameters"
  },
  {
    "name": "utility",
    "category": "execution",
    "formula": "",
    "explanation": "Expected cost + risk penalty.",
    "python_code": "def utility(self) -> float:\n        \"\"\"Expected cost + risk penalty.\"\"\"\n        return self.expected_cost + self.params.lambda_ * self.variance_cost",
    "source_file": "core\\execution\\optimization\\almgren_chriss.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "ACTrajectory"
  },
  {
    "name": "__init__",
    "category": "execution",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self,\n                 config: Optional[ExecutionConfig] = None,\n                 impact_model: Optional[FXMarketImpactModel] = None):\n        self.config = config or ExecutionConfig()\n        self.impact_model = impact_model or FXMarketImpactModel(self.config)",
    "source_file": "core\\execution\\optimization\\almgren_chriss.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "AlmgrenChrissOptimizer"
  },
  {
    "name": "calculate_trajectory",
    "category": "volatility",
    "formula": "trajectory",
    "explanation": "Calculate optimal execution trajectory.\n\nArgs:\n    symbol: Currency pair\n    total_quantity: Total position to liquidate\n    horizon_seconds: Time horizon in seconds\n    num_periods: Number of trading periods\n    mid_price: Current mid price\n    volatility: Price volatility (estimated if None)\n    risk_aversion: Risk aversion parameter (from config if None)\n    session: FX session (auto-detected if None)\n\nReturns:\n    ACTrajectory with optimal trading schedule",
    "python_code": "def calculate_trajectory(self,\n                            symbol: str,\n                            total_quantity: float,\n                            horizon_seconds: float,\n                            num_periods: int,\n                            mid_price: float,\n                            volatility: Optional[float] = None,\n                            risk_aversion: Optional[float] = None,\n                            session: Optional[FXSession] = None) -> ACTrajectory:\n        \"\"\"\n        Calculate optimal execution trajectory.\n\n        Args:\n            symbol: Currency pair\n            total_quantity: Total position to liquidate\n            horizon_seconds: Time horizon in seconds\n            num_periods: Number of trading periods\n            mid_price: Current mid price\n            volatility: Price volatility (estimated if None)\n            risk_aversion: Risk aversion parameter (from config if None)\n            session: FX session (auto-detected if None)\n\n        Returns:\n            ACTrajectory with optimal trading schedule\n        \"\"\"\n        if session is None:\n            session = get_current_session()\n\n        session_cfg = get_session_config(session)\n        symbol_cfg = get_symbol_config(symbol)\n\n        # Estimate volatility if not provided (bps per second)\n        if volatility is None:\n            # Default: 10 bps/day = ~0.0006 bps/second\n            volatility = 10.0 / 86400 * session_cfg.volatility_multiplier\n\n        # Get risk aversion\n        if risk_aversion is None:\n            risk_aversion = self.config.risk_aversion\n\n        # Estimate impact coefficients from market model\n        eta = self.config.temp_impact_coef * session_cfg.spread_multiplier\n        gamma = self.config.perm_impact_coef * session_cfg.spread_multiplier\n\n        # Build parameters\n        params = ACParameters(\n            X=total_quantity,\n            T=horizon_seconds,\n            N=num_periods,\n            sigma=volatility,\n            eta=eta,\n            gamma=gamma,\n            lambda_=risk_aversion\n        )\n\n        # Calculate optimal trajectory\n        trajectory = self._solve_trajectory(params)\n\n        return trajectory",
    "source_file": "core\\execution\\optimization\\almgren_chriss.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "AlmgrenChrissOptimizer"
  },
  {
    "name": "_solve_trajectory",
    "category": "execution",
    "formula": "x_k = X * sinh(kappa * (T - t_k)) / sinh(kappa * T) | x_k = X * (1 - t_k/T) (TWAP) | ACTrajectory(",
    "explanation": "Solve for optimal trajectory using closed-form solution.\n\nThe optimal holdings trajectory is:\nx_k = X * sinh(kappa * (T - t_k)) / sinh(kappa * T)\n\nFor kappa -> 0 (risk-neutral): x_k = X * (1 - t_k/T) (TWAP)\nFor kappa -> inf (infinitely risk-averse): immediate execution",
    "python_code": "def _solve_trajectory(self, params: ACParameters) -> ACTrajectory:\n        \"\"\"\n        Solve for optimal trajectory using closed-form solution.\n\n        The optimal holdings trajectory is:\n        x_k = X * sinh(kappa * (T - t_k)) / sinh(kappa * T)\n\n        For kappa -> 0 (risk-neutral): x_k = X * (1 - t_k/T) (TWAP)\n        For kappa -> inf (infinitely risk-averse): immediate execution\n        \"\"\"\n        tau = params.tau\n        kappa = params.kappa\n\n        # Time points\n        time_points = np.array([k * tau for k in range(params.N + 1)])\n\n        # Calculate holdings at each time point\n        if kappa * params.T < 0.01:\n            # Risk-neutral case (TWAP limit)\n            holdings = params.X * (1 - time_points / params.T)\n        elif kappa * params.T > 100:\n            # Extremely risk-averse (immediate execution)\n            holdings = np.zeros(params.N + 1)\n            holdings[0] = params.X\n        else:\n            # General case\n            sinh_kT = np.sinh(kappa * params.T)\n            holdings = params.X * np.sinh(kappa * (params.T - time_points)) / sinh_kT\n\n        # Ensure boundary conditions\n        holdings[0] = params.X\n        holdings[-1] = 0\n\n        # Calculate trade sizes (difference in holdings)\n        trade_sizes = -np.diff(holdings)  # Positive = selling\n\n        # Trade rates (shares per second in each period)\n        trade_rates = trade_sizes / tau\n\n        # Calculate expected costs\n        expected_perm = self._calculate_permanent_cost(holdings, trade_sizes, params)\n        expected_temp = self._calculate_temporary_cost(trade_rates, params)\n        variance = self._calculate_variance(holdings, params)\n\n        return ACTrajectory(\n            time_points=time_points,\n            holdings=holdings,\n            trade_sizes=trade_sizes,\n            trade_rates=trade_rates,\n            expected_cost=expected_perm + expected_temp,\n            expected_permanent_cost=expected_perm,\n            expected_temporary_cost=expected_temp,\n            variance_cost=variance,\n            params=params\n        )",
    "source_file": "core\\execution\\optimization\\almgren_chriss.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "AlmgrenChrissOptimizer"
  },
  {
    "name": "_calculate_permanent_cost",
    "category": "microstructure",
    "formula": "params.gamma * cost",
    "explanation": "Calculate expected permanent impact cost.\n\nE[Permanent Cost] = gamma * sum(x_k * n_k)",
    "python_code": "def _calculate_permanent_cost(self,\n                                 holdings: np.ndarray,\n                                 trade_sizes: np.ndarray,\n                                 params: ACParameters) -> float:\n        \"\"\"\n        Calculate expected permanent impact cost.\n\n        E[Permanent Cost] = gamma * sum(x_k * n_k)\n        \"\"\"\n        # Permanent cost = gamma * sum over trades of (holdings * trade_size)\n        cost = 0.0\n        for k in range(len(trade_sizes)):\n            cost += holdings[k] * trade_sizes[k]\n\n        return params.gamma * cost",
    "source_file": "core\\execution\\optimization\\almgren_chriss.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "AlmgrenChrissOptimizer"
  },
  {
    "name": "_calculate_temporary_cost",
    "category": "microstructure",
    "formula": "v_k = n_k / tau is the trade rate | cost",
    "explanation": "Calculate expected temporary impact cost.\n\nE[Temporary Cost] = eta * tau * sum(v_k^2)\nwhere v_k = n_k / tau is the trade rate",
    "python_code": "def _calculate_temporary_cost(self,\n                                 trade_rates: np.ndarray,\n                                 params: ACParameters) -> float:\n        \"\"\"\n        Calculate expected temporary impact cost.\n\n        E[Temporary Cost] = eta * tau * sum(v_k^2)\n        where v_k = n_k / tau is the trade rate\n        \"\"\"\n        # Temporary cost = eta * tau * sum(rate^2)\n        cost = params.eta * params.tau * np.sum(trade_rates ** 2)\n        return cost",
    "source_file": "core\\execution\\optimization\\almgren_chriss.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "AlmgrenChrissOptimizer"
  },
  {
    "name": "_calculate_variance",
    "category": "execution",
    "formula": "variance",
    "explanation": "Calculate variance of execution cost.\n\nVar[Cost] = sigma^2 * tau * sum(x_k^2)",
    "python_code": "def _calculate_variance(self,\n                           holdings: np.ndarray,\n                           params: ACParameters) -> float:\n        \"\"\"\n        Calculate variance of execution cost.\n\n        Var[Cost] = sigma^2 * tau * sum(x_k^2)\n        \"\"\"\n        # Variance from price movements while holding\n        # Skip the last point (holdings = 0)\n        variance = params.sigma ** 2 * params.tau * np.sum(holdings[:-1] ** 2)\n        return variance",
    "source_file": "core\\execution\\optimization\\almgren_chriss.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "AlmgrenChrissOptimizer"
  },
  {
    "name": "create_schedule",
    "category": "volatility",
    "formula": "schedule",
    "explanation": "Create an executable schedule from AC trajectory.\n\nArgs:\n    order_id: Unique order identifier\n    symbol: Currency pair\n    direction: 1 = buy, -1 = sell\n    total_quantity: Total to execute\n    mid_price: Current mid price\n    volatility: Price volatility\n    horizon_seconds: Execution window\n    session: FX session\n\nReturns:\n    ExecutionSchedule with slices",
    "python_code": "def create_schedule(self,\n                       order_id: str,\n                       symbol: str,\n                       direction: int,\n                       total_quantity: float,\n                       mid_price: float,\n                       volatility: Optional[float] = None,\n                       horizon_seconds: Optional[int] = None,\n                       session: Optional[FXSession] = None) -> ExecutionSchedule:\n        \"\"\"\n        Create an executable schedule from AC trajectory.\n\n        Args:\n            order_id: Unique order identifier\n            symbol: Currency pair\n            direction: 1 = buy, -1 = sell\n            total_quantity: Total to execute\n            mid_price: Current mid price\n            volatility: Price volatility\n            horizon_seconds: Execution window\n            session: FX session\n\n        Returns:\n            ExecutionSchedule with slices\n        \"\"\"\n        if horizon_seconds is None:\n            horizon_seconds = self.config.default_horizon_seconds\n\n        if session is None:\n            session = get_current_session()\n\n        # Calculate number of periods\n        slice_interval = self.config.slice_interval_seconds\n        num_periods = max(2, horizon_seconds // slice_interval)\n\n        # Calculate optimal trajectory\n        trajectory = self.calculate_trajectory(\n            symbol=symbol,\n            total_quantity=total_quantity,\n            horizon_seconds=horizon_seconds,\n            num_periods=num_periods,\n            mid_price=mid_price,\n            volatility=volatility,\n            session=session\n        )\n\n        # Convert to execution slices\n        now = datetime.now(timezone.utc)\n        slices = []\n\n        for i, (time_offset, qty) in enumerate(zip(trajectory.time_points[:-1],\n                                                    trajectory.trade_sizes)):\n            target_time = now + timedelta(seconds=float(time_offset))\n\n            # Determine strategy for slice based on size and urgency\n            if i == 0 and trajectory.params.kappa * trajectory.params.T > 5:\n                # High urgency, use market for first slice\n                strategy = ExecutionStrategy.MARKET\n            else:\n                strategy = ExecutionStrategy.LIMIT\n\n            slice_obj = ExecutionSlice(\n                slice_id=i,\n                target_time=target_time,\n                target_quantity=float(qty),\n                strategy=strategy\n            )\n            slices.append(slice_obj)\n\n        # Create schedule\n        schedule = ExecutionSchedule(\n            order_id=order_id,\n            symbol=symbol,\n            direction=direction,\n            total_quantity=total_quantity,\n            slices=slices,\n            strategy=ExecutionStrategy.ALMGREN_CHRISS,\n            horizon_seconds=horizon_seconds,\n            expected_cost_bps=trajectory.expected_cost * 10000  # Convert to bps\n        )\n\n        return schedule",
    "source_file": "core\\execution\\optimization\\almgren_chriss.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "AlmgrenChrissOptimizer"
  },
  {
    "name": "update_and_reoptimize",
    "category": "volatility",
    "formula": "current_trajectory",
    "explanation": "Re-optimize trajectory based on current state.\n\nArgs:\n    current_trajectory: Original trajectory\n    time_elapsed: Seconds since start\n    quantity_remaining: Remaining position\n    current_volatility: Current volatility estimate\n    current_session: Current FX session\n\nReturns:\n    New optimized trajectory",
    "python_code": "def update_and_reoptimize(self,\n                             current_trajectory: ACTrajectory,\n                             time_elapsed: float,\n                             quantity_remaining: float,\n                             current_volatility: float,\n                             current_session: FXSession) -> ACTrajectory:\n        \"\"\"\n        Re-optimize trajectory based on current state.\n\n        Args:\n            current_trajectory: Original trajectory\n            time_elapsed: Seconds since start\n            quantity_remaining: Remaining position\n            current_volatility: Current volatility estimate\n            current_session: Current FX session\n\n        Returns:\n            New optimized trajectory\n        \"\"\"\n        params = current_trajectory.params\n\n        # Time remaining\n        time_remaining = params.T - time_elapsed\n        if time_remaining <= params.tau:\n            # Not enough time to re-optimize\n            return current_trajectory\n\n        # Periods remaining\n        periods_remaining = max(2, int(time_remaining / params.tau))\n\n        # New parameters with updated volatility\n        session_cfg = get_session_config(current_session)\n\n        new_params = ACParameters(\n            X=quantity_remaining,\n            T=time_remaining,\n            N=periods_remaining,\n            sigma=current_volatility,\n            eta=params.eta * session_cfg.spread_multiplier,\n            gamma=params.gamma * session_cfg.spread_multiplier,\n            lambda_=params.lambda_\n        )\n\n        # Solve new trajectory\n        return self.optimizer._solve_trajectory(new_params)",
    "source_file": "core\\execution\\optimization\\almgren_chriss.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "AdaptiveAlmgrenChriss"
  },
  {
    "name": "record_fill",
    "category": "execution",
    "formula": "",
    "explanation": "Record a fill for future learning.",
    "python_code": "def record_fill(self,\n                   target_qty: float,\n                   filled_qty: float,\n                   target_price: float,\n                   fill_price: float,\n                   slippage_bps: float):\n        \"\"\"Record a fill for future learning.\"\"\"\n        self.execution_history.append({\n            'target_qty': target_qty,\n            'filled_qty': filled_qty,\n            'target_price': target_price,\n            'fill_price': fill_price,\n            'slippage_bps': slippage_bps,\n            'timestamp': datetime.now(timezone.utc)\n        })\n\n        # Keep limited history\n        if len(self.execution_history) > 1000:\n            self.execution_history = self.execution_history[-500:]",
    "source_file": "core\\execution\\optimization\\almgren_chriss.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "AdaptiveAlmgrenChriss"
  },
  {
    "name": "get_ac_optimizer",
    "category": "execution",
    "formula": "AlmgrenChrissOptimizer(config)",
    "explanation": "Factory function to get AC optimizer.",
    "python_code": "def get_ac_optimizer(config: Optional[ExecutionConfig] = None) -> AlmgrenChrissOptimizer:\n    \"\"\"Factory function to get AC optimizer.\"\"\"\n    return AlmgrenChrissOptimizer(config)",
    "source_file": "core\\execution\\optimization\\almgren_chriss.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": null
  },
  {
    "name": "is_active",
    "category": "execution",
    "formula": "hour >= self.start_hour or hour < self.end_hour",
    "explanation": "Check if session is active at given time.",
    "python_code": "def is_active(self, dt: Optional[datetime] = None) -> bool:\n        \"\"\"Check if session is active at given time.\"\"\"\n        if dt is None:\n            dt = datetime.now(timezone.utc)\n\n        hour = dt.hour\n\n        # Handle sessions crossing midnight\n        if self.start_hour < self.end_hour:\n            return self.start_hour <= hour < self.end_hour\n        else:\n            return hour >= self.start_hour or hour < self.end_hour",
    "source_file": "core\\execution\\optimization\\config.py",
    "academic_reference": null,
    "class_name": "FXSessionConfig"
  },
  {
    "name": "get_current_session",
    "category": "execution",
    "formula": "FXSession.OVERLAP_LN | FXSession.OVERLAP_TL | FXSession.LONDON",
    "explanation": "Get the current FX trading session.\n\nPriority: OVERLAP_LN > OVERLAP_TL > LONDON > NEW_YORK > TOKYO > OFF_HOURS",
    "python_code": "def get_current_session(dt: Optional[datetime] = None) -> FXSession:\n    \"\"\"\n    Get the current FX trading session.\n\n    Priority: OVERLAP_LN > OVERLAP_TL > LONDON > NEW_YORK > TOKYO > OFF_HOURS\n    \"\"\"\n    if dt is None:\n        dt = datetime.now(timezone.utc)\n\n    hour = dt.hour\n\n    # Check overlaps first (highest priority)\n    if 12 <= hour < 16:\n        return FXSession.OVERLAP_LN\n    if 7 <= hour < 9:\n        return FXSession.OVERLAP_TL\n\n    # Check major sessions\n    if 7 <= hour < 16:\n        return FXSession.LONDON\n    if 12 <= hour < 21:\n        return FXSession.NEW_YORK\n    if 0 <= hour < 9:\n        return FXSession.TOKYO\n\n    # Off hours\n    return FXSession.OFF_HOURS",
    "source_file": "core\\execution\\optimization\\config.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "get_session_config",
    "category": "execution",
    "formula": "DEFAULT_SESSIONS.get(session, DEFAULT_SESSIONS[FXSession.OFF_HOURS])",
    "explanation": "Get configuration for a session.",
    "python_code": "def get_session_config(session: Optional[FXSession] = None,\n                       dt: Optional[datetime] = None) -> FXSessionConfig:\n    \"\"\"Get configuration for a session.\"\"\"\n    if session is None:\n        session = get_current_session(dt)\n    return DEFAULT_SESSIONS.get(session, DEFAULT_SESSIONS[FXSession.OFF_HOURS])",
    "source_file": "core\\execution\\optimization\\config.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "get_symbol_config",
    "category": "execution",
    "formula": "DEFAULT_SYMBOL_CONFIGS[symbol] | SymbolExecutionConfig(",
    "explanation": "Get execution configuration for a symbol.",
    "python_code": "def get_symbol_config(symbol: str) -> SymbolExecutionConfig:\n    \"\"\"Get execution configuration for a symbol.\"\"\"\n    if symbol in DEFAULT_SYMBOL_CONFIGS:\n        return DEFAULT_SYMBOL_CONFIGS[symbol]\n\n    # Default config for unknown symbols\n    is_jpy = 'JPY' in symbol\n    return SymbolExecutionConfig(\n        symbol=symbol,\n        pip_value=0.01 if is_jpy else 0.0001,\n        min_lot_size=1000,\n        daily_volume_estimate=0.1e9,\n        avg_spread_bps=2.0,\n        tick_size=0.001 if is_jpy else 0.00001\n    )",
    "source_file": "core\\execution\\optimization\\config.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "num_slices",
    "category": "execution",
    "formula": "len(self.slices)",
    "explanation": "",
    "python_code": "def num_slices(self) -> int:\n        return len(self.slices)",
    "source_file": "core\\execution\\optimization\\config.py",
    "academic_reference": null,
    "class_name": "ExecutionSchedule"
  },
  {
    "name": "executed_quantity",
    "category": "execution",
    "formula": "sum(s.executed_quantity for s in self.slices)",
    "explanation": "",
    "python_code": "def executed_quantity(self) -> float:\n        return sum(s.executed_quantity for s in self.slices)",
    "source_file": "core\\execution\\optimization\\config.py",
    "academic_reference": null,
    "class_name": "ExecutionSchedule"
  },
  {
    "name": "remaining_quantity",
    "category": "execution",
    "formula": "",
    "explanation": "",
    "python_code": "def remaining_quantity(self) -> float:\n        return self.total_quantity - self.executed_quantity",
    "source_file": "core\\execution\\optimization\\config.py",
    "academic_reference": null,
    "class_name": "ExecutionSchedule"
  },
  {
    "name": "is_complete",
    "category": "execution",
    "formula": "all(s.status in ('filled', 'cancelled') for s in self.slices)",
    "explanation": "",
    "python_code": "def is_complete(self) -> bool:\n        return all(s.status in ('filled', 'cancelled') for s in self.slices)",
    "source_file": "core\\execution\\optimization\\config.py",
    "academic_reference": null,
    "class_name": "ExecutionSchedule"
  },
  {
    "name": "vwap",
    "category": "execution",
    "formula": "total_value / total_qty if total_qty > 0 else 0.0",
    "explanation": "Calculate volume-weighted average price of executed slices.",
    "python_code": "def vwap(self) -> float:\n        \"\"\"Calculate volume-weighted average price of executed slices.\"\"\"\n        total_value = sum(s.executed_quantity * s.executed_price\n                        for s in self.slices if s.executed_price > 0)\n        total_qty = sum(s.executed_quantity for s in self.slices if s.executed_price > 0)\n        return total_value / total_qty if total_qty > 0 else 0.0",
    "source_file": "core\\execution\\optimization\\config.py",
    "academic_reference": null,
    "class_name": "ExecutionSchedule"
  },
  {
    "name": "__init__",
    "category": "execution",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, config: Optional[ExecutionConfig] = None):\n        self.config = config or ExecutionConfig()\n\n        # Initialize strategy components\n        self.impact_model = get_market_impact_model(self.config)\n        self.ac_optimizer = get_ac_optimizer(self.config)\n        self.twap_scheduler = get_twap_scheduler(self.config)\n        self.vwap_scheduler = get_vwap_scheduler(self.config)\n        self.rl_optimizer = get_rl_executor() if self.config.use_rl_adaptation else None\n\n        # Strategy thresholds\n        self.small_order_threshold = 100_000    # Below this -> MARKET/LIMIT\n        self.medium_order_threshold = 1_000_000  # Below this -> TWAP\n        self.large_order_threshold = 10_000_000",
    "source_file": "core\\execution\\optimization\\engine.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "ExecutionEngine"
  },
  {
    "name": "optimize",
    "category": "volatility",
    "formula": "higher = faster) | ExecutionDecision(",
    "explanation": "Determine optimal execution strategy.\n\nArgs:\n    symbol: Currency pair\n    direction: 1 = buy, -1 = sell\n    quantity: Order size\n    mid_price: Current mid price\n    spread_bps: Current spread in basis points\n    volatility: Current volatility\n    urgency: Urgency level 0-1 (higher = faster)\n    signal_confidence: ML signal confidence 0-1\n    session: FX session (auto-detected if None)\n\nReturns:\n    ExecutionDecision with strategy and schedule",
    "python_code": "def optimize(self,\n                symbol: str,\n                direction: int,\n                quantity: float,\n                mid_price: float,\n                spread_bps: float,\n                volatility: float = 0.0001,\n                urgency: float = 0.5,\n                signal_confidence: float = 0.5,\n                session: Optional[FXSession] = None) -> ExecutionDecision:\n        \"\"\"\n        Determine optimal execution strategy.\n\n        Args:\n            symbol: Currency pair\n            direction: 1 = buy, -1 = sell\n            quantity: Order size\n            mid_price: Current mid price\n            spread_bps: Current spread in basis points\n            volatility: Current volatility\n            urgency: Urgency level 0-1 (higher = faster)\n            signal_confidence: ML signal confidence 0-1\n            session: FX session (auto-detected if None)\n\n        Returns:\n            ExecutionDecision with strategy and schedule\n        \"\"\"\n        if session is None:\n            session = get_current_session()\n\n        session_cfg = get_session_config(session)\n\n        # Estimate costs for each strategy\n        costs = self._estimate_strategy_costs(\n            symbol=symbol,\n            quantity=quantity,\n            mid_price=mid_price,\n            spread_bps=spread_bps,\n            volatility=volatility,\n            session=session\n        )\n\n        # Select strategy based on order characteristics\n        strategy, reasoning = self._select_strategy(\n            quantity=quantity,\n            urgency=urgency,\n            signal_confidence=signal_confidence,\n            spread_bps=spread_bps,\n            session=session,\n            costs=costs\n        )\n\n        # Generate schedule for selected strategy\n        schedule = self._create_schedule(\n            strategy=strategy,\n            symbol=symbol,\n            direction=direction,\n            quantity=quantity,\n            mid_price=mid_price,\n            volatility=volatility,\n            urgency=urgency,\n            session=session\n        )\n\n        # Apply RL adjustment if enabled\n        if self.rl_optimizer and self.config.use_rl_adaptation:\n            rl_action = self._get_rl_adjustment(\n                symbol=symbol,\n                quantity=quantity,\n                spread_bps=spread_bps,\n                volatility=volatility,\n                session=session,\n                urgency=urgency\n            )\n        else:\n            rl_action = None\n\n        # Build decision\n        expected_cost = costs.get(strategy, spread_bps / 2)\n        use_limit = strategy == ExecutionStrategy.LIMIT or (\n            spread_bps < 1.0 and urgency < 0.5\n        )\n        limit_offset = min(spread_bps * 0.3, self.config.max_limit_offset_bps) if use_limit else 0\n\n        aggressiveness = 0.3 + 0.4 * urgency + 0.3 * signal_confidence\n\n        return ExecutionDecision(\n            strategy=strategy,\n            schedule=schedule,\n            expected_cost_bps=expected_cost,\n            expec",
    "source_file": "core\\execution\\optimization\\engine.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "ExecutionEngine"
  },
  {
    "name": "_estimate_strategy_costs",
    "category": "execution",
    "formula": "costs",
    "explanation": "Estimate execution cost for each strategy.",
    "python_code": "def _estimate_strategy_costs(self,\n                                symbol: str,\n                                quantity: float,\n                                mid_price: float,\n                                spread_bps: float,\n                                volatility: float,\n                                session: FXSession) -> Dict[ExecutionStrategy, float]:\n        \"\"\"Estimate execution cost for each strategy.\"\"\"\n        costs = {}\n\n        # MARKET: immediate, pay full spread + impact\n        impact = self.impact_model.estimate_impact(\n            symbol=symbol,\n            quantity=quantity,\n            direction=1,\n            mid_price=mid_price,\n            spread_bps=spread_bps,\n            session=session\n        )\n        costs[ExecutionStrategy.MARKET] = impact.total_impact_bps\n\n        # LIMIT: pay less spread but risk non-fill\n        # Cost = (1 - fill_prob) * market_cost + fill_prob * (spread/4)\n        fill_prob = 0.7  # Estimated\n        costs[ExecutionStrategy.LIMIT] = (\n            (1 - fill_prob) * impact.total_impact_bps +\n            fill_prob * spread_bps / 4\n        )\n\n        # TWAP: distributed execution\n        num_slices = max(5, int(quantity / 200_000))\n        twap_cost = self.impact_model.estimate_execution_cost(\n            symbol=symbol,\n            quantity=quantity,\n            horizon_seconds=300,\n            num_slices=num_slices,\n            session=session\n        )\n        costs[ExecutionStrategy.TWAP] = twap_cost\n\n        # VWAP: volume-weighted\n        # Similar to TWAP but slightly better in liquid sessions\n        session_cfg = get_session_config(session)\n        vwap_cost = twap_cost * (0.9 + 0.1 / session_cfg.liquidity_multiplier)\n        costs[ExecutionStrategy.VWAP] = vwap_cost\n\n        # AC: optimal trajectory\n        # Best for large orders, accounts for risk\n        ac_cost = twap_cost * 0.85  # AC typically 15% better than naive\n        costs[ExecutionStrategy.ALMGREN_CHRISS] = ac_cost\n\n        return costs",
    "source_file": "core\\execution\\optimization\\engine.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "ExecutionEngine"
  },
  {
    "name": "_select_strategy",
    "category": "execution",
    "formula": "ExecutionStrategy.MARKET, \"High urgency requires immediate execution\" | ExecutionStrategy.LIMIT, \"Small order with tight spread, use limit\" | ExecutionStrategy.MARKET, \"Small order, direct market execution\"",
    "explanation": "Select optimal execution strategy.",
    "python_code": "def _select_strategy(self,\n                        quantity: float,\n                        urgency: float,\n                        signal_confidence: float,\n                        spread_bps: float,\n                        session: FXSession,\n                        costs: Dict[ExecutionStrategy, float]) -> Tuple[ExecutionStrategy, str]:\n        \"\"\"Select optimal execution strategy.\"\"\"\n\n        # High urgency -> MARKET\n        if urgency > self.config.urgency_high_threshold:\n            return ExecutionStrategy.MARKET, \"High urgency requires immediate execution\"\n\n        # Small orders -> MARKET or LIMIT\n        if quantity < self.small_order_threshold:\n            if spread_bps < 1.0 and urgency < 0.5:\n                return ExecutionStrategy.LIMIT, \"Small order with tight spread, use limit\"\n            return ExecutionStrategy.MARKET, \"Small order, direct market execution\"\n\n        # Very low urgency + tight spread -> LIMIT\n        if urgency < self.config.urgency_low_threshold and spread_bps < 1.2:\n            return ExecutionStrategy.LIMIT, \"Low urgency with tight spread, passive execution\"\n\n        # Large institutional orders -> Almgren-Chriss\n        if quantity > self.large_order_threshold:\n            return ExecutionStrategy.ALMGREN_CHRISS, \"Large order, optimal trajectory minimizes impact\"\n\n        # Medium orders -> TWAP or VWAP based on session\n        session_cfg = get_session_config(session)\n\n        if session_cfg.liquidity_multiplier > 1.2:\n            # High liquidity session -> VWAP (track volume)\n            return ExecutionStrategy.VWAP, f\"Medium order in liquid session ({session.value}), use VWAP\"\n        else:\n            # Lower liquidity -> TWAP (spread evenly)\n            return ExecutionStrategy.TWAP, f\"Medium order in {session.value} session, use TWAP\"",
    "source_file": "core\\execution\\optimization\\engine.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "ExecutionEngine"
  },
  {
    "name": "_create_schedule",
    "category": "execution",
    "formula": "ExecutionSchedule( | ExecutionSchedule(",
    "explanation": "Create execution schedule for selected strategy.",
    "python_code": "def _create_schedule(self,\n                        strategy: ExecutionStrategy,\n                        symbol: str,\n                        direction: int,\n                        quantity: float,\n                        mid_price: float,\n                        volatility: float,\n                        urgency: float,\n                        session: FXSession) -> Optional[ExecutionSchedule]:\n        \"\"\"Create execution schedule for selected strategy.\"\"\"\n        order_id = str(uuid.uuid4())\n\n        # Calculate horizon based on urgency\n        base_horizon = self.config.default_horizon_seconds\n        horizon = int(base_horizon * (1 - 0.7 * urgency))\n        horizon = max(60, min(horizon, self.config.max_horizon_seconds))\n\n        if strategy == ExecutionStrategy.MARKET:\n            # Single immediate slice\n            return ExecutionSchedule(\n                order_id=order_id,\n                symbol=symbol,\n                direction=direction,\n                total_quantity=quantity,\n                slices=[ExecutionSlice(\n                    slice_id=0,\n                    target_time=datetime.now(timezone.utc),\n                    target_quantity=quantity,\n                    strategy=ExecutionStrategy.MARKET,\n                    status=\"pending\"\n                )],\n                strategy=strategy,\n                horizon_seconds=0,\n                expected_cost_bps=0\n            )\n\n        elif strategy == ExecutionStrategy.LIMIT:\n            # Single limit slice\n            return ExecutionSchedule(\n                order_id=order_id,\n                symbol=symbol,\n                direction=direction,\n                total_quantity=quantity,\n                slices=[ExecutionSlice(\n                    slice_id=0,\n                    target_time=datetime.now(timezone.utc),\n                    target_quantity=quantity,\n                    strategy=ExecutionStrategy.LIMIT,\n                    status=\"pending\"\n                )],\n                strategy=strategy,\n                horizon_seconds=self.config.limit_order_timeout_seconds,\n                expected_cost_bps=0\n            )\n\n        elif strategy == ExecutionStrategy.TWAP:\n            return self.twap_scheduler.create_schedule(\n                order_id=order_id,\n                symbol=symbol,\n                direction=direction,\n                total_quantity=quantity,\n                horizon_seconds=horizon\n            )\n\n        elif strategy == ExecutionStrategy.VWAP:\n            return self.vwap_scheduler.create_schedule(\n                order_id=order_id,\n                symbol=symbol,\n                direction=direction,\n                total_quantity=quantity,\n                horizon_seconds=horizon\n            )\n\n        elif strategy == ExecutionStrategy.ALMGREN_CHRISS:\n            return self.ac_optimizer.create_schedule(\n                order_id=order_id,\n                symbol=symbol,\n                direction=direction,\n                total_quantity=quantity,\n       ",
    "source_file": "core\\execution\\optimization\\engine.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "ExecutionEngine"
  },
  {
    "name": "_get_rl_adjustment",
    "category": "execution",
    "formula": "",
    "explanation": "Get RL-based adjustment to execution parameters.",
    "python_code": "def _get_rl_adjustment(self,\n                          symbol: str,\n                          quantity: float,\n                          spread_bps: float,\n                          volatility: float,\n                          session: FXSession,\n                          urgency: float) -> Optional[ExecutionAction]:\n        \"\"\"Get RL-based adjustment to execution parameters.\"\"\"\n        if not self.rl_optimizer:\n            return None\n\n        from .rl_executor import ExecutionState\n\n        session_cfg = get_session_config(session)\n\n        state = ExecutionState(\n            remaining_qty_pct=1.0,\n            time_remaining_pct=1.0,\n            spread_bps=spread_bps,\n            volatility=volatility,\n            session_liquidity=session_cfg.liquidity_multiplier,\n            order_flow=0.0,\n            fill_rate=0.8,\n            slippage_so_far_bps=0.0\n        )\n\n        return self.rl_optimizer.agent.select_action(state, explore=False)",
    "source_file": "core\\execution\\optimization\\engine.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "ExecutionEngine"
  },
  {
    "name": "get_execution_status",
    "category": "execution",
    "formula": "",
    "explanation": "Get status of an execution.",
    "python_code": "def get_execution_status(self, exec_id: str) -> Optional[dict]:\n        \"\"\"Get status of an execution.\"\"\"\n        return self.active_executions.get(exec_id)",
    "source_file": "core\\execution\\optimization\\engine.py",
    "academic_reference": null,
    "class_name": "ExecutionInterceptor"
  },
  {
    "name": "cancel_execution",
    "category": "execution",
    "formula": "False | True",
    "explanation": "Cancel an active execution.",
    "python_code": "def cancel_execution(self, exec_id: str) -> bool:\n        \"\"\"Cancel an active execution.\"\"\"\n        if exec_id not in self.active_executions:\n            return False\n\n        exec_state = self.active_executions[exec_id]\n        exec_state['status'] = ExecutionStatus.CANCELLED\n        self._stop_event.set()\n\n        return True",
    "source_file": "core\\execution\\optimization\\engine.py",
    "academic_reference": null,
    "class_name": "ExecutionInterceptor"
  },
  {
    "name": "get_active_executions",
    "category": "execution",
    "formula": "[",
    "explanation": "Get list of active execution IDs.",
    "python_code": "def get_active_executions(self) -> List[str]:\n        \"\"\"Get list of active execution IDs.\"\"\"\n        return [\n            eid for eid, state in self.active_executions.items()\n            if state['status'] == ExecutionStatus.ACTIVE\n        ]",
    "source_file": "core\\execution\\optimization\\engine.py",
    "academic_reference": null,
    "class_name": "ExecutionInterceptor"
  },
  {
    "name": "get_execution_engine",
    "category": "execution",
    "formula": "ExecutionEngine(config)",
    "explanation": "Factory function to get execution engine.",
    "python_code": "def get_execution_engine(config: Optional[ExecutionConfig] = None) -> ExecutionEngine:\n    \"\"\"Factory function to get execution engine.\"\"\"\n    return ExecutionEngine(config)",
    "source_file": "core\\execution\\optimization\\engine.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "get_execution_interceptor",
    "category": "execution",
    "formula": "ExecutionInterceptor(executor, config)",
    "explanation": "Factory function to get execution interceptor.",
    "python_code": "def get_execution_interceptor(executor: Any,\n                             config: Optional[ExecutionConfig] = None) -> ExecutionInterceptor:\n    \"\"\"Factory function to get execution interceptor.\"\"\"\n    return ExecutionInterceptor(executor, config)",
    "source_file": "core\\execution\\optimization\\engine.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "microstructure",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, config: Optional[ExecutionConfig] = None):\n        self.config = config or ExecutionConfig()\n\n        # Calibration factors (from empirical studies)\n        self.base_temp_impact = 1.0    # bps per sqrt(participation)\n        self.base_perm_impact = 0.3    # bps per participation rate\n        self.dealer_count_factor = 0.8 # More dealers = lower impact\n\n        # Session impact adjustments\n        self.session_multipliers = {\n            FXSession.TOKYO: 1.3,      # Lower liquidity, higher impact\n            FXSession.LONDON: 0.85,    # High liquidity, lower impact\n            FXSession.NEW_YORK: 0.9,\n            FXSession.OVERLAP_LN: 0.7, # Best liquidity\n            FXSession.OVERLAP_TL: 1.0,\n            FXSession.OFF_HOURS: 1.8   # Wide spreads, high impact\n        }",
    "source_file": "core\\execution\\optimization\\market_impact_fx.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "FXMarketImpactModel"
  },
  {
    "name": "estimate_impact",
    "category": "volatility",
    "formula": "MarketImpactEstimate(",
    "explanation": "Estimate market impact for an FX order.\n\nArgs:\n    symbol: Currency pair (e.g., 'EURUSD')\n    quantity: Order size in base currency units\n    direction: 1 for buy, -1 for sell\n    mid_price: Current mid price\n    spread_bps: Current spread in basis points (optional)\n    session: FX session (auto-detected if None)\n    volatility: Current volatility (optional)\n\nReturns:\n    MarketImpactEstimate with breakdown of costs",
    "python_code": "def estimate_impact(self,\n                       symbol: str,\n                       quantity: float,\n                       direction: int,\n                       mid_price: float,\n                       spread_bps: Optional[float] = None,\n                       session: Optional[FXSession] = None,\n                       volatility: Optional[float] = None) -> MarketImpactEstimate:\n        \"\"\"\n        Estimate market impact for an FX order.\n\n        Args:\n            symbol: Currency pair (e.g., 'EURUSD')\n            quantity: Order size in base currency units\n            direction: 1 for buy, -1 for sell\n            mid_price: Current mid price\n            spread_bps: Current spread in basis points (optional)\n            session: FX session (auto-detected if None)\n            volatility: Current volatility (optional)\n\n        Returns:\n            MarketImpactEstimate with breakdown of costs\n        \"\"\"\n        # Get configurations\n        symbol_cfg = get_symbol_config(symbol)\n        if session is None:\n            session = get_current_session()\n        session_cfg = get_session_config(session)\n\n        # Use provided spread or default\n        if spread_bps is None:\n            spread_bps = symbol_cfg.avg_spread_bps * session_cfg.spread_multiplier\n\n        # Calculate participation rate (size / estimated session volume)\n        session_volume = self._estimate_session_volume(symbol_cfg, session_cfg)\n        participation_rate = quantity / session_volume if session_volume > 0 else 0.01\n\n        # 1. Dealer spread component (half spread for crossing)\n        dealer_spread_cost = spread_bps / 2.0\n\n        # 2. Temporary impact (square root model)\n        # Impact increases with sqrt of participation\n        temp_impact = self.base_temp_impact * np.sqrt(participation_rate) * 10000\n        temp_impact *= session_cfg.spread_multiplier  # Adjust for session\n\n        # 3. Permanent impact (linear in participation)\n        # Information leakage to dealers\n        perm_impact = self.base_perm_impact * participation_rate * 10000\n\n        # Volatility adjustment (higher vol = more impact)\n        if volatility is not None:\n            vol_factor = 1.0 + (volatility - 0.0001) * 1000  # Normalize to ~1.0\n            temp_impact *= vol_factor\n            perm_impact *= vol_factor\n\n        # Session multiplier\n        session_mult = self.session_multipliers.get(session, 1.0)\n\n        # Total impact\n        total_impact = (dealer_spread_cost + temp_impact + perm_impact) * session_mult\n\n        # Confidence based on data quality\n        confidence = 0.8 if spread_bps == symbol_cfg.avg_spread_bps else 0.9\n\n        return MarketImpactEstimate(\n            total_impact_bps=total_impact,\n            dealer_spread_bps=dealer_spread_cost * session_mult,\n            size_impact_bps=temp_impact * session_mult,\n            info_leakage_bps=perm_impact * session_mult,\n            session_multiplier=session_mult,\n            confidence=confidence\n        )",
    "source_file": "core\\execution\\optimization\\market_impact_fx.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "FXMarketImpactModel"
  },
  {
    "name": "_estimate_session_volume",
    "category": "microstructure",
    "formula": "session_volume",
    "explanation": "Estimate volume available during current session.",
    "python_code": "def _estimate_session_volume(self,\n                                symbol_cfg: SymbolExecutionConfig,\n                                session_cfg: FXSessionConfig) -> float:\n        \"\"\"Estimate volume available during current session.\"\"\"\n        daily_volume = symbol_cfg.daily_volume_estimate\n\n        # Distribute across session based on volume weight\n        session_hours = session_cfg.end_hour - session_cfg.start_hour\n        if session_hours < 0:\n            session_hours += 24\n\n        # Session's share of daily volume\n        session_volume = daily_volume * session_cfg.volume_weight\n\n        # Adjust for liquidity characteristics\n        session_volume *= session_cfg.liquidity_multiplier\n\n        return session_volume",
    "source_file": "core\\execution\\optimization\\market_impact_fx.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "FXMarketImpactModel"
  },
  {
    "name": "optimal_execution_rate",
    "category": "microstructure",
    "formula": "optimal_rate",
    "explanation": "Calculate optimal execution rate to minimize impact.\n\nReturns shares per second.",
    "python_code": "def optimal_execution_rate(self,\n                              symbol: str,\n                              total_quantity: float,\n                              horizon_seconds: float,\n                              session: Optional[FXSession] = None) -> float:\n        \"\"\"\n        Calculate optimal execution rate to minimize impact.\n\n        Returns shares per second.\n        \"\"\"\n        if session is None:\n            session = get_current_session()\n\n        session_cfg = get_session_config(session)\n        symbol_cfg = get_symbol_config(symbol)\n\n        # Estimate volume per second during session\n        session_volume = self._estimate_session_volume(symbol_cfg, session_cfg)\n        session_hours = session_cfg.end_hour - session_cfg.start_hour\n        if session_hours <= 0:\n            session_hours += 24\n        volume_per_second = session_volume / (session_hours * 3600)\n\n        # Max participation rate from config\n        max_rate = volume_per_second * self.config.max_participation_rate\n\n        # Target rate to complete within horizon\n        target_rate = total_quantity / horizon_seconds\n\n        # Use minimum of target and max allowed\n        optimal_rate = min(target_rate, max_rate)\n\n        return optimal_rate",
    "source_file": "core\\execution\\optimization\\market_impact_fx.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "FXMarketImpactModel"
  },
  {
    "name": "estimate_execution_cost",
    "category": "microstructure",
    "formula": "total_cost / num_slices",
    "explanation": "Estimate total execution cost for a sliced execution.\n\nReturns expected cost in basis points.",
    "python_code": "def estimate_execution_cost(self,\n                               symbol: str,\n                               quantity: float,\n                               horizon_seconds: float,\n                               num_slices: int,\n                               session: Optional[FXSession] = None) -> float:\n        \"\"\"\n        Estimate total execution cost for a sliced execution.\n\n        Returns expected cost in basis points.\n        \"\"\"\n        if session is None:\n            session = get_current_session()\n\n        slice_qty = quantity / num_slices\n        total_cost = 0.0\n\n        for i in range(num_slices):\n            # Each slice has its own impact\n            impact = self.estimate_impact(\n                symbol=symbol,\n                quantity=slice_qty,\n                direction=1,\n                mid_price=1.0,  # Normalized\n                session=session\n            )\n            total_cost += impact.total_impact_bps\n\n        # Average cost per unit\n        return total_cost / num_slices",
    "source_file": "core\\execution\\optimization\\market_impact_fx.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "FXMarketImpactModel"
  },
  {
    "name": "impact",
    "category": "volatility",
    "formula": "0.0 | impact",
    "explanation": "Calculate temporary impact in price units.\n\nArgs:\n    trade_rate: Our trading rate (units/second)\n    volume_rate: Market volume rate (units/second)\n    volatility: Price volatility\n    direction: 1 for buy, -1 for sell\n\nReturns:\n    Temporary impact in price units",
    "python_code": "def impact(self,\n              trade_rate: float,\n              volume_rate: float,\n              volatility: float,\n              direction: int) -> float:\n        \"\"\"\n        Calculate temporary impact in price units.\n\n        Args:\n            trade_rate: Our trading rate (units/second)\n            volume_rate: Market volume rate (units/second)\n            volatility: Price volatility\n            direction: 1 for buy, -1 for sell\n\n        Returns:\n            Temporary impact in price units\n        \"\"\"\n        if volume_rate <= 0:\n            return 0.0\n\n        participation = trade_rate / volume_rate\n        impact = self.eta * volatility * direction * np.power(participation, self.alpha)\n\n        return impact",
    "source_file": "core\\execution\\optimization\\market_impact_fx.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "TemporaryImpactModel"
  },
  {
    "name": "estimate_spread",
    "category": "volatility",
    "formula": "(max(0.1, bid_spread), max(0.1, ask_spread))",
    "explanation": "Estimate dealer spread for a given order.\n\nArgs:\n    quantity: Order size\n    typical_size: Typical order size in market\n    session: Current FX session\n    volatility: Current price volatility\n    recent_order_flow: Recent net order flow (+ = buying pressure)\n\nReturns:\n    Tuple of (bid_spread_bps, ask_spread_bps)",
    "python_code": "def estimate_spread(self,\n                       quantity: float,\n                       typical_size: float,\n                       session: FXSession,\n                       volatility: float = 0.0001,\n                       recent_order_flow: float = 0.0) -> Tuple[float, float]:\n        \"\"\"\n        Estimate dealer spread for a given order.\n\n        Args:\n            quantity: Order size\n            typical_size: Typical order size in market\n            session: Current FX session\n            volatility: Current price volatility\n            recent_order_flow: Recent net order flow (+ = buying pressure)\n\n        Returns:\n            Tuple of (bid_spread_bps, ask_spread_bps)\n        \"\"\"\n        session_cfg = get_session_config(session)\n\n        # Base spread adjusted for session\n        spread = self.base_spread_bps * session_cfg.spread_multiplier\n\n        # Size adjustment (larger orders get wider spreads)\n        size_ratio = quantity / typical_size if typical_size > 0 else 1.0\n        size_adj = self.size_sensitivity * np.log1p(size_ratio)\n\n        # Volatility adjustment\n        vol_adj = volatility * 10000 * 0.5  # Convert to bps\n\n        # Order flow adjustment (asymmetric spread based on flow)\n        flow_adj = self.info_sensitivity * recent_order_flow\n\n        total_spread = spread + size_adj + vol_adj\n\n        # Distribute spread (potentially asymmetric)\n        bid_spread = total_spread / 2 + flow_adj / 2\n        ask_spread = total_spread / 2 - flow_adj / 2\n\n        return (max(0.1, bid_spread), max(0.1, ask_spread))",
    "source_file": "core\\execution\\optimization\\market_impact_fx.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "DealerSpreadModel"
  },
  {
    "name": "get_market_impact_model",
    "category": "microstructure",
    "formula": "FXMarketImpactModel(config)",
    "explanation": "Factory function to get market impact model.",
    "python_code": "def get_market_impact_model(config: Optional[ExecutionConfig] = None) -> FXMarketImpactModel:\n    \"\"\"Factory function to get market impact model.\"\"\"\n    return FXMarketImpactModel(config)",
    "source_file": "core\\execution\\optimization\\market_impact_fx.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": null
  },
  {
    "name": "to_tensor",
    "category": "execution",
    "formula": "torch.FloatTensor([",
    "explanation": "Convert to PyTorch tensor.",
    "python_code": "def to_tensor(self) -> 'torch.Tensor':\n        \"\"\"Convert to PyTorch tensor.\"\"\"\n        if not HAS_TORCH:\n            raise RuntimeError(\"PyTorch not available\")\n        return torch.FloatTensor([\n            self.remaining_qty_pct,\n            self.time_remaining_pct,\n            self.spread_bps / 5.0,  # Normalize spread\n            self.volatility * 10000,  # Normalize vol\n            self.session_liquidity,\n            self.order_flow,\n            self.fill_rate,\n            self.slippage_so_far_bps / 10.0  # Normalize slippage\n        ])",
    "source_file": "core\\execution\\optimization\\rl_executor.py",
    "academic_reference": null,
    "class_name": "ExecutionState"
  },
  {
    "name": "to_array",
    "category": "execution",
    "formula": "np.array([",
    "explanation": "Convert to numpy array.",
    "python_code": "def to_array(self) -> np.ndarray:\n        \"\"\"Convert to numpy array.\"\"\"\n        return np.array([\n            self.remaining_qty_pct,\n            self.time_remaining_pct,\n            self.spread_bps / 5.0,\n            self.volatility * 10000,\n            self.session_liquidity,\n            self.order_flow,\n            self.fill_rate,\n            self.slippage_so_far_bps / 10.0\n        ])",
    "source_file": "core\\execution\\optimization\\rl_executor.py",
    "academic_reference": null,
    "class_name": "ExecutionState"
  },
  {
    "name": "from_array",
    "category": "execution",
    "formula": "cls(",
    "explanation": "Create from raw action array.",
    "python_code": "def from_array(cls, arr: np.ndarray) -> 'ExecutionAction':\n        \"\"\"Create from raw action array.\"\"\"\n        return cls(\n            trade_rate=np.clip(arr[0] * 0.75 + 1.25, 0.5, 2.0),  # Map to [0.5, 2.0]\n            use_limit=arr[1] > 0.5,\n            limit_offset_bps=np.clip(arr[2] * 3.0, 0.0, 3.0),  # Map to [0, 3]\n            aggressiveness=np.clip(arr[3], 0.0, 1.0)\n        )",
    "source_file": "core\\execution\\optimization\\rl_executor.py",
    "academic_reference": null,
    "class_name": "ExecutionAction"
  },
  {
    "name": "__init__",
    "category": "execution",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, capacity: int = 100000):\n        self.buffer = deque(maxlen=capacity)",
    "source_file": "core\\execution\\optimization\\rl_executor.py",
    "academic_reference": null,
    "class_name": "ReplayBuffer"
  },
  {
    "name": "push",
    "category": "execution",
    "formula": "",
    "explanation": "Add experience to buffer.",
    "python_code": "def push(self, experience: Experience):\n        \"\"\"Add experience to buffer.\"\"\"\n        self.buffer.append(experience)",
    "source_file": "core\\execution\\optimization\\rl_executor.py",
    "academic_reference": null,
    "class_name": "ReplayBuffer"
  },
  {
    "name": "sample",
    "category": "execution",
    "formula": "[self.buffer[i] for i in indices]",
    "explanation": "Sample batch of experiences.",
    "python_code": "def sample(self, batch_size: int) -> List[Experience]:\n        \"\"\"Sample batch of experiences.\"\"\"\n        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n        return [self.buffer[i] for i in indices]",
    "source_file": "core\\execution\\optimization\\rl_executor.py",
    "academic_reference": null,
    "class_name": "ReplayBuffer"
  },
  {
    "name": "select_action",
    "category": "execution",
    "formula": "ExecutionAction.from_array(action)",
    "explanation": "Select action given current state.\n\nArgs:\n    state: Current execution state\n    explore: Whether to add exploration noise\n\nReturns:\n    ExecutionAction with decisions",
    "python_code": "def select_action(self,\n                     state: ExecutionState,\n                     explore: bool = True) -> ExecutionAction:\n        \"\"\"\n        Select action given current state.\n\n        Args:\n            state: Current execution state\n            explore: Whether to add exploration noise\n\n        Returns:\n            ExecutionAction with decisions\n        \"\"\"\n        if self.use_torch:\n            state_tensor = state.to_tensor().unsqueeze(0).to(self.device)\n\n            with torch.no_grad():\n                action = self.actor(state_tensor).cpu().numpy()[0]\n\n            # Add exploration noise\n            if explore:\n                noise = np.random.normal(0, self.exploration_noise, size=action.shape)\n                action = np.clip(action + noise, -1, 1)\n\n            return ExecutionAction.from_array(action)\n        else:\n            # Rule-based fallback\n            return self._rule_based_action(state)",
    "source_file": "core\\execution\\optimization\\rl_executor.py",
    "academic_reference": null,
    "class_name": "DDPGExecutor"
  },
  {
    "name": "_rule_based_action",
    "category": "execution",
    "formula": "ExecutionAction(",
    "explanation": "Rule-based action selection (fallback when no PyTorch).",
    "python_code": "def _rule_based_action(self, state: ExecutionState) -> ExecutionAction:\n        \"\"\"Rule-based action selection (fallback when no PyTorch).\"\"\"\n        # More aggressive when time running out\n        urgency = 1 - state.time_remaining_pct\n\n        # Use limit when spread is tight and we have time\n        use_limit = (state.spread_bps < 1.5 and\n                    state.time_remaining_pct > 0.3 and\n                    state.remaining_qty_pct > 0.2)\n\n        # Trade rate based on urgency and liquidity\n        trade_rate = 1.0 + urgency * 0.5 * state.session_liquidity\n\n        # Limit offset based on spread\n        limit_offset = state.spread_bps * 0.3 if use_limit else 0\n\n        # Aggressiveness from multiple factors\n        aggressiveness = 0.3 + 0.4 * urgency + 0.3 * (1 - state.remaining_qty_pct)\n\n        return ExecutionAction(\n            trade_rate=np.clip(trade_rate, 0.5, 2.0),\n            use_limit=use_limit,\n            limit_offset_bps=limit_offset,\n            aggressiveness=np.clip(aggressiveness, 0, 1)\n        )",
    "source_file": "core\\execution\\optimization\\rl_executor.py",
    "academic_reference": null,
    "class_name": "DDPGExecutor"
  },
  {
    "name": "store_experience",
    "category": "execution",
    "formula": "",
    "explanation": "Store experience in replay buffer.",
    "python_code": "def store_experience(self, experience: Experience):\n        \"\"\"Store experience in replay buffer.\"\"\"\n        self.replay_buffer.push(experience)",
    "source_file": "core\\execution\\optimization\\rl_executor.py",
    "academic_reference": null,
    "class_name": "DDPGExecutor"
  },
  {
    "name": "train",
    "category": "execution",
    "formula": "{'status': 'no_torch'} | {'status': 'insufficient_data'} | {",
    "explanation": "Train the agent on a batch of experiences.\n\nReturns:\n    Dictionary with training metrics",
    "python_code": "def train(self, batch_size: int = 64) -> Dict[str, float]:\n        \"\"\"\n        Train the agent on a batch of experiences.\n\n        Returns:\n            Dictionary with training metrics\n        \"\"\"\n        if not self.use_torch:\n            return {'status': 'no_torch'}\n\n        if len(self.replay_buffer) < batch_size:\n            return {'status': 'insufficient_data'}\n\n        # Sample batch\n        batch = self.replay_buffer.sample(batch_size)\n\n        # Convert to tensors\n        states = torch.stack([e.state.to_tensor() for e in batch]).to(self.device)\n        actions = torch.FloatTensor([e.action for e in batch]).to(self.device)\n        rewards = torch.FloatTensor([e.reward for e in batch]).unsqueeze(1).to(self.device)\n        next_states = torch.stack([e.next_state.to_tensor() for e in batch]).to(self.device)\n        dones = torch.FloatTensor([float(e.done) for e in batch]).unsqueeze(1).to(self.device)\n\n        # Update critic\n        with torch.no_grad():\n            next_actions = self.actor_target(next_states)\n            target_q = rewards + (1 - dones) * self.gamma * self.critic_target(next_states, next_actions)\n\n        current_q = self.critic(states, actions)\n        critic_loss = F.mse_loss(current_q, target_q)\n\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n\n        # Update actor\n        actor_loss = -self.critic(states, self.actor(states)).mean()\n\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n\n        # Soft update target networks\n        self._soft_update(self.actor, self.actor_target)\n        self._soft_update(self.critic, self.critic_target)\n\n        self.train_steps += 1\n\n        return {\n            'critic_loss': critic_loss.item(),\n            'actor_loss': actor_loss.item(),\n            'train_steps': self.train_steps\n        }",
    "source_file": "core\\execution\\optimization\\rl_executor.py",
    "academic_reference": null,
    "class_name": "DDPGExecutor"
  },
  {
    "name": "_soft_update",
    "category": "execution",
    "formula": "",
    "explanation": "Soft update target network.",
    "python_code": "def _soft_update(self, source: 'nn.Module', target: 'nn.Module'):\n        \"\"\"Soft update target network.\"\"\"\n        for target_param, source_param in zip(target.parameters(), source.parameters()):\n            target_param.data.copy_(\n                self.tau * source_param.data + (1 - self.tau) * target_param.data\n            )",
    "source_file": "core\\execution\\optimization\\rl_executor.py",
    "academic_reference": null,
    "class_name": "DDPGExecutor"
  },
  {
    "name": "save",
    "category": "execution",
    "formula": "",
    "explanation": "Save model to file.",
    "python_code": "def save(self, path: str):\n        \"\"\"Save model to file.\"\"\"\n        path = Path(path)\n        path.parent.mkdir(parents=True, exist_ok=True)\n\n        if self.use_torch:\n            torch.save({\n                'actor': self.actor.state_dict(),\n                'actor_target': self.actor_target.state_dict(),\n                'critic': self.critic.state_dict(),\n                'critic_target': self.critic_target.state_dict(),\n                'actor_optimizer': self.actor_optimizer.state_dict(),\n                'critic_optimizer': self.critic_optimizer.state_dict(),\n                'train_steps': self.train_steps,\n                'episodes': self.episodes\n            }, path)\n        else:\n            # Save empty marker\n            with open(path, 'wb') as f:\n                pickle.dump({'use_torch': False}, f)",
    "source_file": "core\\execution\\optimization\\rl_executor.py",
    "academic_reference": null,
    "class_name": "DDPGExecutor"
  },
  {
    "name": "load",
    "category": "execution",
    "formula": "if self.use_torch:",
    "explanation": "Load model from file.",
    "python_code": "def load(self, path: str):\n        \"\"\"Load model from file.\"\"\"\n        path = Path(path)\n        if not path.exists():\n            logger.warning(f\"Model file not found: {path}\")\n            return\n\n        if self.use_torch:\n            checkpoint = torch.load(path, map_location=self.device)\n            self.actor.load_state_dict(checkpoint['actor'])\n            self.actor_target.load_state_dict(checkpoint['actor_target'])\n            self.critic.load_state_dict(checkpoint['critic'])\n            self.critic_target.load_state_dict(checkpoint['critic_target'])\n            self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])\n            self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])\n            self.train_steps = checkpoint.get('train_steps', 0)\n            self.episodes = checkpoint.get('episodes', 0)",
    "source_file": "core\\execution\\optimization\\rl_executor.py",
    "academic_reference": null,
    "class_name": "DDPGExecutor"
  },
  {
    "name": "start_execution",
    "category": "execution",
    "formula": "",
    "explanation": "Start tracking an execution and get initial action.\n\nReturns initial action recommendation.",
    "python_code": "def start_execution(self,\n                       order_id: str,\n                       total_quantity: float,\n                       horizon_seconds: float,\n                       symbol: str) -> ExecutionAction:\n        \"\"\"\n        Start tracking an execution and get initial action.\n\n        Returns initial action recommendation.\n        \"\"\"\n        initial_state = ExecutionState(\n            remaining_qty_pct=1.0,\n            time_remaining_pct=1.0,\n            spread_bps=1.0,  # Will be updated\n            volatility=0.0001,\n            session_liquidity=1.0,\n            order_flow=0.0,\n            fill_rate=0.8,\n            slippage_so_far_bps=0.0\n        )\n\n        self.current_executions[order_id] = {\n            'total_qty': total_quantity,\n            'executed_qty': 0.0,\n            'horizon': horizon_seconds,\n            'elapsed': 0.0,\n            'symbol': symbol,\n            'last_state': initial_state,\n            'cumulative_cost': 0.0,\n            'experiences': []\n        }\n\n        return self.agent.select_action(initial_state, explore=True)",
    "source_file": "core\\execution\\optimization\\rl_executor.py",
    "academic_reference": null,
    "class_name": "RLExecutionOptimizer"
  },
  {
    "name": "update_and_decide",
    "category": "volatility",
    "formula": "ExecutionAction( | ExecutionAction(",
    "explanation": "Update state and get next action.\n\nArgs:\n    order_id: Order identifier\n    executed_qty: Quantity executed so far\n    elapsed_seconds: Time elapsed\n    current_spread_bps: Current spread\n    current_volatility: Current volatility\n    session_liquidity: Session liquidity factor\n    order_flow: Order flow imbalance\n    execution_cost_bps: Execution cost so far\n\nReturns:\n    Next action recommendation",
    "python_code": "def update_and_decide(self,\n                         order_id: str,\n                         executed_qty: float,\n                         elapsed_seconds: float,\n                         current_spread_bps: float,\n                         current_volatility: float,\n                         session_liquidity: float,\n                         order_flow: float,\n                         execution_cost_bps: float) -> ExecutionAction:\n        \"\"\"\n        Update state and get next action.\n\n        Args:\n            order_id: Order identifier\n            executed_qty: Quantity executed so far\n            elapsed_seconds: Time elapsed\n            current_spread_bps: Current spread\n            current_volatility: Current volatility\n            session_liquidity: Session liquidity factor\n            order_flow: Order flow imbalance\n            execution_cost_bps: Execution cost so far\n\n        Returns:\n            Next action recommendation\n        \"\"\"\n        if order_id not in self.current_executions:\n            # Return default action\n            return ExecutionAction(\n                trade_rate=1.0,\n                use_limit=False,\n                limit_offset_bps=0.0,\n                aggressiveness=0.5\n            )\n\n        exec_state = self.current_executions[order_id]\n\n        # Calculate new state\n        remaining_qty = exec_state['total_qty'] - executed_qty\n        remaining_time = exec_state['horizon'] - elapsed_seconds\n\n        new_state = ExecutionState(\n            remaining_qty_pct=remaining_qty / exec_state['total_qty'],\n            time_remaining_pct=max(0, remaining_time / exec_state['horizon']),\n            spread_bps=current_spread_bps,\n            volatility=current_volatility,\n            session_liquidity=session_liquidity,\n            order_flow=order_flow,\n            fill_rate=executed_qty / max(1, exec_state['elapsed']) * 60,  # Fills per minute\n            slippage_so_far_bps=execution_cost_bps\n        )\n\n        # Calculate reward (negative cost is good)\n        reward = -execution_cost_bps\n\n        # Store experience\n        if exec_state['last_state'] is not None:\n            experience = Experience(\n                state=exec_state['last_state'],\n                action=np.zeros(4),  # Will be filled from last action\n                reward=reward,\n                next_state=new_state,\n                done=remaining_qty <= 0 or remaining_time <= 0\n            )\n            self.agent.store_experience(experience)\n\n        # Update tracking\n        exec_state['executed_qty'] = executed_qty\n        exec_state['elapsed'] = elapsed_seconds\n        exec_state['last_state'] = new_state\n        exec_state['cumulative_cost'] = execution_cost_bps\n\n        # Get next action\n        is_done = new_state.remaining_qty_pct <= 0.01 or new_state.time_remaining_pct <= 0.01\n\n        if is_done:\n            # Cleanup\n            del self.current_executions[order_id]\n            return ExecutionAction(\n                trade_rate=2.0,  # Execute",
    "source_file": "core\\execution\\optimization\\rl_executor.py",
    "academic_reference": null,
    "class_name": "RLExecutionOptimizer"
  },
  {
    "name": "train_from_buffer",
    "category": "execution",
    "formula": "",
    "explanation": "Train agent from replay buffer.",
    "python_code": "def train_from_buffer(self, batch_size: int = 64) -> Dict[str, float]:\n        \"\"\"Train agent from replay buffer.\"\"\"\n        return self.agent.train(batch_size)",
    "source_file": "core\\execution\\optimization\\rl_executor.py",
    "academic_reference": null,
    "class_name": "RLExecutionOptimizer"
  },
  {
    "name": "save_model",
    "category": "execution",
    "formula": "",
    "explanation": "Save model.",
    "python_code": "def save_model(self, path: str):\n        \"\"\"Save model.\"\"\"\n        self.agent.save(path)",
    "source_file": "core\\execution\\optimization\\rl_executor.py",
    "academic_reference": null,
    "class_name": "RLExecutionOptimizer"
  },
  {
    "name": "get_rl_executor",
    "category": "execution",
    "formula": "RLExecutionOptimizer(model_path)",
    "explanation": "Factory function to get RL executor.",
    "python_code": "def get_rl_executor(model_path: Optional[str] = None) -> RLExecutionOptimizer:\n    \"\"\"Factory function to get RL executor.\"\"\"\n    return RLExecutionOptimizer(model_path)",
    "source_file": "core\\execution\\optimization\\rl_executor.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "forward",
    "category": "execution",
    "formula": "x",
    "explanation": "",
    "python_code": "def forward(self, state: torch.Tensor) -> torch.Tensor:\n            x = F.relu(self.fc1(state))\n            x = F.relu(self.fc2(x))\n            x = torch.tanh(self.fc3(x))  # Output in [-1, 1]\n            return x",
    "source_file": "core\\execution\\optimization\\rl_executor.py",
    "academic_reference": null,
    "class_name": "Actor"
  },
  {
    "name": "__init__",
    "category": "execution",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, config: Optional[ExecutionConfig] = None):\n        self.config = config or ExecutionConfig()\n\n        # Session volume weights (normalized to sum to 1 over 24h)\n        self.session_weights = {\n            FXSession.OVERLAP_LN: 1.8,   # Most liquid\n            FXSession.LONDON: 1.4,\n            FXSession.NEW_YORK: 1.2,\n            FXSession.OVERLAP_TL: 1.0,\n            FXSession.TOKYO: 0.4,\n            FXSession.OFF_HOURS: 0.2     # Least liquid\n        }",
    "source_file": "core\\execution\\optimization\\twap.py",
    "academic_reference": null,
    "class_name": "SessionAwareTWAP"
  },
  {
    "name": "create_schedule",
    "category": "execution",
    "formula": "ExecutionSchedule(",
    "explanation": "Create a TWAP execution schedule.\n\nArgs:\n    order_id: Unique order identifier\n    symbol: Currency pair\n    direction: 1 = buy, -1 = sell\n    total_quantity: Total quantity to execute\n    horizon_seconds: Total execution window (default from config)\n    slice_interval_seconds: Time between slices (default from config)\n    start_time: Start time (default: now)\n    session_aware: Whether to weight by session liquidity\n\nReturns:\n    ExecutionSchedule with TWAP slices",
    "python_code": "def create_schedule(self,\n                       order_id: str,\n                       symbol: str,\n                       direction: int,\n                       total_quantity: float,\n                       horizon_seconds: Optional[int] = None,\n                       slice_interval_seconds: Optional[int] = None,\n                       start_time: Optional[datetime] = None,\n                       session_aware: bool = True) -> ExecutionSchedule:\n        \"\"\"\n        Create a TWAP execution schedule.\n\n        Args:\n            order_id: Unique order identifier\n            symbol: Currency pair\n            direction: 1 = buy, -1 = sell\n            total_quantity: Total quantity to execute\n            horizon_seconds: Total execution window (default from config)\n            slice_interval_seconds: Time between slices (default from config)\n            start_time: Start time (default: now)\n            session_aware: Whether to weight by session liquidity\n\n        Returns:\n            ExecutionSchedule with TWAP slices\n        \"\"\"\n        if horizon_seconds is None:\n            horizon_seconds = self.config.default_horizon_seconds\n\n        if slice_interval_seconds is None:\n            slice_interval_seconds = self.config.slice_interval_seconds\n\n        if start_time is None:\n            start_time = datetime.now(timezone.utc)\n\n        # Calculate number of slices\n        num_slices = max(2, horizon_seconds // slice_interval_seconds)\n\n        # Generate time intervals\n        intervals = self._generate_intervals(\n            start_time=start_time,\n            horizon_seconds=horizon_seconds,\n            num_slices=num_slices\n        )\n\n        # Calculate weights for each interval\n        if session_aware:\n            weights = self._calculate_session_weights(intervals)\n        else:\n            weights = np.ones(num_slices) / num_slices\n\n        # Normalize weights\n        weights = weights / weights.sum()\n\n        # Create execution slices\n        slices = []\n        for i, (interval_start, interval_end, weight) in enumerate(zip(\n            intervals[:-1], intervals[1:], weights\n        )):\n            slice_qty = total_quantity * weight\n\n            # Skip tiny slices\n            if slice_qty < 1:\n                continue\n\n            session = get_current_session(interval_start)\n\n            slices.append(ExecutionSlice(\n                slice_id=i,\n                target_time=interval_start,\n                target_quantity=slice_qty,\n                strategy=ExecutionStrategy.MARKET,\n                status=\"pending\"\n            ))\n\n        # Adjust last slice to ensure total matches\n        if slices:\n            executed_so_far = sum(s.target_quantity for s in slices[:-1])\n            slices[-1].target_quantity = total_quantity - executed_so_far\n\n        return ExecutionSchedule(\n            order_id=order_id,\n            symbol=symbol,\n            direction=direction,\n            total_quantity=total_quantity,\n            slices=slices,\n        ",
    "source_file": "core\\execution\\optimization\\twap.py",
    "academic_reference": null,
    "class_name": "SessionAwareTWAP"
  },
  {
    "name": "_generate_intervals",
    "category": "execution",
    "formula": "intervals",
    "explanation": "Generate evenly-spaced time intervals.",
    "python_code": "def _generate_intervals(self,\n                           start_time: datetime,\n                           horizon_seconds: float,\n                           num_slices: int) -> List[datetime]:\n        \"\"\"Generate evenly-spaced time intervals.\"\"\"\n        interval_seconds = horizon_seconds / num_slices\n\n        intervals = []\n        for i in range(num_slices + 1):\n            t = start_time + timedelta(seconds=i * interval_seconds)\n            intervals.append(t)\n\n        return intervals",
    "source_file": "core\\execution\\optimization\\twap.py",
    "academic_reference": null,
    "class_name": "SessionAwareTWAP"
  },
  {
    "name": "_calculate_session_weights",
    "category": "execution",
    "formula": "np.array(weights)",
    "explanation": "Calculate volume weights based on FX sessions.\n\nReturns normalized weights that sum to 1.",
    "python_code": "def _calculate_session_weights(self, intervals: List[datetime]) -> np.ndarray:\n        \"\"\"\n        Calculate volume weights based on FX sessions.\n\n        Returns normalized weights that sum to 1.\n        \"\"\"\n        weights = []\n\n        for i in range(len(intervals) - 1):\n            start = intervals[i]\n            end = intervals[i + 1]\n\n            # Get predominant session for this interval\n            session = get_current_session(start)\n\n            # Apply session weight\n            weight = self.session_weights.get(session, 1.0)\n\n            # Check for weekend (reduced liquidity)\n            if self._is_weekend(start):\n                weight *= 0.1\n\n            weights.append(weight)\n\n        return np.array(weights)",
    "source_file": "core\\execution\\optimization\\twap.py",
    "academic_reference": null,
    "class_name": "SessionAwareTWAP"
  },
  {
    "name": "_is_weekend",
    "category": "execution",
    "formula": "True | True | True",
    "explanation": "Check if datetime falls on FX weekend (Friday 21:00 - Sunday 21:00 UTC).",
    "python_code": "def _is_weekend(self, dt: datetime) -> bool:\n        \"\"\"Check if datetime falls on FX weekend (Friday 21:00 - Sunday 21:00 UTC).\"\"\"\n        weekday = dt.weekday()\n        hour = dt.hour\n\n        # Friday after 21:00 UTC\n        if weekday == 4 and hour >= 21:\n            return True\n\n        # Saturday\n        if weekday == 5:\n            return True\n\n        # Sunday before 21:00 UTC\n        if weekday == 6 and hour < 21:\n            return True\n\n        return False",
    "source_file": "core\\execution\\optimization\\twap.py",
    "academic_reference": null,
    "class_name": "SessionAwareTWAP"
  },
  {
    "name": "_estimate_cost",
    "category": "execution",
    "formula": "spread_cost + participation_cost",
    "explanation": "Estimate execution cost in basis points.",
    "python_code": "def _estimate_cost(self,\n                      symbol: str,\n                      total_quantity: float,\n                      num_slices: int) -> float:\n        \"\"\"Estimate execution cost in basis points.\"\"\"\n        symbol_cfg = get_symbol_config(symbol)\n\n        # TWAP cost  spread/2 + small impact from participation\n        spread_cost = symbol_cfg.avg_spread_bps / 2\n\n        # Participation impact (lower for more slices)\n        slice_qty = total_quantity / num_slices\n        participation_cost = 0.1 * np.sqrt(slice_qty / 1e6)\n\n        return spread_cost + participation_cost",
    "source_file": "core\\execution\\optimization\\twap.py",
    "academic_reference": null,
    "class_name": "SessionAwareTWAP"
  },
  {
    "name": "create_schedule",
    "category": "execution",
    "formula": "session_aware=False). | super().create_schedule(",
    "explanation": "Create simple TWAP schedule (session_aware=False).",
    "python_code": "def create_schedule(self,\n                       order_id: str,\n                       symbol: str,\n                       direction: int,\n                       total_quantity: float,\n                       horizon_seconds: Optional[int] = None,\n                       slice_interval_seconds: Optional[int] = None,\n                       start_time: Optional[datetime] = None,\n                       **kwargs) -> ExecutionSchedule:\n        \"\"\"Create simple TWAP schedule (session_aware=False).\"\"\"\n        return super().create_schedule(\n            order_id=order_id,\n            symbol=symbol,\n            direction=direction,\n            total_quantity=total_quantity,\n            horizon_seconds=horizon_seconds,\n            slice_interval_seconds=slice_interval_seconds,\n            start_time=start_time,\n            session_aware=False  # Always use equal weights\n        )",
    "source_file": "core\\execution\\optimization\\twap.py",
    "academic_reference": null,
    "class_name": "SimpleTWAP"
  },
  {
    "name": "create_adaptive_schedule",
    "category": "execution",
    "formula": "schedule",
    "explanation": "Create initial schedule (will be adapted during execution).",
    "python_code": "def create_adaptive_schedule(self,\n                                order_id: str,\n                                symbol: str,\n                                direction: int,\n                                total_quantity: float,\n                                horizon_seconds: Optional[int] = None) -> ExecutionSchedule:\n        \"\"\"Create initial schedule (will be adapted during execution).\"\"\"\n        schedule = self.base_twap.create_schedule(\n            order_id=order_id,\n            symbol=symbol,\n            direction=direction,\n            total_quantity=total_quantity,\n            horizon_seconds=horizon_seconds\n        )\n\n        # Store state for adaptation\n        self.execution_state[order_id] = {\n            'original_schedule': schedule,\n            'slices_executed': 0,\n            'quantity_executed': 0.0,\n            'vwap_so_far': 0.0,\n            'spread_history': [],\n            'vol_history': []\n        }\n\n        return schedule",
    "source_file": "core\\execution\\optimization\\twap.py",
    "academic_reference": null,
    "class_name": "AdaptiveTWAP"
  },
  {
    "name": "adapt_next_slice",
    "category": "volatility",
    "formula": "adapted_slice",
    "explanation": "Get adapted next slice based on market conditions.\n\nArgs:\n    order_id: Order identifier\n    current_spread_bps: Current bid-ask spread\n    current_volatility: Current realized volatility\n    order_flow: Recent order flow imbalance (-1 to 1)\n\nReturns:\n    Adapted execution slice or None if complete",
    "python_code": "def adapt_next_slice(self,\n                        order_id: str,\n                        current_spread_bps: float,\n                        current_volatility: float,\n                        order_flow: float = 0.0) -> Optional[ExecutionSlice]:\n        \"\"\"\n        Get adapted next slice based on market conditions.\n\n        Args:\n            order_id: Order identifier\n            current_spread_bps: Current bid-ask spread\n            current_volatility: Current realized volatility\n            order_flow: Recent order flow imbalance (-1 to 1)\n\n        Returns:\n            Adapted execution slice or None if complete\n        \"\"\"\n        if order_id not in self.execution_state:\n            return None\n\n        state = self.execution_state[order_id]\n        schedule = state['original_schedule']\n\n        # Get next pending slice\n        next_slice_idx = state['slices_executed']\n        if next_slice_idx >= len(schedule.slices):\n            return None\n\n        next_slice = schedule.slices[next_slice_idx]\n\n        # Record market conditions\n        state['spread_history'].append(current_spread_bps)\n        state['vol_history'].append(current_volatility)\n\n        # Calculate adaptation factor\n        symbol_cfg = get_symbol_config(schedule.symbol)\n\n        # Spread factor: trade more when spread is tight\n        avg_spread = symbol_cfg.avg_spread_bps\n        spread_factor = avg_spread / max(current_spread_bps, 0.1)\n        spread_factor = np.clip(spread_factor, 0.5, 2.0)\n\n        # Volatility factor: trade less when vol is high\n        if len(state['vol_history']) > 1:\n            avg_vol = np.mean(state['vol_history'])\n            vol_factor = avg_vol / max(current_volatility, 1e-8)\n            vol_factor = np.clip(vol_factor, 0.5, 2.0)\n        else:\n            vol_factor = 1.0\n\n        # Combined adaptation\n        adaptation = spread_factor * vol_factor\n\n        # Adjust slice quantity\n        adapted_qty = next_slice.target_quantity * adaptation\n\n        # Ensure we don't over-execute\n        remaining = schedule.total_quantity - state['quantity_executed']\n        adapted_qty = min(adapted_qty, remaining)\n\n        # Create adapted slice\n        adapted_slice = ExecutionSlice(\n            slice_id=next_slice.slice_id,\n            target_time=datetime.now(timezone.utc),\n            target_quantity=adapted_qty,\n            strategy=next_slice.strategy,\n            status=\"active\"\n        )\n\n        return adapted_slice",
    "source_file": "core\\execution\\optimization\\twap.py",
    "academic_reference": null,
    "class_name": "AdaptiveTWAP"
  },
  {
    "name": "record_execution",
    "category": "execution",
    "formula": "state = self.execution_state[order_id]",
    "explanation": "Record an execution for tracking.",
    "python_code": "def record_execution(self,\n                        order_id: str,\n                        executed_qty: float,\n                        executed_price: float):\n        \"\"\"Record an execution for tracking.\"\"\"\n        if order_id not in self.execution_state:\n            return\n\n        state = self.execution_state[order_id]\n        state['slices_executed'] += 1\n        state['quantity_executed'] += executed_qty\n\n        # Update VWAP\n        old_vwap = state['vwap_so_far']\n        old_qty = state['quantity_executed'] - executed_qty\n        if state['quantity_executed'] > 0:\n            state['vwap_so_far'] = (\n                (old_vwap * old_qty + executed_price * executed_qty) /\n                state['quantity_executed']\n            )",
    "source_file": "core\\execution\\optimization\\twap.py",
    "academic_reference": null,
    "class_name": "AdaptiveTWAP"
  },
  {
    "name": "get_twap_scheduler",
    "category": "execution",
    "formula": "SessionAwareTWAP(config) | SimpleTWAP(config)",
    "explanation": "Factory function to get TWAP scheduler.",
    "python_code": "def get_twap_scheduler(config: Optional[ExecutionConfig] = None,\n                      session_aware: bool = True) -> SessionAwareTWAP:\n    \"\"\"Factory function to get TWAP scheduler.\"\"\"\n    if session_aware:\n        return SessionAwareTWAP(config)\n    else:\n        return SimpleTWAP(config)",
    "source_file": "core\\execution\\optimization\\twap.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "get_weight_at",
    "category": "microstructure",
    "formula": "",
    "explanation": "Get volume weight at a specific time.",
    "python_code": "def get_weight_at(self, dt: datetime) -> float:\n        \"\"\"Get volume weight at a specific time.\"\"\"\n        hour = dt.hour\n        return self.hourly_weights[hour]",
    "source_file": "core\\execution\\optimization\\vwap.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VolumeProfile"
  },
  {
    "name": "get_period_weight",
    "category": "microstructure",
    "formula": "total_weight",
    "explanation": "Get total volume weight for a time period.",
    "python_code": "def get_period_weight(self,\n                         start: datetime,\n                         end: datetime,\n                         interval_minutes: int = 5) -> float:\n        \"\"\"Get total volume weight for a time period.\"\"\"\n        total_weight = 0.0\n        current = start\n\n        while current < end:\n            # Weight is proportional to time in this hour\n            minutes_in_period = min(\n                (end - current).total_seconds() / 60,\n                interval_minutes\n            )\n            weight = self.hourly_weights[current.hour] * (minutes_in_period / 60)\n            total_weight += weight\n            current += timedelta(minutes=interval_minutes)\n\n        return total_weight",
    "source_file": "core\\execution\\optimization\\vwap.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VolumeProfile"
  },
  {
    "name": "create_volume_profile",
    "category": "microstructure",
    "formula": "VolumeProfile(",
    "explanation": "Create FX volume profile for a symbol.\n\nBased on BIS Triennial Survey and market structure:\n- London dominates (37% of volume)\n- NY second (19% of volume)\n- Tokyo third (6% of volume)\n- Overlap periods have highest volume",
    "python_code": "def create_volume_profile(symbol: str) -> VolumeProfile:\n    \"\"\"\n    Create FX volume profile for a symbol.\n\n    Based on BIS Triennial Survey and market structure:\n    - London dominates (37% of volume)\n    - NY second (19% of volume)\n    - Tokyo third (6% of volume)\n    - Overlap periods have highest volume\n    \"\"\"\n    # Base hourly profile (normalized, peaks during London-NY overlap)\n    # Hours are UTC\n    base_profile = np.array([\n        0.3,   # 00:00 - Tokyo session start\n        0.4,   # 01:00 - Tokyo\n        0.5,   # 02:00 - Tokyo\n        0.5,   # 03:00 - Tokyo peak\n        0.4,   # 04:00 - Tokyo\n        0.4,   # 05:00 - Tokyo winding down\n        0.5,   # 06:00 - Tokyo-London transition\n        0.8,   # 07:00 - London open, TL overlap\n        1.0,   # 08:00 - London ramp up\n        1.2,   # 09:00 - London peak\n        1.2,   # 10:00 - London\n        1.1,   # 11:00 - London\n        1.5,   # 12:00 - London-NY overlap start\n        1.8,   # 13:00 - LN overlap peak\n        1.8,   # 14:00 - LN overlap peak\n        1.6,   # 15:00 - LN overlap\n        1.2,   # 16:00 - London close\n        1.0,   # 17:00 - NY afternoon\n        0.9,   # 18:00 - NY\n        0.7,   # 19:00 - NY winding down\n        0.5,   # 20:00 - NY close\n        0.3,   # 21:00 - Off hours\n        0.2,   # 22:00 - Off hours\n        0.2,   # 23:00 - Off hours\n    ])\n\n    # Normalize to sum to 1\n    base_profile = base_profile / base_profile.sum()\n\n    # Symbol-specific adjustments\n    if 'JPY' in symbol:\n        # More Tokyo volume for JPY pairs\n        tokyo_boost = np.zeros(24)\n        tokyo_boost[0:9] = 0.3  # Boost Tokyo hours\n        base_profile = base_profile + tokyo_boost * base_profile\n        base_profile = base_profile / base_profile.sum()\n\n    elif 'EUR' in symbol or 'GBP' in symbol:\n        # More London volume for EUR/GBP pairs\n        london_boost = np.zeros(24)\n        london_boost[7:16] = 0.2  # Boost London hours\n        base_profile = base_profile + london_boost * base_profile\n        base_profile = base_profile / base_profile.sum()\n\n    # Session weights\n    session_weights = {\n        FXSession.TOKYO: 0.15,\n        FXSession.LONDON: 0.37,\n        FXSession.NEW_YORK: 0.19,\n        FXSession.OVERLAP_LN: 0.20,\n        FXSession.OVERLAP_TL: 0.05,\n        FXSession.OFF_HOURS: 0.04\n    }\n\n    return VolumeProfile(\n        hourly_weights=base_profile,\n        session_weights=session_weights,\n        symbol=symbol\n    )",
    "source_file": "core\\execution\\optimization\\vwap.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "execution",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, config: Optional[ExecutionConfig] = None):\n        self.config = config or ExecutionConfig()\n        self.volume_profiles: Dict[str, VolumeProfile] = {}",
    "source_file": "core\\execution\\optimization\\vwap.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "FXVWAPScheduler"
  },
  {
    "name": "get_volume_profile",
    "category": "microstructure",
    "formula": "",
    "explanation": "Get or create volume profile for symbol.",
    "python_code": "def get_volume_profile(self, symbol: str) -> VolumeProfile:\n        \"\"\"Get or create volume profile for symbol.\"\"\"\n        if symbol not in self.volume_profiles:\n            self.volume_profiles[symbol] = create_volume_profile(symbol)\n        return self.volume_profiles[symbol]",
    "source_file": "core\\execution\\optimization\\vwap.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "FXVWAPScheduler"
  },
  {
    "name": "create_schedule",
    "category": "execution",
    "formula": "ExecutionSchedule(",
    "explanation": "Create a VWAP execution schedule.\n\nArgs:\n    order_id: Unique order identifier\n    symbol: Currency pair\n    direction: 1 = buy, -1 = sell\n    total_quantity: Total quantity to execute\n    horizon_seconds: Total execution window\n    slice_interval_seconds: Time between slices\n    start_time: Start time (default: now)\n    max_participation_rate: Max percentage of estimated volume\n\nReturns:\n    ExecutionSchedule with VWAP slices",
    "python_code": "def create_schedule(self,\n                       order_id: str,\n                       symbol: str,\n                       direction: int,\n                       total_quantity: float,\n                       horizon_seconds: Optional[int] = None,\n                       slice_interval_seconds: Optional[int] = None,\n                       start_time: Optional[datetime] = None,\n                       max_participation_rate: Optional[float] = None) -> ExecutionSchedule:\n        \"\"\"\n        Create a VWAP execution schedule.\n\n        Args:\n            order_id: Unique order identifier\n            symbol: Currency pair\n            direction: 1 = buy, -1 = sell\n            total_quantity: Total quantity to execute\n            horizon_seconds: Total execution window\n            slice_interval_seconds: Time between slices\n            start_time: Start time (default: now)\n            max_participation_rate: Max percentage of estimated volume\n\n        Returns:\n            ExecutionSchedule with VWAP slices\n        \"\"\"\n        if horizon_seconds is None:\n            horizon_seconds = self.config.default_horizon_seconds\n\n        if slice_interval_seconds is None:\n            slice_interval_seconds = self.config.slice_interval_seconds\n\n        if start_time is None:\n            start_time = datetime.now(timezone.utc)\n\n        if max_participation_rate is None:\n            max_participation_rate = self.config.max_participation_rate\n\n        # Get volume profile\n        profile = self.get_volume_profile(symbol)\n        symbol_cfg = get_symbol_config(symbol)\n\n        # Calculate slices\n        num_slices = max(2, horizon_seconds // slice_interval_seconds)\n        end_time = start_time + timedelta(seconds=horizon_seconds)\n\n        # Generate time intervals\n        intervals = []\n        for i in range(num_slices + 1):\n            t = start_time + timedelta(seconds=i * slice_interval_seconds)\n            intervals.append(t)\n\n        # Calculate volume weights for each interval\n        weights = []\n        for i in range(len(intervals) - 1):\n            weight = profile.get_period_weight(\n                intervals[i],\n                intervals[i + 1],\n                interval_minutes=slice_interval_seconds // 60\n            )\n            weights.append(weight)\n\n        weights = np.array(weights)\n\n        # Normalize weights\n        if weights.sum() > 0:\n            weights = weights / weights.sum()\n        else:\n            weights = np.ones(num_slices) / num_slices\n\n        # Calculate slice quantities\n        slice_quantities = total_quantity * weights\n\n        # Apply participation rate cap\n        slice_quantities = self._apply_participation_cap(\n            slice_quantities=slice_quantities,\n            intervals=intervals,\n            symbol_cfg=symbol_cfg,\n            max_rate=max_participation_rate\n        )\n\n        # Ensure we execute full quantity (redistribute any capped amounts)\n        total_allocated = slice_quantities.sum()\n        if total_allocated",
    "source_file": "core\\execution\\optimization\\vwap.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "FXVWAPScheduler"
  },
  {
    "name": "_apply_participation_cap",
    "category": "execution",
    "formula": "capped",
    "explanation": "Apply participation rate cap to slice quantities.\n\nEnsures we don't exceed max_rate of estimated volume.",
    "python_code": "def _apply_participation_cap(self,\n                                slice_quantities: np.ndarray,\n                                intervals: List[datetime],\n                                symbol_cfg,\n                                max_rate: float) -> np.ndarray:\n        \"\"\"\n        Apply participation rate cap to slice quantities.\n\n        Ensures we don't exceed max_rate of estimated volume.\n        \"\"\"\n        capped = slice_quantities.copy()\n\n        for i, qty in enumerate(slice_quantities):\n            if i >= len(intervals) - 1:\n                continue\n\n            interval_start = intervals[i]\n            interval_end = intervals[i + 1]\n\n            # Estimate volume during this interval\n            session = get_current_session(interval_start)\n            session_cfg = get_session_config(session)\n\n            interval_seconds = (interval_end - interval_start).total_seconds()\n            daily_volume = symbol_cfg.daily_volume_estimate\n\n            # Volume in this interval\n            session_fraction = session_cfg.volume_weight\n            interval_fraction = interval_seconds / (24 * 3600)\n            estimated_volume = daily_volume * session_fraction * interval_fraction\n\n            # Apply cap\n            max_qty = estimated_volume * max_rate\n            capped[i] = min(qty, max_qty)\n\n        return capped",
    "source_file": "core\\execution\\optimization\\vwap.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "FXVWAPScheduler"
  },
  {
    "name": "_estimate_cost",
    "category": "microstructure",
    "formula": "cost = spread/2 + impact from participation | spread_cost + total_impact",
    "explanation": "Estimate VWAP execution cost.\n\nVWAP cost = spread/2 + impact from participation",
    "python_code": "def _estimate_cost(self,\n                      symbol: str,\n                      total_quantity: float,\n                      weights: np.ndarray,\n                      intervals: List[datetime]) -> float:\n        \"\"\"\n        Estimate VWAP execution cost.\n\n        VWAP cost = spread/2 + impact from participation\n        \"\"\"\n        symbol_cfg = get_symbol_config(symbol)\n\n        # Base spread cost\n        spread_cost = symbol_cfg.avg_spread_bps / 2\n\n        # Impact from participation (weighted by session)\n        total_impact = 0.0\n        for i, (weight, interval_start) in enumerate(zip(weights, intervals[:-1])):\n            if weight <= 0:\n                continue\n\n            session = get_current_session(interval_start)\n            session_cfg = get_session_config(session)\n\n            # Slice quantity\n            slice_qty = total_quantity * weight\n\n            # Participation-based impact (lower in liquid sessions)\n            session_impact = 0.1 * np.sqrt(slice_qty / 1e6)\n            session_impact *= session_cfg.spread_multiplier\n\n            total_impact += session_impact * weight\n\n        return spread_cost + total_impact",
    "source_file": "core\\execution\\optimization\\vwap.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "FXVWAPScheduler"
  },
  {
    "name": "start_execution",
    "category": "execution",
    "formula": "schedule",
    "explanation": "Start VWAP execution with tracking.",
    "python_code": "def start_execution(self,\n                       order_id: str,\n                       symbol: str,\n                       direction: int,\n                       total_quantity: float,\n                       horizon_seconds: int) -> ExecutionSchedule:\n        \"\"\"Start VWAP execution with tracking.\"\"\"\n        schedule = self.base_vwap.create_schedule(\n            order_id=order_id,\n            symbol=symbol,\n            direction=direction,\n            total_quantity=total_quantity,\n            horizon_seconds=horizon_seconds\n        )\n\n        # Initialize tracking state\n        self.execution_state[order_id] = {\n            'schedule': schedule,\n            'profile': self.base_vwap.get_volume_profile(symbol),\n            'start_time': datetime.now(timezone.utc),\n            'expected_volume': 0.0,\n            'actual_volume': 0.0,\n            'executed_qty': 0.0,\n            'slices_executed': 0,\n            'vwap': 0.0,\n            'market_vwap': 0.0\n        }\n\n        return schedule",
    "source_file": "core\\execution\\optimization\\vwap.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AdaptiveVWAP"
  },
  {
    "name": "update_volume",
    "category": "execution",
    "formula": "1.0 | adjustment",
    "explanation": "Update with observed market volume.\n\nArgs:\n    order_id: Order identifier\n    observed_volume: Actual volume observed since last update\n    market_vwap: Current market VWAP\n\nReturns:\n    Recommended execution rate adjustment (1.0 = on track)",
    "python_code": "def update_volume(self,\n                     order_id: str,\n                     observed_volume: float,\n                     market_vwap: float) -> float:\n        \"\"\"\n        Update with observed market volume.\n\n        Args:\n            order_id: Order identifier\n            observed_volume: Actual volume observed since last update\n            market_vwap: Current market VWAP\n\n        Returns:\n            Recommended execution rate adjustment (1.0 = on track)\n        \"\"\"\n        if order_id not in self.execution_state:\n            return 1.0\n\n        state = self.execution_state[order_id]\n        state['actual_volume'] += observed_volume\n        state['market_vwap'] = market_vwap\n\n        # Calculate expected volume up to now\n        elapsed = (datetime.now(timezone.utc) - state['start_time']).total_seconds()\n        schedule = state['schedule']\n        profile = state['profile']\n\n        expected = profile.get_period_weight(\n            state['start_time'],\n            datetime.now(timezone.utc)\n        ) * schedule.total_quantity\n\n        state['expected_volume'] = expected\n\n        # Volume ratio\n        if expected > 0:\n            volume_ratio = state['actual_volume'] / expected\n        else:\n            volume_ratio = 1.0\n\n        # If volume is higher than expected, we can execute faster\n        # If volume is lower, we should slow down\n        adjustment = np.clip(volume_ratio, 0.5, 2.0)\n\n        return adjustment",
    "source_file": "core\\execution\\optimization\\vwap.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AdaptiveVWAP"
  },
  {
    "name": "get_next_slice",
    "category": "execution",
    "formula": "ExecutionSlice(",
    "explanation": "Get next execution slice with optional adjustment.",
    "python_code": "def get_next_slice(self,\n                      order_id: str,\n                      volume_adjustment: float = 1.0) -> Optional[ExecutionSlice]:\n        \"\"\"Get next execution slice with optional adjustment.\"\"\"\n        if order_id not in self.execution_state:\n            return None\n\n        state = self.execution_state[order_id]\n        schedule = state['schedule']\n\n        idx = state['slices_executed']\n        if idx >= len(schedule.slices):\n            return None\n\n        base_slice = schedule.slices[idx]\n\n        # Adjust quantity\n        adjusted_qty = base_slice.target_quantity * volume_adjustment\n\n        # Don't exceed remaining\n        remaining = schedule.total_quantity - state['executed_qty']\n        adjusted_qty = min(adjusted_qty, remaining)\n\n        return ExecutionSlice(\n            slice_id=base_slice.slice_id,\n            target_time=datetime.now(timezone.utc),\n            target_quantity=adjusted_qty,\n            strategy=base_slice.strategy,\n            status=\"active\"\n        )",
    "source_file": "core\\execution\\optimization\\vwap.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AdaptiveVWAP"
  },
  {
    "name": "record_fill",
    "category": "execution",
    "formula": "state = self.execution_state[order_id]",
    "explanation": "Record a fill.",
    "python_code": "def record_fill(self,\n                   order_id: str,\n                   filled_qty: float,\n                   fill_price: float):\n        \"\"\"Record a fill.\"\"\"\n        if order_id not in self.execution_state:\n            return\n\n        state = self.execution_state[order_id]\n\n        # Update VWAP\n        old_value = state['vwap'] * state['executed_qty']\n        state['executed_qty'] += filled_qty\n        if state['executed_qty'] > 0:\n            state['vwap'] = (old_value + fill_price * filled_qty) / state['executed_qty']\n\n        state['slices_executed'] += 1",
    "source_file": "core\\execution\\optimization\\vwap.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AdaptiveVWAP"
  },
  {
    "name": "get_tracking_error",
    "category": "execution",
    "formula": "0.0 | 0.0 | error",
    "explanation": "Calculate VWAP tracking error.\n\nReturns difference between our VWAP and market VWAP in bps.",
    "python_code": "def get_tracking_error(self, order_id: str) -> float:\n        \"\"\"\n        Calculate VWAP tracking error.\n\n        Returns difference between our VWAP and market VWAP in bps.\n        \"\"\"\n        if order_id not in self.execution_state:\n            return 0.0\n\n        state = self.execution_state[order_id]\n\n        if state['vwap'] <= 0 or state['market_vwap'] <= 0:\n            return 0.0\n\n        # Tracking error in bps\n        error = (state['vwap'] - state['market_vwap']) / state['market_vwap'] * 10000\n\n        return error",
    "source_file": "core\\execution\\optimization\\vwap.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AdaptiveVWAP"
  },
  {
    "name": "get_vwap_scheduler",
    "category": "execution",
    "formula": "FXVWAPScheduler(config)",
    "explanation": "Factory function to get VWAP scheduler.",
    "python_code": "def get_vwap_scheduler(config: Optional[ExecutionConfig] = None) -> FXVWAPScheduler:\n    \"\"\"Factory function to get VWAP scheduler.\"\"\"\n    return FXVWAPScheduler(config)",
    "source_file": "core\\execution\\optimization\\vwap.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "prepare_forex_data",
    "category": "machine_learning",
    "formula": "{",
    "explanation": "Prepare forex data for Time-Series-Library models.\n\nArgs:\n    df: DataFrame with OHLCV data\n    target_col: Target column name\n    feature_cols: Feature column names\n    seq_len: Lookback length\n    pred_len: Prediction length\n\nReturns:\n    Dictionary with prepared data arrays",
    "python_code": "def prepare_forex_data(\n        df: pd.DataFrame,\n        target_col: str = 'close',\n        feature_cols: List[str] = None,\n        seq_len: int = 96,\n        pred_len: int = 24\n    ) -> Dict:\n        \"\"\"\n        Prepare forex data for Time-Series-Library models.\n\n        Args:\n            df: DataFrame with OHLCV data\n            target_col: Target column name\n            feature_cols: Feature column names\n            seq_len: Lookback length\n            pred_len: Prediction length\n\n        Returns:\n            Dictionary with prepared data arrays\n        \"\"\"\n        if feature_cols is None:\n            feature_cols = ['open', 'high', 'low', 'close', 'volume']\n\n        # Filter to available columns\n        feature_cols = [c for c in feature_cols if c in df.columns]\n\n        # Create sequences\n        data = df[feature_cols].values\n        target_idx = feature_cols.index(target_col) if target_col in feature_cols else 0\n\n        X, y = [], []\n        for i in range(len(data) - seq_len - pred_len + 1):\n            X.append(data[i:i+seq_len])\n            y.append(data[i+seq_len:i+seq_len+pred_len, target_idx])\n\n        return {\n            'X': np.array(X),\n            'y': np.array(y),\n            'feature_cols': feature_cols,\n            'target_col': target_col,\n            'seq_len': seq_len,\n            'pred_len': pred_len\n        }",
    "source_file": "core\\features\\academic_deep_learning.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "TimeSeriesLibraryModels"
  },
  {
    "name": "get_model_config",
    "category": "machine_learning",
    "formula": "base_config",
    "explanation": "Get default configuration for a Time-Series-Library model.\n\nArgs:\n    model_name: Model name\n    seq_len: Lookback length\n    pred_len: Prediction length\n    enc_in: Number of input variates\n\nReturns:\n    Configuration dictionary",
    "python_code": "def get_model_config(\n        model_name: str,\n        seq_len: int = 96,\n        pred_len: int = 24,\n        enc_in: int = 7\n    ) -> Dict:\n        \"\"\"\n        Get default configuration for a Time-Series-Library model.\n\n        Args:\n            model_name: Model name\n            seq_len: Lookback length\n            pred_len: Prediction length\n            enc_in: Number of input variates\n\n        Returns:\n            Configuration dictionary\n        \"\"\"\n        base_config = {\n            'seq_len': seq_len,\n            'pred_len': pred_len,\n            'enc_in': enc_in,\n            'dec_in': enc_in,\n            'c_out': enc_in,\n            'd_model': 512,\n            'd_ff': 512,\n            'n_heads': 8,\n            'e_layers': 2,\n            'd_layers': 1,\n            'dropout': 0.1,\n            'embed': 'timeF',\n            'freq': 'h',\n            'factor': 3,\n            'output_attention': False,\n        }\n\n        # Model-specific overrides\n        if model_name == 'iTransformer':\n            base_config['class_strategy'] = 'projection'\n        elif model_name == 'TimeXer':\n            base_config['patch_len'] = 16\n            base_config['stride'] = 8\n        elif model_name == 'PatchTST':\n            base_config['patch_len'] = 16\n            base_config['stride'] = 8\n            base_config['padding_patch'] = 'end'\n\n        return base_config",
    "source_file": "core\\features\\academic_deep_learning.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "TimeSeriesLibraryModels"
  },
  {
    "name": "generate_time_features",
    "category": "feature_engineering",
    "formula": "features",
    "explanation": "Generate time-based features for TFT.\n\nThese are \"known future inputs\" that the model can use\nfor forecasting.\n\nArgs:\n    df: DataFrame with DatetimeIndex\n\nReturns:\n    DataFrame with time features",
    "python_code": "def generate_time_features(df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate time-based features for TFT.\n\n        These are \"known future inputs\" that the model can use\n        for forecasting.\n\n        Args:\n            df: DataFrame with DatetimeIndex\n\n        Returns:\n            DataFrame with time features\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n\n        if isinstance(df.index, pd.DatetimeIndex):\n            # Cyclical encoding (better for neural networks)\n            features['hour_sin'] = np.sin(2 * np.pi * df.index.hour / 24)\n            features['hour_cos'] = np.cos(2 * np.pi * df.index.hour / 24)\n\n            features['day_sin'] = np.sin(2 * np.pi * df.index.dayofweek / 7)\n            features['day_cos'] = np.cos(2 * np.pi * df.index.dayofweek / 7)\n\n            features['month_sin'] = np.sin(2 * np.pi * df.index.month / 12)\n            features['month_cos'] = np.cos(2 * np.pi * df.index.month / 12)\n\n            # Forex session indicators\n            hour = df.index.hour\n            features['is_asian_session'] = ((hour >= 0) & (hour < 8)).astype(float)\n            features['is_london_session'] = ((hour >= 8) & (hour < 16)).astype(float)\n            features['is_ny_session'] = ((hour >= 13) & (hour < 22)).astype(float)\n\n            # Weekend proximity (forex closes Friday, opens Sunday)\n            features['day_of_week'] = df.index.dayofweek\n            features['is_friday'] = (df.index.dayofweek == 4).astype(float)\n            features['is_monday'] = (df.index.dayofweek == 0).astype(float)\n\n        return features",
    "source_file": "core\\features\\academic_deep_learning.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": "DeepLearningFeatures"
  },
  {
    "name": "normalize_for_dl",
    "category": "statistical",
    "formula": "None = global) | normalized.fillna(0), params",
    "explanation": "Normalize data for deep learning.\n\nArgs:\n    df: DataFrame to normalize\n    method: 'zscore', 'minmax', or 'robust'\n    window: Rolling window (None = global)\n\nReturns:\n    Tuple of (normalized DataFrame, normalization parameters)",
    "python_code": "def normalize_for_dl(\n        df: pd.DataFrame,\n        method: str = 'zscore',\n        window: int = None\n    ) -> Tuple[pd.DataFrame, Dict]:\n        \"\"\"\n        Normalize data for deep learning.\n\n        Args:\n            df: DataFrame to normalize\n            method: 'zscore', 'minmax', or 'robust'\n            window: Rolling window (None = global)\n\n        Returns:\n            Tuple of (normalized DataFrame, normalization parameters)\n        \"\"\"\n        params = {}\n\n        if method == 'zscore':\n            if window:\n                mean = df.rolling(window).mean()\n                std = df.rolling(window).std()\n            else:\n                mean = df.mean()\n                std = df.std()\n\n            normalized = (df - mean) / (std + 1e-8)\n            params = {'mean': mean, 'std': std}\n\n        elif method == 'minmax':\n            if window:\n                min_val = df.rolling(window).min()\n                max_val = df.rolling(window).max()\n            else:\n                min_val = df.min()\n                max_val = df.max()\n\n            normalized = (df - min_val) / (max_val - min_val + 1e-8)\n            params = {'min': min_val, 'max': max_val}\n\n        elif method == 'robust':\n            if window:\n                median = df.rolling(window).median()\n                q75 = df.rolling(window).quantile(0.75)\n                q25 = df.rolling(window).quantile(0.25)\n                iqr = q75 - q25\n            else:\n                median = df.median()\n                q75 = df.quantile(0.75)\n                q25 = df.quantile(0.25)\n                iqr = q75 - q25\n\n            normalized = (df - median) / (iqr + 1e-8)\n            params = {'median': median, 'iqr': iqr}\n\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n\n        return normalized.fillna(0), params",
    "source_file": "core\\features\\academic_deep_learning.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": "DeepLearningFeatures"
  },
  {
    "name": "__init__",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self):\n        self.dl_features = DeepLearningFeatures()\n        self.tsl_models = TimeSeriesLibraryModels()",
    "source_file": "core\\features\\academic_deep_learning.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": "AcademicDeepLearningFeatures"
  },
  {
    "name": "generate_all",
    "category": "feature_engineering",
    "formula": "features.fillna(0)",
    "explanation": "Generate all features suitable for deep learning.\n\nArgs:\n    df: DataFrame with OHLCV data\n\nReturns:\n    DataFrame with DL-ready features",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all features suitable for deep learning.\n\n        Args:\n            df: DataFrame with OHLCV data\n\n        Returns:\n            DataFrame with DL-ready features\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n\n        # Time features (for TFT known future inputs)\n        time_feats = self.dl_features.generate_time_features(df)\n        features = pd.concat([features, time_feats], axis=1)\n\n        # Price features\n        close = df['close']\n        returns = close.pct_change()\n\n        # Log returns (better for DL)\n        features['log_return'] = np.log(close / close.shift(1))\n\n        # Normalized price (relative to rolling mean)\n        for w in [20, 50, 100]:\n            features[f'price_norm_{w}'] = close / close.rolling(w).mean() - 1\n\n        # Volatility features (normalized)\n        features['volatility_20'] = returns.rolling(20).std()\n        features['volatility_zscore'] = (\n            features['volatility_20'] -\n            features['volatility_20'].rolling(100).mean()\n        ) / (features['volatility_20'].rolling(100).std() + 1e-8)\n\n        # Volume features (if available)\n        if 'volume' in df.columns:\n            volume = df['volume']\n            features['volume_norm'] = volume / volume.rolling(20).mean() - 1\n\n        # Range features\n        if 'high' in df.columns and 'low' in df.columns:\n            features['range_pct'] = (df['high'] - df['low']) / close\n\n        return features.fillna(0)",
    "source_file": "core\\features\\academic_deep_learning.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": "AcademicDeepLearningFeatures"
  },
  {
    "name": "generate_deep_learning_features",
    "category": "deep_learning",
    "formula": "generator.generate_all(df)",
    "explanation": "Convenience function to generate deep learning features.\n\nCitations:\n- Lim et al. (2021) - Temporal Fusion Transformer\n- Liu et al. (2024) - iTransformer\n- Wang et al. (2024) - TimeXer\n- Oreshkin et al. (2020) - N-BEATS\n\nArgs:\n    df: DataFrame with OHLCV data\n    **kwargs: Additional arguments\n\nReturns:\n    DataFrame with DL-ready features",
    "python_code": "def generate_deep_learning_features(df: pd.DataFrame, **kwargs) -> pd.DataFrame:\n    \"\"\"\n    Convenience function to generate deep learning features.\n\n    Citations:\n    - Lim et al. (2021) - Temporal Fusion Transformer\n    - Liu et al. (2024) - iTransformer\n    - Wang et al. (2024) - TimeXer\n    - Oreshkin et al. (2020) - N-BEATS\n\n    Args:\n        df: DataFrame with OHLCV data\n        **kwargs: Additional arguments\n\n    Returns:\n        DataFrame with DL-ready features\n    \"\"\"\n    generator = AcademicDeepLearningFeatures()\n    return generator.generate_all(df)",
    "source_file": "core\\features\\academic_deep_learning.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Initialize Avellaneda-Stoikov model.\n\nArgs:\n    config: Model configuration\n\nNote on parameters:\n    \"The  parameter is a value that must be defined by the\n    market maker, considering how much inventory risk he is\n    willing to be exposed. A value closer to zero means more\n    aggressive quoting.\"\n    - Avellaneda & Stoikov (2008)",
    "python_code": "def __init__(self, config: MarketMakingConfig = None):\n        \"\"\"\n        Initialize Avellaneda-Stoikov model.\n\n        Args:\n            config: Model configuration\n\n        Note on parameters:\n            \"The  parameter is a value that must be defined by the\n            market maker, considering how much inventory risk he is\n            willing to be exposed. A value closer to zero means more\n            aggressive quoting.\"\n            - Avellaneda & Stoikov (2008)\n        \"\"\"\n        self.config = config or MarketMakingConfig()",
    "source_file": "core\\features\\academic_market_making.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "AvellanedaStoikov"
  },
  {
    "name": "reservation_price",
    "category": "technical",
    "formula": "r(s, q, t) = s - q(T-t) | inventory = 0: reservation = mid | positive = long)",
    "explanation": "Compute reservation (indifference) price.\n\nThe reservation price is the market maker's personal valuation\nof the asset given their current inventory position.\n\nFormula:\n    r(s, q, t) = s - q(T-t)\n\nInterpretation:\n    - If inventory > 0 (long): reservation < mid (want to sell)\n    - If inventory < 0 (short): reservation > mid (want to buy)\n    - If inventory = 0: reservation = mid\n\n\"First, the dealer computes a personal indifference valuation\nfor the stock, given his current inventory.\"\n- Avellaneda & Stoikov (2008)\n\nArgs:\n    mid_price: Current mid price\n    inventory: Current inventory (positive = long)\n    time_remaining: Time remaining until terminal (default: T)\n\nReturns:\n    Reservation price",
    "python_code": "def reservation_price(\n        self,\n        mid_price: float,\n        inventory: float,\n        time_remaining: float = None\n    ) -> float:\n        \"\"\"\n        Compute reservation (indifference) price.\n\n        The reservation price is the market maker's personal valuation\n        of the asset given their current inventory position.\n\n        Formula:\n            r(s, q, t) = s - q(T-t)\n\n        Interpretation:\n            - If inventory > 0 (long): reservation < mid (want to sell)\n            - If inventory < 0 (short): reservation > mid (want to buy)\n            - If inventory = 0: reservation = mid\n\n        \"First, the dealer computes a personal indifference valuation\n        for the stock, given his current inventory.\"\n        - Avellaneda & Stoikov (2008)\n\n        Args:\n            mid_price: Current mid price\n            inventory: Current inventory (positive = long)\n            time_remaining: Time remaining until terminal (default: T)\n\n        Returns:\n            Reservation price\n        \"\"\"\n        time_remaining = time_remaining if time_remaining is not None else self.config.T\n\n        gamma = self.config.gamma\n        sigma = self.config.sigma\n\n        # r = s - q(T-t)\n        reservation = mid_price - inventory * gamma * sigma**2 * time_remaining\n\n        return reservation",
    "source_file": "core\\features\\academic_market_making.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "AvellanedaStoikov"
  },
  {
    "name": "optimal_spread",
    "category": "microstructure",
    "formula": "^a + ^b = (T-t) + (2/)ln(1 + /k) | b = (T-t) + (2/)ln(1 + /k) | term1 + term2",
    "explanation": "Compute optimal total spread.\n\nFormula:\n    ^a + ^b = (T-t) + (2/)ln(1 + /k)\n\n\"The bid-ask spread is independent of the inventory.\"\n- Avellaneda & Stoikov (2008)\n\nThe first term depends on time and risk aversion.\nThe second term comes from the order arrival intensity model.\n\nArgs:\n    time_remaining: Time remaining until terminal\n\nReturns:\n    Total spread (ask_delta + bid_delta)",
    "python_code": "def optimal_spread(self, time_remaining: float = None) -> float:\n        \"\"\"\n        Compute optimal total spread.\n\n        Formula:\n            ^a + ^b = (T-t) + (2/)ln(1 + /k)\n\n        \"The bid-ask spread is independent of the inventory.\"\n        - Avellaneda & Stoikov (2008)\n\n        The first term depends on time and risk aversion.\n        The second term comes from the order arrival intensity model.\n\n        Args:\n            time_remaining: Time remaining until terminal\n\n        Returns:\n            Total spread (ask_delta + bid_delta)\n        \"\"\"\n        time_remaining = time_remaining if time_remaining is not None else self.config.T\n\n        gamma = self.config.gamma\n        sigma = self.config.sigma\n        k = self.config.k\n\n        # Term 1: Time-dependent component\n        term1 = gamma * sigma**2 * time_remaining\n\n        # Term 2: Arrival intensity component\n        term2 = (2.0 / gamma) * np.log(1 + gamma / k)\n\n        return term1 + term2",
    "source_file": "core\\features\\academic_market_making.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "AvellanedaStoikov"
  },
  {
    "name": "optimal_quotes",
    "category": "microstructure",
    "formula": "positive = long) | bid, ask",
    "explanation": "Compute optimal bid and ask prices.\n\n\"To minimize inventory risk, prices should be skewed to favor\nthe inventory to come back to its targeted ideal balance point.\nTo maximize trade profitability, spreads should be enlarged such\nthat the expected future value of the account is maximized.\"\n- Avellaneda & Stoikov (2008)\n\nArgs:\n    mid_price: Current mid price\n    inventory: Current inventory (positive = long)\n    time_remaining: Time remaining until terminal\n\nReturns:\n    Tuple of (optimal_bid, optimal_ask)",
    "python_code": "def optimal_quotes(\n        self,\n        mid_price: float,\n        inventory: float,\n        time_remaining: float = None\n    ) -> Tuple[float, float]:\n        \"\"\"\n        Compute optimal bid and ask prices.\n\n        \"To minimize inventory risk, prices should be skewed to favor\n        the inventory to come back to its targeted ideal balance point.\n        To maximize trade profitability, spreads should be enlarged such\n        that the expected future value of the account is maximized.\"\n        - Avellaneda & Stoikov (2008)\n\n        Args:\n            mid_price: Current mid price\n            inventory: Current inventory (positive = long)\n            time_remaining: Time remaining until terminal\n\n        Returns:\n            Tuple of (optimal_bid, optimal_ask)\n        \"\"\"\n        time_remaining = time_remaining if time_remaining is not None else self.config.T\n\n        # Step 1: Compute reservation price\n        r = self.reservation_price(mid_price, inventory, time_remaining)\n\n        # Step 2: Compute optimal spread\n        spread = self.optimal_spread(time_remaining)\n\n        # Step 3: Compute inventory skew\n        # Positive inventory  lower bid, higher ask (encourage selling)\n        # Negative inventory  higher bid, lower ask (encourage buying)\n        gamma = self.config.gamma\n        sigma = self.config.sigma\n        skew = inventory * gamma * sigma**2 * time_remaining\n\n        # Optimal quotes (symmetric spread around reservation, skewed by inventory)\n        bid = r - spread / 2 + skew\n        ask = r + spread / 2 + skew\n\n        # Alternative: quotes directly around mid with skew\n        # bid = mid_price - spread / 2 - skew\n        # ask = mid_price + spread / 2 - skew\n\n        return bid, ask",
    "source_file": "core\\features\\academic_market_making.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "AvellanedaStoikov"
  },
  {
    "name": "expected_fill_rate",
    "category": "feature_engineering",
    "formula": "A * np.exp(-k * delta)",
    "explanation": "Expected order fill rate at distance delta from mid.\n\n\"Avellaneda and Stoikov showed that, given the empirical evidence,\nwe can assume that () = Aexp(-k) for order arrival rates.\"\n\nArgs:\n    delta: Distance from mid price\n\nReturns:\n    Expected fill rate (arrivals per unit time)",
    "python_code": "def expected_fill_rate(self, delta: float) -> float:\n        \"\"\"\n        Expected order fill rate at distance delta from mid.\n\n        \"Avellaneda and Stoikov showed that, given the empirical evidence,\n        we can assume that () = Aexp(-k) for order arrival rates.\"\n\n        Args:\n            delta: Distance from mid price\n\n        Returns:\n            Expected fill rate (arrivals per unit time)\n        \"\"\"\n        A = self.config.A\n        k = self.config.k\n\n        return A * np.exp(-k * delta)",
    "source_file": "core\\features\\academic_market_making.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "AvellanedaStoikov"
  },
  {
    "name": "inventory_limits_adjustment",
    "category": "feature_engineering",
    "formula": "bid, ask",
    "explanation": "Adjust quotes based on inventory limits.\n\n\"When inventory approaches the maximum limit, quotes are\nadjusted more aggressively to reduce position.\"\n- Guant, Lehalle, Fernandez-Tapia (2012)\n\nArgs:\n    bid: Current optimal bid\n    ask: Current optimal ask\n    inventory: Current inventory\n\nReturns:\n    Adjusted (bid, ask)",
    "python_code": "def inventory_limits_adjustment(\n        self,\n        bid: float,\n        ask: float,\n        inventory: float\n    ) -> Tuple[float, float]:\n        \"\"\"\n        Adjust quotes based on inventory limits.\n\n        \"When inventory approaches the maximum limit, quotes are\n        adjusted more aggressively to reduce position.\"\n        - Guant, Lehalle, Fernandez-Tapia (2012)\n\n        Args:\n            bid: Current optimal bid\n            ask: Current optimal ask\n            inventory: Current inventory\n\n        Returns:\n            Adjusted (bid, ask)\n        \"\"\"\n        max_inv = self.config.max_inventory\n\n        # Inventory as fraction of limit\n        inv_ratio = abs(inventory) / max_inv if max_inv > 0 else 0\n\n        # Exponential adjustment as inventory approaches limit\n        adjustment_factor = np.exp(2 * inv_ratio) - 1\n\n        if inventory > 0:  # Long position, want to sell\n            # Widen ask less, tighten bid (encourage sells)\n            ask = ask - adjustment_factor * (ask - bid) * 0.1\n            bid = bid - adjustment_factor * (ask - bid) * 0.2\n        elif inventory < 0:  # Short position, want to buy\n            # Tighten ask, widen bid (encourage buys)\n            ask = ask + adjustment_factor * (ask - bid) * 0.2\n            bid = bid + adjustment_factor * (ask - bid) * 0.1\n\n        return bid, ask",
    "source_file": "core\\features\\academic_market_making.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "AvellanedaStoikov"
  },
  {
    "name": "estimate_market_maker_inventory",
    "category": "feature_engineering",
    "formula": "positive = MM is long) | inv_signal",
    "explanation": "Estimate market maker inventory from quote asymmetry.\n\nIf market maker is long, they quote lower bids (discourage buying)\nand lower asks (encourage selling), creating negative skew.\n\nArgs:\n    bid: Bid price series\n    ask: Ask price series\n    mid: Mid price (if None, computed from bid/ask)\n\nReturns:\n    Estimated inventory signal (positive = MM is long)",
    "python_code": "def estimate_market_maker_inventory(\n        self,\n        bid: pd.Series,\n        ask: pd.Series,\n        mid: pd.Series = None\n    ) -> pd.Series:\n        \"\"\"\n        Estimate market maker inventory from quote asymmetry.\n\n        If market maker is long, they quote lower bids (discourage buying)\n        and lower asks (encourage selling), creating negative skew.\n\n        Args:\n            bid: Bid price series\n            ask: Ask price series\n            mid: Mid price (if None, computed from bid/ask)\n\n        Returns:\n            Estimated inventory signal (positive = MM is long)\n        \"\"\"\n        if mid is None:\n            mid = (bid + ask) / 2\n\n        # Quote skew: negative means MM wants to sell (is long)\n        bid_dist = mid - bid\n        ask_dist = ask - mid\n\n        # Asymmetry: positive = ask is further from mid = MM is short\n        asymmetry = ask_dist - bid_dist\n\n        # Normalized by spread\n        spread = ask - bid\n        inv_signal = asymmetry / (spread + 1e-10)\n\n        return inv_signal",
    "source_file": "core\\features\\academic_market_making.py",
    "academic_reference": "arXiv:1206.4810",
    "class_name": "MarketMakingFeatures"
  },
  {
    "name": "generate_features",
    "category": "feature_engineering",
    "formula": "features.fillna(0)",
    "explanation": "Generate market making features.\n\nArgs:\n    df: DataFrame with bid/ask/close data\n\nReturns:\n    DataFrame with market making features",
    "python_code": "def generate_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate market making features.\n\n        Args:\n            df: DataFrame with bid/ask/close data\n\n        Returns:\n            DataFrame with market making features\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n\n        # Get prices\n        close = df['close']\n        bid = df.get('bid', close - 0.00005)\n        ask = df.get('ask', close + 0.00005)\n        mid = (bid + ask) / 2\n\n        # =========================================================\n        # Spread Features\n        # =========================================================\n        spread = ask - bid\n        features['SPREAD'] = spread\n        features['SPREAD_PCT'] = spread / mid\n        features['SPREAD_BPS'] = spread / mid * 10000\n\n        # Spread relative to theoretical (Avellaneda-Stoikov)\n        # Estimate volatility\n        returns = close.pct_change()\n        vol = returns.rolling(20).std()\n\n        # Theoretical spread (simplified)\n        gamma = self.config.gamma\n        k = self.config.k\n        theoretical_spread = (2.0 / gamma) * np.log(1 + gamma / k) * mid\n\n        features['SPREAD_VS_THEORETICAL'] = spread / (theoretical_spread + 1e-10)\n\n        # =========================================================\n        # Inventory Estimation Features\n        # =========================================================\n        features['MM_INVENTORY_SIGNAL'] = self.estimate_market_maker_inventory(\n            bid, ask, mid\n        )\n\n        # Rolling inventory pressure\n        for w in [5, 10, 20]:\n            features[f'MM_INV_PRESSURE_{w}'] = features['MM_INVENTORY_SIGNAL'].rolling(w).mean()\n\n        # =========================================================\n        # Quote Quality Features\n        # =========================================================\n\n        # Bid-ask midpoint vs last close (microstructure)\n        features['MID_VS_CLOSE'] = mid - close.shift(1)\n\n        # Quote stability\n        features['BID_STABILITY'] = bid.rolling(10).std() / (spread + 1e-10)\n        features['ASK_STABILITY'] = ask.rolling(10).std() / (spread + 1e-10)\n\n        # =========================================================\n        # Optimal Quote Computation\n        # =========================================================\n\n        # Compute what AS model would suggest\n        # Assuming zero inventory (neutral market maker)\n        optimal_bids = []\n        optimal_asks = []\n\n        for i in range(len(df)):\n            m = mid.iloc[i]\n            # Use rolling volatility\n            if i >= 20:\n                self.model.config.sigma = vol.iloc[i] if not pd.isna(vol.iloc[i]) else 0.0002\n            else:\n                self.model.config.sigma = 0.0002\n\n            b, a = self.model.optimal_quotes(m, inventory=0, time_remaining=1.0)\n            optimal_bids.append(b)\n            optimal_asks.append(a)\n\n        features['AS_OPTIMAL_BID'] = optimal_bids\n        features['AS_OPTIM",
    "source_file": "core\\features\\academic_market_making.py",
    "academic_reference": "arXiv:1206.4810",
    "class_name": "MarketMakingFeatures"
  },
  {
    "name": "generate_all",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Generate all market making features.\n\nArgs:\n    df: DataFrame with OHLCV + bid/ask data\n\nReturns:\n    DataFrame with market making features",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all market making features.\n\n        Args:\n            df: DataFrame with OHLCV + bid/ask data\n\n        Returns:\n            DataFrame with market making features\n        \"\"\"\n        return self.mm_features.generate_features(df)",
    "source_file": "core\\features\\academic_market_making.py",
    "academic_reference": "arXiv:1206.4810",
    "class_name": "AcademicMarketMakingFeatures"
  },
  {
    "name": "generate_market_making_features",
    "category": "deep_learning",
    "formula": "generator.generate_all(df)",
    "explanation": "Convenience function to generate market making features.\n\nCitations:\n- Avellaneda & Stoikov (2008) - Optimal market making\n- Guant et al. (2012) - Inventory risk\n- Lehalle et al. (2013) - Market microstructure\n\nArgs:\n    df: DataFrame with price data\n    **kwargs: Additional arguments\n\nReturns:\n    DataFrame with market making features",
    "python_code": "def generate_market_making_features(df: pd.DataFrame, **kwargs) -> pd.DataFrame:\n    \"\"\"\n    Convenience function to generate market making features.\n\n    Citations:\n    - Avellaneda & Stoikov (2008) - Optimal market making\n    - Guant et al. (2012) - Inventory risk\n    - Lehalle et al. (2013) - Market microstructure\n\n    Args:\n        df: DataFrame with price data\n        **kwargs: Additional arguments\n\n    Returns:\n        DataFrame with market making features\n    \"\"\"\n    generator = AcademicMarketMakingFeatures()\n    return generator.generate_all(df)",
    "source_file": "core\\features\\academic_market_making.py",
    "academic_reference": "arXiv:1206.4810",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, config: MicrostructureConfig = None):\n        self.config = config or MicrostructureConfig()",
    "source_file": "core\\features\\academic_microstructure.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OrderFlowImbalance"
  },
  {
    "name": "compute_ofi",
    "category": "microstructure",
    "formula": "ofi.fillna(0)",
    "explanation": "Compute Order Flow Imbalance per Cont et al. (2014).\n\nArgs:\n    bid_price: Best bid price series\n    ask_price: Best ask price series\n    bid_volume: Volume at best bid\n    ask_volume: Volume at best ask\n\nReturns:\n    OFI series",
    "python_code": "def compute_ofi(\n        self,\n        bid_price: pd.Series,\n        ask_price: pd.Series,\n        bid_volume: pd.Series,\n        ask_volume: pd.Series\n    ) -> pd.Series:\n        \"\"\"\n        Compute Order Flow Imbalance per Cont et al. (2014).\n\n        Args:\n            bid_price: Best bid price series\n            ask_price: Best ask price series\n            bid_volume: Volume at best bid\n            ask_volume: Volume at best ask\n\n        Returns:\n            OFI series\n        \"\"\"\n        # Changes in volume\n        delta_bid_vol = bid_volume.diff()\n        delta_ask_vol = ask_volume.diff()\n\n        # Price improvement indicators\n        bid_improve = (bid_price.diff() >= 0).astype(float)\n        ask_improve = (ask_price.diff() <= 0).astype(float)\n\n        # OFI = bid contribution - ask contribution\n        # Eq (8) from Cont et al. (2014)\n        ofi = (delta_bid_vol * bid_improve) - (delta_ask_vol * ask_improve)\n\n        return ofi.fillna(0)",
    "source_file": "core\\features\\academic_microstructure.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OrderFlowImbalance"
  },
  {
    "name": "compute_ofi_from_trades",
    "category": "microstructure",
    "formula": "positive = buying pressure) | ofi",
    "explanation": "Compute OFI from trade data using Lee-Ready classification.\n\nLee, C.M.C., & Ready, M.J. (1991).\n\"Inferring Trade Direction from Intraday Data\"\nJournal of Finance, 46(2), 733-746.\n\nArgs:\n    price: Trade prices\n    volume: Trade volumes\n    bid: Best bid at trade time\n    ask: Best ask at trade time\n\nReturns:\n    OFI series (positive = buying pressure)",
    "python_code": "def compute_ofi_from_trades(\n        self,\n        price: pd.Series,\n        volume: pd.Series,\n        bid: pd.Series,\n        ask: pd.Series\n    ) -> pd.Series:\n        \"\"\"\n        Compute OFI from trade data using Lee-Ready classification.\n\n        Lee, C.M.C., & Ready, M.J. (1991).\n        \"Inferring Trade Direction from Intraday Data\"\n        Journal of Finance, 46(2), 733-746.\n\n        Args:\n            price: Trade prices\n            volume: Trade volumes\n            bid: Best bid at trade time\n            ask: Best ask at trade time\n\n        Returns:\n            OFI series (positive = buying pressure)\n        \"\"\"\n        mid = (bid + ask) / 2\n\n        # Lee-Ready classification\n        # Trade at ask = buy, trade at bid = sell\n        # Trade at mid = use tick rule\n        buy_indicator = np.where(\n            price > mid, 1,\n            np.where(price < mid, -1, np.sign(price.diff()))\n        )\n\n        # Signed volume\n        signed_volume = volume * buy_indicator\n\n        # Rolling OFI\n        ofi = pd.Series(signed_volume, index=price.index)\n\n        return ofi",
    "source_file": "core\\features\\academic_microstructure.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OrderFlowImbalance"
  },
  {
    "name": "estimate_lambda",
    "category": "microstructure",
    "formula": "P_t =  * OFI_t + _t | result",
    "explanation": "Estimate Kyle's lambda using OFI regression.\n\nP_t =  * OFI_t + _t\n\nCont et al. (2014) show  is inversely proportional to market depth:\n\"slope inversely proportional to the market depth\"\n\nArgs:\n    price_changes: Price change series\n    ofi: Order flow imbalance series\n    window: Rolling window for estimation\n\nReturns:\n    Rolling lambda estimates (price impact per unit OFI)",
    "python_code": "def estimate_lambda(\n        self,\n        price_changes: pd.Series,\n        ofi: pd.Series,\n        window: int = None\n    ) -> pd.Series:\n        \"\"\"\n        Estimate Kyle's lambda using OFI regression.\n\n        P_t =  * OFI_t + _t\n\n        Cont et al. (2014) show  is inversely proportional to market depth:\n        \"slope inversely proportional to the market depth\"\n\n        Args:\n            price_changes: Price change series\n            ofi: Order flow imbalance series\n            window: Rolling window for estimation\n\n        Returns:\n            Rolling lambda estimates (price impact per unit OFI)\n        \"\"\"\n        window = window or self.config.kyle_window\n\n        lambdas = []\n        for i in range(window, len(ofi)):\n            X = ofi.iloc[i-window:i].values.reshape(-1, 1)\n            y = price_changes.iloc[i-window:i].values\n\n            # OLS:  = (X'X)^(-1) X'y\n            XtX = np.dot(X.T, X)\n            if XtX > 1e-10:\n                lambda_est = np.dot(X.T, y) / XtX\n                lambdas.append(lambda_est[0])\n            else:\n                lambdas.append(np.nan)\n\n        # Pad beginning with NaN\n        result = pd.Series(\n            [np.nan] * window + lambdas,\n            index=ofi.index\n        )\n\n        return result",
    "source_file": "core\\features\\academic_microstructure.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OrderFlowImbalance"
  },
  {
    "name": "bulk_volume_classification",
    "category": "volatility",
    "formula": "buy_volume, sell_volume",
    "explanation": "Bulk Volume Classification (BVC) per Easley et al. (2012a).\n\n\"They advocate a 'bulk volume' classification strategy,\nreferred to as BV-VPIN.\"\n\nArgs:\n    open_price: Opening prices\n    close_price: Closing prices\n    volume: Total volume\n    sigma: Volatility (if None, computed from returns)\n\nReturns:\n    Tuple of (buy_volume, sell_volume)",
    "python_code": "def bulk_volume_classification(\n        self,\n        open_price: pd.Series,\n        close_price: pd.Series,\n        volume: pd.Series,\n        sigma: pd.Series = None\n    ) -> Tuple[pd.Series, pd.Series]:\n        \"\"\"\n        Bulk Volume Classification (BVC) per Easley et al. (2012a).\n\n        \"They advocate a 'bulk volume' classification strategy,\n        referred to as BV-VPIN.\"\n\n        Args:\n            open_price: Opening prices\n            close_price: Closing prices\n            volume: Total volume\n            sigma: Volatility (if None, computed from returns)\n\n        Returns:\n            Tuple of (buy_volume, sell_volume)\n        \"\"\"\n        # Compute returns\n        if sigma is None:\n            returns = np.log(close_price / open_price)\n            sigma = returns.rolling(20).std()\n            sigma = sigma.fillna(returns.std())\n\n        # Standardized price change\n        z = (close_price - open_price) / (sigma * open_price + 1e-10)\n\n        # Buy probability from CDF\n        buy_prob = pd.Series(norm.cdf(z), index=close_price.index)\n\n        # Classify volume\n        buy_volume = volume * buy_prob\n        sell_volume = volume * (1 - buy_prob)\n\n        return buy_volume, sell_volume",
    "source_file": "core\\features\\academic_microstructure.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VPIN"
  },
  {
    "name": "compute_vpin",
    "category": "microstructure",
    "formula": "higher = more toxic) | vpin",
    "explanation": "Compute VPIN metric.\n\n\"VPIN captures the market dynamics in event time, i.e., equal\nincrements of trading volume rather than calendar time.\"\n- Easley et al. (2012)\n\nArgs:\n    buy_volume: Classified buy volume\n    sell_volume: Classified sell volume\n    bucket_size: Volume per bucket\n    n_buckets: Number of buckets in rolling window\n\nReturns:\n    VPIN series (0-1, higher = more toxic)",
    "python_code": "def compute_vpin(\n        self,\n        buy_volume: pd.Series,\n        sell_volume: pd.Series,\n        bucket_size: int = None,\n        n_buckets: int = None\n    ) -> pd.Series:\n        \"\"\"\n        Compute VPIN metric.\n\n        \"VPIN captures the market dynamics in event time, i.e., equal\n        increments of trading volume rather than calendar time.\"\n        - Easley et al. (2012)\n\n        Args:\n            buy_volume: Classified buy volume\n            sell_volume: Classified sell volume\n            bucket_size: Volume per bucket\n            n_buckets: Number of buckets in rolling window\n\n        Returns:\n            VPIN series (0-1, higher = more toxic)\n        \"\"\"\n        bucket_size = bucket_size or self.config.vpin_bucket_size\n        n_buckets = n_buckets or self.config.vpin_n_buckets\n\n        total_volume = buy_volume + sell_volume\n\n        # Create volume buckets\n        cum_volume = total_volume.cumsum()\n        bucket_idx = (cum_volume // bucket_size).astype(int)\n\n        # Aggregate by bucket\n        df = pd.DataFrame({\n            'buy': buy_volume,\n            'sell': sell_volume,\n            'bucket': bucket_idx\n        })\n\n        bucket_agg = df.groupby('bucket').agg({\n            'buy': 'sum',\n            'sell': 'sum'\n        })\n\n        # Absolute imbalance per bucket\n        imbalance = np.abs(bucket_agg['buy'] - bucket_agg['sell'])\n\n        # Rolling VPIN\n        vpin_buckets = imbalance.rolling(n_buckets).sum() / (n_buckets * bucket_size)\n\n        # Map back to original index\n        vpin = bucket_idx.map(vpin_buckets).fillna(method='ffill')\n\n        return vpin",
    "source_file": "core\\features\\academic_microstructure.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VPIN"
  },
  {
    "name": "compute_vpin_fast",
    "category": "microstructure",
    "formula": "vpin",
    "explanation": "Fast VPIN approximation for tick data without OHLC.\n\nUses tick rule for trade classification.\n\nArgs:\n    close: Price series\n    volume: Volume series\n    window: Rolling window\n\nReturns:\n    VPIN approximation",
    "python_code": "def compute_vpin_fast(\n        self,\n        close: pd.Series,\n        volume: pd.Series,\n        window: int = 50\n    ) -> pd.Series:\n        \"\"\"\n        Fast VPIN approximation for tick data without OHLC.\n\n        Uses tick rule for trade classification.\n\n        Args:\n            close: Price series\n            volume: Volume series\n            window: Rolling window\n\n        Returns:\n            VPIN approximation\n        \"\"\"\n        # Tick rule classification\n        price_change = close.diff()\n        buy_indicator = np.sign(price_change).fillna(0)\n        buy_indicator = np.where(buy_indicator == 0, 1, buy_indicator)  # Ties go to buyer\n\n        buy_volume = volume * (buy_indicator == 1)\n        sell_volume = volume * (buy_indicator == -1)\n\n        # Rolling imbalance\n        total_vol = volume.rolling(window).sum()\n        net_vol = (buy_volume - sell_volume).rolling(window).sum()\n\n        vpin = np.abs(net_vol) / (total_vol + 1e-10)\n\n        return vpin",
    "source_file": "core\\features\\academic_microstructure.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VPIN"
  },
  {
    "name": "estimate_lambda_roll",
    "category": "microstructure",
    "formula": "positive = buying) | pd.Series(",
    "explanation": "Rolling estimation of Kyle's lambda.\n\nArgs:\n    price: Price series\n    order_flow: Signed order flow (positive = buying)\n    window: Rolling window\n\nReturns:\n    Rolling lambda estimates",
    "python_code": "def estimate_lambda_roll(\n        self,\n        price: pd.Series,\n        order_flow: pd.Series,\n        window: int = None\n    ) -> pd.Series:\n        \"\"\"\n        Rolling estimation of Kyle's lambda.\n\n        Args:\n            price: Price series\n            order_flow: Signed order flow (positive = buying)\n            window: Rolling window\n\n        Returns:\n            Rolling lambda estimates\n        \"\"\"\n        window = window or self.config.kyle_window\n\n        price_change = price.diff()\n\n        # Rolling regression: P =  * OrderFlow\n        lambdas = []\n        for i in range(window, len(price)):\n            X = order_flow.iloc[i-window:i].values\n            y = price_change.iloc[i-window:i].values\n\n            # Handle NaN\n            mask = ~(np.isnan(X) | np.isnan(y))\n            if mask.sum() < window // 2:\n                lambdas.append(np.nan)\n                continue\n\n            X, y = X[mask], y[mask]\n\n            # OLS\n            XtX = np.dot(X, X)\n            if XtX > 1e-10:\n                lambda_est = np.dot(X, y) / XtX\n                lambdas.append(lambda_est)\n            else:\n                lambdas.append(np.nan)\n\n        return pd.Series(\n            [np.nan] * window + lambdas,\n            index=price.index\n        )",
    "source_file": "core\\features\\academic_microstructure.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "KyleLambda"
  },
  {
    "name": "compute_market_depth",
    "category": "microstructure",
    "formula": "depth = 1 /  (inverse of price impact). | 1.0 / (lambda_.abs() + 1e-10)",
    "explanation": "Market depth = 1 /  (inverse of price impact).\n\n\"Market depth reflects the informativeness of the order flow\nand the size of trades by noise traders.\"\n- Kyle (1985)\n\nArgs:\n    lambda_: Kyle's lambda series\n\nReturns:\n    Market depth series",
    "python_code": "def compute_market_depth(self, lambda_: pd.Series) -> pd.Series:\n        \"\"\"\n        Market depth = 1 /  (inverse of price impact).\n\n        \"Market depth reflects the informativeness of the order flow\n        and the size of trades by noise traders.\"\n        - Kyle (1985)\n\n        Args:\n            lambda_: Kyle's lambda series\n\n        Returns:\n            Market depth series\n        \"\"\"\n        return 1.0 / (lambda_.abs() + 1e-10)",
    "source_file": "core\\features\\academic_microstructure.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "KyleLambda"
  },
  {
    "name": "compute_illiquidity",
    "category": "microstructure",
    "formula": "higher = less liquid) | illiq * 1e6",
    "explanation": "Compute Amihud illiquidity ratio.\n\nArgs:\n    returns: Return series\n    dollar_volume: Dollar volume (price * volume)\n    window: Rolling window\n\nReturns:\n    Illiquidity series (higher = less liquid)",
    "python_code": "def compute_illiquidity(\n        self,\n        returns: pd.Series,\n        dollar_volume: pd.Series,\n        window: int = None\n    ) -> pd.Series:\n        \"\"\"\n        Compute Amihud illiquidity ratio.\n\n        Args:\n            returns: Return series\n            dollar_volume: Dollar volume (price * volume)\n            window: Rolling window\n\n        Returns:\n            Illiquidity series (higher = less liquid)\n        \"\"\"\n        window = window or self.config.amihud_window\n\n        # |Return| / Dollar Volume\n        illiq_daily = returns.abs() / (dollar_volume + 1e-10)\n\n        # Rolling average\n        illiq = illiq_daily.rolling(window).mean()\n\n        # Scale by 1e6 for interpretability\n        return illiq * 1e6",
    "source_file": "core\\features\\academic_microstructure.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AmihudIlliquidity"
  },
  {
    "name": "compute_spread",
    "category": "microstructure",
    "formula": "pd.Series(spread, index=price.index)",
    "explanation": "Compute Roll spread estimator.\n\nArgs:\n    price: Price series\n    window: Rolling window for covariance\n\nReturns:\n    Estimated spread series",
    "python_code": "def compute_spread(\n        self,\n        price: pd.Series,\n        window: int = 20\n    ) -> pd.Series:\n        \"\"\"\n        Compute Roll spread estimator.\n\n        Args:\n            price: Price series\n            window: Rolling window for covariance\n\n        Returns:\n            Estimated spread series\n        \"\"\"\n        price_change = price.diff()\n\n        # Rolling covariance of consecutive price changes\n        cov = price_change.rolling(window).cov(price_change.shift(1))\n\n        # Spread = 2 * sqrt(-cov) if cov < 0\n        spread = np.where(cov < 0, 2 * np.sqrt(-cov), 0)\n\n        return pd.Series(spread, index=price.index)",
    "source_file": "core\\features\\academic_microstructure.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RollSpread"
  },
  {
    "name": "generate_all",
    "category": "feature_engineering",
    "formula": "features.fillna(0)",
    "explanation": "Generate all microstructure features.\n\nArgs:\n    df: DataFrame with OHLCV data\n    has_orderbook: Whether L2 orderbook data is available\n\nReturns:\n    DataFrame with microstructure features",
    "python_code": "def generate_all(\n        self,\n        df: pd.DataFrame,\n        has_orderbook: bool = False\n    ) -> pd.DataFrame:\n        \"\"\"\n        Generate all microstructure features.\n\n        Args:\n            df: DataFrame with OHLCV data\n            has_orderbook: Whether L2 orderbook data is available\n\n        Returns:\n            DataFrame with microstructure features\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n\n        # Required columns\n        close = df['close']\n        volume = df.get('volume', pd.Series(1, index=df.index))\n\n        # Optional columns\n        open_price = df.get('open', close.shift(1).fillna(close))\n        high = df.get('high', close)\n        low = df.get('low', close)\n        bid = df.get('bid', close - 0.00005)\n        ask = df.get('ask', close + 0.00005)\n\n        # =========================================================\n        # VPIN Features - Easley, Lpez de Prado, O'Hara (2012)\n        # =========================================================\n        buy_vol, sell_vol = self.vpin.bulk_volume_classification(\n            open_price, close, volume\n        )\n\n        features['VPIN_50'] = self.vpin.compute_vpin_fast(close, volume, 50)\n        features['VPIN_100'] = self.vpin.compute_vpin_fast(close, volume, 100)\n\n        # Volume imbalance (simplified VPIN)\n        features['VOL_IMBALANCE_20'] = (\n            (buy_vol - sell_vol).rolling(20).sum() /\n            (volume.rolling(20).sum() + 1e-10)\n        )\n\n        # =========================================================\n        # OFI Features - Cont, Kukanov, Stoikov (2014)\n        # =========================================================\n        ofi = self.ofi.compute_ofi_from_trades(close, volume, bid, ask)\n\n        for w in [5, 10, 20, 50]:\n            features[f'OFI_{w}'] = ofi.rolling(w).sum()\n            features[f'OFI_STD_{w}'] = ofi.rolling(w).std()\n\n        # Normalized OFI\n        features['OFI_NORM_20'] = (\n            ofi.rolling(20).sum() / (ofi.rolling(20).std() + 1e-10)\n        )\n\n        # =========================================================\n        # Kyle Lambda - Kyle (1985)\n        # =========================================================\n        order_flow = ofi.cumsum()  # Cumulative order flow\n\n        for w in [50, 100]:\n            lambda_est = self.kyle.estimate_lambda_roll(close, order_flow, w)\n            features[f'KYLE_LAMBDA_{w}'] = lambda_est\n            features[f'MARKET_DEPTH_{w}'] = self.kyle.compute_market_depth(lambda_est)\n\n        # =========================================================\n        # Amihud Illiquidity - Amihud (2002)\n        # =========================================================\n        returns = close.pct_change()\n        dollar_volume = close * volume\n\n        for w in [10, 20, 50]:\n            features[f'AMIHUD_{w}'] = self.amihud.compute_illiquidity(\n                returns, dollar_volume, w\n            )\n\n        # =========================================================\n       ",
    "source_file": "core\\features\\academic_microstructure.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AcademicMicrostructureFeatures"
  },
  {
    "name": "generate_microstructure_features",
    "category": "microstructure",
    "formula": "generator.generate_all(df, **kwargs)",
    "explanation": "Convenience function to generate microstructure features.\n\nCitations:\n- Cont et al. (2014) - Order Flow Imbalance\n- Easley et al. (2012) - VPIN\n- Kyle (1985) - Market Impact\n- Amihud (2002) - Illiquidity\n- Roll (1984) - Spread Estimation\n\nArgs:\n    df: DataFrame with price/volume data\n    **kwargs: Additional arguments\n\nReturns:\n    DataFrame with microstructure features",
    "python_code": "def generate_microstructure_features(df: pd.DataFrame, **kwargs) -> pd.DataFrame:\n    \"\"\"\n    Convenience function to generate microstructure features.\n\n    Citations:\n    - Cont et al. (2014) - Order Flow Imbalance\n    - Easley et al. (2012) - VPIN\n    - Kyle (1985) - Market Impact\n    - Amihud (2002) - Illiquidity\n    - Roll (1984) - Spread Estimation\n\n    Args:\n        df: DataFrame with price/volume data\n        **kwargs: Additional arguments\n\n    Returns:\n        DataFrame with microstructure features\n    \"\"\"\n    generator = AcademicMicrostructureFeatures()\n    return generator.generate_all(df, **kwargs)",
    "source_file": "core\\features\\academic_microstructure.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "regime",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, config: RegimeConfig = None):\n        self.config = config or RegimeConfig()\n        self.n_states = self.config.n_states\n\n        # Model parameters\n        self.means_ = None\n        self.covars_ = None\n        self.transmat_ = None\n        self.startprob_ = None\n\n        # State labels\n        self.state_labels_ = None\n\n        # Use hmmlearn if available\n        self._model = None\n        if HAS_HMMLEARN:\n            self._model = hmm.GaussianHMM(\n                n_components=self.n_states,\n                covariance_type=self.config.covariance_type,\n                n_iter=self.config.n_iter,\n                random_state=self.config.random_state\n            )",
    "source_file": "core\\features\\academic_regime.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "GaussianHMM"
  },
  {
    "name": "fit",
    "category": "machine_learning",
    "formula": "series using EM algorithm.",
    "explanation": "Fit HMM to return series using EM algorithm.\n\nThe EM algorithm alternates between:\n- E-step: Compute expected state occupancies\n- M-step: Update parameters (means, variances, transitions)\n\nArgs:\n    returns: Return series\n\nReturns:\n    self",
    "python_code": "def fit(self, returns: pd.Series) -> 'GaussianHMM':\n        \"\"\"\n        Fit HMM to return series using EM algorithm.\n\n        The EM algorithm alternates between:\n        - E-step: Compute expected state occupancies\n        - M-step: Update parameters (means, variances, transitions)\n\n        Args:\n            returns: Return series\n\n        Returns:\n            self\n        \"\"\"\n        X = returns.dropna().values.reshape(-1, 1)\n\n        if len(X) < 50:\n            raise ValueError(\"Insufficient data for HMM fitting (need >= 50 obs)\")\n\n        if HAS_HMMLEARN and self._model is not None:\n            self._model.fit(X)\n            self.means_ = self._model.means_.flatten()\n            self.covars_ = self._model.covars_.flatten()\n            self.transmat_ = self._model.transmat_\n            self.startprob_ = self._model.startprob_\n        else:\n            # Fallback: Simple GMM-based initialization\n            self._fit_simple(X)\n\n        # Label states by mean return\n        self._label_states()\n\n        return self",
    "source_file": "core\\features\\academic_regime.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "GaussianHMM"
  },
  {
    "name": "_fit_simple",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Simple HMM fitting without hmmlearn.\n\nUses K-means initialization and simplified EM.",
    "python_code": "def _fit_simple(self, X: np.ndarray):\n        \"\"\"\n        Simple HMM fitting without hmmlearn.\n\n        Uses K-means initialization and simplified EM.\n        \"\"\"\n        from sklearn.cluster import KMeans\n\n        # K-means initialization\n        kmeans = KMeans(\n            n_clusters=self.n_states,\n            random_state=self.config.random_state,\n            n_init=10\n        )\n        labels = kmeans.fit_predict(X)\n\n        # Compute means and variances per state\n        self.means_ = np.array([X[labels == k].mean() for k in range(self.n_states)])\n        self.covars_ = np.array([X[labels == k].var() + 1e-6 for k in range(self.n_states)])\n\n        # Estimate transition matrix from labels\n        self.transmat_ = np.zeros((self.n_states, self.n_states))\n        for i in range(len(labels) - 1):\n            self.transmat_[labels[i], labels[i+1]] += 1\n\n        # Normalize rows\n        self.transmat_ = self.transmat_ / (self.transmat_.sum(axis=1, keepdims=True) + 1e-10)\n\n        # Start probabilities from label frequencies\n        self.startprob_ = np.bincount(labels, minlength=self.n_states) / len(labels)",
    "source_file": "core\\features\\academic_regime.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "GaussianHMM"
  },
  {
    "name": "_label_states",
    "category": "regime",
    "formula": "",
    "explanation": "Label states as Bull, Neutral, Bear based on mean returns.\n\n\"Three hidden states representing bull, bear, and neutral\nmarket regimes.\"\n- Nystrup et al. (2016)",
    "python_code": "def _label_states(self):\n        \"\"\"\n        Label states as Bull, Neutral, Bear based on mean returns.\n\n        \"Three hidden states representing bull, bear, and neutral\n        market regimes.\"\n        - Nystrup et al. (2016)\n        \"\"\"\n        sorted_idx = np.argsort(self.means_)\n\n        self.state_labels_ = {}\n        if self.n_states >= 3:\n            self.state_labels_[sorted_idx[0]] = 'Bear'\n            self.state_labels_[sorted_idx[-1]] = 'Bull'\n            for i in sorted_idx[1:-1]:\n                self.state_labels_[i] = 'Neutral'\n        elif self.n_states == 2:\n            self.state_labels_[sorted_idx[0]] = 'Bear'\n            self.state_labels_[sorted_idx[1]] = 'Bull'",
    "source_file": "core\\features\\academic_regime.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "GaussianHMM"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "pd.Series(labels, index=returns.dropna().index)",
    "explanation": "Predict most likely regime sequence (Viterbi algorithm).\n\nThe Viterbi algorithm finds the most likely state sequence:\nS* = argmax_S P(S | r_1, ..., r_T)\n\nArgs:\n    returns: Return series\n\nReturns:\n    Series of regime labels",
    "python_code": "def predict(self, returns: pd.Series) -> pd.Series:\n        \"\"\"\n        Predict most likely regime sequence (Viterbi algorithm).\n\n        The Viterbi algorithm finds the most likely state sequence:\n        S* = argmax_S P(S | r_1, ..., r_T)\n\n        Args:\n            returns: Return series\n\n        Returns:\n            Series of regime labels\n        \"\"\"\n        X = returns.dropna().values.reshape(-1, 1)\n\n        if HAS_HMMLEARN and self._model is not None:\n            states = self._model.predict(X)\n        else:\n            states = self._predict_simple(X)\n\n        # Map to labels\n        labels = [self.state_labels_.get(s, 'Unknown') for s in states]\n\n        return pd.Series(labels, index=returns.dropna().index)",
    "source_file": "core\\features\\academic_regime.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "GaussianHMM"
  },
  {
    "name": "_predict_simple",
    "category": "machine_learning",
    "formula": "np.argmax(log_probs, axis=1)",
    "explanation": "Simple state prediction using maximum likelihood.",
    "python_code": "def _predict_simple(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Simple state prediction using maximum likelihood.\"\"\"\n        # Compute log-likelihood for each state\n        log_probs = np.zeros((len(X), self.n_states))\n\n        for k in range(self.n_states):\n            log_probs[:, k] = stats.norm.logpdf(\n                X.flatten(),\n                loc=self.means_[k],\n                scale=np.sqrt(self.covars_[k])\n            )\n\n        return np.argmax(log_probs, axis=1)",
    "source_file": "core\\features\\academic_regime.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "GaussianHMM"
  },
  {
    "name": "predict_proba",
    "category": "machine_learning",
    "formula": "S_t = k | r_1, ..., r_T) for each state k. | pd.DataFrame(",
    "explanation": "Compute probability of each regime (Forward-Backward algorithm).\n\nReturns P(S_t = k | r_1, ..., r_T) for each state k.\n\nArgs:\n    returns: Return series\n\nReturns:\n    DataFrame with regime probabilities",
    "python_code": "def predict_proba(self, returns: pd.Series) -> pd.DataFrame:\n        \"\"\"\n        Compute probability of each regime (Forward-Backward algorithm).\n\n        Returns P(S_t = k | r_1, ..., r_T) for each state k.\n\n        Args:\n            returns: Return series\n\n        Returns:\n            DataFrame with regime probabilities\n        \"\"\"\n        X = returns.dropna().values.reshape(-1, 1)\n\n        if HAS_HMMLEARN and self._model is not None:\n            probs = self._model.predict_proba(X)\n        else:\n            probs = self._predict_proba_simple(X)\n\n        # Create DataFrame with labeled columns\n        columns = [self.state_labels_.get(i, f'State_{i}') for i in range(self.n_states)]\n\n        return pd.DataFrame(\n            probs,\n            index=returns.dropna().index,\n            columns=columns\n        )",
    "source_file": "core\\features\\academic_regime.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "GaussianHMM"
  },
  {
    "name": "_predict_proba_simple",
    "category": "machine_learning",
    "formula": "probs",
    "explanation": "Simple probability estimation using Gaussian likelihood.",
    "python_code": "def _predict_proba_simple(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Simple probability estimation using Gaussian likelihood.\"\"\"\n        log_probs = np.zeros((len(X), self.n_states))\n\n        for k in range(self.n_states):\n            log_probs[:, k] = stats.norm.logpdf(\n                X.flatten(),\n                loc=self.means_[k],\n                scale=np.sqrt(self.covars_[k])\n            )\n\n        # Normalize to probabilities (softmax)\n        probs = np.exp(log_probs - logsumexp(log_probs, axis=1, keepdims=True))\n\n        return probs",
    "source_file": "core\\features\\academic_regime.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "GaussianHMM"
  },
  {
    "name": "get_regime_statistics",
    "category": "regime",
    "formula": "stats_dict",
    "explanation": "Get statistics for each regime.\n\nReturns:\n    Dictionary with mean, std, and label for each regime",
    "python_code": "def get_regime_statistics(self) -> Dict[str, Dict]:\n        \"\"\"\n        Get statistics for each regime.\n\n        Returns:\n            Dictionary with mean, std, and label for each regime\n        \"\"\"\n        stats_dict = {}\n\n        for i in range(self.n_states):\n            label = self.state_labels_.get(i, f'State_{i}')\n            stats_dict[label] = {\n                'state_id': i,\n                'mean_return': self.means_[i],\n                'volatility': np.sqrt(self.covars_[i]),\n                'sharpe_approx': self.means_[i] / (np.sqrt(self.covars_[i]) + 1e-10)\n            }\n\n        return stats_dict",
    "source_file": "core\\features\\academic_regime.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "GaussianHMM"
  },
  {
    "name": "fit",
    "category": "machine_learning",
    "formula": "series",
    "explanation": "Fit regime model to historical returns.\n\nArgs:\n    returns: Historical return series\n\nReturns:\n    self",
    "python_code": "def fit(self, returns: pd.Series) -> 'RegimeFeatures':\n        \"\"\"\n        Fit regime model to historical returns.\n\n        Args:\n            returns: Historical return series\n\n        Returns:\n            self\n        \"\"\"\n        self.hmm.fit(returns)\n        return self",
    "source_file": "core\\features\\academic_regime.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "RegimeFeatures"
  },
  {
    "name": "generate_features",
    "category": "regime",
    "formula": "features.fillna(method='ffill').fillna(0)",
    "explanation": "Generate regime-based features.\n\nArgs:\n    returns: Return series\n\nReturns:\n    DataFrame with regime features",
    "python_code": "def generate_features(self, returns: pd.Series) -> pd.DataFrame:\n        \"\"\"\n        Generate regime-based features.\n\n        Args:\n            returns: Return series\n\n        Returns:\n            DataFrame with regime features\n        \"\"\"\n        features = pd.DataFrame(index=returns.index)\n\n        # Predict regimes\n        regime_labels = self.hmm.predict(returns)\n        regime_probs = self.hmm.predict_proba(returns)\n\n        # =========================================================\n        # Regime Indicators\n        # =========================================================\n        features['REGIME_LABEL'] = regime_labels\n\n        # Binary indicators\n        features['IS_BULL'] = (regime_labels == 'Bull').astype(float)\n        features['IS_BEAR'] = (regime_labels == 'Bear').astype(float)\n        features['IS_NEUTRAL'] = (regime_labels == 'Neutral').astype(float)\n\n        # =========================================================\n        # Regime Probabilities\n        # =========================================================\n        for col in regime_probs.columns:\n            features[f'PROB_{col.upper()}'] = regime_probs[col]\n\n        # =========================================================\n        # Regime Duration\n        # =========================================================\n        # Count consecutive days in same regime\n        regime_change = (regime_labels != regime_labels.shift(1)).astype(int)\n        regime_group = regime_change.cumsum()\n\n        duration = regime_labels.groupby(regime_group).cumcount() + 1\n        features['REGIME_DURATION'] = duration\n\n        # =========================================================\n        # Regime Transition Features\n        # =========================================================\n        # Probability of leaving current regime\n        features['REGIME_STABILITY'] = features[[\n            c for c in features.columns if c.startswith('PROB_')\n        ]].max(axis=1)\n\n        # Regime momentum (increasing or decreasing probability)\n        for col in regime_probs.columns:\n            prob_col = f'PROB_{col.upper()}'\n            features[f'{prob_col}_MOM'] = features[prob_col].diff(5)\n\n        # =========================================================\n        # Regime-Adjusted Features\n        # =========================================================\n\n        # Position sizing multiplier based on regime\n        # Bull: 1.5x, Neutral: 1.0x, Bear: 0.5x\n        regime_multiplier = {\n            'Bull': 1.5,\n            'Neutral': 1.0,\n            'Bear': 0.5\n        }\n        features['REGIME_MULTIPLIER'] = regime_labels.map(regime_multiplier).fillna(1.0)\n\n        # Regime-adjusted volatility expectation\n        regime_stats = self.hmm.get_regime_statistics()\n        vol_map = {k: v['volatility'] for k, v in regime_stats.items()}\n        features['REGIME_EXPECTED_VOL'] = regime_labels.map(vol_map).fillna(\n            returns.std()\n        )\n\n        return features.fillna(method",
    "source_file": "core\\features\\academic_regime.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "RegimeFeatures"
  },
  {
    "name": "fit_predict",
    "category": "statistical",
    "formula": "= returns.iloc[i:i+1] | pd.DataFrame(results).set_index('date')",
    "explanation": "Perform rolling regime detection.\n\nArgs:\n    returns: Return series\n\nReturns:\n    DataFrame with regime predictions and features",
    "python_code": "def fit_predict(self, returns: pd.Series) -> pd.DataFrame:\n        \"\"\"\n        Perform rolling regime detection.\n\n        Args:\n            returns: Return series\n\n        Returns:\n            DataFrame with regime predictions and features\n        \"\"\"\n        window = self.config.lookback_window\n        results = []\n\n        for i in range(window, len(returns)):\n            # Refit periodically\n            if i == window or (i - self._last_fit_idx) >= self.refit_frequency:\n                train_data = returns.iloc[i-window:i]\n                try:\n                    self.current_hmm = GaussianHMM(self.config)\n                    self.current_hmm.fit(train_data)\n                    self._last_fit_idx = i\n                except Exception:\n                    pass  # Keep previous model\n\n            if self.current_hmm is None:\n                results.append({\n                    'date': returns.index[i],\n                    'regime': 'Unknown',\n                    'prob_bull': 0.33,\n                    'prob_neutral': 0.34,\n                    'prob_bear': 0.33\n                })\n                continue\n\n            # Predict current regime\n            current_return = returns.iloc[i:i+1]\n\n            try:\n                regime = self.current_hmm.predict(current_return).iloc[0]\n                probs = self.current_hmm.predict_proba(current_return).iloc[0]\n\n                results.append({\n                    'date': returns.index[i],\n                    'regime': regime,\n                    'prob_bull': probs.get('Bull', 0),\n                    'prob_neutral': probs.get('Neutral', 0),\n                    'prob_bear': probs.get('Bear', 0)\n                })\n            except Exception:\n                results.append({\n                    'date': returns.index[i],\n                    'regime': 'Unknown',\n                    'prob_bull': 0.33,\n                    'prob_neutral': 0.34,\n                    'prob_bear': 0.33\n                })\n\n        return pd.DataFrame(results).set_index('date')",
    "source_file": "core\\features\\academic_regime.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "RollingRegimeDetector"
  },
  {
    "name": "generate_all",
    "category": "machine_learning",
    "formula": "features",
    "explanation": "Generate all regime detection features.\n\nArgs:\n    df: DataFrame with price data\n    fit: Whether to fit the model (True) or use existing (False)\n\nReturns:\n    DataFrame with regime features",
    "python_code": "def generate_all(\n        self,\n        df: pd.DataFrame,\n        fit: bool = True\n    ) -> pd.DataFrame:\n        \"\"\"\n        Generate all regime detection features.\n\n        Args:\n            df: DataFrame with price data\n            fit: Whether to fit the model (True) or use existing (False)\n\n        Returns:\n            DataFrame with regime features\n        \"\"\"\n        close = df['close']\n        returns = close.pct_change().fillna(0)\n\n        if fit or not self._fitted:\n            try:\n                self.regime_detector.fit(returns)\n                self._fitted = True\n            except Exception as e:\n                # Return empty features if fitting fails\n                features = pd.DataFrame(index=df.index)\n                features['REGIME_LABEL'] = 'Unknown'\n                features['IS_BULL'] = 0.0\n                features['IS_BEAR'] = 0.0\n                features['IS_NEUTRAL'] = 1.0\n                features['REGIME_MULTIPLIER'] = 1.0\n                return features\n\n        return self.regime_detector.generate_features(returns)",
    "source_file": "core\\features\\academic_regime.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "AcademicRegimeFeatures"
  },
  {
    "name": "generate_regime_features",
    "category": "deep_learning",
    "formula": "generator.generate_all(df, **kwargs)",
    "explanation": "Convenience function to generate regime detection features.\n\nCitations:\n- Hamilton (1989) - Markov-switching models\n- Rydn et al. (1998) - HMM for financial data\n- Ang & Bekaert (2002) - Regime-based asset allocation\n\nArgs:\n    df: DataFrame with price data\n    **kwargs: Additional arguments\n\nReturns:\n    DataFrame with regime features",
    "python_code": "def generate_regime_features(df: pd.DataFrame, **kwargs) -> pd.DataFrame:\n    \"\"\"\n    Convenience function to generate regime detection features.\n\n    Citations:\n    - Hamilton (1989) - Markov-switching models\n    - Rydn et al. (1998) - HMM for financial data\n    - Ang & Bekaert (2002) - Regime-based asset allocation\n\n    Args:\n        df: DataFrame with price data\n        **kwargs: Additional arguments\n\n    Returns:\n        DataFrame with regime features\n    \"\"\"\n    generator = AcademicRegimeFeatures()\n    return generator.generate_all(df, **kwargs)",
    "source_file": "core\\features\\academic_regime.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "volatility",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, config: VolatilityConfig = None):\n        self.config = config or VolatilityConfig()",
    "source_file": "core\\features\\academic_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RealizedVolatility"
  },
  {
    "name": "compute_realized_vol",
    "category": "volatility",
    "formula": "rv",
    "explanation": "Compute realized volatility from returns.\n\nArgs:\n    returns: Return series\n    window: Rolling window\n    annualize: Whether to annualize\n\nReturns:\n    Realized volatility series",
    "python_code": "def compute_realized_vol(\n        self,\n        returns: pd.Series,\n        window: int = None,\n        annualize: bool = True\n    ) -> pd.Series:\n        \"\"\"\n        Compute realized volatility from returns.\n\n        Args:\n            returns: Return series\n            window: Rolling window\n            annualize: Whether to annualize\n\n        Returns:\n            Realized volatility series\n        \"\"\"\n        window = window or self.config.default_window\n\n        # Sum of squared returns\n        rv = returns.rolling(window).apply(\n            lambda x: np.sqrt(np.sum(x**2))\n        )\n\n        if annualize:\n            rv = rv * np.sqrt(self.config.annualization_factor / window)\n\n        return rv",
    "source_file": "core\\features\\academic_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RealizedVolatility"
  },
  {
    "name": "compute_har_components",
    "category": "volatility",
    "formula": "df",
    "explanation": "Compute HAR components (daily, weekly, monthly).\n\n\"Three primary volatility components can be identified: the\nshort-term traders with daily or higher trading frequency,\nthe medium-term investors who typically rebalance their\npositions weekly, and the long-term agents with a\ncharacteristic time of one or more months.\"\n- Corsi (2009)\n\nArgs:\n    rv_daily: Daily realized volatility series\n\nReturns:\n    DataFrame with RV_d, RV_w, RV_m columns",
    "python_code": "def compute_har_components(\n        self,\n        rv_daily: pd.Series\n    ) -> pd.DataFrame:\n        \"\"\"\n        Compute HAR components (daily, weekly, monthly).\n\n        \"Three primary volatility components can be identified: the\n        short-term traders with daily or higher trading frequency,\n        the medium-term investors who typically rebalance their\n        positions weekly, and the long-term agents with a\n        characteristic time of one or more months.\"\n        - Corsi (2009)\n\n        Args:\n            rv_daily: Daily realized volatility series\n\n        Returns:\n            DataFrame with RV_d, RV_w, RV_m columns\n        \"\"\"\n        df = pd.DataFrame(index=rv_daily.index)\n\n        # Daily component\n        df['RV_d'] = rv_daily\n\n        # Weekly component (5-day average)\n        df['RV_w'] = rv_daily.rolling(self.config.har_weekly).mean()\n\n        # Monthly component (22-day average)\n        df['RV_m'] = rv_daily.rolling(self.config.har_monthly).mean()\n\n        return df",
    "source_file": "core\\features\\academic_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HARRV"
  },
  {
    "name": "fit",
    "category": "volatility",
    "formula": "{'beta_0': 0, 'beta_d': 1, 'beta_w': 0, 'beta_m': 0}",
    "explanation": "Fit HAR-RV model using OLS.\n\nArgs:\n    rv_daily: Daily realized volatility\n    forecast_horizon: Forecast horizon (default: 1 day)\n\nReturns:\n    Dictionary of fitted coefficients",
    "python_code": "def fit(\n        self,\n        rv_daily: pd.Series,\n        forecast_horizon: int = 1\n    ) -> Dict[str, float]:\n        \"\"\"\n        Fit HAR-RV model using OLS.\n\n        Args:\n            rv_daily: Daily realized volatility\n            forecast_horizon: Forecast horizon (default: 1 day)\n\n        Returns:\n            Dictionary of fitted coefficients\n        \"\"\"\n        # Compute components\n        df = self.compute_har_components(rv_daily)\n\n        # Target: h-day ahead RV\n        df['RV_target'] = df['RV_d'].shift(-forecast_horizon)\n\n        # Drop NaN\n        df = df.dropna()\n\n        if len(df) < 50:\n            return {'beta_0': 0, 'beta_d': 1, 'beta_w': 0, 'beta_m': 0}\n\n        # OLS regression\n        X = df[['RV_d', 'RV_w', 'RV_m']].values\n        y = df['RV_target'].values\n\n        # Add constant\n        X = np.column_stack([np.ones(len(X)), X])\n\n        # OLS:  = (X'X)^(-1) X'y\n        try:\n            beta = np.linalg.lstsq(X, y, rcond=None)[0]\n        except np.linalg.LinAlgError:\n            beta = [0, 1, 0, 0]\n\n        self.coefficients = {\n            'beta_0': beta[0],\n            'beta_d': beta[1],\n            'beta_w': beta[2],\n            'beta_m': beta[3]\n        }\n\n        return self.coefficients",
    "source_file": "core\\features\\academic_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HARRV"
  },
  {
    "name": "predict",
    "category": "volatility",
    "formula": "forecast",
    "explanation": "Forecast volatility using fitted HAR model.\n\nArgs:\n    rv_daily: Daily realized volatility\n\nReturns:\n    Volatility forecast series",
    "python_code": "def predict(\n        self,\n        rv_daily: pd.Series\n    ) -> pd.Series:\n        \"\"\"\n        Forecast volatility using fitted HAR model.\n\n        Args:\n            rv_daily: Daily realized volatility\n\n        Returns:\n            Volatility forecast series\n        \"\"\"\n        if self.coefficients is None:\n            self.fit(rv_daily)\n\n        df = self.compute_har_components(rv_daily)\n\n        forecast = (\n            self.coefficients['beta_0'] +\n            self.coefficients['beta_d'] * df['RV_d'] +\n            self.coefficients['beta_w'] * df['RV_w'] +\n            self.coefficients['beta_m'] * df['RV_m']\n        )\n\n        return forecast",
    "source_file": "core\\features\\academic_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HARRV"
  },
  {
    "name": "parkinson",
    "category": "volatility",
    "formula": "_P = (1/4ln(2)) * (H - L) | _P = (1/4ln(2)) * (H - L) | of a common stock.\"",
    "explanation": "Parkinson (1980) Range Volatility Estimator.\n\nFormula:\n    _P = (1/4ln(2)) * (H - L)\n\nWhere H, L are log high/low prices.\n\n\"The extreme value method for estimating the variance of the\nrate of return of a common stock.\"\n- Parkinson (1980)\n\nEfficiency: 5.2x more efficient than close-to-close\n\nCitation:\n    Parkinson, M. (1980).\n    Journal of Business, 53(1), 61-65.\n\nArgs:\n    high: High prices\n    low: Low prices\n    window: Rolling window\n    annualize: Whether to annualize\n\nReturns:\n    Volatility series",
    "python_code": "def parkinson(\n        self,\n        high: pd.Series,\n        low: pd.Series,\n        window: int = None,\n        annualize: bool = True\n    ) -> pd.Series:\n        \"\"\"\n        Parkinson (1980) Range Volatility Estimator.\n\n        Formula:\n            _P = (1/4ln(2)) * (H - L)\n\n        Where H, L are log high/low prices.\n\n        \"The extreme value method for estimating the variance of the\n        rate of return of a common stock.\"\n        - Parkinson (1980)\n\n        Efficiency: 5.2x more efficient than close-to-close\n\n        Citation:\n            Parkinson, M. (1980).\n            Journal of Business, 53(1), 61-65.\n\n        Args:\n            high: High prices\n            low: Low prices\n            window: Rolling window\n            annualize: Whether to annualize\n\n        Returns:\n            Volatility series\n        \"\"\"\n        window = window or self.config.default_window\n\n        # Log high-low range\n        log_hl = np.log(high / low)\n\n        # Parkinson constant\n        factor = 1.0 / (4.0 * np.log(2))\n\n        # Rolling variance\n        var = factor * (log_hl ** 2).rolling(window).mean()\n\n        vol = np.sqrt(var)\n\n        if annualize:\n            vol = vol * np.sqrt(self.config.annualization_factor)\n\n        return vol",
    "source_file": "core\\features\\academic_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RangeBasedVolatility"
  },
  {
    "name": "garman_klass",
    "category": "volatility",
    "formula": "_GK = 0.5*(H-L) - (2ln(2)-1)*(C-O) | _GK = 0.5*(H-L) - (2ln(2)-1)*(C-O) | vol",
    "explanation": "Garman-Klass (1980) Volatility Estimator.\n\nFormula:\n    _GK = 0.5*(H-L) - (2ln(2)-1)*(C-O)\n\nWhere H, L, O, C are log prices.\n\n\"On the estimation of security price volatilities from\nhistorical data.\"\n- Garman & Klass (1980)\n\nEfficiency: 7.4x more efficient than close-to-close\nAssumption: No drift, continuous prices\n\nCitation:\n    Garman, M.B., & Klass, M.J. (1980).\n    Journal of Business, 53(1), 67-78.\n\nArgs:\n    open_: Open prices\n    high: High prices\n    low: Low prices\n    close: Close prices\n    window: Rolling window\n    annualize: Whether to annualize\n\nReturns:\n    Volatility series",
    "python_code": "def garman_klass(\n        self,\n        open_: pd.Series,\n        high: pd.Series,\n        low: pd.Series,\n        close: pd.Series,\n        window: int = None,\n        annualize: bool = True\n    ) -> pd.Series:\n        \"\"\"\n        Garman-Klass (1980) Volatility Estimator.\n\n        Formula:\n            _GK = 0.5*(H-L) - (2ln(2)-1)*(C-O)\n\n        Where H, L, O, C are log prices.\n\n        \"On the estimation of security price volatilities from\n        historical data.\"\n        - Garman & Klass (1980)\n\n        Efficiency: 7.4x more efficient than close-to-close\n        Assumption: No drift, continuous prices\n\n        Citation:\n            Garman, M.B., & Klass, M.J. (1980).\n            Journal of Business, 53(1), 67-78.\n\n        Args:\n            open_: Open prices\n            high: High prices\n            low: Low prices\n            close: Close prices\n            window: Rolling window\n            annualize: Whether to annualize\n\n        Returns:\n            Volatility series\n        \"\"\"\n        window = window or self.config.default_window\n\n        # Log prices\n        log_hl = np.log(high / low)\n        log_co = np.log(close / open_)\n\n        # Garman-Klass formula\n        term1 = 0.5 * (log_hl ** 2)\n        term2 = (2 * np.log(2) - 1) * (log_co ** 2)\n\n        var = (term1 - term2).rolling(window).mean()\n\n        # Handle negative variance (can happen with noisy data)\n        var = var.clip(lower=0)\n        vol = np.sqrt(var)\n\n        if annualize:\n            vol = vol * np.sqrt(self.config.annualization_factor)\n\n        return vol",
    "source_file": "core\\features\\academic_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RangeBasedVolatility"
  },
  {
    "name": "rogers_satchell",
    "category": "volatility",
    "formula": "_RS = (H-C)*(H-O) + (L-C)*(L-O) | _RS = (H-C)*(H-O) + (L-C)*(L-O) | vol",
    "explanation": "Rogers-Satchell (1991) Volatility Estimator.\n\nFormula:\n    _RS = (H-C)*(H-O) + (L-C)*(L-O)\n\nWhere H, L, O, C are log prices.\n\nKey Advantage: Handles drift (non-zero mean returns)\n\n\"Rogers and Satchell (1991) proposed a formula that allows\nfor drifts.\"\n- Yang & Zhang (2000)\n\nCitation:\n    Rogers, L.C.G., & Satchell, S.E. (1991).\n    Annals of Applied Probability, 1(4), 504-512.\n\nArgs:\n    open_: Open prices\n    high: High prices\n    low: Low prices\n    close: Close prices\n    window: Rolling window\n    annualize: Whether to annualize\n\nReturns:\n    Volatility series",
    "python_code": "def rogers_satchell(\n        self,\n        open_: pd.Series,\n        high: pd.Series,\n        low: pd.Series,\n        close: pd.Series,\n        window: int = None,\n        annualize: bool = True\n    ) -> pd.Series:\n        \"\"\"\n        Rogers-Satchell (1991) Volatility Estimator.\n\n        Formula:\n            _RS = (H-C)*(H-O) + (L-C)*(L-O)\n\n        Where H, L, O, C are log prices.\n\n        Key Advantage: Handles drift (non-zero mean returns)\n\n        \"Rogers and Satchell (1991) proposed a formula that allows\n        for drifts.\"\n        - Yang & Zhang (2000)\n\n        Citation:\n            Rogers, L.C.G., & Satchell, S.E. (1991).\n            Annals of Applied Probability, 1(4), 504-512.\n\n        Args:\n            open_: Open prices\n            high: High prices\n            low: Low prices\n            close: Close prices\n            window: Rolling window\n            annualize: Whether to annualize\n\n        Returns:\n            Volatility series\n        \"\"\"\n        window = window or self.config.default_window\n\n        # Log price ratios\n        log_ho = np.log(high / open_)\n        log_hc = np.log(high / close)\n        log_lo = np.log(low / open_)\n        log_lc = np.log(low / close)\n\n        # Rogers-Satchell formula\n        rs_var = (log_ho * log_hc) + (log_lo * log_lc)\n\n        var = rs_var.rolling(window).mean()\n        var = var.clip(lower=0)\n        vol = np.sqrt(var)\n\n        if annualize:\n            vol = vol * np.sqrt(self.config.annualization_factor)\n\n        return vol",
    "source_file": "core\\features\\academic_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RangeBasedVolatility"
  },
  {
    "name": "yang_zhang",
    "category": "volatility",
    "formula": "_YZ = _overnight + k*_open + (1-k)*_RS | _YZ = _overnight + k*_open + (1-k)*_RS | k = 0.34 / (1.34 + (n+1)/(n-1))",
    "explanation": "Yang-Zhang (2000) Volatility Estimator.\n\nFormula:\n    _YZ = _overnight + k*_open + (1-k)*_RS\n\nWhere:\n    k = 0.34 / (1.34 + (n+1)/(n-1))\n    _overnight = Var(log(O_t/C_{t-1}))\n    _open = Var(log(C_t/O_t))\n    _RS = Rogers-Satchell variance\n\nKey Advantage: Handles overnight jumps AND drift\n\n\"Yang and Zhang (2000) published a formula that is unbiased,\ndrift independent, and consistent in dealing with opening\njumps; this latter feature is unique among the formulas examined.\"\n- Bali & Weinbaum (2005)\n\nCitation:\n    Yang, D., & Zhang, Q. (2000).\n    Journal of Business, 73(3), 477-492.\n\nArgs:\n    open_: Open prices\n    high: High prices\n    low: Low prices\n    close: Close prices\n    window: Rolling window\n    annualize: Whether to annualize\n\nReturns:\n    Volatility series",
    "python_code": "def yang_zhang(\n        self,\n        open_: pd.Series,\n        high: pd.Series,\n        low: pd.Series,\n        close: pd.Series,\n        window: int = None,\n        annualize: bool = True\n    ) -> pd.Series:\n        \"\"\"\n        Yang-Zhang (2000) Volatility Estimator.\n\n        Formula:\n            _YZ = _overnight + k*_open + (1-k)*_RS\n\n        Where:\n            k = 0.34 / (1.34 + (n+1)/(n-1))\n            _overnight = Var(log(O_t/C_{t-1}))\n            _open = Var(log(C_t/O_t))\n            _RS = Rogers-Satchell variance\n\n        Key Advantage: Handles overnight jumps AND drift\n\n        \"Yang and Zhang (2000) published a formula that is unbiased,\n        drift independent, and consistent in dealing with opening\n        jumps; this latter feature is unique among the formulas examined.\"\n        - Bali & Weinbaum (2005)\n\n        Citation:\n            Yang, D., & Zhang, Q. (2000).\n            Journal of Business, 73(3), 477-492.\n\n        Args:\n            open_: Open prices\n            high: High prices\n            low: Low prices\n            close: Close prices\n            window: Rolling window\n            annualize: Whether to annualize\n\n        Returns:\n            Volatility series\n        \"\"\"\n        window = window or self.config.default_window\n\n        # Yang-Zhang k parameter\n        k = 0.34 / (1.34 + (window + 1) / (window - 1))\n\n        # Overnight volatility: O_t vs C_{t-1}\n        log_oc_overnight = np.log(open_ / close.shift(1))\n        overnight_var = log_oc_overnight.rolling(window).var()\n\n        # Open-to-close volatility\n        log_co = np.log(close / open_)\n        open_close_var = log_co.rolling(window).var()\n\n        # Rogers-Satchell component\n        rs_var = self.rogers_satchell(\n            open_, high, low, close, window, annualize=False\n        ) ** 2\n\n        # Yang-Zhang formula\n        var = overnight_var + k * open_close_var + (1 - k) * rs_var\n        var = var.clip(lower=0)\n        vol = np.sqrt(var)\n\n        if annualize:\n            vol = vol * np.sqrt(self.config.annualization_factor)\n\n        return vol",
    "source_file": "core\\features\\academic_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RangeBasedVolatility"
  },
  {
    "name": "close_to_close",
    "category": "volatility",
    "formula": " = std(log(C_t/C_{t-1})) | vol",
    "explanation": "Simple close-to-close volatility (baseline).\n\nFormula:\n     = std(log(C_t/C_{t-1}))\n\nUsed as baseline for efficiency comparison.\n\nArgs:\n    close: Close prices\n    window: Rolling window\n    annualize: Whether to annualize\n\nReturns:\n    Volatility series",
    "python_code": "def close_to_close(\n        self,\n        close: pd.Series,\n        window: int = None,\n        annualize: bool = True\n    ) -> pd.Series:\n        \"\"\"\n        Simple close-to-close volatility (baseline).\n\n        Formula:\n             = std(log(C_t/C_{t-1}))\n\n        Used as baseline for efficiency comparison.\n\n        Args:\n            close: Close prices\n            window: Rolling window\n            annualize: Whether to annualize\n\n        Returns:\n            Volatility series\n        \"\"\"\n        window = window or self.config.default_window\n\n        log_returns = np.log(close / close.shift(1))\n        vol = log_returns.rolling(window).std()\n\n        if annualize:\n            vol = vol * np.sqrt(self.config.annualization_factor)\n\n        return vol",
    "source_file": "core\\features\\academic_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RangeBasedVolatility"
  },
  {
    "name": "generate_all",
    "category": "volatility",
    "formula": "",
    "explanation": "Generate all volatility features.\n\nArgs:\n    df: DataFrame with OHLCV data\n\nReturns:\n    DataFrame with volatility features",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all volatility features.\n\n        Args:\n            df: DataFrame with OHLCV data\n\n        Returns:\n            DataFrame with volatility features\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n\n        # Required columns\n        close = df['close']\n\n        # Optional columns (use close if not available)\n        open_ = df.get('open', close.shift(1).fillna(close))\n        high = df.get('high', close)\n        low = df.get('low', close)\n\n        # =========================================================\n        # Close-to-Close (Baseline)\n        # =========================================================\n        for w in [5, 10, 20, 60]:\n            features[f'VOL_CC_{w}'] = self.range_based.close_to_close(close, w)\n\n        # =========================================================\n        # Parkinson (1980) - 5.2x efficient\n        # =========================================================\n        for w in [5, 10, 20, 60]:\n            features[f'VOL_PARKINSON_{w}'] = self.range_based.parkinson(high, low, w)\n\n        # =========================================================\n        # Garman-Klass (1980) - 7.4x efficient\n        # =========================================================\n        for w in [5, 10, 20, 60]:\n            features[f'VOL_GK_{w}'] = self.range_based.garman_klass(\n                open_, high, low, close, w\n            )\n\n        # =========================================================\n        # Rogers-Satchell (1991) - Drift independent\n        # =========================================================\n        for w in [5, 10, 20, 60]:\n            features[f'VOL_RS_{w}'] = self.range_based.rogers_satchell(\n                open_, high, low, close, w\n            )\n\n        # =========================================================\n        # Yang-Zhang (2000) - Handles overnight jumps\n        # =========================================================\n        for w in [10, 20, 60]:\n            features[f'VOL_YZ_{w}'] = self.range_based.yang_zhang(\n                open_, high, low, close, w\n            )\n\n        # =========================================================\n        # Realized Volatility\n        # =========================================================\n        returns = close.pct_change()\n        for w in [5, 10, 20, 60]:\n            features[f'RV_{w}'] = self.realized.compute_realized_vol(returns, w)\n\n        # =========================================================\n        # HAR-RV Components - Corsi (2009)\n        # =========================================================\n        rv_daily = self.realized.compute_realized_vol(returns, 1, annualize=False)\n\n        har_components = self.har.compute_har_components(rv_daily)\n        features['HAR_RV_D'] = har_components['RV_d']\n        features['HAR_RV_W'] = har_components['RV_w']\n        features['HAR_RV_M'] = har_components['RV_m']\n\n        # HAR fo",
    "source_file": "core\\features\\academic_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AcademicVolatilityFeatures"
  },
  {
    "name": "generate_volatility_features",
    "category": "volatility",
    "formula": "generator.generate_all(df)",
    "explanation": "Convenience function to generate volatility features.\n\nCitations:\n- Corsi (2009) - HAR-RV\n- Parkinson (1980) - Range volatility\n- Garman & Klass (1980) - OHLC volatility\n- Rogers & Satchell (1991) - Drift-independent\n- Yang & Zhang (2000) - Overnight jumps\n\nArgs:\n    df: DataFrame with OHLCV data\n    **kwargs: Additional arguments\n\nReturns:\n    DataFrame with volatility features",
    "python_code": "def generate_volatility_features(df: pd.DataFrame, **kwargs) -> pd.DataFrame:\n    \"\"\"\n    Convenience function to generate volatility features.\n\n    Citations:\n    - Corsi (2009) - HAR-RV\n    - Parkinson (1980) - Range volatility\n    - Garman & Klass (1980) - OHLC volatility\n    - Rogers & Satchell (1991) - Drift-independent\n    - Yang & Zhang (2000) - Overnight jumps\n\n    Args:\n        df: DataFrame with OHLCV data\n        **kwargs: Additional arguments\n\n    Returns:\n        DataFrame with volatility features\n    \"\"\"\n    generator = AcademicVolatilityFeatures()\n    return generator.generate_all(df)",
    "source_file": "core\\features\\academic_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "rank",
    "category": "alpha_factor",
    "formula": "x.rank(pct=True)",
    "explanation": "Cross-sectional rank (percentile).",
    "python_code": "def rank(x: pd.Series) -> pd.Series:\n        \"\"\"Cross-sectional rank (percentile).\"\"\"\n        return x.rank(pct=True)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "delta",
    "category": "alpha_factor",
    "formula": "x.diff(d)",
    "explanation": "Difference from d periods ago.",
    "python_code": "def delta(x: pd.Series, d: int = 1) -> pd.Series:\n        \"\"\"Difference from d periods ago.\"\"\"\n        return x.diff(d)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "delay",
    "category": "alpha_factor",
    "formula": "x.shift(d)",
    "explanation": "Lag by d periods.",
    "python_code": "def delay(x: pd.Series, d: int = 1) -> pd.Series:\n        \"\"\"Lag by d periods.\"\"\"\n        return x.shift(d)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "correlation",
    "category": "alpha_factor",
    "formula": "x.rolling(d).corr(y)",
    "explanation": "Rolling correlation.",
    "python_code": "def correlation(x: pd.Series, y: pd.Series, d: int) -> pd.Series:\n        \"\"\"Rolling correlation.\"\"\"\n        return x.rolling(d).corr(y)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "covariance",
    "category": "alpha_factor",
    "formula": "x.rolling(d).cov(y)",
    "explanation": "Rolling covariance.",
    "python_code": "def covariance(x: pd.Series, y: pd.Series, d: int) -> pd.Series:\n        \"\"\"Rolling covariance.\"\"\"\n        return x.rolling(d).cov(y)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "scale",
    "category": "alpha_factor",
    "formula": "x * a / x.abs().sum()",
    "explanation": "Scale to sum to a.",
    "python_code": "def scale(x: pd.Series, a: float = 1) -> pd.Series:\n        \"\"\"Scale to sum to a.\"\"\"\n        return x * a / x.abs().sum()",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "ts_rank",
    "category": "alpha_factor",
    "formula": "x.rolling(d).apply(lambda arr: pd.Series(arr).rank().iloc[-1] / len(arr), raw=False)",
    "explanation": "Time-series rank over d periods.",
    "python_code": "def ts_rank(x: pd.Series, d: int) -> pd.Series:\n        \"\"\"Time-series rank over d periods.\"\"\"\n        return x.rolling(d).apply(lambda arr: pd.Series(arr).rank().iloc[-1] / len(arr), raw=False)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "ts_max",
    "category": "alpha_factor",
    "formula": "x.rolling(d).max()",
    "explanation": "Rolling maximum.",
    "python_code": "def ts_max(x: pd.Series, d: int) -> pd.Series:\n        \"\"\"Rolling maximum.\"\"\"\n        return x.rolling(d).max()",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "ts_min",
    "category": "alpha_factor",
    "formula": "x.rolling(d).min()",
    "explanation": "Rolling minimum.",
    "python_code": "def ts_min(x: pd.Series, d: int) -> pd.Series:\n        \"\"\"Rolling minimum.\"\"\"\n        return x.rolling(d).min()",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "ts_argmax",
    "category": "alpha_factor",
    "formula": "x.rolling(d).apply(np.argmax, raw=True) + 1",
    "explanation": "Days since maximum.",
    "python_code": "def ts_argmax(x: pd.Series, d: int) -> pd.Series:\n        \"\"\"Days since maximum.\"\"\"\n        return x.rolling(d).apply(np.argmax, raw=True) + 1",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "ts_argmin",
    "category": "alpha_factor",
    "formula": "x.rolling(d).apply(np.argmin, raw=True) + 1",
    "explanation": "Days since minimum.",
    "python_code": "def ts_argmin(x: pd.Series, d: int) -> pd.Series:\n        \"\"\"Days since minimum.\"\"\"\n        return x.rolling(d).apply(np.argmin, raw=True) + 1",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "ts_sum",
    "category": "alpha_factor",
    "formula": "x.rolling(d).sum()",
    "explanation": "Rolling sum.",
    "python_code": "def ts_sum(x: pd.Series, d: int) -> pd.Series:\n        \"\"\"Rolling sum.\"\"\"\n        return x.rolling(d).sum()",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "ts_product",
    "category": "alpha_factor",
    "formula": "x.rolling(d).apply(np.prod, raw=True)",
    "explanation": "Rolling product.",
    "python_code": "def ts_product(x: pd.Series, d: int) -> pd.Series:\n        \"\"\"Rolling product.\"\"\"\n        return x.rolling(d).apply(np.prod, raw=True)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "stddev",
    "category": "alpha_factor",
    "formula": "x.rolling(d).std()",
    "explanation": "Rolling standard deviation.",
    "python_code": "def stddev(x: pd.Series, d: int) -> pd.Series:\n        \"\"\"Rolling standard deviation.\"\"\"\n        return x.rolling(d).std()",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "decay_linear",
    "category": "alpha_factor",
    "formula": "x.rolling(d).apply(lambda arr: np.dot(arr, weights) / weights.sum(), raw=True)",
    "explanation": "Linear decay weighted average.",
    "python_code": "def decay_linear(x: pd.Series, d: int) -> pd.Series:\n        \"\"\"Linear decay weighted average.\"\"\"\n        weights = np.arange(1, d + 1)\n        return x.rolling(d).apply(lambda arr: np.dot(arr, weights) / weights.sum(), raw=True)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "sign",
    "category": "alpha_factor",
    "formula": "np.sign(x)",
    "explanation": "Sign function.",
    "python_code": "def sign(x: pd.Series) -> pd.Series:\n        \"\"\"Sign function.\"\"\"\n        return np.sign(x)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "log",
    "category": "alpha_factor",
    "formula": "np.log(x.replace(0, np.nan))",
    "explanation": "Natural log.",
    "python_code": "def log(x: pd.Series) -> pd.Series:\n        \"\"\"Natural log.\"\"\"\n        return np.log(x.replace(0, np.nan))",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "abs_",
    "category": "alpha_factor",
    "formula": "x.abs()",
    "explanation": "Absolute value.",
    "python_code": "def abs_(x: pd.Series) -> pd.Series:\n        \"\"\"Absolute value.\"\"\"\n        return x.abs()",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "adv",
    "category": "alpha_factor",
    "formula": "volume.rolling(d).mean()",
    "explanation": "Average daily volume.",
    "python_code": "def adv(self, volume: pd.Series, d: int) -> pd.Series:\n        \"\"\"Average daily volume.\"\"\"\n        return volume.rolling(d).mean()",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha001",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "(rank(Ts_ArgMax(SignedPower(((returns < 0) ? stddev(returns, 20) : close), 2.), 5)) - 0.5)",
    "python_code": "def alpha001(self, close: pd.Series, returns: pd.Series) -> pd.Series:\n        \"\"\"(rank(Ts_ArgMax(SignedPower(((returns < 0) ? stddev(returns, 20) : close), 2.), 5)) - 0.5)\"\"\"\n        cond = returns < 0\n        inner = np.where(cond, self.stddev(returns, 20), close)\n        return self.rank(self.ts_argmax(pd.Series(inner, index=close.index) ** 2, 5)) - 0.5",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha002",
    "category": "alpha_factor",
    "formula": "-1 * self.correlation(",
    "explanation": "(-1 * correlation(rank(delta(log(volume), 2)), rank(((close - open) / open)), 6))",
    "python_code": "def alpha002(self, open_: pd.Series, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(-1 * correlation(rank(delta(log(volume), 2)), rank(((close - open) / open)), 6))\"\"\"\n        return -1 * self.correlation(\n            self.rank(self.delta(self.log(volume + 1), 2)),\n            self.rank((close - open_) / (open_ + 1e-8)),\n            6\n        )",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha003",
    "category": "alpha_factor",
    "formula": "-1 * self.correlation(self.rank(open_), self.rank(volume + 1), 10)",
    "explanation": "(-1 * correlation(rank(open), rank(volume), 10))",
    "python_code": "def alpha003(self, open_: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(-1 * correlation(rank(open), rank(volume), 10))\"\"\"\n        return -1 * self.correlation(self.rank(open_), self.rank(volume + 1), 10)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha004",
    "category": "alpha_factor",
    "formula": "-1 * self.ts_rank(self.rank(low), 9)",
    "explanation": "(-1 * Ts_Rank(rank(low), 9))",
    "python_code": "def alpha004(self, low: pd.Series) -> pd.Series:\n        \"\"\"(-1 * Ts_Rank(rank(low), 9))\"\"\"\n        return -1 * self.ts_rank(self.rank(low), 9)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha005",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "(rank((open - (sum(vwap, 10) / 10))) * (-1 * abs(rank((close - vwap)))))",
    "python_code": "def alpha005(self, open_: pd.Series, vwap: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"(rank((open - (sum(vwap, 10) / 10))) * (-1 * abs(rank((close - vwap)))))\"\"\"\n        return self.rank(open_ - self.ts_sum(vwap, 10) / 10) * (-1 * self.abs_(self.rank(close - vwap)))",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha006",
    "category": "alpha_factor",
    "formula": "-1 * self.correlation(open_, volume + 1, 10)",
    "explanation": "(-1 * correlation(open, volume, 10))",
    "python_code": "def alpha006(self, open_: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(-1 * correlation(open, volume, 10))\"\"\"\n        return -1 * self.correlation(open_, volume + 1, 10)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha007",
    "category": "alpha_factor",
    "formula": "np.where(",
    "explanation": "((adv20 < volume) ? ((-1 * ts_rank(abs(delta(close, 7)), 60)) * sign(delta(close, 7))) : (-1 * 1))",
    "python_code": "def alpha007(self, volume: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"((adv20 < volume) ? ((-1 * ts_rank(abs(delta(close, 7)), 60)) * sign(delta(close, 7))) : (-1 * 1))\"\"\"\n        adv20 = self.adv(volume, 20)\n        cond = adv20 < volume\n        return np.where(\n            cond,\n            -1 * self.ts_rank(self.abs_(self.delta(close, 7)), 60) * self.sign(self.delta(close, 7)),\n            -1\n        )",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha008",
    "category": "alpha_factor",
    "formula": "-1 * self.rank(inner - self.delay(inner, 10))",
    "explanation": "(-1 * rank(((sum(open, 5) * sum(returns, 5)) - delay((sum(open, 5) * sum(returns, 5)), 10))))",
    "python_code": "def alpha008(self, open_: pd.Series, returns: pd.Series) -> pd.Series:\n        \"\"\"(-1 * rank(((sum(open, 5) * sum(returns, 5)) - delay((sum(open, 5) * sum(returns, 5)), 10))))\"\"\"\n        inner = self.ts_sum(open_, 5) * self.ts_sum(returns, 5)\n        return -1 * self.rank(inner - self.delay(inner, 10))",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha009",
    "category": "alpha_factor",
    "formula": "np.where(cond1, delta_close, np.where(cond2, delta_close, -1 * delta_close))",
    "explanation": "((0 < ts_min(delta(close, 1), 5)) ? delta(close, 1) : ((ts_max(delta(close, 1), 5) < 0) ? delta(close, 1) : (-1 * delta(close, 1))))",
    "python_code": "def alpha009(self, close: pd.Series) -> pd.Series:\n        \"\"\"((0 < ts_min(delta(close, 1), 5)) ? delta(close, 1) : ((ts_max(delta(close, 1), 5) < 0) ? delta(close, 1) : (-1 * delta(close, 1))))\"\"\"\n        delta_close = self.delta(close, 1)\n        cond1 = self.ts_min(delta_close, 5) > 0\n        cond2 = self.ts_max(delta_close, 5) < 0\n        return np.where(cond1, delta_close, np.where(cond2, delta_close, -1 * delta_close))",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha010",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "rank(((0 < ts_min(delta(close, 1), 4)) ? delta(close, 1) : ((ts_max(delta(close, 1), 4) < 0) ? delta(close, 1) : (-1 * delta(close, 1)))))",
    "python_code": "def alpha010(self, close: pd.Series) -> pd.Series:\n        \"\"\"rank(((0 < ts_min(delta(close, 1), 4)) ? delta(close, 1) : ((ts_max(delta(close, 1), 4) < 0) ? delta(close, 1) : (-1 * delta(close, 1)))))\"\"\"\n        delta_close = self.delta(close, 1)\n        cond1 = self.ts_min(delta_close, 4) > 0\n        cond2 = self.ts_max(delta_close, 4) < 0\n        inner = np.where(cond1, delta_close, np.where(cond2, delta_close, -1 * delta_close))\n        return self.rank(pd.Series(inner, index=close.index))",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha011",
    "category": "alpha_factor",
    "formula": "(",
    "explanation": "((rank(ts_max((vwap - close), 3)) + rank(ts_min((vwap - close), 3))) * rank(delta(volume, 3)))",
    "python_code": "def alpha011(self, vwap: pd.Series, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"((rank(ts_max((vwap - close), 3)) + rank(ts_min((vwap - close), 3))) * rank(delta(volume, 3)))\"\"\"\n        return (\n            (self.rank(self.ts_max(vwap - close, 3)) + self.rank(self.ts_min(vwap - close, 3))) *\n            self.rank(self.delta(volume, 3))\n        )",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha012",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "(sign(delta(volume, 1)) * (-1 * delta(close, 1)))",
    "python_code": "def alpha012(self, volume: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"(sign(delta(volume, 1)) * (-1 * delta(close, 1)))\"\"\"\n        return self.sign(self.delta(volume, 1)) * (-1 * self.delta(close, 1))",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha013",
    "category": "alpha_factor",
    "formula": "-1 * self.rank(self.covariance(self.rank(close), self.rank(volume + 1), 5))",
    "explanation": "(-1 * rank(covariance(rank(close), rank(volume), 5)))",
    "python_code": "def alpha013(self, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(-1 * rank(covariance(rank(close), rank(volume), 5)))\"\"\"\n        return -1 * self.rank(self.covariance(self.rank(close), self.rank(volume + 1), 5))",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha014",
    "category": "alpha_factor",
    "formula": "-1 * self.rank(self.delta(returns, 3)) * self.correlation(open_, volume + 1, 10)",
    "explanation": "((-1 * rank(delta(returns, 3))) * correlation(open, volume, 10))",
    "python_code": "def alpha014(self, open_: pd.Series, volume: pd.Series, returns: pd.Series) -> pd.Series:\n        \"\"\"((-1 * rank(delta(returns, 3))) * correlation(open, volume, 10))\"\"\"\n        return -1 * self.rank(self.delta(returns, 3)) * self.correlation(open_, volume + 1, 10)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha015",
    "category": "alpha_factor",
    "formula": "-1 * self.ts_sum(self.rank(self.correlation(self.rank(high), self.rank(volume + 1), 3)), 3)",
    "explanation": "(-1 * sum(rank(correlation(rank(high), rank(volume), 3)), 3))",
    "python_code": "def alpha015(self, high: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(-1 * sum(rank(correlation(rank(high), rank(volume), 3)), 3))\"\"\"\n        return -1 * self.ts_sum(self.rank(self.correlation(self.rank(high), self.rank(volume + 1), 3)), 3)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha016",
    "category": "alpha_factor",
    "formula": "-1 * self.rank(self.covariance(self.rank(high), self.rank(volume + 1), 5))",
    "explanation": "(-1 * rank(covariance(rank(high), rank(volume), 5)))",
    "python_code": "def alpha016(self, high: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(-1 * rank(covariance(rank(high), rank(volume), 5)))\"\"\"\n        return -1 * self.rank(self.covariance(self.rank(high), self.rank(volume + 1), 5))",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha017",
    "category": "alpha_factor",
    "formula": "(",
    "explanation": "(((-1 * rank(ts_rank(close, 10))) * rank(delta(delta(close, 1), 1))) * rank(ts_rank((volume / adv20), 5)))",
    "python_code": "def alpha017(self, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(((-1 * rank(ts_rank(close, 10))) * rank(delta(delta(close, 1), 1))) * rank(ts_rank((volume / adv20), 5)))\"\"\"\n        adv20 = self.adv(volume, 20)\n        return (\n            -1 * self.rank(self.ts_rank(close, 10)) *\n            self.rank(self.delta(self.delta(close, 1), 1)) *\n            self.rank(self.ts_rank(volume / (adv20 + 1e-8), 5))\n        )",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha018",
    "category": "alpha_factor",
    "formula": "-1 * self.rank(",
    "explanation": "(-1 * rank(((stddev(abs((close - open)), 5) + (close - open)) + correlation(close, open, 10))))",
    "python_code": "def alpha018(self, close: pd.Series, open_: pd.Series) -> pd.Series:\n        \"\"\"(-1 * rank(((stddev(abs((close - open)), 5) + (close - open)) + correlation(close, open, 10))))\"\"\"\n        return -1 * self.rank(\n            self.stddev(self.abs_(close - open_), 5) +\n            (close - open_) +\n            self.correlation(close, open_, 10)\n        )",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha019",
    "category": "alpha_factor",
    "formula": "(",
    "explanation": "((-1 * sign(((close - delay(close, 7)) + delta(close, 7)))) * (1 + rank((1 + sum(returns, 250)))))",
    "python_code": "def alpha019(self, close: pd.Series, returns: pd.Series) -> pd.Series:\n        \"\"\"((-1 * sign(((close - delay(close, 7)) + delta(close, 7)))) * (1 + rank((1 + sum(returns, 250)))))\"\"\"\n        return (\n            -1 * self.sign((close - self.delay(close, 7)) + self.delta(close, 7)) *\n            (1 + self.rank(1 + self.ts_sum(returns, 250)))\n        )",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha020",
    "category": "alpha_factor",
    "formula": "(",
    "explanation": "(((-1 * rank((open - delay(high, 1)))) * rank((open - delay(close, 1)))) * rank((open - delay(low, 1))))",
    "python_code": "def alpha020(self, open_: pd.Series, high: pd.Series, close: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"(((-1 * rank((open - delay(high, 1)))) * rank((open - delay(close, 1)))) * rank((open - delay(low, 1))))\"\"\"\n        return (\n            -1 * self.rank(open_ - self.delay(high, 1)) *\n            self.rank(open_ - self.delay(close, 1)) *\n            self.rank(open_ - self.delay(low, 1))\n        )",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha021",
    "category": "alpha_factor",
    "formula": "np.where(cond1, -1, np.where(cond2, 1, np.where(cond3, 1, -1)))",
    "explanation": "((((sum(close, 8) / 8) + stddev(close, 8)) < (sum(close, 2) / 2)) ? (-1 * 1) : (((sum(close, 2) / 2) < ((sum(close, 8) / 8) - stddev(close, 8))) ? 1 : (((1 < (volume / adv20)) | ((volume / adv20) == 1)) ? 1 : (-1 * 1))))",
    "python_code": "def alpha021(self, volume: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"((((sum(close, 8) / 8) + stddev(close, 8)) < (sum(close, 2) / 2)) ? (-1 * 1) : (((sum(close, 2) / 2) < ((sum(close, 8) / 8) - stddev(close, 8))) ? 1 : (((1 < (volume / adv20)) | ((volume / adv20) == 1)) ? 1 : (-1 * 1))))\"\"\"\n        adv20 = self.adv(volume, 20)\n        mean8 = self.ts_sum(close, 8) / 8\n        std8 = self.stddev(close, 8)\n        mean2 = self.ts_sum(close, 2) / 2\n\n        cond1 = (mean8 + std8) < mean2\n        cond2 = mean2 < (mean8 - std8)\n        cond3 = (volume / (adv20 + 1e-8)) >= 1\n\n        return np.where(cond1, -1, np.where(cond2, 1, np.where(cond3, 1, -1)))",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha022",
    "category": "alpha_factor",
    "formula": "-1 * self.delta(self.correlation(high, volume + 1, 5), 5) * self.rank(self.stddev(close, 20))",
    "explanation": "(-1 * (delta(correlation(high, volume, 5), 5) * rank(stddev(close, 20))))",
    "python_code": "def alpha022(self, high: pd.Series, volume: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"(-1 * (delta(correlation(high, volume, 5), 5) * rank(stddev(close, 20))))\"\"\"\n        return -1 * self.delta(self.correlation(high, volume + 1, 5), 5) * self.rank(self.stddev(close, 20))",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha023",
    "category": "alpha_factor",
    "formula": "np.where(",
    "explanation": "(((sum(high, 20) / 20) < high) ? (-1 * delta(high, 2)) : 0)",
    "python_code": "def alpha023(self, high: pd.Series) -> pd.Series:\n        \"\"\"(((sum(high, 20) / 20) < high) ? (-1 * delta(high, 2)) : 0)\"\"\"\n        return np.where(\n            (self.ts_sum(high, 20) / 20) < high,\n            -1 * self.delta(high, 2),\n            0\n        )",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha024",
    "category": "alpha_factor",
    "formula": "np.where(cond, -1 * (close - self.ts_min(close, 100)), -1 * self.delta(close, 3))",
    "explanation": "((((delta((sum(close, 100) / 100), 100) / delay(close, 100)) < 0.05) | ((delta((sum(close, 100) / 100), 100) / delay(close, 100)) == 0.05)) ? (-1 * (close - ts_min(close, 100))) : (-1 * delta(close, 3)))",
    "python_code": "def alpha024(self, close: pd.Series) -> pd.Series:\n        \"\"\"((((delta((sum(close, 100) / 100), 100) / delay(close, 100)) < 0.05) | ((delta((sum(close, 100) / 100), 100) / delay(close, 100)) == 0.05)) ? (-1 * (close - ts_min(close, 100))) : (-1 * delta(close, 3)))\"\"\"\n        mean100 = self.ts_sum(close, 100) / 100\n        cond = (self.delta(mean100, 100) / (self.delay(close, 100) + 1e-8)) <= 0.05\n        return np.where(cond, -1 * (close - self.ts_min(close, 100)), -1 * self.delta(close, 3))",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha025",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "rank(((((-1 * returns) * adv20) * vwap) * (high - close)))",
    "python_code": "def alpha025(self, volume: pd.Series, returns: pd.Series, vwap: pd.Series, high: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"rank(((((-1 * returns) * adv20) * vwap) * (high - close)))\"\"\"\n        adv20 = self.adv(volume, 20)\n        return self.rank(-1 * returns * adv20 * vwap * (high - close))",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha026",
    "category": "alpha_factor",
    "formula": "-1 * self.ts_max(",
    "explanation": "(-1 * ts_max(correlation(ts_rank(volume, 5), ts_rank(high, 5), 5), 3))",
    "python_code": "def alpha026(self, volume: pd.Series, high: pd.Series) -> pd.Series:\n        \"\"\"(-1 * ts_max(correlation(ts_rank(volume, 5), ts_rank(high, 5), 5), 3))\"\"\"\n        return -1 * self.ts_max(\n            self.correlation(self.ts_rank(volume + 1, 5), self.ts_rank(high, 5), 5),\n            3\n        )",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha027",
    "category": "alpha_factor",
    "formula": "np.where(self.rank(inner) > 0.5, -1, 1)",
    "explanation": "((0.5 < rank((sum(correlation(rank(volume), rank(vwap), 6), 2) / 2.0))) ? (-1 * 1) : 1)",
    "python_code": "def alpha027(self, volume: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"((0.5 < rank((sum(correlation(rank(volume), rank(vwap), 6), 2) / 2.0))) ? (-1 * 1) : 1)\"\"\"\n        inner = self.ts_sum(self.correlation(self.rank(volume + 1), self.rank(vwap), 6), 2) / 2\n        return np.where(self.rank(inner) > 0.5, -1, 1)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha028",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "scale(((correlation(adv20, low, 5) + ((high + low) / 2)) - close))",
    "python_code": "def alpha028(self, volume: pd.Series, high: pd.Series, low: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"scale(((correlation(adv20, low, 5) + ((high + low) / 2)) - close))\"\"\"\n        adv20 = self.adv(volume, 20)\n        return self.scale(self.correlation(adv20, low, 5) + (high + low) / 2 - close)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha029",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "(min(product(rank(rank(scale(log(sum(ts_min(rank(rank((-1 * rank(delta((close - 1), 5))))), 2), 1))))), 1), 5) + ts_rank(delay((-1 * returns), 6), 5))",
    "python_code": "def alpha029(self, close: pd.Series, returns: pd.Series) -> pd.Series:\n        \"\"\"(min(product(rank(rank(scale(log(sum(ts_min(rank(rank((-1 * rank(delta((close - 1), 5))))), 2), 1))))), 1), 5) + ts_rank(delay((-1 * returns), 6), 5))\"\"\"\n        inner = -1 * self.rank(self.delta(close - 1, 5))\n        inner2 = self.ts_sum(self.ts_min(self.rank(self.rank(inner)), 2), 1)\n        inner3 = self.ts_product(self.rank(self.rank(self.scale(self.log(inner2 + 1)))), 1)\n        return self.ts_min(inner3, 5) + self.ts_rank(self.delay(-1 * returns, 6), 5)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha030",
    "category": "alpha_factor",
    "formula": "(1 - self.rank(sign_sum)) * self.ts_sum(volume, 5) / (self.ts_sum(volume, 20) + 1e-8)",
    "explanation": "(((1.0 - rank(((sign((close - delay(close, 1))) + sign((delay(close, 1) - delay(close, 2)))) + sign((delay(close, 2) - delay(close, 3)))))) * sum(volume, 5)) / sum(volume, 20))",
    "python_code": "def alpha030(self, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(((1.0 - rank(((sign((close - delay(close, 1))) + sign((delay(close, 1) - delay(close, 2)))) + sign((delay(close, 2) - delay(close, 3)))))) * sum(volume, 5)) / sum(volume, 20))\"\"\"\n        sign_sum = (\n            self.sign(close - self.delay(close, 1)) +\n            self.sign(self.delay(close, 1) - self.delay(close, 2)) +\n            self.sign(self.delay(close, 2) - self.delay(close, 3))\n        )\n        return (1 - self.rank(sign_sum)) * self.ts_sum(volume, 5) / (self.ts_sum(volume, 20) + 1e-8)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha031",
    "category": "alpha_factor",
    "formula": "(",
    "explanation": "((rank(rank(rank(decay_linear((-1 * rank(rank(delta(close, 10)))), 10)))) + rank((-1 * delta(close, 3)))) + sign(scale(correlation(adv20, low, 12))))",
    "python_code": "def alpha031(self, close: pd.Series, low: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"((rank(rank(rank(decay_linear((-1 * rank(rank(delta(close, 10)))), 10)))) + rank((-1 * delta(close, 3)))) + sign(scale(correlation(adv20, low, 12))))\"\"\"\n        adv20 = self.adv(volume, 20)\n        inner = self.decay_linear(-1 * self.rank(self.rank(self.delta(close, 10))), 10)\n        return (\n            self.rank(self.rank(self.rank(inner))) +\n            self.rank(-1 * self.delta(close, 3)) +\n            self.sign(self.scale(self.correlation(adv20, low, 12)))\n        )",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha032",
    "category": "alpha_factor",
    "formula": "(",
    "explanation": "(scale(((sum(close, 7) / 7) - close)) + (20 * scale(correlation(vwap, delay(close, 5), 230))))",
    "python_code": "def alpha032(self, close: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"(scale(((sum(close, 7) / 7) - close)) + (20 * scale(correlation(vwap, delay(close, 5), 230))))\"\"\"\n        return (\n            self.scale(self.ts_sum(close, 7) / 7 - close) +\n            20 * self.scale(self.correlation(vwap, self.delay(close, 5), 230))\n        )",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha033",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "rank((-1 * ((1 - (open / close))^1)))",
    "python_code": "def alpha033(self, open_: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"rank((-1 * ((1 - (open / close))^1)))\"\"\"\n        return self.rank(-1 * (1 - open_ / (close + 1e-8)))",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha034",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "rank(((1 - rank((stddev(returns, 2) / stddev(returns, 5)))) + (1 - rank(delta(close, 1)))))",
    "python_code": "def alpha034(self, close: pd.Series, returns: pd.Series) -> pd.Series:\n        \"\"\"rank(((1 - rank((stddev(returns, 2) / stddev(returns, 5)))) + (1 - rank(delta(close, 1)))))\"\"\"\n        return self.rank(\n            (1 - self.rank(self.stddev(returns, 2) / (self.stddev(returns, 5) + 1e-8))) +\n            (1 - self.rank(self.delta(close, 1)))\n        )",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha035",
    "category": "alpha_factor",
    "formula": "(",
    "explanation": "((Ts_Rank(volume, 32) * (1 - Ts_Rank(((close + high) - low), 16))) * (1 - Ts_Rank(returns, 32)))",
    "python_code": "def alpha035(self, volume: pd.Series, close: pd.Series, high: pd.Series, low: pd.Series, returns: pd.Series) -> pd.Series:\n        \"\"\"((Ts_Rank(volume, 32) * (1 - Ts_Rank(((close + high) - low), 16))) * (1 - Ts_Rank(returns, 32)))\"\"\"\n        return (\n            self.ts_rank(volume + 1, 32) *\n            (1 - self.ts_rank(close + high - low, 16)) *\n            (1 - self.ts_rank(returns, 32))\n        )",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha036",
    "category": "alpha_factor",
    "formula": "(",
    "explanation": "Long formula - simplified correlation-based signal",
    "python_code": "def alpha036(self, open_: pd.Series, close: pd.Series, volume: pd.Series, vwap: pd.Series, returns: pd.Series) -> pd.Series:\n        \"\"\"Long formula - simplified correlation-based signal\"\"\"\n        adv20 = self.adv(volume, 20)\n        return (\n            self.rank(self.correlation(close - open_, self.delay(volume, 1), 15)) *\n            self.rank(open_ - close) *\n            self.rank(self.ts_rank(self.delay(-1 * returns, 6), 5))\n        )",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha037",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "(rank(correlation(delay((open - close), 1), close, 200)) + rank((open - close)))",
    "python_code": "def alpha037(self, open_: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"(rank(correlation(delay((open - close), 1), close, 200)) + rank((open - close)))\"\"\"\n        return self.rank(self.correlation(self.delay(open_ - close, 1), close, 200)) + self.rank(open_ - close)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha038",
    "category": "alpha_factor",
    "formula": "-1 * self.rank(self.ts_rank(close, 10)) * self.rank(close / (open_ + 1e-8))",
    "explanation": "((-1 * rank(Ts_Rank(close, 10))) * rank((close / open)))",
    "python_code": "def alpha038(self, close: pd.Series, open_: pd.Series) -> pd.Series:\n        \"\"\"((-1 * rank(Ts_Rank(close, 10))) * rank((close / open)))\"\"\"\n        return -1 * self.rank(self.ts_rank(close, 10)) * self.rank(close / (open_ + 1e-8))",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha039",
    "category": "alpha_factor",
    "formula": "(",
    "explanation": "((-1 * rank((delta(close, 7) * (1 - rank(decay_linear((volume / adv20), 9)))))) * (1 + rank(sum(returns, 250))))",
    "python_code": "def alpha039(self, volume: pd.Series, close: pd.Series, returns: pd.Series) -> pd.Series:\n        \"\"\"((-1 * rank((delta(close, 7) * (1 - rank(decay_linear((volume / adv20), 9)))))) * (1 + rank(sum(returns, 250))))\"\"\"\n        adv20 = self.adv(volume, 20)\n        return (\n            -1 * self.rank(self.delta(close, 7) * (1 - self.rank(self.decay_linear(volume / (adv20 + 1e-8), 9)))) *\n            (1 + self.rank(self.ts_sum(returns, 250)))\n        )",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha040",
    "category": "alpha_factor",
    "formula": "-1 * self.rank(self.stddev(high, 10)) * self.correlation(high, volume + 1, 10)",
    "explanation": "((-1 * rank(stddev(high, 10))) * correlation(high, volume, 10))",
    "python_code": "def alpha040(self, high: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"((-1 * rank(stddev(high, 10))) * correlation(high, volume, 10))\"\"\"\n        return -1 * self.rank(self.stddev(high, 10)) * self.correlation(high, volume + 1, 10)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha041",
    "category": "alpha_factor",
    "formula": "np.sqrt(high * low) - vwap",
    "explanation": "(((high * low)^0.5) - vwap)",
    "python_code": "def alpha041(self, high: pd.Series, low: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"(((high * low)^0.5) - vwap)\"\"\"\n        return np.sqrt(high * low) - vwap",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha042",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "(rank((vwap - close)) / rank((vwap + close)))",
    "python_code": "def alpha042(self, vwap: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"(rank((vwap - close)) / rank((vwap + close)))\"\"\"\n        return self.rank(vwap - close) / (self.rank(vwap + close) + 1e-8)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha043",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "(ts_rank((volume / adv20), 20) * ts_rank((-1 * delta(close, 7)), 8))",
    "python_code": "def alpha043(self, volume: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"(ts_rank((volume / adv20), 20) * ts_rank((-1 * delta(close, 7)), 8))\"\"\"\n        adv20 = self.adv(volume, 20)\n        return self.ts_rank(volume / (adv20 + 1e-8), 20) * self.ts_rank(-1 * self.delta(close, 7), 8)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha044",
    "category": "alpha_factor",
    "formula": "-1 * self.correlation(high, self.rank(volume + 1), 5)",
    "explanation": "(-1 * correlation(high, rank(volume), 5))",
    "python_code": "def alpha044(self, high: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(-1 * correlation(high, rank(volume), 5))\"\"\"\n        return -1 * self.correlation(high, self.rank(volume + 1), 5)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha045",
    "category": "alpha_factor",
    "formula": "-1 * (",
    "explanation": "(-1 * ((rank((sum(delay(close, 5), 20) / 20)) * correlation(close, volume, 2)) * rank(correlation(sum(close, 5), sum(close, 20), 2))))",
    "python_code": "def alpha045(self, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(-1 * ((rank((sum(delay(close, 5), 20) / 20)) * correlation(close, volume, 2)) * rank(correlation(sum(close, 5), sum(close, 20), 2))))\"\"\"\n        return -1 * (\n            self.rank(self.ts_sum(self.delay(close, 5), 20) / 20) *\n            self.correlation(close, volume + 1, 2) *\n            self.rank(self.correlation(self.ts_sum(close, 5), self.ts_sum(close, 20), 2))\n        )",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha046",
    "category": "alpha_factor",
    "formula": "np.where(diff > 0.25, -1, np.where(diff < 0, 1, -1 * (close - self.delay(close, 1))))",
    "explanation": "((0.25 < (((delay(close, 20) - delay(close, 10)) / 10) - ((delay(close, 10) - close) / 10))) ? (-1 * 1) : (((((delay(close, 20) - delay(close, 10)) / 10) - ((delay(close, 10) - close) / 10)) < 0) ? 1 : ((-1 * 1) * (close - delay(close, 1)))))",
    "python_code": "def alpha046(self, close: pd.Series) -> pd.Series:\n        \"\"\"((0.25 < (((delay(close, 20) - delay(close, 10)) / 10) - ((delay(close, 10) - close) / 10))) ? (-1 * 1) : (((((delay(close, 20) - delay(close, 10)) / 10) - ((delay(close, 10) - close) / 10)) < 0) ? 1 : ((-1 * 1) * (close - delay(close, 1)))))\"\"\"\n        diff = (self.delay(close, 20) - self.delay(close, 10)) / 10 - (self.delay(close, 10) - close) / 10\n        return np.where(diff > 0.25, -1, np.where(diff < 0, 1, -1 * (close - self.delay(close, 1))))",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha047",
    "category": "alpha_factor",
    "formula": "(",
    "explanation": "((((rank((1 / close)) * volume) / adv20) * ((high * rank((high - close))) / (sum(high, 5) / 5))) - rank((vwap - delay(vwap, 5))))",
    "python_code": "def alpha047(self, volume: pd.Series, vwap: pd.Series, high: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"((((rank((1 / close)) * volume) / adv20) * ((high * rank((high - close))) / (sum(high, 5) / 5))) - rank((vwap - delay(vwap, 5))))\"\"\"\n        adv20 = self.adv(volume, 20)\n        return (\n            (self.rank(1 / (close + 1e-8)) * volume / (adv20 + 1e-8)) *\n            (high * self.rank(high - close) / (self.ts_sum(high, 5) / 5 + 1e-8)) -\n            self.rank(vwap - self.delay(vwap, 5))\n        )",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha048",
    "category": "alpha_factor",
    "formula": "corr * self.delta(close, 1) / (close + 1e-8)",
    "explanation": "(indneutralize(((correlation(delta(close, 1), delta(delay(close, 1), 1), 250) * delta(close, 1)) / close), IndClass.subindustry) / sum(((delta(close, 1) / delay(close, 1))^2), 250))",
    "python_code": "def alpha048(self, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(indneutralize(((correlation(delta(close, 1), delta(delay(close, 1), 1), 250) * delta(close, 1)) / close), IndClass.subindustry) / sum(((delta(close, 1) / delay(close, 1))^2), 250))\"\"\"\n        # Simplified without industry neutralization\n        corr = self.correlation(self.delta(close, 1), self.delta(self.delay(close, 1), 1), 250)\n        return corr * self.delta(close, 1) / (close + 1e-8)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha049",
    "category": "alpha_factor",
    "formula": "np.where(cond, 1, -1 * (close - self.delay(close, 1)))",
    "explanation": "Trend strength",
    "python_code": "def alpha049(self, close: pd.Series) -> pd.Series:\n        \"\"\"Trend strength\"\"\"\n        diff = self.delay(close, 20) - self.delay(close, 10)\n        cond = (diff / 10 - (self.delay(close, 10) - close) / 10) < -0.1\n        return np.where(cond, 1, -1 * (close - self.delay(close, 1)))",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha050",
    "category": "alpha_factor",
    "formula": "-1 * self.ts_max(self.correlation(self.rank(volume + 1), self.rank(vwap), 5), 5)",
    "explanation": "(-1 * ts_max(rank(correlation(rank(volume), rank(vwap), 5)), 5))",
    "python_code": "def alpha050(self, volume: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"(-1 * ts_max(rank(correlation(rank(volume), rank(vwap), 5)), 5))\"\"\"\n        return -1 * self.ts_max(self.correlation(self.rank(volume + 1), self.rank(vwap), 5), 5)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha051",
    "category": "alpha_factor",
    "formula": "np.where(diff < -0.05, 1, -1 * (close - self.delay(close, 1)))",
    "explanation": "Similar to alpha049 with different threshold",
    "python_code": "def alpha051(self, close: pd.Series) -> pd.Series:\n        \"\"\"Similar to alpha049 with different threshold\"\"\"\n        diff = (self.delay(close, 20) - self.delay(close, 10)) / 10 - (self.delay(close, 10) - close) / 10\n        return np.where(diff < -0.05, 1, -1 * (close - self.delay(close, 1)))",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha052",
    "category": "alpha_factor",
    "formula": "(",
    "explanation": "((((-1 * ts_min(low, 5)) + delay(ts_min(low, 5), 5)) * rank(((sum(returns, 240) - sum(returns, 20)) / 220))) * ts_rank(volume, 5))",
    "python_code": "def alpha052(self, returns: pd.Series, volume: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"((((-1 * ts_min(low, 5)) + delay(ts_min(low, 5), 5)) * rank(((sum(returns, 240) - sum(returns, 20)) / 220))) * ts_rank(volume, 5))\"\"\"\n        ts_min_low = self.ts_min(low, 5)\n        return (\n            (-1 * ts_min_low + self.delay(ts_min_low, 5)) *\n            self.rank((self.ts_sum(returns, 240) - self.ts_sum(returns, 20)) / 220) *\n            self.ts_rank(volume + 1, 5)\n        )",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha053",
    "category": "alpha_factor",
    "formula": "-1 * self.delta(inner, 9)",
    "explanation": "((-1 * delta((((close - low) - (high - close)) / (close - low)), 9)))",
    "python_code": "def alpha053(self, close: pd.Series, high: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"((-1 * delta((((close - low) - (high - close)) / (close - low)), 9)))\"\"\"\n        inner = ((close - low) - (high - close)) / (close - low + 1e-8)\n        return -1 * self.delta(inner, 9)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha054",
    "category": "alpha_factor",
    "formula": "-1 * (low - close) * (open_ ** 5) / ((low - high + 1e-8) * (close ** 5 + 1e-8))",
    "explanation": "((-1 * ((low - close) * (open^5))) / ((low - high) * (close^5)))",
    "python_code": "def alpha054(self, open_: pd.Series, close: pd.Series, high: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"((-1 * ((low - close) * (open^5))) / ((low - high) * (close^5)))\"\"\"\n        return -1 * (low - close) * (open_ ** 5) / ((low - high + 1e-8) * (close ** 5 + 1e-8))",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha055",
    "category": "alpha_factor",
    "formula": "-1 * self.correlation(self.rank(inner), self.rank(volume + 1), 6)",
    "explanation": "(-1 * correlation(rank(((close - ts_min(low, 12)) / (ts_max(high, 12) - ts_min(low, 12)))), rank(volume), 6))",
    "python_code": "def alpha055(self, high: pd.Series, low: pd.Series, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(-1 * correlation(rank(((close - ts_min(low, 12)) / (ts_max(high, 12) - ts_min(low, 12)))), rank(volume), 6))\"\"\"\n        inner = (close - self.ts_min(low, 12)) / (self.ts_max(high, 12) - self.ts_min(low, 12) + 1e-8)\n        return -1 * self.correlation(self.rank(inner), self.rank(volume + 1), 6)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha056",
    "category": "alpha_factor",
    "formula": "-1 * self.rank(self.ts_sum(returns, 10) / (self.ts_sum(self.ts_sum(returns, 2), 3) + 1e-8)) * self.rank(returns)",
    "explanation": "(0 - (1 * (rank((sum(returns, 10) / sum(sum(returns, 2), 3))) * rank((returns * cap)))))",
    "python_code": "def alpha056(self, returns: pd.Series) -> pd.Series:\n        \"\"\"(0 - (1 * (rank((sum(returns, 10) / sum(sum(returns, 2), 3))) * rank((returns * cap)))))\"\"\"\n        # Simplified without cap\n        return -1 * self.rank(self.ts_sum(returns, 10) / (self.ts_sum(self.ts_sum(returns, 2), 3) + 1e-8)) * self.rank(returns)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha057",
    "category": "alpha_factor",
    "formula": "-1 * (close - vwap) / (self.decay_linear(self.rank(self.ts_argmax(close, 30)), 2) + 1e-8)",
    "explanation": "(0 - (1 * ((close - vwap) / decay_linear(rank(ts_argmax(close, 30)), 2))))",
    "python_code": "def alpha057(self, close: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"(0 - (1 * ((close - vwap) / decay_linear(rank(ts_argmax(close, 30)), 2))))\"\"\"\n        return -1 * (close - vwap) / (self.decay_linear(self.rank(self.ts_argmax(close, 30)), 2) + 1e-8)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha058",
    "category": "alpha_factor",
    "formula": "-1 * self.ts_rank(self.decay_linear(self.correlation(vwap, volume + 1, 4), 8), 6)",
    "explanation": "Sector neutralized - simplified",
    "python_code": "def alpha058(self, volume: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"Sector neutralized - simplified\"\"\"\n        return -1 * self.ts_rank(self.decay_linear(self.correlation(vwap, volume + 1, 4), 8), 6)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha059",
    "category": "alpha_factor",
    "formula": "-1 * self.ts_rank(self.decay_linear(self.correlation(vwap, volume + 1, 5), 12), 8)",
    "explanation": "Similar to alpha058 with different parameters",
    "python_code": "def alpha059(self, volume: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"Similar to alpha058 with different parameters\"\"\"\n        return -1 * self.ts_rank(self.decay_linear(self.correlation(vwap, volume + 1, 5), 12), 8)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha060",
    "category": "alpha_factor",
    "formula": "-1 * (2 * self.scale(self.rank(inner)) - self.scale(self.rank(self.ts_argmax(close, 10))))",
    "explanation": "(0 - (1 * ((2 * scale(rank(((((close - low) - (high - close)) / (high - low)) * volume)))) - scale(rank(ts_argmax(close, 10))))))",
    "python_code": "def alpha060(self, close: pd.Series, high: pd.Series, low: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(0 - (1 * ((2 * scale(rank(((((close - low) - (high - close)) / (high - low)) * volume)))) - scale(rank(ts_argmax(close, 10))))))\"\"\"\n        inner = ((close - low) - (high - close)) / (high - low + 1e-8) * volume\n        return -1 * (2 * self.scale(self.rank(inner)) - self.scale(self.rank(self.ts_argmax(close, 10))))",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha061",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Volume-VWAP relationship",
    "python_code": "def alpha061(self, vwap: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"Volume-VWAP relationship\"\"\"\n        adv180 = self.adv(volume, 180)\n        return self.rank(vwap - self.ts_min(vwap, 16)) < self.rank(self.correlation(vwap, adv180, 18))",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "alpha062",
    "category": "alpha_factor",
    "formula": "-1 * self.correlation(high, self.rank(adv20), 5) * self.rank(open_ - (high + vwap) / 2)",
    "explanation": "Volume-high correlation",
    "python_code": "def alpha062(self, volume: pd.Series, high: pd.Series, open_: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"Volume-high correlation\"\"\"\n        adv20 = self.adv(volume, 20)\n        return -1 * self.correlation(high, self.rank(adv20), 5) * self.rank(open_ - (high + vwap) / 2)",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "generate_all_alphas",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Generate all 101 Alpha signals.\n\nArgs:\n    df: DataFrame with OHLCV columns\n\nReturns:\n    DataFrame with alpha signals added",
    "python_code": "def generate_all_alphas(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all 101 Alpha signals.\n\n        Args:\n            df: DataFrame with OHLCV columns\n\n        Returns:\n            DataFrame with alpha signals added\n        \"\"\"\n        result = df.copy()\n\n        # Extract columns\n        open_ = df.get('open', df.get('mid', df['close']))\n        high = df.get('high', df.get('ask', df['close']))\n        low = df.get('low', df.get('bid', df['close']))\n        close = df['close']\n        volume = df.get('volume', df.get('tick_count', pd.Series(1, index=df.index)))\n\n        # Calculate derived values\n        returns = close.pct_change()\n        vwap = (high + low + close) / 3  # Simplified VWAP\n\n        # Generate alphas (with error handling)\n        alpha_funcs = [\n            ('alpha001', lambda: self.alpha001(close, returns)),\n            ('alpha002', lambda: self.alpha002(open_, close, volume)),\n            ('alpha003', lambda: self.alpha003(open_, volume)),\n            ('alpha004', lambda: self.alpha004(low)),\n            ('alpha005', lambda: self.alpha005(open_, vwap, close)),\n            ('alpha006', lambda: self.alpha006(open_, volume)),\n            ('alpha007', lambda: self.alpha007(volume, close)),\n            ('alpha008', lambda: self.alpha008(open_, returns)),\n            ('alpha009', lambda: self.alpha009(close)),\n            ('alpha010', lambda: self.alpha010(close)),\n            ('alpha011', lambda: self.alpha011(vwap, close, volume)),\n            ('alpha012', lambda: self.alpha012(volume, close)),\n            ('alpha013', lambda: self.alpha013(close, volume)),\n            ('alpha014', lambda: self.alpha014(open_, volume, returns)),\n            ('alpha015', lambda: self.alpha015(high, volume)),\n            ('alpha016', lambda: self.alpha016(high, volume)),\n            ('alpha017', lambda: self.alpha017(close, volume)),\n            ('alpha018', lambda: self.alpha018(close, open_)),\n            ('alpha019', lambda: self.alpha019(close, returns)),\n            ('alpha020', lambda: self.alpha020(open_, high, close, low)),\n            ('alpha021', lambda: self.alpha021(volume, close)),\n            ('alpha022', lambda: self.alpha022(high, volume, close)),\n            ('alpha023', lambda: self.alpha023(high)),\n            ('alpha024', lambda: self.alpha024(close)),\n            ('alpha025', lambda: self.alpha025(volume, returns, vwap, high, close)),\n            ('alpha026', lambda: self.alpha026(volume, high)),\n            ('alpha027', lambda: self.alpha027(volume, vwap)),\n            ('alpha028', lambda: self.alpha028(volume, high, low, close)),\n            ('alpha029', lambda: self.alpha029(close, returns)),\n            ('alpha030', lambda: self.alpha030(close, volume)),\n            ('alpha031', lambda: self.alpha031(close, low, volume)),\n            ('alpha032', lambda: self.alpha032(close, vwap)),\n            ('alpha033', lambda: self.alpha033(open_, close)),\n            ('alpha034', lambda: self.alpha034(close, r",
    "source_file": "core\\features\\alpha101.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Complete"
  },
  {
    "name": "__init__",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Initialize Alpha158.\n\nArgs:\n    windows: Rolling windows for technical indicators.\n             Default: [5, 10, 20, 30, 60]",
    "python_code": "def __init__(self, windows: List[int] = None):\n        \"\"\"\n        Initialize Alpha158.\n\n        Args:\n            windows: Rolling windows for technical indicators.\n                     Default: [5, 10, 20, 30, 60]\n        \"\"\"\n        self.windows = windows or [5, 10, 20, 30, 60]",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha158"
  },
  {
    "name": "_ref",
    "category": "alpha_factor",
    "formula": "series.shift(n)",
    "explanation": "Reference value n periods ago.",
    "python_code": "def _ref(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"Reference value n periods ago.\"\"\"\n        return series.shift(n)",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha158"
  },
  {
    "name": "_delta",
    "category": "alpha_factor",
    "formula": "series.diff(n)",
    "explanation": "Difference from n periods ago.",
    "python_code": "def _delta(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"Difference from n periods ago.\"\"\"\n        return series.diff(n)",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha158"
  },
  {
    "name": "_mean",
    "category": "alpha_factor",
    "formula": "series.rolling(n, min_periods=1).mean()",
    "explanation": "Rolling mean.",
    "python_code": "def _mean(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"Rolling mean.\"\"\"\n        return series.rolling(n, min_periods=1).mean()",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha158"
  },
  {
    "name": "_std",
    "category": "alpha_factor",
    "formula": "series.rolling(n, min_periods=2).std()",
    "explanation": "Rolling standard deviation.",
    "python_code": "def _std(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"Rolling standard deviation.\"\"\"\n        return series.rolling(n, min_periods=2).std()",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha158"
  },
  {
    "name": "_max",
    "category": "alpha_factor",
    "formula": "series.rolling(n, min_periods=1).max()",
    "explanation": "Rolling maximum.",
    "python_code": "def _max(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"Rolling maximum.\"\"\"\n        return series.rolling(n, min_periods=1).max()",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha158"
  },
  {
    "name": "_min",
    "category": "alpha_factor",
    "formula": "series.rolling(n, min_periods=1).min()",
    "explanation": "Rolling minimum.",
    "python_code": "def _min(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"Rolling minimum.\"\"\"\n        return series.rolling(n, min_periods=1).min()",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha158"
  },
  {
    "name": "_sum",
    "category": "alpha_factor",
    "formula": "series.rolling(n, min_periods=1).sum()",
    "explanation": "Rolling sum.",
    "python_code": "def _sum(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"Rolling sum.\"\"\"\n        return series.rolling(n, min_periods=1).sum()",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha158"
  },
  {
    "name": "_corr",
    "category": "alpha_factor",
    "formula": "x.rolling(n, min_periods=2).corr(y)",
    "explanation": "Rolling correlation.",
    "python_code": "def _corr(x: pd.Series, y: pd.Series, n: int) -> pd.Series:\n        \"\"\"Rolling correlation.\"\"\"\n        return x.rolling(n, min_periods=2).corr(y)",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha158"
  },
  {
    "name": "_rank",
    "category": "alpha_factor",
    "formula": "0.5 | (arr[-1] > arr[:-1]).sum() / (len(arr) - 1) | series.rolling(n, min_periods=2).apply(rank_pct, raw=True)",
    "explanation": "Rolling rank (percentile position).",
    "python_code": "def _rank(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"Rolling rank (percentile position).\"\"\"\n        def rank_pct(arr):\n            if len(arr) < 2:\n                return 0.5\n            return (arr[-1] > arr[:-1]).sum() / (len(arr) - 1)\n        return series.rolling(n, min_periods=2).apply(rank_pct, raw=True)",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha158"
  },
  {
    "name": "_slope",
    "category": "alpha_factor",
    "formula": "0 | np.polyfit(x, arr, 1)[0] | series.rolling(n, min_periods=2).apply(calc_slope, raw=True)",
    "explanation": "Rolling slope (linear regression).",
    "python_code": "def _slope(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"Rolling slope (linear regression).\"\"\"\n        def calc_slope(arr):\n            if len(arr) < 2:\n                return 0\n            x = np.arange(len(arr))\n            return np.polyfit(x, arr, 1)[0]\n        return series.rolling(n, min_periods=2).apply(calc_slope, raw=True)",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha158"
  },
  {
    "name": "_rsquare",
    "category": "alpha_factor",
    "formula": "0 | corr ** 2 if not np.isnan(corr) else 0 | series.rolling(n, min_periods=2).apply(calc_rsq, raw=True)",
    "explanation": "Rolling R-squared of linear regression.",
    "python_code": "def _rsquare(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"Rolling R-squared of linear regression.\"\"\"\n        def calc_rsq(arr):\n            if len(arr) < 2:\n                return 0\n            x = np.arange(len(arr))\n            corr = np.corrcoef(x, arr)[0, 1]\n            return corr ** 2 if not np.isnan(corr) else 0\n        return series.rolling(n, min_periods=2).apply(calc_rsq, raw=True)",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha158"
  },
  {
    "name": "_resi",
    "category": "alpha_factor",
    "formula": "0 | arr[-1] - pred | series.rolling(n, min_periods=2).apply(calc_resi, raw=True)",
    "explanation": "Rolling residual from linear regression.",
    "python_code": "def _resi(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"Rolling residual from linear regression.\"\"\"\n        def calc_resi(arr):\n            if len(arr) < 2:\n                return 0\n            x = np.arange(len(arr))\n            coef = np.polyfit(x, arr, 1)\n            pred = coef[0] * (len(arr) - 1) + coef[1]\n            return arr[-1] - pred\n        return series.rolling(n, min_periods=2).apply(calc_resi, raw=True)",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha158"
  },
  {
    "name": "_quantile",
    "category": "alpha_factor",
    "formula": "series.rolling(n, min_periods=1).quantile(q)",
    "explanation": "Rolling quantile.",
    "python_code": "def _quantile(series: pd.Series, n: int, q: float) -> pd.Series:\n        \"\"\"Rolling quantile.\"\"\"\n        return series.rolling(n, min_periods=1).quantile(q)",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha158"
  },
  {
    "name": "_idxmax",
    "category": "alpha_factor",
    "formula": "(len(arr) - 1 - np.argmax(arr)) / len(arr) | series.rolling(n, min_periods=1).apply(idx_max, raw=True)",
    "explanation": "Days since rolling max.",
    "python_code": "def _idxmax(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"Days since rolling max.\"\"\"\n        def idx_max(arr):\n            return (len(arr) - 1 - np.argmax(arr)) / len(arr)\n        return series.rolling(n, min_periods=1).apply(idx_max, raw=True)",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha158"
  },
  {
    "name": "_idxmin",
    "category": "alpha_factor",
    "formula": "(len(arr) - 1 - np.argmin(arr)) / len(arr) | series.rolling(n, min_periods=1).apply(idx_min, raw=True)",
    "explanation": "Days since rolling min.",
    "python_code": "def _idxmin(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"Days since rolling min.\"\"\"\n        def idx_min(arr):\n            return (len(arr) - 1 - np.argmin(arr)) / len(arr)\n        return series.rolling(n, min_periods=1).apply(idx_min, raw=True)",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha158"
  },
  {
    "name": "_kbar_features",
    "category": "alpha_factor",
    "formula": "features",
    "explanation": "Generate KBAR features from OHLC data.",
    "python_code": "def _kbar_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Generate KBAR features from OHLC data.\"\"\"\n        o, h, l, c = df['open'], df['high'], df['low'], df['close']\n\n        features = pd.DataFrame(index=df.index)\n\n        # KMID: (close-open)/open\n        features['KMID'] = (c - o) / (o + 1e-12)\n\n        # KLEN: (high-low)/open\n        features['KLEN'] = (h - l) / (o + 1e-12)\n\n        # KMID2: (close-open)/(high-low)\n        features['KMID2'] = (c - o) / (h - l + 1e-12)\n\n        # KUP: (high-max(open,close))/open\n        features['KUP'] = (h - np.maximum(o, c)) / (o + 1e-12)\n\n        # KUP2: (high-max(open,close))/(high-low)\n        features['KUP2'] = (h - np.maximum(o, c)) / (h - l + 1e-12)\n\n        # KLOW: (min(open,close)-low)/open\n        features['KLOW'] = (np.minimum(o, c) - l) / (o + 1e-12)\n\n        # KLOW2: (min(open,close)-low)/(high-low)\n        features['KLOW2'] = (np.minimum(o, c) - l) / (h - l + 1e-12)\n\n        # KSFT: (2*close-high-low)/open\n        features['KSFT'] = (2 * c - h - l) / (o + 1e-12)\n\n        # KSFT2: (2*close-high-low)/(high-low)\n        features['KSFT2'] = (2 * c - h - l) / (h - l + 1e-12)\n\n        return features",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha158"
  },
  {
    "name": "_price_features",
    "category": "alpha_factor",
    "formula": "features",
    "explanation": "Generate normalized price features.",
    "python_code": "def _price_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Generate normalized price features.\"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n\n        for field in ['open', 'high', 'low', 'close']:\n            for i in range(5):  # Windows 0-4\n                if i == 0:\n                    features[f'{field.upper()}0'] = df[field] / (close + 1e-12)\n                else:\n                    features[f'{field.upper()}{i}'] = self._ref(df[field], i) / (close + 1e-12)\n\n        return features",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha158"
  },
  {
    "name": "_volume_features",
    "category": "alpha_factor",
    "formula": "features",
    "explanation": "Generate normalized volume features.",
    "python_code": "def _volume_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Generate normalized volume features.\"\"\"\n        features = pd.DataFrame(index=df.index)\n        vol = df['volume']\n\n        for i in range(5):  # Windows 0-4\n            if i == 0:\n                features[f'VOLUME0'] = vol / (vol + 1e-12)\n            else:\n                features[f'VOLUME{i}'] = self._ref(vol, i) / (vol + 1e-12)\n\n        return features",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha158"
  },
  {
    "name": "_rolling_features",
    "category": "alpha_factor",
    "formula": "features",
    "explanation": "Generate rolling statistical features.",
    "python_code": "def _rolling_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Generate rolling statistical features.\"\"\"\n        features = pd.DataFrame(index=df.index)\n\n        close = df['close']\n        high = df['high']\n        low = df['low']\n        volume = df['volume']\n\n        for d in self.windows:\n            # ROC: Rate of change\n            features[f'ROC{d}'] = self._ref(close, d) / (close + 1e-12)\n\n            # MA: Moving average\n            features[f'MA{d}'] = self._mean(close, d) / (close + 1e-12)\n\n            # STD: Standard deviation\n            features[f'STD{d}'] = self._std(close, d) / (close + 1e-12)\n\n            # BETA: Slope\n            features[f'BETA{d}'] = self._slope(close, d) / (close + 1e-12)\n\n            # RSQR: R-squared\n            features[f'RSQR{d}'] = self._rsquare(close, d)\n\n            # RESI: Residual\n            features[f'RESI{d}'] = self._resi(close, d) / (close + 1e-12)\n\n            # MAX: Rolling max\n            features[f'MAX{d}'] = self._max(high, d) / (close + 1e-12)\n\n            # MIN: Rolling min\n            features[f'MIN{d}'] = self._min(low, d) / (close + 1e-12)\n\n            # QTLU: 80th percentile\n            features[f'QTLU{d}'] = self._quantile(close, d, 0.8) / (close + 1e-12)\n\n            # QTLD: 20th percentile\n            features[f'QTLD{d}'] = self._quantile(close, d, 0.2) / (close + 1e-12)\n\n            # RANK: Percentile rank\n            features[f'RANK{d}'] = self._rank(close, d)\n\n            # RSV: Raw stochastic value\n            max_high = self._max(high, d)\n            min_low = self._min(low, d)\n            features[f'RSV{d}'] = (close - min_low) / (max_high - min_low + 1e-12)\n\n            # IMAX: Days since max\n            features[f'IMAX{d}'] = self._idxmax(high, d)\n\n            # IMIN: Days since min\n            features[f'IMIN{d}'] = self._idxmin(low, d)\n\n            # IMXD: Difference between max and min positions\n            features[f'IMXD{d}'] = features[f'IMAX{d}'] - features[f'IMIN{d}']\n\n            # CORR: Correlation between close and log volume\n            features[f'CORR{d}'] = self._corr(close, np.log(volume + 1), d)\n\n            # CORD: Correlation of returns and volume changes\n            ret = close / self._ref(close, 1) - 1\n            vol_ret = np.log(volume / self._ref(volume, 1) + 1)\n            features[f'CORD{d}'] = self._corr(ret, vol_ret, d)\n\n            # CNTP: Percentage of up days\n            up = (close > self._ref(close, 1)).astype(float)\n            features[f'CNTP{d}'] = self._mean(up, d)\n\n            # CNTN: Percentage of down days\n            down = (close < self._ref(close, 1)).astype(float)\n            features[f'CNTN{d}'] = self._mean(down, d)\n\n            # CNTD: Up days minus down days percentage\n            features[f'CNTD{d}'] = features[f'CNTP{d}'] - features[f'CNTN{d}']\n\n            # SUMP: Sum of gains / total absolute change (RSI-like)\n            gains = np.maximum(close - self._ref(close, 1), 0)\n            abs_change = np.abs(close - s",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha158"
  },
  {
    "name": "generate_all",
    "category": "alpha_factor",
    "formula": "features",
    "explanation": "Generate all 158 Alpha158 factors.\n\nArgs:\n    df: DataFrame with columns: open, high, low, close, volume\n\nReturns:\n    DataFrame with 158 factor columns",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all 158 Alpha158 factors.\n\n        Args:\n            df: DataFrame with columns: open, high, low, close, volume\n\n        Returns:\n            DataFrame with 158 factor columns\n        \"\"\"\n        # Ensure required columns\n        required = ['open', 'high', 'low', 'close', 'volume']\n        for col in required:\n            if col not in df.columns:\n                raise ValueError(f\"Missing required column: {col}\")\n\n        # Generate all feature groups\n        kbar = self._kbar_features(df)\n        price = self._price_features(df)\n        volume = self._volume_features(df)\n        rolling = self._rolling_features(df)\n\n        # Combine all features\n        features = pd.concat([kbar, price, volume, rolling], axis=1)\n\n        # Clean up\n        features = features.replace([np.inf, -np.inf], np.nan)\n        features = features.fillna(0)\n\n        return features",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha158"
  },
  {
    "name": "get_feature_names",
    "category": "alpha_factor",
    "formula": "names",
    "explanation": "Get list of all feature names.",
    "python_code": "def get_feature_names(self) -> List[str]:\n        \"\"\"Get list of all feature names.\"\"\"\n        names = []\n\n        # KBAR (9)\n        names.extend(['KMID', 'KLEN', 'KMID2', 'KUP', 'KUP2',\n                      'KLOW', 'KLOW2', 'KSFT', 'KSFT2'])\n\n        # Price (20)\n        for field in ['OPEN', 'HIGH', 'LOW', 'CLOSE']:\n            for i in range(5):\n                names.append(f'{field}{i}')\n\n        # Volume (5)\n        for i in range(5):\n            names.append(f'VOLUME{i}')\n\n        # Rolling (28 operators x 5 windows = 140)\n        operators = ['ROC', 'MA', 'STD', 'BETA', 'RSQR', 'RESI', 'MAX', 'MIN',\n                    'QTLU', 'QTLD', 'RANK', 'RSV', 'IMAX', 'IMIN', 'IMXD',\n                    'CORR', 'CORD', 'CNTP', 'CNTN', 'CNTD', 'SUMP', 'SUMN',\n                    'SUMD', 'VMA', 'VSTD', 'WVMA', 'VSUMP', 'VSUMN', 'VSUMD']\n\n        for op in operators:\n            for d in self.windows:\n                names.append(f'{op}{d}')\n\n        return names",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha158"
  },
  {
    "name": "generate_alpha158",
    "category": "alpha_factor",
    "formula": "alpha.generate_all(df)",
    "explanation": "Generate Alpha158 factors.\n\nArgs:\n    df: OHLCV DataFrame\n    windows: Rolling windows (default: [5, 10, 20, 30, 60])\n\nReturns:\n    DataFrame with 158 factors",
    "python_code": "def generate_alpha158(df: pd.DataFrame, windows: List[int] = None) -> pd.DataFrame:\n    \"\"\"\n    Generate Alpha158 factors.\n\n    Args:\n        df: OHLCV DataFrame\n        windows: Rolling windows (default: [5, 10, 20, 30, 60])\n\n    Returns:\n        DataFrame with 158 factors\n    \"\"\"\n    alpha = Alpha158(windows=windows)\n    return alpha.generate_all(df)",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": null
  },
  {
    "name": "rank_pct",
    "category": "alpha_factor",
    "formula": "0.5 | (arr[-1] > arr[:-1]).sum() / (len(arr) - 1)",
    "explanation": "",
    "python_code": "def rank_pct(arr):\n            if len(arr) < 2:\n                return 0.5\n            return (arr[-1] > arr[:-1]).sum() / (len(arr) - 1)",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": null
  },
  {
    "name": "calc_slope",
    "category": "alpha_factor",
    "formula": "0 | np.polyfit(x, arr, 1)[0]",
    "explanation": "",
    "python_code": "def calc_slope(arr):\n            if len(arr) < 2:\n                return 0\n            x = np.arange(len(arr))\n            return np.polyfit(x, arr, 1)[0]",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": null
  },
  {
    "name": "calc_rsq",
    "category": "alpha_factor",
    "formula": "0 | corr ** 2 if not np.isnan(corr) else 0",
    "explanation": "",
    "python_code": "def calc_rsq(arr):\n            if len(arr) < 2:\n                return 0\n            x = np.arange(len(arr))\n            corr = np.corrcoef(x, arr)[0, 1]\n            return corr ** 2 if not np.isnan(corr) else 0",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": null
  },
  {
    "name": "calc_resi",
    "category": "alpha_factor",
    "formula": "0 | arr[-1] - pred",
    "explanation": "",
    "python_code": "def calc_resi(arr):\n            if len(arr) < 2:\n                return 0\n            x = np.arange(len(arr))\n            coef = np.polyfit(x, arr, 1)\n            pred = coef[0] * (len(arr) - 1) + coef[1]\n            return arr[-1] - pred",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": null
  },
  {
    "name": "idx_max",
    "category": "alpha_factor",
    "formula": "(len(arr) - 1 - np.argmax(arr)) / len(arr)",
    "explanation": "",
    "python_code": "def idx_max(arr):\n            return (len(arr) - 1 - np.argmax(arr)) / len(arr)",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": null
  },
  {
    "name": "idx_min",
    "category": "alpha_factor",
    "formula": "(len(arr) - 1 - np.argmin(arr)) / len(arr)",
    "explanation": "",
    "python_code": "def idx_min(arr):\n            return (len(arr) - 1 - np.argmin(arr)) / len(arr)",
    "source_file": "core\\features\\alpha158.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Initialize Alpha360.\n\nArgs:\n    lookback: Number of days to look back (default: 60)",
    "python_code": "def __init__(self, lookback: int = 60):\n        \"\"\"\n        Initialize Alpha360.\n\n        Args:\n            lookback: Number of days to look back (default: 60)\n        \"\"\"\n        self.lookback = lookback",
    "source_file": "core\\features\\alpha360.py",
    "academic_reference": "Qlib (Microsoft) Alpha360 Extended Factors",
    "class_name": "Alpha360"
  },
  {
    "name": "generate_all",
    "category": "alpha_factor",
    "formula": "features",
    "explanation": "Generate all 360 Alpha360 factors.\n\nArgs:\n    df: DataFrame with columns: open, high, low, close, volume\n        Optional: vwap (will be calculated if missing)\n\nReturns:\n    DataFrame with 360 factor columns",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all 360 Alpha360 factors.\n\n        Args:\n            df: DataFrame with columns: open, high, low, close, volume\n                Optional: vwap (will be calculated if missing)\n\n        Returns:\n            DataFrame with 360 factor columns\n        \"\"\"\n        # Ensure required columns\n        required = ['open', 'high', 'low', 'close', 'volume']\n        for col in required:\n            if col not in df.columns:\n                raise ValueError(f\"Missing required column: {col}\")\n\n        features = pd.DataFrame(index=df.index)\n\n        close = df['close']\n        open_ = df['open']\n        high = df['high']\n        low = df['low']\n        volume = df['volume']\n\n        # VWAP (calculate if not present)\n        if 'vwap' in df.columns:\n            vwap = df['vwap']\n        else:\n            # Approximate VWAP as (high + low + close) / 3\n            vwap = (high + low + close) / 3\n\n        # Generate CLOSE features (60)\n        for i in range(self.lookback - 1, 0, -1):\n            features[f'CLOSE{i}'] = close.shift(i) / (close + 1e-12)\n        features['CLOSE0'] = close / (close + 1e-12)  # Always 1\n\n        # Generate OPEN features (60)\n        for i in range(self.lookback - 1, 0, -1):\n            features[f'OPEN{i}'] = open_.shift(i) / (close + 1e-12)\n        features['OPEN0'] = open_ / (close + 1e-12)\n\n        # Generate HIGH features (60)\n        for i in range(self.lookback - 1, 0, -1):\n            features[f'HIGH{i}'] = high.shift(i) / (close + 1e-12)\n        features['HIGH0'] = high / (close + 1e-12)\n\n        # Generate LOW features (60)\n        for i in range(self.lookback - 1, 0, -1):\n            features[f'LOW{i}'] = low.shift(i) / (close + 1e-12)\n        features['LOW0'] = low / (close + 1e-12)\n\n        # Generate VWAP features (60)\n        for i in range(self.lookback - 1, 0, -1):\n            features[f'VWAP{i}'] = vwap.shift(i) / (close + 1e-12)\n        features['VWAP0'] = vwap / (close + 1e-12)\n\n        # Generate VOLUME features (60)\n        for i in range(self.lookback - 1, 0, -1):\n            features[f'VOLUME{i}'] = volume.shift(i) / (volume + 1e-12)\n        features['VOLUME0'] = volume / (volume + 1e-12)  # Always 1\n\n        # Clean up\n        features = features.replace([np.inf, -np.inf], np.nan)\n        features = features.fillna(0)\n\n        # Reorder columns properly\n        ordered_cols = []\n        for field in ['CLOSE', 'OPEN', 'HIGH', 'LOW', 'VWAP', 'VOLUME']:\n            for i in range(self.lookback - 1, -1, -1):\n                ordered_cols.append(f'{field}{i}')\n\n        features = features[ordered_cols]\n\n        return features",
    "source_file": "core\\features\\alpha360.py",
    "academic_reference": "Qlib (Microsoft) Alpha360 Extended Factors",
    "class_name": "Alpha360"
  },
  {
    "name": "get_feature_names",
    "category": "alpha_factor",
    "formula": "names",
    "explanation": "Get list of all feature names.",
    "python_code": "def get_feature_names(self) -> List[str]:\n        \"\"\"Get list of all feature names.\"\"\"\n        names = []\n        for field in ['CLOSE', 'OPEN', 'HIGH', 'LOW', 'VWAP', 'VOLUME']:\n            for i in range(self.lookback - 1, -1, -1):\n                names.append(f'{field}{i}')\n        return names",
    "source_file": "core\\features\\alpha360.py",
    "academic_reference": "Qlib (Microsoft) Alpha360 Extended Factors",
    "class_name": "Alpha360"
  },
  {
    "name": "generate_all",
    "category": "alpha_factor",
    "formula": "alpha.generate_all(df)",
    "explanation": "Generate compact Alpha360 features.",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Generate compact Alpha360 features.\"\"\"\n        alpha = Alpha360(lookback=self.lookback)\n        return alpha.generate_all(df)",
    "source_file": "core\\features\\alpha360.py",
    "academic_reference": "Qlib (Microsoft) Alpha360 Extended Factors",
    "class_name": "Alpha360Compact"
  },
  {
    "name": "generate_alpha360",
    "category": "alpha_factor",
    "formula": "alpha.generate_all(df)",
    "explanation": "Generate Alpha360 factors.\n\nArgs:\n    df: OHLCV DataFrame\n    lookback: Days to look back (default: 60)\n\nReturns:\n    DataFrame with 360 factors (6 fields x 60 days)",
    "python_code": "def generate_alpha360(df: pd.DataFrame, lookback: int = 60) -> pd.DataFrame:\n    \"\"\"\n    Generate Alpha360 factors.\n\n    Args:\n        df: OHLCV DataFrame\n        lookback: Days to look back (default: 60)\n\n    Returns:\n        DataFrame with 360 factors (6 fields x 60 days)\n    \"\"\"\n    alpha = Alpha360(lookback=lookback)\n    return alpha.generate_all(df)",
    "source_file": "core\\features\\alpha360.py",
    "academic_reference": "Qlib (Microsoft) Alpha360 Extended Factors",
    "class_name": null
  },
  {
    "name": "generate_alpha360_compact",
    "category": "alpha_factor",
    "formula": "alpha.generate_all(df)",
    "explanation": "Generate compact Alpha360 for HFT.\n\nArgs:\n    df: OHLCV DataFrame\n    lookback: Periods to look back (default: 20)\n\nReturns:\n    DataFrame with 120 factors (6 fields x 20 periods)",
    "python_code": "def generate_alpha360_compact(df: pd.DataFrame, lookback: int = 20) -> pd.DataFrame:\n    \"\"\"\n    Generate compact Alpha360 for HFT.\n\n    Args:\n        df: OHLCV DataFrame\n        lookback: Periods to look back (default: 20)\n\n    Returns:\n        DataFrame with 120 factors (6 fields x 20 periods)\n    \"\"\"\n    alpha = Alpha360Compact(lookback=lookback)\n    return alpha.generate_all(df)",
    "source_file": "core\\features\\alpha360.py",
    "academic_reference": "Qlib (Microsoft) Alpha360 Extended Factors",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "microstructure",
    "formula": "",
    "explanation": "Initialize Asia FX Spread Features.\n\nArgs:\n    hkd_lower_band: HKD/USD lower band (strong side)\n    hkd_upper_band: HKD/USD upper band (weak side)\n    spread_window: Window for spread calculations\n    asian_session_start: Asian session start (UTC hour)\n    asian_session_end: Asian session end (UTC hour)",
    "python_code": "def __init__(\n        self,\n        hkd_lower_band: float = 7.75,\n        hkd_upper_band: float = 7.85,\n        spread_window: int = 20,\n        asian_session_start: int = 0,   # UTC midnight = 8 AM HK/SG\n        asian_session_end: int = 8,     # UTC 8 AM\n    ):\n        \"\"\"\n        Initialize Asia FX Spread Features.\n\n        Args:\n            hkd_lower_band: HKD/USD lower band (strong side)\n            hkd_upper_band: HKD/USD upper band (weak side)\n            spread_window: Window for spread calculations\n            asian_session_start: Asian session start (UTC hour)\n            asian_session_end: Asian session end (UTC hour)\n        \"\"\"\n        self.hkd_lower = hkd_lower_band\n        self.hkd_upper = hkd_upper_band\n        self.spread_window = spread_window\n        self.asian_start = asian_session_start\n        self.asian_end = asian_session_end",
    "source_file": "core\\features\\asia_fx_spread.py",
    "academic_reference": null,
    "class_name": "AsiaFXSpreadFeatures"
  },
  {
    "name": "_cnh_cny_features",
    "category": "microstructure",
    "formula": "features",
    "explanation": "CNH-CNY Spread Features.\n\nCNH (offshore yuan) and CNY (onshore yuan) often diverge due to:\n- Capital controls\n- Different liquidity conditions\n- PBOC intervention in onshore market\n\nReference:\nCheung et al. (2019). \"A Decade of RMB Internationalisation\"\n\nNote: For non-CNH pairs, we create proxy features based on\nprice divergence patterns that would be similar to CNH-CNY dynamics.",
    "python_code": "def _cnh_cny_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        CNH-CNY Spread Features.\n\n        CNH (offshore yuan) and CNY (onshore yuan) often diverge due to:\n        - Capital controls\n        - Different liquidity conditions\n        - PBOC intervention in onshore market\n\n        Reference:\n        Cheung et al. (2019). \"A Decade of RMB Internationalisation\"\n\n        Note: For non-CNH pairs, we create proxy features based on\n        price divergence patterns that would be similar to CNH-CNY dynamics.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n        volume = df.get('volume', pd.Series(1, index=df.index))\n\n        # 1. Spread proxy (price vs smoothed price as offshore/onshore analog)\n        # This mimics CNH-CNY divergence behavior\n        ema_fast = close.ewm(span=5, min_periods=1).mean()\n        ema_slow = close.ewm(span=20, min_periods=1).mean()\n        features['ASIA_SPREAD_diff'] = (ema_fast - ema_slow) / (ema_slow + 1e-10)\n\n        # 2. Spread mean-reversion signal\n        # CNH-CNY spread typically mean-reverts with 2-3 day half-life\n        spread = features['ASIA_SPREAD_diff']\n        spread_ma = spread.rolling(self.spread_window, min_periods=5).mean()\n        spread_std = spread.rolling(self.spread_window, min_periods=5).std()\n        features['ASIA_SPREAD_zscore'] = -(spread - spread_ma) / (spread_std + 1e-10)\n\n        # 3. Intervention probability proxy\n        # Large sudden moves suggest intervention\n        ret_vol = returns.rolling(self.spread_window, min_periods=2).std()\n        standardized = returns / (ret_vol + 1e-10)\n        intervention_proxy = (np.abs(standardized) > 2.5).rolling(5, min_periods=1).sum()\n        features['ASIA_INTV_prob'] = intervention_proxy / 5\n\n        # 4. Capital flow proxy (volume-price divergence)\n        # High volume + low price change = absorption (intervention-like)\n        vol_z = (volume - volume.rolling(self.spread_window, min_periods=5).mean()) / \\\n                (volume.rolling(self.spread_window, min_periods=5).std() + 1e-10)\n        ret_z = np.abs(standardized)\n        features['ASIA_FLOW_proxy'] = vol_z - ret_z\n\n        return features",
    "source_file": "core\\features\\asia_fx_spread.py",
    "academic_reference": null,
    "class_name": "AsiaFXSpreadFeatures"
  },
  {
    "name": "_hkd_peg_features",
    "category": "microstructure",
    "formula": "features",
    "explanation": "HKD Peg Band Features.\n\nHong Kong maintains a currency board with USD:\n- Strong-side: 7.75 (HKD strengthens, HKMA sells HKD/buys USD)\n- Weak-side: 7.85 (HKD weakens, HKMA buys HKD/sells USD)\n\nFor non-HKD pairs, creates band-like features based on price ranges.\n\nReference:\nHui et al. (2017). \"The Sustainability of Hong Kong's Currency Board\"",
    "python_code": "def _hkd_peg_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        HKD Peg Band Features.\n\n        Hong Kong maintains a currency board with USD:\n        - Strong-side: 7.75 (HKD strengthens, HKMA sells HKD/buys USD)\n        - Weak-side: 7.85 (HKD weakens, HKMA buys HKD/sells USD)\n\n        For non-HKD pairs, creates band-like features based on price ranges.\n\n        Reference:\n        Hui et al. (2017). \"The Sustainability of Hong Kong's Currency Board\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        high = df.get('high', close)\n        low = df.get('low', close)\n\n        # Define dynamic band based on rolling range (peg band analog)\n        range_low = close.rolling(60, min_periods=10).quantile(0.05)\n        range_high = close.rolling(60, min_periods=10).quantile(0.95)\n        band_width = range_high - range_low\n\n        # 1. Band position (where price is relative to range)\n        # 0 = at lower band (buy signal), 1 = at upper band (sell signal)\n        features['ASIA_BAND_pos'] = (close - range_low) / (band_width + 1e-10)\n\n        # 2. Distance to bands (potential for intervention/reversion)\n        dist_to_lower = (close - range_low) / (close + 1e-10)\n        dist_to_upper = (range_high - close) / (close + 1e-10)\n        features['ASIA_BAND_dist'] = np.minimum(dist_to_lower, dist_to_upper)\n\n        # 3. Band pressure (direction of pressure)\n        # Positive = pressure toward upper band, negative = toward lower\n        momentum = close.pct_change(5)\n        features['ASIA_BAND_pressure'] = momentum * (2 * features['ASIA_BAND_pos'] - 1)\n\n        # 4. Band touch probability\n        # How likely to touch a band based on current trajectory\n        vol = close.pct_change().rolling(20, min_periods=2).std()\n        time_to_band = features['ASIA_BAND_dist'] / (vol * np.sqrt(5) + 1e-10)\n        # Higher probability if closer and more volatile\n        features['ASIA_BAND_touch_prob'] = 1 / (1 + time_to_band)\n\n        return features",
    "source_file": "core\\features\\asia_fx_spread.py",
    "academic_reference": null,
    "class_name": "AsiaFXSpreadFeatures"
  },
  {
    "name": "_asian_session_features",
    "category": "microstructure",
    "formula": "accumulation | features",
    "explanation": "Asian Session Microstructure Features.\n\nBased on research showing distinct Asian session characteristics:\n- Lower liquidity but more information-driven trading\n- Trend continuation from NY close\n- Pre-European session positioning\n\nReference:\nRime & Schrimpf (2013). \"The Anatomy of the Global FX Market\"",
    "python_code": "def _asian_session_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Asian Session Microstructure Features.\n\n        Based on research showing distinct Asian session characteristics:\n        - Lower liquidity but more information-driven trading\n        - Trend continuation from NY close\n        - Pre-European session positioning\n\n        Reference:\n        Rime & Schrimpf (2013). \"The Anatomy of the Global FX Market\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n        volume = df.get('volume', pd.Series(1, index=df.index))\n\n        # 1. Session return accumulation\n        # Track returns over Asian session period (~8 hours)\n        features['ASIA_SESSION_ret'] = returns.rolling(8, min_periods=1).sum()\n\n        # 2. Session volume profile\n        # Normalized volume relative to recent average\n        vol_ma = volume.rolling(self.spread_window, min_periods=5).mean()\n        features['ASIA_SESSION_vol'] = volume / (vol_ma + 1e-10)\n\n        # 3. NY-to-Asia gap signal\n        # Price action from NY close to Asia open often reverses\n        gap = returns.rolling(5, min_periods=1).sum()\n        features['ASIA_NY_gap'] = -gap  # Contrarian signal\n\n        # 4. Pre-Europe positioning\n        # Build-up before European session\n        ret_recent = returns.rolling(3, min_periods=1).sum()\n        vol_recent = returns.abs().rolling(3, min_periods=1).sum()\n        features['ASIA_PREEUR_pos'] = ret_recent / (vol_recent + 1e-10)\n\n        return features",
    "source_file": "core\\features\\asia_fx_spread.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AsiaFXSpreadFeatures"
  },
  {
    "name": "_regional_dynamics_features",
    "category": "microstructure",
    "formula": "0 | np.corrcoef(x[:-1], x[1:])[0, 1] if len(x) > 2 else 0 | features",
    "explanation": "Regional Cross-Asian Currency Dynamics.\n\nAsian currencies often move together due to:\n- Similar economic exposures\n- Regional trade flows\n- Risk sentiment (risk-on/off dynamics)\n\nReference:\nBIS research on Asian currency co-movements.",
    "python_code": "def _regional_dynamics_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Regional Cross-Asian Currency Dynamics.\n\n        Asian currencies often move together due to:\n        - Similar economic exposures\n        - Regional trade flows\n        - Risk sentiment (risk-on/off dynamics)\n\n        Reference:\n        BIS research on Asian currency co-movements.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Regional beta proxy (sensitivity to regional moves)\n        # Rolling correlation with lagged self as stability measure\n        def rolling_autocorr(x):\n            if len(x) < 5:\n                return 0\n            return np.corrcoef(x[:-1], x[1:])[0, 1] if len(x) > 2 else 0\n\n        features['ASIA_REG_beta'] = returns.rolling(\n            self.spread_window, min_periods=5\n        ).apply(rolling_autocorr, raw=True)\n\n        # 2. Risk sentiment proxy (based on volatility)\n        vol = returns.rolling(10, min_periods=2).std()\n        vol_change = vol.pct_change(5)\n        # Rising vol = risk-off, falling vol = risk-on\n        features['ASIA_RISK_sent'] = -vol_change.clip(-1, 1)\n\n        # 3. Regional momentum (trend in Asian session)\n        # Persistent trends suggest regional flow\n        returns_sign = np.sign(returns)\n        consistency = returns_sign.rolling(10, min_periods=2).mean()\n        features['ASIA_REG_mom'] = consistency\n\n        return features",
    "source_file": "core\\features\\asia_fx_spread.py",
    "academic_reference": null,
    "class_name": "AsiaFXSpreadFeatures"
  },
  {
    "name": "generate_all",
    "category": "microstructure",
    "formula": "features",
    "explanation": "Generate all Asia FX Spread features.\n\nArgs:\n    df: DataFrame with columns: open, high, low, close, volume\n\nReturns:\n    DataFrame with 15 factor columns",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all Asia FX Spread features.\n\n        Args:\n            df: DataFrame with columns: open, high, low, close, volume\n\n        Returns:\n            DataFrame with 15 factor columns\n        \"\"\"\n        # Ensure required columns\n        if 'close' not in df.columns:\n            raise ValueError(\"Missing required column: 'close'\")\n\n        # Fill missing OHLC from close\n        df = df.copy()\n        if 'open' not in df.columns:\n            df['open'] = df['close'].shift(1).fillna(df['close'])\n        if 'high' not in df.columns:\n            df['high'] = df['close']\n        if 'low' not in df.columns:\n            df['low'] = df['close']\n        if 'volume' not in df.columns:\n            df['volume'] = 1\n\n        # Generate all factor groups\n        cnh_cny = self._cnh_cny_features(df)\n        hkd_peg = self._hkd_peg_features(df)\n        asian_session = self._asian_session_features(df)\n        regional = self._regional_dynamics_features(df)\n\n        # Combine all features\n        features = pd.concat([\n            cnh_cny, hkd_peg, asian_session, regional\n        ], axis=1)\n\n        # Clean up\n        features = features.replace([np.inf, -np.inf], np.nan)\n        features = features.fillna(0)\n\n        return features",
    "source_file": "core\\features\\asia_fx_spread.py",
    "academic_reference": null,
    "class_name": "AsiaFXSpreadFeatures"
  },
  {
    "name": "get_feature_names",
    "category": "microstructure",
    "formula": "names",
    "explanation": "Get list of all feature names.",
    "python_code": "def get_feature_names(self) -> List[str]:\n        \"\"\"Get list of all feature names.\"\"\"\n        names = []\n\n        # CNH-CNY Spread (4)\n        names.extend(['ASIA_SPREAD_diff', 'ASIA_SPREAD_zscore',\n                      'ASIA_INTV_prob', 'ASIA_FLOW_proxy'])\n\n        # HKD Peg Band (4)\n        names.extend(['ASIA_BAND_pos', 'ASIA_BAND_dist',\n                      'ASIA_BAND_pressure', 'ASIA_BAND_touch_prob'])\n\n        # Asian Session (4)\n        names.extend(['ASIA_SESSION_ret', 'ASIA_SESSION_vol',\n                      'ASIA_NY_gap', 'ASIA_PREEUR_pos'])\n\n        # Regional Dynamics (3)\n        names.extend(['ASIA_REG_beta', 'ASIA_RISK_sent', 'ASIA_REG_mom'])\n\n        return names",
    "source_file": "core\\features\\asia_fx_spread.py",
    "academic_reference": null,
    "class_name": "AsiaFXSpreadFeatures"
  },
  {
    "name": "get_citations",
    "category": "microstructure",
    "formula": "{",
    "explanation": "Get academic citations for Asian FX spread trading.",
    "python_code": "def get_citations() -> Dict[str, str]:\n        \"\"\"Get academic citations for Asian FX spread trading.\"\"\"\n        return {\n            'CNH_CNY': \"\"\"Cheung, Y.W. et al. (2019). \"A Decade of RMB\n                          Internationalisation\" Economic Policy.\n                          CNH-CNY spread mean-reverts with 2-3 day half-life.\"\"\",\n            'HKD_Peg': \"\"\"Hui, C.H. et al. (2017). \"The Sustainability of Hong Kong's\n                          Currency Board\" Journal of Banking & Finance.\n                          Analysis of HKD peg credibility and intervention.\"\"\",\n            'FX_Anatomy': \"\"\"Rime, D. & Schrimpf, A. (2013). \"The Anatomy of the\n                            Global FX Market Through the Lens of the 2013 Triennial\n                            Survey\" BIS Quarterly Review.\n                            Asian session microstructure patterns.\"\"\",\n            'VeighNa': \"\"\"VeighNa/vnpy Community (2024). \"Event-Driven Trading\n                         Framework\" Open-source quant platform from Taiwan/China.\"\"\"\n        }",
    "source_file": "core\\features\\asia_fx_spread.py",
    "academic_reference": null,
    "class_name": "AsiaFXSpreadFeatures"
  },
  {
    "name": "generate_asia_fx_features",
    "category": "microstructure",
    "formula": "features.generate_all(df)",
    "explanation": "Generate Asia FX Spread features.\n\nArgs:\n    df: OHLCV DataFrame\n\nReturns:\n    DataFrame with 15 Asian FX market factors",
    "python_code": "def generate_asia_fx_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generate Asia FX Spread features.\n\n    Args:\n        df: OHLCV DataFrame\n\n    Returns:\n        DataFrame with 15 Asian FX market factors\n    \"\"\"\n    features = AsiaFXSpreadFeatures()\n    return features.generate_all(df)",
    "source_file": "core\\features\\asia_fx_spread.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "rolling_autocorr",
    "category": "microstructure",
    "formula": "0 | np.corrcoef(x[:-1], x[1:])[0, 1] if len(x) > 2 else 0",
    "explanation": "",
    "python_code": "def rolling_autocorr(x):\n            if len(x) < 5:\n                return 0\n            return np.corrcoef(x[:-1], x[1:])[0, 1] if len(x) > 2 else 0",
    "source_file": "core\\features\\asia_fx_spread.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "statistical",
    "formula": "",
    "explanation": "Initialize Barra CNE6 Forex.\n\nArgs:\n    windows: Rolling windows (default: [5, 10, 20, 60])",
    "python_code": "def __init__(self, windows: List[int] = None):\n        \"\"\"\n        Initialize Barra CNE6 Forex.\n\n        Args:\n            windows: Rolling windows (default: [5, 10, 20, 60])\n        \"\"\"\n        self.windows = windows or [5, 10, 20, 60]",
    "source_file": "core\\features\\barra_cne6.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BarraCNE6Forex"
  },
  {
    "name": "_size_factors",
    "category": "volatility",
    "formula": "features",
    "explanation": "SIZE factors - magnitude of price movements.\nIn equities: market cap. In forex: volatility scale.",
    "python_code": "def _size_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        SIZE factors - magnitude of price movements.\n        In equities: market cap. In forex: volatility scale.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        high = df['high']\n        low = df['low']\n\n        # LNCAP equivalent: Log of ATR (Average True Range)\n        tr = np.maximum(high - low,\n                       np.abs(high - close.shift(1)),\n                       np.abs(low - close.shift(1)))\n        atr_20 = tr.rolling(20, min_periods=1).mean()\n        features['SIZE_LNCAP'] = np.log(atr_20 + 1e-12)\n\n        # MIDCAP: Nonlinear size (squared deviation from mean)\n        features['SIZE_MIDCAP'] = (atr_20 - atr_20.rolling(60, min_periods=1).mean()) ** 2\n\n        # Size relative to long-term average\n        atr_60 = tr.rolling(60, min_periods=1).mean()\n        features['SIZE_RELATIVE'] = atr_20 / (atr_60 + 1e-12)\n\n        # Log range as size proxy\n        daily_range = (high - low) / (close + 1e-12)\n        features['SIZE_RANGE'] = np.log(daily_range.rolling(20, min_periods=1).mean() + 1e-12)\n\n        # Size percentile rank\n        features['SIZE_RANK'] = atr_20.rolling(60, min_periods=1).apply(\n            lambda x: (x[-1] > x[:-1]).sum() / len(x) if len(x) > 1 else 0.5, raw=True\n        )\n\n        return features",
    "source_file": "core\\features\\barra_cne6.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BarraCNE6Forex"
  },
  {
    "name": "_beta_factors",
    "category": "statistical",
    "formula": "as market proxy | features",
    "explanation": "BETA factors - sensitivity to market movements.\nUses rolling correlation/regression with lagged returns.",
    "python_code": "def _beta_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        BETA factors - sensitivity to market movements.\n        Uses rolling correlation/regression with lagged returns.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # Historical beta: regression coefficient on market (use lagged self as proxy)\n        market_proxy = returns.shift(1)  # Lagged return as market proxy\n\n        for d in [20, 60]:\n            # Rolling beta (correlation * vol ratio)\n            corr = returns.rolling(d, min_periods=5).corr(market_proxy)\n            vol_ret = returns.rolling(d, min_periods=2).std()\n            vol_mkt = market_proxy.rolling(d, min_periods=2).std()\n            features[f'BETA_HIST{d}'] = corr * (vol_ret / (vol_mkt + 1e-12))\n\n        # Daily standard deviation of returns (DASTD)\n        features['BETA_DASTD'] = returns.rolling(20, min_periods=2).std()\n\n        # Cumulative range (CMRA)\n        log_ret = np.log(close / close.shift(1) + 1e-12)\n        features['BETA_CMRA'] = (\n            log_ret.rolling(12, min_periods=1).max() -\n            log_ret.rolling(12, min_periods=1).min()\n        )\n\n        return features",
    "source_file": "core\\features\\barra_cne6.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BarraCNE6Forex"
  },
  {
    "name": "_momentum_factors",
    "category": "technical",
    "formula": "features",
    "explanation": "MOMENTUM factors - trend strength and persistence.\nClassic Barra momentum uses 12-1 month return.",
    "python_code": "def _momentum_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        MOMENTUM factors - trend strength and persistence.\n        Classic Barra momentum uses 12-1 month return.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n\n        # RSTR: Relative strength (cumulative return)\n        for d in [5, 10, 20, 60]:\n            features[f'MOM_RSTR{d}'] = close / close.shift(d) - 1\n\n        # Short-term reversal (STREV)\n        features['MOM_STREV'] = close.shift(1) / close.shift(5) - 1\n\n        # Momentum quality: consistency of direction\n        returns = close.pct_change()\n        up_ratio = (returns > 0).rolling(20, min_periods=1).mean()\n        features['MOM_QUALITY'] = 2 * up_ratio - 1  # -1 to 1 scale\n\n        return features",
    "source_file": "core\\features\\barra_cne6.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BarraCNE6Forex"
  },
  {
    "name": "_volatility_factors",
    "category": "volatility",
    "formula": "features",
    "explanation": "VOLATILITY factors - various volatility measures.",
    "python_code": "def _volatility_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        VOLATILITY factors - various volatility measures.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        high = df['high']\n        low = df['low']\n        returns = close.pct_change()\n\n        # Realized volatility at different horizons\n        for d in [5, 20, 60]:\n            features[f'VOL_RVOL{d}'] = returns.rolling(d, min_periods=2).std()\n\n        # Parkinson volatility (uses high-low)\n        log_hl = np.log(high / low + 1e-12)\n        features['VOL_PARKINSON'] = np.sqrt(\n            (log_hl ** 2).rolling(20, min_periods=1).mean() / (4 * np.log(2))\n        )\n\n        # Volatility of volatility\n        vol_20 = returns.rolling(20, min_periods=2).std()\n        features['VOL_VOLVOL'] = vol_20.rolling(20, min_periods=2).std()\n\n        # Volatility skew (asymmetry)\n        pos_ret = returns.where(returns > 0, 0)\n        neg_ret = returns.where(returns < 0, 0)\n        up_vol = pos_ret.rolling(20, min_periods=2).std()\n        down_vol = np.abs(neg_ret).rolling(20, min_periods=2).std()\n        features['VOL_SKEW'] = (up_vol - down_vol) / (up_vol + down_vol + 1e-12)\n\n        return features",
    "source_file": "core\\features\\barra_cne6.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BarraCNE6Forex"
  },
  {
    "name": "_liquidity_factors",
    "category": "reinforcement_learning",
    "formula": "features",
    "explanation": "LIQUIDITY factors - volume and turnover metrics.",
    "python_code": "def _liquidity_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        LIQUIDITY factors - volume and turnover metrics.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        volume = df['volume']\n        close = df['close']\n\n        # Share turnover (STOM)\n        for d in [5, 20, 60]:\n            avg_vol = volume.rolling(d, min_periods=1).mean()\n            features[f'LIQ_STOM{d}'] = np.log(avg_vol + 1)\n\n        # Volume coefficient of variation\n        vol_std = volume.rolling(20, min_periods=2).std()\n        vol_mean = volume.rolling(20, min_periods=1).mean()\n        features['LIQ_VOLCOV'] = vol_std / (vol_mean + 1e-12)\n\n        # Amihud illiquidity (return/volume ratio)\n        returns = close.pct_change()\n        amihud = np.abs(returns) / (volume + 1e-12)\n        features['LIQ_AMIHUD'] = -np.log(amihud.rolling(20, min_periods=1).mean() + 1e-12)\n\n        return features",
    "source_file": "core\\features\\barra_cne6.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BarraCNE6Forex"
  },
  {
    "name": "_value_factors",
    "category": "technical",
    "formula": "features",
    "explanation": "VALUE factors - mean reversion and valuation proxies.\nIn equities: P/E, P/B. In forex: deviation from MA.",
    "python_code": "def _value_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        VALUE factors - mean reversion and valuation proxies.\n        In equities: P/E, P/B. In forex: deviation from MA.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n\n        # Book-to-price proxy: distance from moving average\n        for d in [20, 60]:\n            ma = close.rolling(d, min_periods=1).mean()\n            features[f'VAL_BTOP{d}'] = (ma - close) / (close + 1e-12)\n\n        # Z-score (standardized deviation)\n        ma_60 = close.rolling(60, min_periods=1).mean()\n        std_60 = close.rolling(60, min_periods=2).std()\n        features['VAL_ZSCORE'] = (close - ma_60) / (std_60 + 1e-12)\n\n        # Relative strength index as value indicator\n        delta = close.diff()\n        gain = delta.where(delta > 0, 0).rolling(14, min_periods=1).mean()\n        loss = (-delta.where(delta < 0, 0)).rolling(14, min_periods=1).mean()\n        rs = gain / (loss + 1e-12)\n        features['VAL_RSI'] = (100 - (100 / (1 + rs))) / 100 - 0.5  # Centered\n\n        # Bollinger position\n        bb_upper = ma_60 + 2 * std_60\n        bb_lower = ma_60 - 2 * std_60\n        features['VAL_BBPOS'] = (close - bb_lower) / (bb_upper - bb_lower + 1e-12)\n\n        return features",
    "source_file": "core\\features\\barra_cne6.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BarraCNE6Forex"
  },
  {
    "name": "_growth_factors",
    "category": "reinforcement_learning",
    "formula": "0 | np.polyfit(x, arr, 1)[0] | 0",
    "explanation": "GROWTH factors - trend strength and acceleration.\nIn equities: earnings growth. In forex: price trend strength.",
    "python_code": "def _growth_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        GROWTH factors - trend strength and acceleration.\n        In equities: earnings growth. In forex: price trend strength.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n\n        # Earnings growth proxy: trend slope\n        def calc_slope(arr):\n            if len(arr) < 2:\n                return 0\n            x = np.arange(len(arr))\n            return np.polyfit(x, arr, 1)[0]\n\n        for d in [20, 60]:\n            features[f'GRO_SLOPE{d}'] = close.rolling(d, min_periods=2).apply(calc_slope, raw=True)\n\n        # Sales growth proxy: rate of change acceleration\n        roc_20 = close / close.shift(20) - 1\n        features['GRO_ACC'] = roc_20 - roc_20.shift(20)\n\n        # R-squared of trend (quality of fit)\n        def calc_rsq(arr):\n            if len(arr) < 2:\n                return 0\n            x = np.arange(len(arr))\n            corr = np.corrcoef(x, arr)[0, 1]\n            return corr ** 2 if not np.isnan(corr) else 0\n\n        features['GRO_RSQ'] = close.rolling(20, min_periods=2).apply(calc_rsq, raw=True)\n\n        return features",
    "source_file": "core\\features\\barra_cne6.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BarraCNE6Forex"
  },
  {
    "name": "_quality_factors",
    "category": "microstructure",
    "formula": "ratio | features",
    "explanation": "QUALITY factors - consistency and reliability.\nIn equities: ROE, profit margins. In forex: signal quality.",
    "python_code": "def _quality_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        QUALITY factors - consistency and reliability.\n        In equities: ROE, profit margins. In forex: signal quality.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # ROE proxy: Sharpe-like metric (return/risk)\n        for d in [20, 60]:\n            ret_sum = returns.rolling(d, min_periods=1).sum()\n            ret_std = returns.rolling(d, min_periods=2).std()\n            features[f'QUA_SHARPE{d}'] = ret_sum / (ret_std * np.sqrt(d) + 1e-12)\n\n        # Profit margin proxy: positive return ratio\n        features['QUA_WINRATE'] = (returns > 0).rolling(20, min_periods=1).mean()\n\n        # Accruals proxy: difference between recent and longer-term performance\n        ret_5 = returns.rolling(5, min_periods=1).sum()\n        ret_20 = returns.rolling(20, min_periods=1).sum()\n        features['QUA_ACCRUAL'] = ret_5 - (ret_20 / 4)  # Short vs normalized long\n\n        return features",
    "source_file": "core\\features\\barra_cne6.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BarraCNE6Forex"
  },
  {
    "name": "_leverage_factors",
    "category": "risk",
    "formula": "features",
    "explanation": "LEVERAGE factors - concentration and drawdown risk.\nIn equities: debt/equity. In forex: drawdown metrics.",
    "python_code": "def _leverage_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        LEVERAGE factors - concentration and drawdown risk.\n        In equities: debt/equity. In forex: drawdown metrics.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n\n        # Debt-to-equity proxy: drawdown depth\n        rolling_max = close.rolling(60, min_periods=1).max()\n        drawdown = (close - rolling_max) / (rolling_max + 1e-12)\n        features['LEV_DRAWDOWN'] = drawdown\n\n        # Book leverage proxy: distance from range midpoint\n        rolling_min = close.rolling(60, min_periods=1).min()\n        range_pos = (close - rolling_min) / (rolling_max - rolling_min + 1e-12)\n        features['LEV_RANGEPOS'] = range_pos - 0.5  # Centered at 0\n\n        # Market leverage: recent volatility vs historical\n        returns = close.pct_change()\n        vol_5 = returns.rolling(5, min_periods=2).std()\n        vol_60 = returns.rolling(60, min_periods=2).std()\n        features['LEV_VOLRATIO'] = vol_5 / (vol_60 + 1e-12)\n\n        return features",
    "source_file": "core\\features\\barra_cne6.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BarraCNE6Forex"
  },
  {
    "name": "_sentiment_factors",
    "category": "reinforcement_learning",
    "formula": "features",
    "explanation": "SENTIMENT factors - market mood and positioning.",
    "python_code": "def _sentiment_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        SENTIMENT factors - market mood and positioning.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        high = df['high']\n        low = df['low']\n        volume = df['volume']\n        returns = close.pct_change()\n\n        # Analyst sentiment proxy: recent price trend strength\n        ma_5 = close.rolling(5, min_periods=1).mean()\n        ma_20 = close.rolling(20, min_periods=1).mean()\n        features['SENT_TREND'] = (ma_5 - ma_20) / (ma_20 + 1e-12)\n\n        # On-balance volume proxy\n        obv_sign = np.sign(returns)\n        obv = (obv_sign * volume).rolling(20, min_periods=1).sum()\n        features['SENT_OBV'] = obv / (volume.rolling(20, min_periods=1).sum() + 1e-12)\n\n        # Money flow proxy (using price position in range)\n        typical_price = (high + low + close) / 3\n        mf_ratio = typical_price / close.shift(1)\n        features['SENT_MFI'] = mf_ratio.rolling(14, min_periods=1).mean() - 1\n\n        # Short interest proxy: down volume ratio\n        down_vol = volume.where(returns < 0, 0)\n        features['SENT_SHORT'] = (\n            down_vol.rolling(10, min_periods=1).sum() /\n            (volume.rolling(10, min_periods=1).sum() + 1e-12)\n        )\n\n        return features",
    "source_file": "core\\features\\barra_cne6.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BarraCNE6Forex"
  },
  {
    "name": "generate_all",
    "category": "reinforcement_learning",
    "formula": "features",
    "explanation": "Generate all 46 Barra CNE6 factors.\n\nArgs:\n    df: DataFrame with columns: open, high, low, close, volume\n\nReturns:\n    DataFrame with 46 factor columns",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all 46 Barra CNE6 factors.\n\n        Args:\n            df: DataFrame with columns: open, high, low, close, volume\n\n        Returns:\n            DataFrame with 46 factor columns\n        \"\"\"\n        # Ensure required columns\n        required = ['open', 'high', 'low', 'close', 'volume']\n        for col in required:\n            if col not in df.columns:\n                raise ValueError(f\"Missing required column: {col}\")\n\n        # Generate all factor groups\n        size = self._size_factors(df)\n        beta = self._beta_factors(df)\n        momentum = self._momentum_factors(df)\n        volatility = self._volatility_factors(df)\n        liquidity = self._liquidity_factors(df)\n        value = self._value_factors(df)\n        growth = self._growth_factors(df)\n        quality = self._quality_factors(df)\n        leverage = self._leverage_factors(df)\n        sentiment = self._sentiment_factors(df)\n\n        # Combine all features\n        features = pd.concat([\n            size, beta, momentum, volatility, liquidity,\n            value, growth, quality, leverage, sentiment\n        ], axis=1)\n\n        # Clean up\n        features = features.replace([np.inf, -np.inf], np.nan)\n        features = features.fillna(0)\n\n        return features",
    "source_file": "core\\features\\barra_cne6.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BarraCNE6Forex"
  },
  {
    "name": "get_feature_names",
    "category": "feature_engineering",
    "formula": "names",
    "explanation": "Get list of all feature names.",
    "python_code": "def get_feature_names(self) -> List[str]:\n        \"\"\"Get list of all feature names.\"\"\"\n        names = []\n\n        # Size (5)\n        names.extend(['SIZE_LNCAP', 'SIZE_MIDCAP', 'SIZE_RELATIVE',\n                      'SIZE_RANGE', 'SIZE_RANK'])\n\n        # Beta (4)\n        names.extend(['BETA_HIST20', 'BETA_HIST60', 'BETA_DASTD', 'BETA_CMRA'])\n\n        # Momentum (6)\n        names.extend(['MOM_RSTR5', 'MOM_RSTR10', 'MOM_RSTR20',\n                      'MOM_RSTR60', 'MOM_STREV', 'MOM_QUALITY'])\n\n        # Volatility (6)\n        names.extend(['VOL_RVOL5', 'VOL_RVOL20', 'VOL_RVOL60',\n                      'VOL_PARKINSON', 'VOL_VOLVOL', 'VOL_SKEW'])\n\n        # Liquidity (5)\n        names.extend(['LIQ_STOM5', 'LIQ_STOM20', 'LIQ_STOM60',\n                      'LIQ_VOLCOV', 'LIQ_AMIHUD'])\n\n        # Value (5)\n        names.extend(['VAL_BTOP20', 'VAL_BTOP60', 'VAL_ZSCORE',\n                      'VAL_RSI', 'VAL_BBPOS'])\n\n        # Growth (4)\n        names.extend(['GRO_SLOPE20', 'GRO_SLOPE60', 'GRO_ACC', 'GRO_RSQ'])\n\n        # Quality (4)\n        names.extend(['QUA_SHARPE20', 'QUA_SHARPE60', 'QUA_WINRATE', 'QUA_ACCRUAL'])\n\n        # Leverage (3)\n        names.extend(['LEV_DRAWDOWN', 'LEV_RANGEPOS', 'LEV_VOLRATIO'])\n\n        # Sentiment (4)\n        names.extend(['SENT_TREND', 'SENT_OBV', 'SENT_MFI', 'SENT_SHORT'])\n\n        return names",
    "source_file": "core\\features\\barra_cne6.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BarraCNE6Forex"
  },
  {
    "name": "get_factor_category",
    "category": "reinforcement_learning",
    "formula": "categories.get(prefix, 'Unknown')",
    "explanation": "Get the category of a factor by name.",
    "python_code": "def get_factor_category(self, factor_name: str) -> str:\n        \"\"\"Get the category of a factor by name.\"\"\"\n        categories = {\n            'SIZE': 'Size',\n            'BETA': 'Beta',\n            'MOM': 'Momentum',\n            'VOL': 'Volatility',\n            'LIQ': 'Liquidity',\n            'VAL': 'Value',\n            'GRO': 'Growth',\n            'QUA': 'Quality',\n            'LEV': 'Leverage',\n            'SENT': 'Sentiment'\n        }\n        prefix = factor_name.split('_')[0]\n        return categories.get(prefix, 'Unknown')",
    "source_file": "core\\features\\barra_cne6.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BarraCNE6Forex"
  },
  {
    "name": "generate_barra_cne6",
    "category": "reinforcement_learning",
    "formula": "barra.generate_all(df)",
    "explanation": "Generate Barra CNE6 factors for forex.\n\nArgs:\n    df: OHLCV DataFrame\n\nReturns:\n    DataFrame with 46 Barra factors",
    "python_code": "def generate_barra_cne6(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generate Barra CNE6 factors for forex.\n\n    Args:\n        df: OHLCV DataFrame\n\n    Returns:\n        DataFrame with 46 Barra factors\n    \"\"\"\n    barra = BarraCNE6Forex()\n    return barra.generate_all(df)",
    "source_file": "core\\features\\barra_cne6.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "calc_slope",
    "category": "feature_engineering",
    "formula": "0 | np.polyfit(x, arr, 1)[0]",
    "explanation": "",
    "python_code": "def calc_slope(arr):\n            if len(arr) < 2:\n                return 0\n            x = np.arange(len(arr))\n            return np.polyfit(x, arr, 1)[0]",
    "source_file": "core\\features\\barra_cne6.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "calc_rsq",
    "category": "feature_engineering",
    "formula": "0 | corr ** 2 if not np.isnan(corr) else 0",
    "explanation": "",
    "python_code": "def calc_rsq(arr):\n            if len(arr) < 2:\n                return 0\n            x = np.arange(len(arr))\n            corr = np.corrcoef(x, arr)[0, 1]\n            return corr ** 2 if not np.isnan(corr) else 0",
    "source_file": "core\\features\\barra_cne6.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "microstructure",
    "formula": "",
    "explanation": "Args:\n    bucket_size: Volume per bucket (default 50 units)\n    n_buckets: Number of buckets for VPIN calculation (default 50)",
    "python_code": "def __init__(self, bucket_size: int = 50, n_buckets: int = 50):\n        \"\"\"\n        Args:\n            bucket_size: Volume per bucket (default 50 units)\n            n_buckets: Number of buckets for VPIN calculation (default 50)\n        \"\"\"\n        self.bucket_size = bucket_size\n        self.n_buckets = n_buckets",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "VPIN"
  },
  {
    "name": "bulk_volume_classification",
    "category": "microstructure",
    "formula": "V_buy = V * CDF(Z)\n    V_sell = V * (1 - CDF(Z))\n    where Z = (P_close - P_open) / sigma | V_buy = V * CDF(Z) | V_sell = V * (1 - CDF(Z))",
    "explanation": "Bulk Volume Classification (BVC) method.\n\nClassifies volume as buy/sell using price changes and\nnormal distribution CDF.\n\nFormula:\n    V_buy = V * CDF(Z)\n    V_sell = V * (1 - CDF(Z))\n    where Z = (P_close - P_open) / sigma",
    "python_code": "def bulk_volume_classification(self,\n                                    prices: pd.Series,\n                                    volumes: pd.Series,\n                                    sigma: float = None) -> Tuple[pd.Series, pd.Series]:\n        \"\"\"\n        Bulk Volume Classification (BVC) method.\n\n        Classifies volume as buy/sell using price changes and\n        normal distribution CDF.\n\n        Formula:\n            V_buy = V * CDF(Z)\n            V_sell = V * (1 - CDF(Z))\n            where Z = (P_close - P_open) / sigma\n        \"\"\"\n        if sigma is None:\n            sigma = prices.pct_change().std()\n\n        price_change = prices.diff()\n        z_score = price_change / (sigma * prices.shift(1) + 1e-10)\n\n        # CDF of standard normal\n        buy_prob = stats.norm.cdf(z_score)\n\n        v_buy = volumes * buy_prob\n        v_sell = volumes * (1 - buy_prob)\n\n        return v_buy, v_sell",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "VPIN"
  },
  {
    "name": "calculate_vpin",
    "category": "microstructure",
    "formula": "OI_bucket = |V_buy - V_sell| / (V_buy + V_sell)\n    VPIN = mean(OI_buckets[-n:]) | OI_bucket = |V_buy - V_sell| / (V_buy + V_sell) | VPIN = mean(OI_buckets[-n:])",
    "explanation": "Calculate VPIN time series.\n\nFormula:\n    OI_bucket = |V_buy - V_sell| / (V_buy + V_sell)\n    VPIN = mean(OI_buckets[-n:])\n\nReturns:\n    Series of VPIN values (0 to 1, higher = more toxic flow)",
    "python_code": "def calculate_vpin(self,\n                       prices: pd.Series,\n                       volumes: pd.Series) -> pd.Series:\n        \"\"\"\n        Calculate VPIN time series.\n\n        Formula:\n            OI_bucket = |V_buy - V_sell| / (V_buy + V_sell)\n            VPIN = mean(OI_buckets[-n:])\n\n        Returns:\n            Series of VPIN values (0 to 1, higher = more toxic flow)\n        \"\"\"\n        v_buy, v_sell = self.bulk_volume_classification(prices, volumes)\n\n        # Order imbalance per observation\n        total_vol = v_buy + v_sell + 1e-10\n        oi = np.abs(v_buy - v_sell) / total_vol\n\n        # Rolling VPIN over n_buckets\n        vpin = oi.rolling(self.n_buckets, min_periods=10).mean()\n\n        return vpin",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "VPIN"
  },
  {
    "name": "calculate_toxicity_signal",
    "category": "volatility",
    "formula": "signal",
    "explanation": "Generate trading signal from VPIN.\n\nHigh VPIN (>0.7) indicates toxic flow, expect volatility.",
    "python_code": "def calculate_toxicity_signal(self,\n                                   prices: pd.Series,\n                                   volumes: pd.Series,\n                                   threshold: float = 0.7) -> pd.Series:\n        \"\"\"\n        Generate trading signal from VPIN.\n\n        High VPIN (>0.7) indicates toxic flow, expect volatility.\n        \"\"\"\n        vpin = self.calculate_vpin(prices, volumes)\n\n        # Signal: 1 = high toxicity expected, 0 = normal\n        signal = (vpin > threshold).astype(int)\n\n        return signal",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "VPIN"
  },
  {
    "name": "calculate_level_ofi",
    "category": "microstructure",
    "formula": "OFI = Q_bid * I(P_bid >= P_bid_prev) - Q_ask * I(P_ask <= P_ask_prev) | Q = change in quantity | bid_ofi + ask_ofi",
    "explanation": "Calculate OFI for a single level.\n\nFormula (Cont et al. 2014):\n    OFI = Q_bid * I(P_bid >= P_bid_prev) - Q_ask * I(P_ask <= P_ask_prev)\n\nWhere:\n    Q = change in quantity\n    I() = indicator function",
    "python_code": "def calculate_level_ofi(self,\n                            bid_price: float, prev_bid_price: float,\n                            bid_size: float, prev_bid_size: float,\n                            ask_price: float, prev_ask_price: float,\n                            ask_size: float, prev_ask_size: float) -> float:\n        \"\"\"\n        Calculate OFI for a single level.\n\n        Formula (Cont et al. 2014):\n            OFI = Q_bid * I(P_bid >= P_bid_prev) - Q_ask * I(P_ask <= P_ask_prev)\n\n        Where:\n            Q = change in quantity\n            I() = indicator function\n        \"\"\"\n        # Bid side contribution\n        if bid_price >= prev_bid_price:\n            bid_ofi = bid_size - prev_bid_size\n        elif bid_price < prev_bid_price:\n            bid_ofi = -prev_bid_size\n        else:\n            bid_ofi = 0\n\n        # Ask side contribution\n        if ask_price <= prev_ask_price:\n            ask_ofi = -(ask_size - prev_ask_size)\n        elif ask_price > prev_ask_price:\n            ask_ofi = prev_ask_size\n        else:\n            ask_ofi = 0\n\n        return bid_ofi + ask_ofi",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "MultiLevelOFI"
  },
  {
    "name": "calculate_integrated_ofi",
    "category": "microstructure",
    "formula": "normalized_ofi",
    "explanation": "Calculate integrated OFI for single-level data (tick data).\n\nApproximates multi-level OFI using price momentum and volume.",
    "python_code": "def calculate_integrated_ofi(self,\n                                  prices: pd.Series,\n                                  volumes: pd.Series,\n                                  window: int = 20) -> pd.Series:\n        \"\"\"\n        Calculate integrated OFI for single-level data (tick data).\n\n        Approximates multi-level OFI using price momentum and volume.\n        \"\"\"\n        returns = prices.pct_change()\n\n        # Classify trades\n        buy_volume = volumes.where(returns > 0, 0)\n        sell_volume = volumes.where(returns < 0, 0)\n\n        # OFI = cumulative buy - sell pressure\n        ofi = (buy_volume - sell_volume).rolling(window, min_periods=5).sum()\n\n        # Normalize by total volume\n        total_vol = volumes.rolling(window, min_periods=5).sum() + 1e-10\n        normalized_ofi = ofi / total_vol\n\n        return normalized_ofi",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "MultiLevelOFI"
  },
  {
    "name": "calculate_ofi_momentum",
    "category": "microstructure",
    "formula": "Positive = increasing buy pressure | Negative = increasing sell pressure | ofi_short - ofi_long",
    "explanation": "OFI momentum signal (short-term vs long-term).\n\nPositive = increasing buy pressure\nNegative = increasing sell pressure",
    "python_code": "def calculate_ofi_momentum(self,\n                                prices: pd.Series,\n                                volumes: pd.Series,\n                                short_window: int = 10,\n                                long_window: int = 50) -> pd.Series:\n        \"\"\"\n        OFI momentum signal (short-term vs long-term).\n\n        Positive = increasing buy pressure\n        Negative = increasing sell pressure\n        \"\"\"\n        ofi_short = self.calculate_integrated_ofi(prices, volumes, short_window)\n        ofi_long = self.calculate_integrated_ofi(prices, volumes, long_window)\n\n        return ofi_short - ofi_long",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "MultiLevelOFI"
  },
  {
    "name": "__init__",
    "category": "volatility",
    "formula": "to consider (filters noise)",
    "explanation": "Args:\n    pt_sl: (profit_taking, stop_loss) multipliers of volatility\n    min_ret: Minimum return to consider (filters noise)\n    vertical_barrier: Maximum holding period in bars",
    "python_code": "def __init__(self,\n                 pt_sl: Tuple[float, float] = (1.0, 1.0),\n                 min_ret: float = 0.0,\n                 vertical_barrier: int = 10):\n        \"\"\"\n        Args:\n            pt_sl: (profit_taking, stop_loss) multipliers of volatility\n            min_ret: Minimum return to consider (filters noise)\n            vertical_barrier: Maximum holding period in bars\n        \"\"\"\n        self.pt_sl = pt_sl\n        self.min_ret = min_ret\n        self.vertical_barrier = vertical_barrier",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "TripleBarrierLabeling"
  },
  {
    "name": "get_daily_volatility",
    "category": "volatility",
    "formula": "returns.ewm(span=window).std()",
    "explanation": "Estimate daily volatility using exponential moving std.",
    "python_code": "def get_daily_volatility(self, prices: pd.Series, window: int = 20) -> pd.Series:\n        \"\"\"Estimate daily volatility using exponential moving std.\"\"\"\n        returns = prices.pct_change()\n        return returns.ewm(span=window).std()",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "TripleBarrierLabeling"
  },
  {
    "name": "apply_triple_barrier",
    "category": "microstructure",
    "formula": "labels",
    "explanation": "Apply triple barrier labeling.\n\nReturns:\n    Series with labels:\n     1 = hit upper barrier (profit)\n    -1 = hit lower barrier (loss)\n     0 = hit vertical barrier (timeout)",
    "python_code": "def apply_triple_barrier(self,\n                              prices: pd.Series,\n                              events: pd.Series = None) -> pd.Series:\n        \"\"\"\n        Apply triple barrier labeling.\n\n        Returns:\n            Series with labels:\n             1 = hit upper barrier (profit)\n            -1 = hit lower barrier (loss)\n             0 = hit vertical barrier (timeout)\n        \"\"\"\n        if events is None:\n            events = pd.Series(index=prices.index, data=True)\n\n        vol = self.get_daily_volatility(prices)\n        labels = pd.Series(index=prices.index, dtype=float)\n\n        for i, (idx, is_event) in enumerate(events.items()):\n            if not is_event or i >= len(prices) - 1:\n                labels[idx] = np.nan\n                continue\n\n            entry_price = prices.iloc[i]\n            entry_vol = vol.iloc[i] if not np.isnan(vol.iloc[i]) else 0.01\n\n            # Set barriers\n            upper = entry_price * (1 + self.pt_sl[0] * entry_vol)\n            lower = entry_price * (1 - self.pt_sl[1] * entry_vol)\n            max_idx = min(i + self.vertical_barrier, len(prices) - 1)\n\n            # Check which barrier hit first\n            label = 0  # Default: vertical barrier\n            for j in range(i + 1, max_idx + 1):\n                price = prices.iloc[j]\n                if price >= upper:\n                    label = 1\n                    break\n                elif price <= lower:\n                    label = -1\n                    break\n\n            labels[idx] = label\n\n        return labels",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "TripleBarrierLabeling"
  },
  {
    "name": "create_meta_labels",
    "category": "machine_learning",
    "formula": "label = 1 if primary prediction was correct | label = 0 if primary prediction was wrong | meta_labels",
    "explanation": "Create meta-labels for training the meta-labeling model.\n\nMeta-label = 1 if primary prediction was correct\nMeta-label = 0 if primary prediction was wrong\n\nThis teaches the model WHEN to trust the primary model.",
    "python_code": "def create_meta_labels(self,\n                            primary_predictions: pd.Series,\n                            actual_returns: pd.Series,\n                            holding_period: int = 5) -> pd.Series:\n        \"\"\"\n        Create meta-labels for training the meta-labeling model.\n\n        Meta-label = 1 if primary prediction was correct\n        Meta-label = 0 if primary prediction was wrong\n\n        This teaches the model WHEN to trust the primary model.\n        \"\"\"\n        # Forward returns\n        forward_returns = actual_returns.rolling(holding_period).sum().shift(-holding_period)\n\n        # Check if primary was correct\n        primary_direction = np.sign(primary_predictions)\n        actual_direction = np.sign(forward_returns)\n\n        meta_labels = (primary_direction == actual_direction).astype(int)\n\n        return meta_labels",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "MetaLabeling"
  },
  {
    "name": "calculate_bet_size",
    "category": "feature_engineering",
    "formula": "bet_size = primary_signal * meta_probability * max_leverage | bet_size = primary_signal * meta_probability * max_leverage | primary_signal * meta_probability * max_leverage",
    "explanation": "Calculate position size using meta-labeling probabilities.\n\nFormula:\n    bet_size = primary_signal * meta_probability * max_leverage",
    "python_code": "def calculate_bet_size(self,\n                           meta_probability: pd.Series,\n                           primary_signal: pd.Series,\n                           max_leverage: float = 1.0) -> pd.Series:\n        \"\"\"\n        Calculate position size using meta-labeling probabilities.\n\n        Formula:\n            bet_size = primary_signal * meta_probability * max_leverage\n        \"\"\"\n        return primary_signal * meta_probability * max_leverage",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "MetaLabeling"
  },
  {
    "name": "prepare_features",
    "category": "volatility",
    "formula": "features.values, features.index",
    "explanation": "Prepare features for HMM training.\n\nFeatures: [returns, volatility, momentum]",
    "python_code": "def prepare_features(self,\n                         prices: pd.Series,\n                         window: int = 20) -> np.ndarray:\n        \"\"\"\n        Prepare features for HMM training.\n\n        Features: [returns, volatility, momentum]\n        \"\"\"\n        returns = prices.pct_change()\n        volatility = returns.rolling(window).std()\n        momentum = prices.pct_change(window)\n\n        features = pd.DataFrame({\n            'returns': returns,\n            'volatility': volatility,\n            'momentum': momentum\n        }).dropna()\n\n        return features.values, features.index",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "HMMRegimeDetector"
  },
  {
    "name": "fit_predict",
    "category": "volatility",
    "formula": "result",
    "explanation": "Fit HMM and predict regimes.\n\nReturns:\n    Series with regime labels (0, 1, 2)\n\nInterpretation (typically):\n    0: Low volatility / trending\n    1: High volatility / mean-reverting\n    2: Transitional / uncertain",
    "python_code": "def fit_predict(self, prices: pd.Series) -> pd.Series:\n        \"\"\"\n        Fit HMM and predict regimes.\n\n        Returns:\n            Series with regime labels (0, 1, 2)\n\n        Interpretation (typically):\n            0: Low volatility / trending\n            1: High volatility / mean-reverting\n            2: Transitional / uncertain\n        \"\"\"\n        try:\n            from hmmlearn.hmm import GaussianHMM\n        except ImportError:\n            # Fallback: simple volatility-based regime\n            return self._simple_regime(prices)\n\n        features, idx = self.prepare_features(prices)\n\n        if len(features) < 50:\n            return self._simple_regime(prices)\n\n        try:\n            self.model = GaussianHMM(\n                n_components=self.n_states,\n                covariance_type=\"diag\",  # Use diagonal covariance (more robust)\n                n_iter=self.n_iter,\n                random_state=42\n            )\n\n            self.model.fit(features)\n            regimes = self.model.predict(features)\n\n            result = pd.Series(index=prices.index, dtype=float)\n            result[idx] = regimes\n            result = result.ffill().fillna(0)\n\n            return result\n        except Exception:\n            # Fallback on any HMM failure\n            return self._simple_regime(prices)",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "HMMRegimeDetector"
  },
  {
    "name": "_simple_regime",
    "category": "volatility",
    "formula": "regime.astype(float).fillna(0)",
    "explanation": "Simple volatility-based regime detection (fallback).",
    "python_code": "def _simple_regime(self, prices: pd.Series, window: int = 20) -> pd.Series:\n        \"\"\"Simple volatility-based regime detection (fallback).\"\"\"\n        returns = prices.pct_change()\n        vol = returns.rolling(window).std()\n        vol_percentile = vol.rolling(100).rank(pct=True)\n\n        # 0: low vol, 1: medium vol, 2: high vol\n        regime = pd.cut(vol_percentile, bins=[0, 0.33, 0.66, 1.0], labels=[0, 1, 2])\n        return regime.astype(float).fillna(0)",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "HMMRegimeDetector"
  },
  {
    "name": "filter",
    "category": "filtering",
    "formula": "x_t = x_{t-1} + w_t  (random walk)\nObservation equation: z_t = x_t + v_t | x_t = x_{t-1} + w_t  (random walk) | z_t = x_t + v_t",
    "explanation": "Apply Kalman filter to observations.\n\nState equation: x_t = x_{t-1} + w_t  (random walk)\nObservation equation: z_t = x_t + v_t\n\nReturns:\n    (filtered_state, kalman_gain)",
    "python_code": "def filter(self, observations: pd.Series) -> Tuple[pd.Series, pd.Series]:\n        \"\"\"\n        Apply Kalman filter to observations.\n\n        State equation: x_t = x_{t-1} + w_t  (random walk)\n        Observation equation: z_t = x_t + v_t\n\n        Returns:\n            (filtered_state, kalman_gain)\n        \"\"\"\n        n = len(observations)\n\n        # Initialize\n        x = observations.iloc[0]  # State estimate\n        P = 1.0  # Error covariance\n\n        filtered = np.zeros(n)\n        gains = np.zeros(n)\n\n        for i, z in enumerate(observations):\n            # Predict\n            x_pred = x\n            P_pred = P + self.Q\n\n            # Update\n            K = P_pred / (P_pred + self.R)  # Kalman gain\n            x = x_pred + K * (z - x_pred)\n            P = (1 - K) * P_pred\n\n            filtered[i] = x\n            gains[i] = K\n\n        return pd.Series(filtered, index=observations.index), \\\n               pd.Series(gains, index=observations.index)",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "KalmanFilterForex"
  },
  {
    "name": "calculate_spread_zscore",
    "category": "microstructure",
    "formula": "zscore",
    "explanation": "Calculate z-score of spread using Kalman-filtered mean.\n\nFor pairs/spread trading.",
    "python_code": "def calculate_spread_zscore(self,\n                                 series1: pd.Series,\n                                 series2: pd.Series,\n                                 window: int = 50) -> pd.Series:\n        \"\"\"\n        Calculate z-score of spread using Kalman-filtered mean.\n\n        For pairs/spread trading.\n        \"\"\"\n        spread = series1 - series2\n        filtered_mean, _ = self.filter(spread)\n\n        # Rolling std for z-score\n        spread_std = spread.rolling(window).std()\n        zscore = (spread - filtered_mean) / (spread_std + 1e-10)\n\n        return zscore",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "KalmanFilterForex"
  },
  {
    "name": "reservation_price",
    "category": "volatility",
    "formula": "r = s - q * gamma * sigma^2 * (T - t) | r = s - q * gamma * sigma^2 * (T - t) | s = mid price",
    "explanation": "Calculate reservation price (market maker's internal valuation).\n\nFormula:\n    r = s - q * gamma * sigma^2 * (T - t)\n\nWhere:\n    s = mid price\n    q = current inventory\n    gamma = risk aversion\n    sigma = volatility\n    T - t = time remaining",
    "python_code": "def reservation_price(self,\n                          mid_price: float,\n                          inventory: float,\n                          time_remaining: float) -> float:\n        \"\"\"\n        Calculate reservation price (market maker's internal valuation).\n\n        Formula:\n            r = s - q * gamma * sigma^2 * (T - t)\n\n        Where:\n            s = mid price\n            q = current inventory\n            gamma = risk aversion\n            sigma = volatility\n            T - t = time remaining\n        \"\"\"\n        return mid_price - inventory * self.gamma * self.sigma**2 * time_remaining",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "AvellanedaStoikov"
  },
  {
    "name": "optimal_spread",
    "category": "microstructure",
    "formula": "delta = gamma * sigma^2 * (T-t) + (2/gamma) * log(1 + gamma/k) | delta = gamma * sigma^2 * (T-t) + (2/gamma) * log(1 + gamma/k) | inventory_component + arrival_component",
    "explanation": "Calculate optimal bid-ask spread.\n\nFormula:\n    delta = gamma * sigma^2 * (T-t) + (2/gamma) * log(1 + gamma/k)",
    "python_code": "def optimal_spread(self, time_remaining: float) -> float:\n        \"\"\"\n        Calculate optimal bid-ask spread.\n\n        Formula:\n            delta = gamma * sigma^2 * (T-t) + (2/gamma) * log(1 + gamma/k)\n        \"\"\"\n        inventory_component = self.gamma * self.sigma**2 * time_remaining\n        arrival_component = (2 / self.gamma) * np.log(1 + self.gamma / self.k)\n\n        return inventory_component + arrival_component",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "AvellanedaStoikov"
  },
  {
    "name": "optimal_quotes",
    "category": "feature_engineering",
    "formula": "bid, ask",
    "explanation": "Calculate optimal bid and ask prices.\n\nReturns:\n    (bid_price, ask_price)",
    "python_code": "def optimal_quotes(self,\n                       mid_price: float,\n                       inventory: float,\n                       time_remaining: float = 1.0) -> Tuple[float, float]:\n        \"\"\"\n        Calculate optimal bid and ask prices.\n\n        Returns:\n            (bid_price, ask_price)\n        \"\"\"\n        r = self.reservation_price(mid_price, inventory, time_remaining)\n        delta = self.optimal_spread(time_remaining)\n\n        bid = r - delta / 2\n        ask = r + delta / 2\n\n        return bid, ask",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "AvellanedaStoikov"
  },
  {
    "name": "inventory_signal",
    "category": "feature_engineering",
    "formula": "Positive = price above fair value (sell) | Negative = price below fair value (buy) | pd.Series(signals, index=prices.index)",
    "explanation": "Generate trading signal based on inventory-adjusted fair value.\n\nReturns signal:\n    Positive = price above fair value (sell)\n    Negative = price below fair value (buy)",
    "python_code": "def inventory_signal(self,\n                         prices: pd.Series,\n                         position_series: pd.Series) -> pd.Series:\n        \"\"\"\n        Generate trading signal based on inventory-adjusted fair value.\n\n        Returns signal:\n            Positive = price above fair value (sell)\n            Negative = price below fair value (buy)\n        \"\"\"\n        vol = prices.pct_change().rolling(20).std() * np.sqrt(252)\n\n        signals = []\n        for i in range(len(prices)):\n            if i < 20:\n                signals.append(0)\n                continue\n\n            mid = prices.iloc[i]\n            inventory = position_series.iloc[i] if i < len(position_series) else 0\n            sigma = vol.iloc[i] if not np.isnan(vol.iloc[i]) else self.sigma\n\n            r = mid - inventory * self.gamma * sigma**2\n            signal = (mid - r) / (sigma + 1e-10)  # Z-score from fair value\n            signals.append(signal)\n\n        return pd.Series(signals, index=prices.index)",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "AvellanedaStoikov"
  },
  {
    "name": "attention_weights_proxy",
    "category": "volatility",
    "formula": "weight = more important time step for prediction. | weights",
    "explanation": "Proxy for attention weights using price importance.\n\nHigher weight = more important time step for prediction.\nBased on price volatility contribution.",
    "python_code": "def attention_weights_proxy(prices: pd.Series, window: int = 20) -> pd.Series:\n        \"\"\"\n        Proxy for attention weights using price importance.\n\n        Higher weight = more important time step for prediction.\n        Based on price volatility contribution.\n        \"\"\"\n        returns = prices.pct_change().abs()\n        weights = returns.rolling(window).apply(\n            lambda x: x / (x.sum() + 1e-10) if len(x) > 0 else 0\n        )\n        return weights",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "DeepLearningFeatures"
  },
  {
    "name": "temporal_attention_context",
    "category": "deep_learning",
    "formula": "context = sum(attention_weights * features) | context = sum(attention_weights * features) | context",
    "explanation": "Attention-weighted context vector (simplified).\n\nFormula:\n    context = sum(attention_weights * features)",
    "python_code": "def temporal_attention_context(prices: pd.Series,\n                                    features: pd.DataFrame,\n                                    window: int = 20) -> pd.Series:\n        \"\"\"\n        Attention-weighted context vector (simplified).\n\n        Formula:\n            context = sum(attention_weights * features)\n        \"\"\"\n        weights = DeepLearningFeatures.attention_weights_proxy(prices, window)\n\n        # Weight each feature by attention\n        context = (features.T * weights).T.sum(axis=1)\n        return context",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "DeepLearningFeatures"
  },
  {
    "name": "multiscale_decomposition",
    "category": "feature_engineering",
    "formula": "result",
    "explanation": "Multiscale trend decomposition (TimeMixer inspired).\n\nDecomposes price into multiple time scales.\n\nCitation: TimeMixer (ICLR 2024)",
    "python_code": "def multiscale_decomposition(prices: pd.Series,\n                                  scales: List[int] = [5, 10, 20, 50]) -> pd.DataFrame:\n        \"\"\"\n        Multiscale trend decomposition (TimeMixer inspired).\n\n        Decomposes price into multiple time scales.\n\n        Citation: TimeMixer (ICLR 2024)\n        \"\"\"\n        result = pd.DataFrame(index=prices.index)\n\n        for scale in scales:\n            # Trend at this scale\n            trend = prices.rolling(scale, min_periods=1).mean()\n            result[f'trend_{scale}'] = trend\n\n            # Residual at this scale\n            result[f'residual_{scale}'] = prices - trend\n\n            # Scale interaction (mixing)\n            if scale > scales[0]:\n                prev_scale = scales[scales.index(scale) - 1]\n                result[f'mix_{prev_scale}_{scale}'] = \\\n                    result[f'trend_{prev_scale}'] - result[f'trend_{scale}']\n\n        return result",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "DeepLearningFeatures"
  },
  {
    "name": "exogenous_features",
    "category": "feature_engineering",
    "formula": "result",
    "explanation": "Prepare exogenous variable features (TimeXer inspired).\n\nCitation: TimeXer (NeurIPS 2024)\n\nArgs:\n    exog_dict: Dictionary of exogenous series (e.g., DXY, VIX)",
    "python_code": "def exogenous_features(prices: pd.Series,\n                           exog_dict: Dict[str, pd.Series] = None) -> pd.DataFrame:\n        \"\"\"\n        Prepare exogenous variable features (TimeXer inspired).\n\n        Citation: TimeXer (NeurIPS 2024)\n\n        Args:\n            exog_dict: Dictionary of exogenous series (e.g., DXY, VIX)\n        \"\"\"\n        result = pd.DataFrame(index=prices.index)\n\n        if exog_dict is None:\n            # Create synthetic exogenous features\n            returns = prices.pct_change()\n\n            result['volatility_regime'] = returns.rolling(20).std().rank(pct=True)\n            result['momentum_regime'] = returns.rolling(10).sum().rank(pct=True)\n            result['mean_reversion'] = -returns.rolling(5).sum()\n        else:\n            for name, series in exog_dict.items():\n                aligned = series.reindex(prices.index).ffill()\n                result[f'exog_{name}'] = aligned\n                result[f'exog_{name}_ret'] = aligned.pct_change()\n                result[f'exog_{name}_zscore'] = (\n                    (aligned - aligned.rolling(20).mean()) /\n                    (aligned.rolling(20).std() + 1e-10)\n                )\n\n        return result",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "DeepLearningFeatures"
  },
  {
    "name": "generate_all",
    "category": "feature_engineering",
    "formula": "result",
    "explanation": "Generate all gold standard features.\n\nArgs:\n    df: DataFrame with 'close', 'volume' columns\n\nReturns:\n    DataFrame with all features",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all gold standard features.\n\n        Args:\n            df: DataFrame with 'close', 'volume' columns\n\n        Returns:\n            DataFrame with all features\n        \"\"\"\n        result = pd.DataFrame(index=df.index)\n\n        close = df['close']\n        volume = df.get('volume', pd.Series(1, index=df.index))\n\n        # [1] VPIN Features\n        try:\n            result['vpin'] = self.vpin.calculate_vpin(close, volume)\n            result['vpin_toxicity'] = self.vpin.calculate_toxicity_signal(close, volume)\n        except Exception:\n            result['vpin'] = 0\n            result['vpin_toxicity'] = 0\n\n        # [2] OFI Features\n        try:\n            result['ofi_integrated'] = self.ofi.calculate_integrated_ofi(close, volume)\n            result['ofi_momentum'] = self.ofi.calculate_ofi_momentum(close, volume)\n        except Exception:\n            result['ofi_integrated'] = 0\n            result['ofi_momentum'] = 0\n\n        # [3] Triple Barrier Labels (for training, not prediction)\n        try:\n            result['tb_labels'] = self.triple_barrier.apply_triple_barrier(close)\n        except Exception:\n            result['tb_labels'] = 0\n\n        # [4] HMM Regime\n        try:\n            result['hmm_regime'] = self.hmm.fit_predict(close)\n        except Exception:\n            result['hmm_regime'] = 0\n\n        # [5] Kalman Filter\n        try:\n            kalman_state, kalman_gain = self.kalman.filter(close)\n            result['kalman_filtered'] = kalman_state\n            result['kalman_gain'] = kalman_gain\n            result['kalman_deviation'] = close - kalman_state\n        except Exception:\n            result['kalman_filtered'] = close\n            result['kalman_gain'] = 0\n            result['kalman_deviation'] = 0\n\n        # [6] Avellaneda-Stoikov (with zero inventory assumption)\n        try:\n            position_proxy = pd.Series(0, index=df.index)  # No position\n            result['as_inventory_signal'] = self.as_model.inventory_signal(close, position_proxy)\n        except Exception:\n            result['as_inventory_signal'] = 0\n\n        # [7-9] Deep Learning Features\n        try:\n            result['attention_proxy'] = self.dl_features.attention_weights_proxy(close)\n        except Exception:\n            result['attention_proxy'] = 0\n\n        try:\n            multiscale = self.dl_features.multiscale_decomposition(close)\n            for col in multiscale.columns:\n                result[f'dl_{col}'] = multiscale[col]\n        except Exception:\n            pass\n\n        try:\n            exog = self.dl_features.exogenous_features(close)\n            for col in exog.columns:\n                result[f'dl_{col}'] = exog[col]\n        except Exception:\n            pass\n\n        # Derived signals\n        try:\n            result['regime_adjusted_signal'] = result['ofi_momentum'] * (1 + result['hmm_regime'] * 0.1)\n            result['toxicity_adjusted_signal'] = result['ofi_mome",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ChineseGoldStandardFeatures"
  },
  {
    "name": "generate_chinese_gold_standard_features",
    "category": "feature_engineering",
    "formula": "generator.generate_all(df)",
    "explanation": "Generate all Chinese gold standard features in one call.\n\nCitations: See module docstring for full list.",
    "python_code": "def generate_chinese_gold_standard_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generate all Chinese gold standard features in one call.\n\n    Citations: See module docstring for full list.\n    \"\"\"\n    generator = ChineseGoldStandardFeatures()\n    return generator.generate_all(df)",
    "source_file": "core\\features\\chinese_gold_standard.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self):\n        self.signals: Dict[str, CrossAssetSignal] = {}",
    "source_file": "core\\features\\cross_asset.py",
    "academic_reference": null,
    "class_name": "CrossAssetSignalGenerator"
  },
  {
    "name": "generate_all_signals",
    "category": "feature_engineering",
    "formula": "df",
    "explanation": "Generate all cross-asset signals.\n\nArgs:\n    forex_data: Forex OHLCV data with pair column or pair-named columns\n    cross_asset_data: Dict of DataFrames for DXY, VIX, SPX, etc.\n\nReturns:\n    DataFrame with cross-asset signals added",
    "python_code": "def generate_all_signals(self,\n                             forex_data: pd.DataFrame,\n                             cross_asset_data: Optional[Dict[str, pd.DataFrame]] = None) -> pd.DataFrame:\n        \"\"\"\n        Generate all cross-asset signals.\n\n        Args:\n            forex_data: Forex OHLCV data with pair column or pair-named columns\n            cross_asset_data: Dict of DataFrames for DXY, VIX, SPX, etc.\n\n        Returns:\n            DataFrame with cross-asset signals added\n        \"\"\"\n        df = forex_data.copy()\n\n        # DXY signals\n        if cross_asset_data and 'DXY' in cross_asset_data:\n            df = self._add_dxy_signals(df, cross_asset_data['DXY'])\n\n        # VIX signals\n        if cross_asset_data and 'VIX' in cross_asset_data:\n            df = self._add_vix_signals(df, cross_asset_data['VIX'])\n\n        # SPX signals\n        if cross_asset_data and 'SPX' in cross_asset_data:\n            df = self._add_spx_signals(df, cross_asset_data['SPX'])\n\n        # Gold signals\n        if cross_asset_data and 'GOLD' in cross_asset_data:\n            df = self._add_gold_signals(df, cross_asset_data['GOLD'])\n\n        # Oil signals\n        if cross_asset_data and 'OIL' in cross_asset_data:\n            df = self._add_oil_signals(df, cross_asset_data['OIL'])\n\n        # Bond yield signals\n        if cross_asset_data and 'US10Y' in cross_asset_data:\n            df = self._add_yield_signals(df, cross_asset_data['US10Y'])\n\n        # Inter-pair signals (even without external data)\n        df = self._add_inter_pair_signals(df)\n\n        # Risk sentiment composite\n        df = self._add_risk_sentiment(df)\n\n        return df",
    "source_file": "core\\features\\cross_asset.py",
    "academic_reference": null,
    "class_name": "CrossAssetSignalGenerator"
  },
  {
    "name": "_add_dxy_signals",
    "category": "feature_engineering",
    "formula": "df",
    "explanation": "Add Dollar Index signals.",
    "python_code": "def _add_dxy_signals(self, df: pd.DataFrame, dxy: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Add Dollar Index signals.\"\"\"\n\n        # Align DXY to forex data\n        if 'close' in dxy.columns:\n            dxy_close = dxy['close']\n        else:\n            dxy_close = dxy.iloc[:, 0]\n\n        # Resample/align if needed\n        dxy_aligned = dxy_close.reindex(df.index, method='ffill')\n\n        # DXY momentum\n        dxy_mom = dxy_aligned.pct_change(20)\n        df['signal_dxy_momentum'] = np.where(dxy_mom > 0.01, -1,  # DXY up = sell EUR/USD\n                                             np.where(dxy_mom < -0.01, 1, 0))\n\n        # DXY vs MA\n        dxy_ma = dxy_aligned.rolling(50).mean()\n        df['signal_dxy_trend'] = np.where(dxy_aligned > dxy_ma, -1, 1)\n\n        # DXY breakout\n        dxy_high = dxy_aligned.rolling(20).max()\n        dxy_low = dxy_aligned.rolling(20).min()\n        df['signal_dxy_breakout'] = np.where(dxy_aligned >= dxy_high * 0.99, -1,  # DXY breaking out = sell EUR\n                                             np.where(dxy_aligned <= dxy_low * 1.01, 1, 0))\n\n        # DXY RSI\n        dxy_delta = dxy_aligned.diff()\n        dxy_gain = dxy_delta.where(dxy_delta > 0, 0).rolling(14).mean()\n        dxy_loss = (-dxy_delta.where(dxy_delta < 0, 0)).rolling(14).mean()\n        dxy_rsi = 100 - (100 / (1 + dxy_gain / (dxy_loss + 1e-8)))\n        df['signal_dxy_rsi'] = np.where(dxy_rsi > 70, 1,  # DXY overbought = buy EUR\n                                        np.where(dxy_rsi < 30, -1, 0))\n\n        return df",
    "source_file": "core\\features\\cross_asset.py",
    "academic_reference": null,
    "class_name": "CrossAssetSignalGenerator"
  },
  {
    "name": "_add_vix_signals",
    "category": "volatility",
    "formula": "df",
    "explanation": "Add VIX (volatility/fear) signals.",
    "python_code": "def _add_vix_signals(self, df: pd.DataFrame, vix: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Add VIX (volatility/fear) signals.\"\"\"\n\n        if 'close' in vix.columns:\n            vix_close = vix['close']\n        else:\n            vix_close = vix.iloc[:, 0]\n\n        vix_aligned = vix_close.reindex(df.index, method='ffill')\n\n        # VIX level\n        df['signal_vix_level'] = np.where(vix_aligned > 30, -1,  # High fear = risk-off = sell risk currencies\n                                          np.where(vix_aligned < 15, 1, 0))\n\n        # VIX spike (sudden fear)\n        vix_change = vix_aligned.pct_change(5)\n        df['signal_vix_spike'] = np.where(vix_change > 0.20, -1,  # VIX spike = risk-off\n                                          np.where(vix_change < -0.15, 1, 0))\n\n        # VIX vs historical\n        vix_percentile = vix_aligned.rolling(252).apply(lambda x: (x.iloc[-1] > x).mean())\n        df['signal_vix_percentile'] = np.where(vix_percentile > 0.8, -1,\n                                               np.where(vix_percentile < 0.2, 1, 0))\n\n        # VIX term structure (if available)\n        # Contango = complacency, Backwardation = fear\n\n        return df",
    "source_file": "core\\features\\cross_asset.py",
    "academic_reference": null,
    "class_name": "CrossAssetSignalGenerator"
  },
  {
    "name": "_add_spx_signals",
    "category": "feature_engineering",
    "formula": "df",
    "explanation": "Add S&P 500 signals (risk appetite).",
    "python_code": "def _add_spx_signals(self, df: pd.DataFrame, spx: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Add S&P 500 signals (risk appetite).\"\"\"\n\n        if 'close' in spx.columns:\n            spx_close = spx['close']\n        else:\n            spx_close = spx.iloc[:, 0]\n\n        spx_aligned = spx_close.reindex(df.index, method='ffill')\n\n        # SPX momentum (risk-on/off)\n        spx_mom = spx_aligned.pct_change(20)\n        df['signal_spx_momentum'] = np.where(spx_mom > 0.03, 1,  # Stocks up = risk-on = buy risk currencies\n                                             np.where(spx_mom < -0.03, -1, 0))\n\n        # SPX vs 200 MA (bull/bear market)\n        spx_ma200 = spx_aligned.rolling(200).mean()\n        df['signal_spx_trend'] = np.where(spx_aligned > spx_ma200, 1, -1)\n\n        # SPX new high (euphoria)\n        spx_high52 = spx_aligned.rolling(252).max()\n        df['signal_spx_new_high'] = np.where(spx_aligned >= spx_high52 * 0.99, 1, 0)\n\n        # SPX drawdown\n        spx_drawdown = (spx_aligned / spx_high52 - 1)\n        df['signal_spx_drawdown'] = np.where(spx_drawdown < -0.10, -1,  # Correction\n                                             np.where(spx_drawdown < -0.20, -2, 0))  # Bear market\n\n        return df",
    "source_file": "core\\features\\cross_asset.py",
    "academic_reference": null,
    "class_name": "CrossAssetSignalGenerator"
  },
  {
    "name": "_add_gold_signals",
    "category": "feature_engineering",
    "formula": "df",
    "explanation": "Add Gold signals (safe haven).",
    "python_code": "def _add_gold_signals(self, df: pd.DataFrame, gold: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Add Gold signals (safe haven).\"\"\"\n\n        if 'close' in gold.columns:\n            gold_close = gold['close']\n        else:\n            gold_close = gold.iloc[:, 0]\n\n        gold_aligned = gold_close.reindex(df.index, method='ffill')\n\n        # Gold momentum\n        gold_mom = gold_aligned.pct_change(20)\n        df['signal_gold_momentum'] = np.where(gold_mom > 0.02, 1,  # Gold up = buy safe havens (JPY, CHF)\n                                              np.where(gold_mom < -0.02, -1, 0))\n\n        # Gold vs USD divergence\n        # If gold and USD both strong = uncertainty\n        # If gold up, USD down = classic risk-off\n        if 'close' in df.columns:\n            forex_ret = df['close'].pct_change(20)\n            gold_ret = gold_aligned.pct_change(20)\n\n            # Correlation-based signal\n            corr = forex_ret.rolling(60).corr(gold_ret)\n            df['signal_gold_forex_corr'] = np.where(corr > 0.5, 1,\n                                                    np.where(corr < -0.5, -1, 0))\n\n        return df",
    "source_file": "core\\features\\cross_asset.py",
    "academic_reference": null,
    "class_name": "CrossAssetSignalGenerator"
  },
  {
    "name": "_add_oil_signals",
    "category": "feature_engineering",
    "formula": "df",
    "explanation": "Add Oil signals (commodity currencies).",
    "python_code": "def _add_oil_signals(self, df: pd.DataFrame, oil: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Add Oil signals (commodity currencies).\"\"\"\n\n        if 'close' in oil.columns:\n            oil_close = oil['close']\n        else:\n            oil_close = oil.iloc[:, 0]\n\n        oil_aligned = oil_close.reindex(df.index, method='ffill')\n\n        # Oil momentum (affects CAD, NOK, RUB)\n        oil_mom = oil_aligned.pct_change(20)\n        df['signal_oil_momentum'] = np.where(oil_mom > 0.05, 1,  # Oil up = buy CAD (sell USD/CAD)\n                                             np.where(oil_mom < -0.05, -1, 0))\n\n        # Oil volatility\n        oil_vol = oil_aligned.pct_change().rolling(20).std()\n        oil_vol_percentile = oil_vol.rolling(252).apply(lambda x: (x.iloc[-1] > x).mean())\n        df['signal_oil_volatility'] = np.where(oil_vol_percentile > 0.8, -1, 0)  # High oil vol = uncertainty\n\n        return df",
    "source_file": "core\\features\\cross_asset.py",
    "academic_reference": null,
    "class_name": "CrossAssetSignalGenerator"
  },
  {
    "name": "_add_yield_signals",
    "category": "feature_engineering",
    "formula": "df",
    "explanation": "Add bond yield signals.",
    "python_code": "def _add_yield_signals(self, df: pd.DataFrame, us10y: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Add bond yield signals.\"\"\"\n\n        if 'close' in us10y.columns:\n            yield_close = us10y['close']\n        else:\n            yield_close = us10y.iloc[:, 0]\n\n        yield_aligned = yield_close.reindex(df.index, method='ffill')\n\n        # Yield direction (higher yields = USD strength)\n        yield_change = yield_aligned.diff(20)\n        df['signal_yield_direction'] = np.where(yield_change > 0.1, 1,  # Rising yields = USD strength\n                                                np.where(yield_change < -0.1, -1, 0))\n\n        # Yield level\n        df['signal_yield_level'] = np.where(yield_aligned > 4.5, 1,  # High yields = USD attractive\n                                            np.where(yield_aligned < 2.0, -1, 0))\n\n        # Yield curve (if 2Y available)\n        # Inversion = recession signal = risk-off\n\n        return df",
    "source_file": "core\\features\\cross_asset.py",
    "academic_reference": null,
    "class_name": "CrossAssetSignalGenerator"
  },
  {
    "name": "_add_inter_pair_signals",
    "category": "feature_engineering",
    "formula": "df",
    "explanation": "Add signals from related forex pairs.",
    "python_code": "def _add_inter_pair_signals(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Add signals from related forex pairs.\"\"\"\n\n        # These work even without external data\n        if 'close' in df.columns:\n            close = df['close']\n\n            # Momentum divergence within pair\n            mom_5 = close.pct_change(5)\n            mom_20 = close.pct_change(20)\n\n            # Short-term vs long-term momentum divergence\n            df['signal_momentum_divergence'] = np.where(\n                (mom_5 > 0) & (mom_20 < 0), 1,  # Short-term reversal up\n                np.where((mom_5 < 0) & (mom_20 > 0), -1, 0)\n            )\n\n            # Acceleration\n            mom_accel = mom_5 - mom_5.shift(5)\n            df['signal_momentum_accel'] = np.where(mom_accel > 0.005, 1,\n                                                   np.where(mom_accel < -0.005, -1, 0))\n\n        return df",
    "source_file": "core\\features\\cross_asset.py",
    "academic_reference": null,
    "class_name": "CrossAssetSignalGenerator"
  },
  {
    "name": "_add_risk_sentiment",
    "category": "feature_engineering",
    "formula": "df",
    "explanation": "Create composite risk sentiment signal.",
    "python_code": "def _add_risk_sentiment(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Create composite risk sentiment signal.\"\"\"\n\n        # Combine available signals\n        risk_signals = []\n\n        if 'signal_vix_level' in df.columns:\n            risk_signals.append(df['signal_vix_level'])\n        if 'signal_spx_momentum' in df.columns:\n            risk_signals.append(df['signal_spx_momentum'])\n        if 'signal_dxy_momentum' in df.columns:\n            risk_signals.append(-df['signal_dxy_momentum'])  # Inverse for EUR/USD\n\n        if risk_signals:\n            df['signal_risk_composite'] = np.sign(sum(risk_signals))\n            df['signal_risk_strength'] = sum(np.abs(s) for s in risk_signals) / len(risk_signals)\n        else:\n            df['signal_risk_composite'] = 0\n            df['signal_risk_strength'] = 0\n\n        return df",
    "source_file": "core\\features\\cross_asset.py",
    "academic_reference": null,
    "class_name": "CrossAssetSignalGenerator"
  },
  {
    "name": "get_signal_weights",
    "category": "statistical",
    "formula": "{} | weights",
    "explanation": "Get optimal signal weights for a pair based on correlations.",
    "python_code": "def get_signal_weights(self, pair: str) -> Dict[str, float]:\n        \"\"\"Get optimal signal weights for a pair based on correlations.\"\"\"\n\n        if pair not in self.CORRELATIONS:\n            return {}\n\n        correlations = self.CORRELATIONS[pair]\n\n        # Convert correlations to weights\n        weights = {}\n        for asset, corr in correlations.items():\n            signal_name = f'signal_{asset.lower()}_momentum'\n            weights[signal_name] = abs(corr)  # Use absolute correlation as weight\n\n        return weights",
    "source_file": "core\\features\\cross_asset.py",
    "academic_reference": null,
    "class_name": "CrossAssetSignalGenerator"
  },
  {
    "name": "create_cross_asset_features",
    "category": "feature_engineering",
    "formula": "generator.generate_all_signals(forex_data, cross_asset_data)",
    "explanation": "Quick function to create cross-asset features for a forex pair.\n\nArgs:\n    forex_pair: Target pair (e.g., 'EURUSD')\n    forex_data: Target pair data\n    related_pairs: Dict of related pair data\n\nReturns:\n    DataFrame with cross-asset features",
    "python_code": "def create_cross_asset_features(forex_pair: str,\n                                forex_data: pd.DataFrame,\n                                related_pairs: Optional[Dict[str, pd.DataFrame]] = None) -> pd.DataFrame:\n    \"\"\"\n    Quick function to create cross-asset features for a forex pair.\n\n    Args:\n        forex_pair: Target pair (e.g., 'EURUSD')\n        forex_data: Target pair data\n        related_pairs: Dict of related pair data\n\n    Returns:\n        DataFrame with cross-asset features\n    \"\"\"\n    generator = CrossAssetSignalGenerator()\n\n    # Convert related pairs to cross-asset format\n    cross_asset_data = {}\n\n    # Map forex pairs to cross-asset equivalent\n    if related_pairs:\n        for pair, data in related_pairs.items():\n            if 'DXY' in pair.upper():\n                cross_asset_data['DXY'] = data\n            elif 'SPX' in pair.upper() or 'SPY' in pair.upper():\n                cross_asset_data['SPX'] = data\n            elif 'VIX' in pair.upper():\n                cross_asset_data['VIX'] = data\n            elif 'GOLD' in pair.upper() or 'XAU' in pair.upper():\n                cross_asset_data['GOLD'] = data\n            elif 'OIL' in pair.upper() or 'CL' in pair.upper() or 'WTI' in pair.upper():\n                cross_asset_data['OIL'] = data\n\n    return generator.generate_all_signals(forex_data, cross_asset_data)",
    "source_file": "core\\features\\cross_asset.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, capacity: int = 1000000):\n        self.buffer = deque(maxlen=capacity)",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "ReplayBuffer"
  },
  {
    "name": "push",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "",
    "python_code": "def push(self, state: np.ndarray, action: np.ndarray, reward: float,\n             next_state: np.ndarray, done: bool):\n        self.buffer.append((state, action, reward, next_state, done))",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "ReplayBuffer"
  },
  {
    "name": "sample",
    "category": "feature_engineering",
    "formula": "states, actions, rewards, next_states, dones",
    "explanation": "",
    "python_code": "def sample(self, batch_size: int) -> Tuple[np.ndarray, ...]:\n        batch = random.sample(self.buffer, batch_size)\n        states = np.array([e[0] for e in batch])\n        actions = np.array([e[1] for e in batch])\n        rewards = np.array([e[2] for e in batch])\n        next_states = np.array([e[3] for e in batch])\n        dones = np.array([e[4] for e in batch])\n        return states, actions, rewards, next_states, dones",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "ReplayBuffer"
  },
  {
    "name": "sample",
    "category": "feature_engineering",
    "formula": "states, actions, rewards, next_states, dones, indices, weights",
    "explanation": "",
    "python_code": "def sample(self, batch_size: int) -> Tuple:\n        size = len(self.buffer)\n        priorities = self.priorities[:size]\n        probs = priorities ** self.alpha\n        probs /= probs.sum()\n\n        indices = np.random.choice(size, batch_size, p=probs)\n        weights = (size * probs[indices]) ** (-self.beta)\n        weights /= weights.max()\n        self.beta = min(1.0, self.beta + self.beta_increment)\n\n        batch = [self.buffer[i] for i in indices]\n        states = np.array([e[0] for e in batch])\n        actions = np.array([e[1] for e in batch])\n        rewards = np.array([e[2] for e in batch])\n        next_states = np.array([e[3] for e in batch])\n        dones = np.array([e[4] for e in batch])\n\n        return states, actions, rewards, next_states, dones, indices, weights",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "PrioritizedReplayBuffer"
  },
  {
    "name": "update_priorities",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "",
    "python_code": "def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):\n        for idx, priority in zip(indices, priorities):\n            self.priorities[idx] = priority + 1e-6\n            self.max_priority = max(self.max_priority, priority)",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "PrioritizedReplayBuffer"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def forward(self, state: torch.Tensor) -> torch.Tensor:\n        return self.max_action * self.net(state)",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "TD3Actor"
  },
  {
    "name": "q1_forward",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def q1_forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n        sa = torch.cat([state, action], dim=-1)\n        return self.q1(sa)",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "TD3Critic"
  },
  {
    "name": "__init__",
    "category": "feature_engineering",
    "formula": "if device == 'auto':",
    "explanation": "",
    "python_code": "def __init__(self, config: TD3Config = None, device: str = 'auto'):\n        self.config = config or TD3Config()\n\n        if not HAS_TORCH:\n            logger.error(\"PyTorch required for TD3\")\n            return\n\n        if device == 'auto':\n            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        else:\n            self.device = torch.device(device)\n\n        # Actor\n        self.actor = TD3Actor(\n            self.config.state_dim,\n            self.config.action_dim,\n            self.config.hidden_dim\n        ).to(self.device)\n        self.actor_target = copy.deepcopy(self.actor)\n        self.actor_optimizer = optim.Adam(\n            self.actor.parameters(), lr=self.config.actor_lr\n        )\n\n        # Twin Critics\n        self.critic = TD3Critic(\n            self.config.state_dim,\n            self.config.action_dim,\n            self.config.hidden_dim\n        ).to(self.device)\n        self.critic_target = copy.deepcopy(self.critic)\n        self.critic_optimizer = optim.Adam(\n            self.critic.parameters(), lr=self.config.critic_lr\n        )\n\n        # Replay buffer\n        self.buffer = ReplayBuffer(self.config.buffer_size)\n\n        # Training state\n        self.total_steps = 0\n        self.is_fitted = False",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "TD3Trader"
  },
  {
    "name": "select_action",
    "category": "feature_engineering",
    "formula": "np.array([0.0]) | action",
    "explanation": "Select action with optional exploration noise.",
    "python_code": "def select_action(self, state: np.ndarray, add_noise: bool = True) -> np.ndarray:\n        \"\"\"Select action with optional exploration noise.\"\"\"\n        if not HAS_TORCH:\n            return np.array([0.0])\n\n        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n\n        with torch.no_grad():\n            action = self.actor(state_tensor).cpu().numpy()[0]\n\n        if add_noise:\n            noise = np.random.normal(0, self.config.exploration_noise,\n                                    size=self.config.action_dim)\n            action = np.clip(action + noise, -1.0, 1.0)\n\n        return action",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "TD3Trader"
  },
  {
    "name": "train_step",
    "category": "reinforcement_learning",
    "formula": "{} | metrics",
    "explanation": "Perform one training step.\n\nImplements TD3 algorithm:\n1. Sample mini-batch\n2. Compute target Q with clipped double Q-learning\n3. Update critics\n4. Delayed policy update with smoothing",
    "python_code": "def train_step(self) -> Dict[str, float]:\n        \"\"\"\n        Perform one training step.\n\n        Implements TD3 algorithm:\n        1. Sample mini-batch\n        2. Compute target Q with clipped double Q-learning\n        3. Update critics\n        4. Delayed policy update with smoothing\n        \"\"\"\n        if len(self.buffer) < self.config.batch_size:\n            return {}\n\n        # Sample batch\n        states, actions, rewards, next_states, dones = self.buffer.sample(\n            self.config.batch_size\n        )\n\n        states = torch.FloatTensor(states).to(self.device)\n        actions = torch.FloatTensor(actions).to(self.device)\n        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n        next_states = torch.FloatTensor(next_states).to(self.device)\n        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n\n        with torch.no_grad():\n            # Target policy smoothing\n            noise = (\n                torch.randn_like(actions) * self.config.policy_noise\n            ).clamp(-self.config.noise_clip, self.config.noise_clip)\n\n            next_actions = (\n                self.actor_target(next_states) + noise\n            ).clamp(-1.0, 1.0)\n\n            # Clipped double Q-learning\n            target_q1, target_q2 = self.critic_target(next_states, next_actions)\n            target_q = torch.min(target_q1, target_q2)\n            target_q = rewards + (1 - dones) * self.config.gamma * target_q\n\n        # Update critics\n        current_q1, current_q2 = self.critic(states, actions)\n        critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n\n        metrics = {'critic_loss': critic_loss.item()}\n\n        # Delayed policy update\n        if self.total_steps % self.config.policy_delay == 0:\n            # Actor loss: maximize Q\n            actor_loss = -self.critic.q1_forward(states, self.actor(states)).mean()\n\n            self.actor_optimizer.zero_grad()\n            actor_loss.backward()\n            self.actor_optimizer.step()\n\n            # Soft update targets\n            for param, target_param in zip(self.critic.parameters(),\n                                          self.critic_target.parameters()):\n                target_param.data.copy_(\n                    self.config.tau * param.data +\n                    (1 - self.config.tau) * target_param.data\n                )\n\n            for param, target_param in zip(self.actor.parameters(),\n                                          self.actor_target.parameters()):\n                target_param.data.copy_(\n                    self.config.tau * param.data +\n                    (1 - self.config.tau) * target_param.data\n                )\n\n            metrics['actor_loss'] = actor_loss.item()\n\n        self.total_steps += 1\n        return metrics",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "TD3Trader"
  },
  {
    "name": "store_transition",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Store experience in replay buffer.",
    "python_code": "def store_transition(self, state: np.ndarray, action: np.ndarray,\n                        reward: float, next_state: np.ndarray, done: bool):\n        \"\"\"Store experience in replay buffer.\"\"\"\n        self.buffer.push(state, action, reward, next_state, done)",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "TD3Trader"
  },
  {
    "name": "save",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Save model weights.",
    "python_code": "def save(self, path: str):\n        \"\"\"Save model weights.\"\"\"\n        torch.save({\n            'actor': self.actor.state_dict(),\n            'critic': self.critic.state_dict(),\n            'actor_target': self.actor_target.state_dict(),\n            'critic_target': self.critic_target.state_dict(),\n        }, path)",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "TD3Trader"
  },
  {
    "name": "load",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Load model weights.",
    "python_code": "def load(self, path: str):\n        \"\"\"Load model weights.\"\"\"\n        checkpoint = torch.load(path, map_location=self.device)\n        self.actor.load_state_dict(checkpoint['actor'])\n        self.critic.load_state_dict(checkpoint['critic'])\n        self.actor_target.load_state_dict(checkpoint['actor_target'])\n        self.critic_target.load_state_dict(checkpoint['critic_target'])\n        self.is_fitted = True",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "TD3Trader"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "mean, log_std",
    "explanation": "",
    "python_code": "def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        x = self.net(state)\n        mean = self.mean_layer(x)\n        log_std = self.log_std_layer(x)\n        log_std = torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n        return mean, log_std",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "SACGaussianActor"
  },
  {
    "name": "sample",
    "category": "reinforcement_learning",
    "formula": "action, log_prob",
    "explanation": "Sample action and compute log probability.",
    "python_code": "def sample(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Sample action and compute log probability.\"\"\"\n        mean, log_std = self.forward(state)\n        std = log_std.exp()\n\n        # Reparameterization trick\n        normal = Normal(mean, std)\n        x_t = normal.rsample()\n        action = torch.tanh(x_t)\n\n        # Log probability with tanh squashing correction\n        log_prob = normal.log_prob(x_t)\n        log_prob -= torch.log(1 - action.pow(2) + 1e-6)\n        log_prob = log_prob.sum(dim=-1, keepdim=True)\n\n        return action, log_prob",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "SACGaussianActor"
  },
  {
    "name": "get_action",
    "category": "reinforcement_learning",
    "formula": "torch.tanh(mean) | torch.tanh(x_t)",
    "explanation": "Get action for evaluation.",
    "python_code": "def get_action(self, state: torch.Tensor, deterministic: bool = False) -> torch.Tensor:\n        \"\"\"Get action for evaluation.\"\"\"\n        mean, log_std = self.forward(state)\n\n        if deterministic:\n            return torch.tanh(mean)\n        else:\n            std = log_std.exp()\n            normal = Normal(mean, std)\n            x_t = normal.rsample()\n            return torch.tanh(x_t)",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "SACGaussianActor"
  },
  {
    "name": "alpha",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "",
    "python_code": "def alpha(self) -> torch.Tensor:\n        return self.log_alpha.exp()",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "SACTrader"
  },
  {
    "name": "select_action",
    "category": "technical",
    "formula": "np.array([0.0]) | action.cpu().numpy()[0]",
    "explanation": "Select action from stochastic policy.",
    "python_code": "def select_action(self, state: np.ndarray, deterministic: bool = False) -> np.ndarray:\n        \"\"\"Select action from stochastic policy.\"\"\"\n        if not HAS_TORCH:\n            return np.array([0.0])\n\n        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n\n        with torch.no_grad():\n            action = self.actor.get_action(state_tensor, deterministic)\n\n        return action.cpu().numpy()[0]",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "SACTrader"
  },
  {
    "name": "get_action",
    "category": "feature_engineering",
    "formula": "q_values.argmax(dim=-1).item()",
    "explanation": "",
    "python_code": "def get_action(self, state: torch.Tensor) -> int:\n        q_values = self.forward(state)\n        return q_values.argmax(dim=-1).item()",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "EarnHFTSubAgent"
  },
  {
    "name": "select_agent",
    "category": "feature_engineering",
    "formula": "probs.argmax(dim=-1).item()",
    "explanation": "",
    "python_code": "def select_agent(self, state: torch.Tensor) -> int:\n        probs = self.forward(state)\n        return probs.argmax(dim=-1).item()",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "EarnHFTRouter"
  },
  {
    "name": "detect_trend",
    "category": "feature_engineering",
    "formula": "1  # Default to neutral | = np.sum(returns) | > trend_threshold:",
    "explanation": "Detect market trend from recent prices.\n\nReturns:\n    0: Bull (uptrend)\n    1: Neutral (sideways)\n    2: Bear (downtrend)",
    "python_code": "def detect_trend(self, prices: np.ndarray) -> int:\n        \"\"\"\n        Detect market trend from recent prices.\n\n        Returns:\n            0: Bull (uptrend)\n            1: Neutral (sideways)\n            2: Bear (downtrend)\n        \"\"\"\n        if len(prices) < self.config.trend_window:\n            return 1  # Default to neutral\n\n        returns = np.diff(prices[-self.config.trend_window:]) / prices[-self.config.trend_window:-1]\n        cumulative_return = np.sum(returns)\n        volatility = np.std(returns)\n\n        # Trend detection thresholds\n        trend_threshold = volatility * 2\n\n        if cumulative_return > trend_threshold:\n            return 0  # Bull\n        elif cumulative_return < -trend_threshold:\n            return 2  # Bear\n        else:\n            return 1",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "EarnHFTTrader"
  },
  {
    "name": "select_action",
    "category": "feature_engineering",
    "formula": "1, 1  # HOLD, neutral agent | action, trend",
    "explanation": "Select action using hierarchical structure.\n\nReturns:\n    (action, selected_agent_idx)",
    "python_code": "def select_action(self, state: np.ndarray, prices: np.ndarray = None,\n                     epsilon: float = 0.0) -> Tuple[int, int]:\n        \"\"\"\n        Select action using hierarchical structure.\n\n        Returns:\n            (action, selected_agent_idx)\n        \"\"\"\n        if not HAS_TORCH:\n            return 1, 1  # HOLD, neutral agent\n\n        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n\n        # Determine market condition\n        if prices is not None:\n            trend = self.detect_trend(prices)\n        else:\n            # Use router if no price history\n            with torch.no_grad():\n                trend = self.router.select_agent(state_tensor)\n\n        # Epsilon-greedy exploration\n        if random.random() < epsilon:\n            action = random.randrange(self.config.action_dim)\n        else:\n            with torch.no_grad():\n                action = self.sub_agents[trend].get_action(state_tensor)\n\n        return action, trend",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "EarnHFTTrader"
  },
  {
    "name": "train_step",
    "category": "machine_learning",
    "formula": "{} | {f'sub_agent_{trend}_loss': loss.item()}",
    "explanation": "Train sub-agent for specific market trend.\n\n\"Each sub-agent learns independently on its respective\nmarket trend data.\"\n- Qin et al. (2024)",
    "python_code": "def train_step(self, trend: int) -> Dict[str, float]:\n        \"\"\"\n        Train sub-agent for specific market trend.\n\n        \"Each sub-agent learns independently on its respective\n        market trend data.\"\n        - Qin et al. (2024)\n        \"\"\"\n        buffer = self.buffers[trend]\n\n        if len(buffer) < self.config.batch_size:\n            return {}\n\n        # Sample batch\n        states, actions, rewards, next_states, dones = buffer.sample(\n            self.config.batch_size\n        )\n\n        states = torch.FloatTensor(states).to(self.device)\n        actions = torch.LongTensor(actions).to(self.device)\n        rewards = torch.FloatTensor(rewards).to(self.device)\n        next_states = torch.FloatTensor(next_states).to(self.device)\n        dones = torch.FloatTensor(dones).to(self.device)\n\n        agent = self.sub_agents[trend]\n        target_agent = self.sub_agents_target[trend]\n        optimizer = self.sub_agent_optimizers[trend]\n\n        # DQN update\n        q_values = agent(states).gather(1, actions.unsqueeze(1)).squeeze()\n\n        with torch.no_grad():\n            next_q_values = target_agent(next_states).max(1)[0]\n            target_q = rewards + (1 - dones) * self.config.gamma * next_q_values\n\n        loss = F.mse_loss(q_values, target_q)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Soft update target\n        for param, target_param in zip(agent.parameters(), target_agent.parameters()):\n            target_param.data.copy_(0.005 * param.data + 0.995 * target_param.data)\n\n        self.total_steps += 1\n\n        return {f'sub_agent_{trend}_loss': loss.item()}",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "EarnHFTTrader"
  },
  {
    "name": "train_router",
    "category": "reinforcement_learning",
    "formula": "{'router_loss': loss.item()}",
    "explanation": "Train router to select appropriate sub-agent.\n\nUses the reward as supervision signal for router learning.",
    "python_code": "def train_router(self, state: np.ndarray, trend: int, reward: float):\n        \"\"\"\n        Train router to select appropriate sub-agent.\n\n        Uses the reward as supervision signal for router learning.\n        \"\"\"\n        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n\n        # Router should predict the trend that led to good reward\n        target = torch.zeros(self.config.num_sub_agents, device=self.device)\n        target[trend] = 1.0\n\n        probs = self.router(state_tensor).squeeze()\n\n        # Weighted cross-entropy based on reward\n        weight = max(0.1, reward + 1)  # Positive weight for good rewards\n        loss = -weight * (target * torch.log(probs + 1e-8)).sum()\n\n        self.router_optimizer.zero_grad()\n        loss.backward()\n        self.router_optimizer.step()\n\n        return {'router_loss': loss.item()}",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "EarnHFTTrader"
  },
  {
    "name": "forward",
    "category": "feature_engineering",
    "formula": "x + memory_output",
    "explanation": "Read from memory and augment input.",
    "python_code": "def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"Read from memory and augment input.\"\"\"\n        batch_size = x.shape[0]\n\n        # Project input to query\n        query = self.query_proj(x).unsqueeze(1)  # [batch, 1, memory_dim]\n\n        # Expand memory for batch\n        memory = self.memory.unsqueeze(0).expand(batch_size, -1, -1)\n\n        # Attention read\n        attended, _ = self.attention(query, memory, memory)\n\n        # Project back and combine with input\n        memory_output = self.output_proj(attended.squeeze(1))\n\n        return x + memory_output",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "MemoryAugmentedNetwork"
  },
  {
    "name": "forward",
    "category": "feature_engineering",
    "formula": "x + up",
    "explanation": "",
    "python_code": "def forward(self, x: torch.Tensor, condition: torch.Tensor) -> torch.Tensor:\n        # Encode condition\n        cond_embedding = self.condition_encoder(condition)\n\n        # Adapter transformation\n        down = self.down_proj(x)\n        adapted = down * torch.sigmoid(cond_embedding)  # Conditional gating\n        up = self.up_proj(adapted)\n\n        return x + up",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "ConditionalAdapter"
  },
  {
    "name": "compute_market_condition",
    "category": "volatility",
    "formula": "condition",
    "explanation": "Extract market condition features from state.\n\nReturns 4D vector: [trend, volatility, mean_rev, momentum]",
    "python_code": "def compute_market_condition(self, state: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Extract market condition features from state.\n\n        Returns 4D vector: [trend, volatility, mean_rev, momentum]\n        \"\"\"\n        # Simplified extraction (in practice, use more sophisticated features)\n        condition = np.zeros(4)\n\n        if len(state) >= 10:\n            # Trend indicator\n            condition[0] = np.tanh(state[0] if len(state) > 0 else 0)\n            # Volatility indicator\n            condition[1] = np.tanh(state[2] if len(state) > 2 else 0)\n            # Mean reversion indicator\n            condition[2] = np.tanh(state[5] if len(state) > 5 else 0)\n            # Momentum indicator\n            condition[3] = np.tanh(state[3] if len(state) > 3 else 0)\n\n        return condition",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "MacroHFTTrader"
  },
  {
    "name": "select_action",
    "category": "feature_engineering",
    "formula": "1  # HOLD | random.randrange(self.config.action_dim) | action",
    "explanation": "Select action using memory-augmented hierarchical structure.",
    "python_code": "def select_action(self, state: np.ndarray, epsilon: float = 0.0) -> int:\n        \"\"\"\n        Select action using memory-augmented hierarchical structure.\n        \"\"\"\n        if not HAS_TORCH:\n            return 1  # HOLD\n\n        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n        condition = self.compute_market_condition(state)\n        condition_tensor = torch.FloatTensor(condition).unsqueeze(0).to(self.device)\n\n        if random.random() < epsilon:\n            return random.randrange(self.config.action_dim)\n\n        with torch.no_grad():\n            # Memory augmentation\n            augmented_state = self.memory_net(state_tensor)\n\n            # Get sub-agent Q-values\n            sub_q_values = []\n            for i, (agent, adapter) in enumerate(zip(self.sub_agents, self.adapters)):\n                # Pass through first layers\n                x = agent[0](augmented_state)  # Linear\n                x = agent[1](x)  # ReLU\n                x = agent[2](x)  # Linear\n                x = agent[3](x)  # ReLU\n\n                # Apply conditional adapter\n                x = adapter(x, condition_tensor)\n\n                # Final layer\n                q = agent[4](x)\n                sub_q_values.append(q)\n\n            # Concatenate sub-agent outputs with state\n            sub_outputs = torch.cat(sub_q_values, dim=-1)\n            hyper_input = torch.cat([state_tensor, sub_outputs], dim=-1)\n\n            # Hyper-agent decision\n            final_q = self.hyper_agent(hyper_input)\n            action = final_q.argmax(dim=-1).item()\n\n        return action",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "MacroHFTTrader"
  },
  {
    "name": "train_step",
    "category": "machine_learning",
    "formula": "{} | {'macrohft_loss': loss.item()}",
    "explanation": "Train MacroHFT with memory and adapters.",
    "python_code": "def train_step(self) -> Dict[str, float]:\n        \"\"\"Train MacroHFT with memory and adapters.\"\"\"\n        if len(self.buffer) < self.config.batch_size:\n            return {}\n\n        states, actions, rewards, next_states, dones = self.buffer.sample(\n            self.config.batch_size\n        )\n\n        states = torch.FloatTensor(states).to(self.device)\n        actions = torch.LongTensor(actions).squeeze().to(self.device)\n        rewards = torch.FloatTensor(rewards).to(self.device)\n        next_states = torch.FloatTensor(next_states).to(self.device)\n        dones = torch.FloatTensor(dones).to(self.device)\n\n        # Compute conditions for batch\n        conditions = torch.FloatTensor([\n            self.compute_market_condition(s) for s in states.cpu().numpy()\n        ]).to(self.device)\n\n        # Forward pass\n        augmented_states = self.memory_net(states)\n\n        sub_q_values = []\n        for agent, adapter in zip(self.sub_agents, self.adapters):\n            x = agent[0](augmented_states)\n            x = agent[1](x)\n            x = agent[2](x)\n            x = agent[3](x)\n            x = adapter(x, conditions)\n            q = agent[4](x)\n            sub_q_values.append(q)\n\n        sub_outputs = torch.cat(sub_q_values, dim=-1)\n        hyper_input = torch.cat([states, sub_outputs], dim=-1)\n        q_values = self.hyper_agent(hyper_input)\n\n        q_selected = q_values.gather(1, actions.unsqueeze(1)).squeeze()\n\n        # Target computation (simplified - no target network for brevity)\n        with torch.no_grad():\n            next_augmented = self.memory_net(next_states)\n            next_conditions = torch.FloatTensor([\n                self.compute_market_condition(s) for s in next_states.cpu().numpy()\n            ]).to(self.device)\n\n            next_sub_q = []\n            for agent, adapter in zip(self.sub_agents, self.adapters):\n                x = agent[0](next_augmented)\n                x = agent[1](x)\n                x = agent[2](x)\n                x = agent[3](x)\n                x = adapter(x, next_conditions)\n                q = agent[4](x)\n                next_sub_q.append(q)\n\n            next_sub_outputs = torch.cat(next_sub_q, dim=-1)\n            next_hyper_input = torch.cat([next_states, next_sub_outputs], dim=-1)\n            next_q = self.hyper_agent(next_hyper_input).max(1)[0]\n\n            target_q = rewards + (1 - dones) * self.config.gamma * next_q\n\n        loss = F.mse_loss(q_selected, target_q)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.optimizer.param_groups[0]['params'], 1.0)\n        self.optimizer.step()\n\n        self.total_steps += 1\n\n        return {'macrohft_loss': loss.item()}",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "MacroHFTTrader"
  },
  {
    "name": "forward",
    "category": "feature_engineering",
    "formula": "q_values, risk",
    "explanation": "",
    "python_code": "def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        features = self.encoder(state)\n        q_values = self.q_head(features)\n        risk = self.risk_head(features)\n        return q_values, risk",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "RiskAwareNetwork"
  },
  {
    "name": "get_q_values",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "",
    "python_code": "def get_q_values(self, state: torch.Tensor) -> torch.Tensor:\n        features = self.encoder(state)\n        return self.q_head(features)",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "RiskAwareNetwork"
  },
  {
    "name": "compute_actual_risk",
    "category": "feature_engineering",
    "formula": "0.5 | neg_ratio = np.mean(returns < 0) | 0.5 * neg_ratio + 0.5 * vol_risk",
    "explanation": "Compute actual risk from returns (for auxiliary task supervision).",
    "python_code": "def compute_actual_risk(self, returns: np.ndarray) -> float:\n        \"\"\"Compute actual risk from returns (for auxiliary task supervision).\"\"\"\n        if len(returns) < 2:\n            return 0.5\n\n        # Risk as probability of negative return\n        neg_ratio = np.mean(returns < 0)\n\n        # Also consider volatility\n        vol = np.std(returns) if len(returns) > 1 else 0\n        vol_risk = min(1.0, vol * 100)  # Scale volatility\n\n        return 0.5 * neg_ratio + 0.5 * vol_risk",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "DeepScalperTrader"
  },
  {
    "name": "compute_hindsight_bonus",
    "category": "reinforcement_learning",
    "formula": "> 0: | < 0: | 0.0",
    "explanation": "Compute hindsight bonus reward.\n\n\"The hindsight bonus encourages the agent to learn from\nwhat would have been the optimal action in hindsight.\"\n- Sun et al. (2022)",
    "python_code": "def compute_hindsight_bonus(self, action: int, actual_return: float) -> float:\n        \"\"\"\n        Compute hindsight bonus reward.\n\n        \"The hindsight bonus encourages the agent to learn from\n        what would have been the optimal action in hindsight.\"\n        - Sun et al. (2022)\n        \"\"\"\n        # Optimal action in hindsight\n        if actual_return > 0:\n            optimal_action = 0  # BUY\n        elif actual_return < 0:\n            optimal_action = 2  # SELL\n        else:\n            optimal_action = 1  # HOLD\n\n        # Bonus if action matches optimal\n        if action == optimal_action:\n            return self.config.hindsight_bonus\n        else:\n            return 0.0",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "DeepScalperTrader"
  },
  {
    "name": "select_action",
    "category": "machine_learning",
    "formula": "1, 0.5 | random.randrange(self.config.action_dim), 0.5 | action, risk.item()",
    "explanation": "Select action with risk-awareness.\n\nReturns:\n    (action, predicted_risk)",
    "python_code": "def select_action(self, state: np.ndarray, epsilon: float = 0.0) -> Tuple[int, float]:\n        \"\"\"\n        Select action with risk-awareness.\n\n        Returns:\n            (action, predicted_risk)\n        \"\"\"\n        if not HAS_TORCH:\n            return 1, 0.5\n\n        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n\n        if random.random() < epsilon:\n            return random.randrange(self.config.action_dim), 0.5\n\n        with torch.no_grad():\n            q_values, risk = self.network(state_tensor)\n\n            # Risk-adjusted Q-values\n            risk_penalty = risk.item() * self.config.risk_penalty\n            adjusted_q = q_values - risk_penalty\n\n            action = adjusted_q.argmax(dim=-1).item()\n\n        return action, risk.item()",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "DeepScalperTrader"
  },
  {
    "name": "train_step",
    "category": "machine_learning",
    "formula": "{} | {",
    "explanation": "Train DeepScalper with risk-aware learning.",
    "python_code": "def train_step(self) -> Dict[str, float]:\n        \"\"\"Train DeepScalper with risk-aware learning.\"\"\"\n        if len(self.buffer) < self.config.batch_size:\n            return {}\n\n        states, action_risk, rewards, next_states, dones = self.buffer.sample(\n            self.config.batch_size\n        )\n\n        actions = action_risk[:, 0].astype(int)\n        actual_risks = action_risk[:, 1]\n\n        states = torch.FloatTensor(states).to(self.device)\n        actions = torch.LongTensor(actions).to(self.device)\n        rewards = torch.FloatTensor(rewards).to(self.device)\n        next_states = torch.FloatTensor(next_states).to(self.device)\n        dones = torch.FloatTensor(dones).to(self.device)\n        actual_risks = torch.FloatTensor(actual_risks).to(self.device)\n\n        # Forward pass\n        q_values, predicted_risk = self.network(states)\n        q_selected = q_values.gather(1, actions.unsqueeze(1)).squeeze()\n\n        # Target Q-values\n        with torch.no_grad():\n            next_q = self.target_network.get_q_values(next_states).max(1)[0]\n            target_q = rewards + (1 - dones) * self.config.gamma * next_q\n\n        # Q-learning loss\n        q_loss = F.mse_loss(q_selected, target_q)\n\n        # Risk prediction loss (auxiliary task)\n        risk_loss = F.mse_loss(predicted_risk.squeeze(), actual_risks)\n\n        # Combined loss\n        total_loss = q_loss + 0.5 * risk_loss\n\n        self.optimizer.zero_grad()\n        total_loss.backward()\n        self.optimizer.step()\n\n        # Soft update target\n        for param, target_param in zip(self.network.parameters(),\n                                      self.target_network.parameters()):\n            target_param.data.copy_(0.005 * param.data + 0.995 * target_param.data)\n\n        self.total_steps += 1\n\n        return {\n            'q_loss': q_loss.item(),\n            'risk_loss': risk_loss.item(),\n            'total_loss': total_loss.item()\n        }",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "DeepScalperTrader"
  },
  {
    "name": "generate_features",
    "category": "machine_learning",
    "formula": "features",
    "explanation": "Generate deep RL-derived features.\n\nFeatures:\n- Q-value estimates\n- Action probabilities\n- Risk predictions\n- Confidence scores",
    "python_code": "def generate_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate deep RL-derived features.\n\n        Features:\n        - Q-value estimates\n        - Action probabilities\n        - Risk predictions\n        - Confidence scores\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n\n        # Placeholder features (actual values require trained models)\n        # These represent the feature structure for integration\n\n        # TD3 features\n        features['TD3_Q_VALUE'] = 0.0\n        features['TD3_ACTION'] = 0.0\n        features['TD3_ACTOR_CONFIDENCE'] = 0.5\n\n        # SAC features\n        features['SAC_Q_VALUE'] = 0.0\n        features['SAC_ENTROPY'] = 0.5\n        features['SAC_ACTION_MEAN'] = 0.0\n        features['SAC_ACTION_STD'] = 0.1\n\n        # EarnHFT features\n        features['EARNHFT_TREND'] = 1  # 0=bull, 1=neutral, 2=bear\n        features['EARNHFT_ACTION'] = 1  # 0=buy, 1=hold, 2=sell\n        features['EARNHFT_ROUTER_CONFIDENCE'] = 0.5\n\n        # MacroHFT features\n        features['MACROHFT_MEMORY_ATTENTION'] = 0.5\n        features['MACROHFT_ADAPTER_GATE'] = 0.5\n        features['MACROHFT_ACTION'] = 1\n\n        # DeepScalper features\n        features['DEEPSCALPER_RISK'] = 0.5\n        features['DEEPSCALPER_Q_ADJUSTED'] = 0.0\n        features['DEEPSCALPER_ACTION'] = 1\n\n        return features",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "DeepRLFeatureGenerator"
  },
  {
    "name": "generate_deep_rl_features",
    "category": "deep_learning",
    "formula": "generator.generate_features(df)",
    "explanation": "Convenience function to generate deep RL features.",
    "python_code": "def generate_deep_rl_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Convenience function to generate deep RL features.\"\"\"\n    generator = DeepRLFeatureGenerator()\n    return generator.generate_features(df)",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": null
  },
  {
    "name": "to",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "",
    "python_code": "def to(self, device):\n            return self",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "_PlaceholderModule"
  },
  {
    "name": "parameters",
    "category": "feature_engineering",
    "formula": "[]",
    "explanation": "",
    "python_code": "def parameters(self):\n            return []",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "_PlaceholderModule"
  },
  {
    "name": "state_dict",
    "category": "feature_engineering",
    "formula": "{}",
    "explanation": "",
    "python_code": "def state_dict(self):\n            return {}",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "_PlaceholderModule"
  },
  {
    "name": "load_state_dict",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "",
    "python_code": "def load_state_dict(self, state):\n            pass",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "_PlaceholderModule"
  },
  {
    "name": "eval",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "",
    "python_code": "def eval(self):\n            pass",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "_PlaceholderModule"
  },
  {
    "name": "train",
    "category": "machine_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def train(self, mode=True):\n            pass",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "_PlaceholderModule"
  },
  {
    "name": "mse_loss",
    "category": "reinforcement_learning",
    "formula": "0.0",
    "explanation": "",
    "python_code": "def mse_loss(*args, **kwargs):\n            return 0.0",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "F"
  },
  {
    "name": "linear",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "",
    "python_code": "def linear(*args, **kwargs):\n            return None",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "F"
  },
  {
    "name": "relu",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "",
    "python_code": "def relu(*args, **kwargs):\n            return None",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "F"
  },
  {
    "name": "device",
    "category": "feature_engineering",
    "formula": "name",
    "explanation": "",
    "python_code": "def device(name):\n            return name",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "torch"
  },
  {
    "name": "cuda_is_available",
    "category": "feature_engineering",
    "formula": "False",
    "explanation": "",
    "python_code": "def cuda_is_available():\n            return False",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "torch"
  },
  {
    "name": "randn",
    "category": "feature_engineering",
    "formula": "np.random.randn(*args)",
    "explanation": "",
    "python_code": "def randn(*args, **kwargs):\n            return np.random.randn(*args)",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "torch"
  },
  {
    "name": "zeros",
    "category": "feature_engineering",
    "formula": "np.zeros(*args)",
    "explanation": "",
    "python_code": "def zeros(*args, **kwargs):\n            return np.zeros(*args)",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "torch"
  },
  {
    "name": "cat",
    "category": "feature_engineering",
    "formula": "np.concatenate(tensors, axis=dim)",
    "explanation": "",
    "python_code": "def cat(tensors, dim=-1):\n            return np.concatenate(tensors, axis=dim)",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "torch"
  },
  {
    "name": "no_grad",
    "category": "feature_engineering",
    "formula": "nullcontext()",
    "explanation": "",
    "python_code": "def no_grad():\n            from contextlib import nullcontext\n            return nullcontext()",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "torch"
  },
  {
    "name": "load",
    "category": "feature_engineering",
    "formula": "{}",
    "explanation": "",
    "python_code": "def load(*args, **kwargs):\n            return {}",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "torch"
  },
  {
    "name": "rsample",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "",
    "python_code": "def rsample(self):\n            return self.mean",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "Normal"
  },
  {
    "name": "log_prob",
    "category": "feature_engineering",
    "formula": "np.zeros_like(x) if hasattr(x, 'shape') else 0.0",
    "explanation": "",
    "python_code": "def log_prob(self, x):\n            return np.zeros_like(x) if hasattr(x, 'shape') else 0.0",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "Normal"
  },
  {
    "name": "entropy",
    "category": "feature_engineering",
    "formula": "np.zeros(1)",
    "explanation": "",
    "python_code": "def entropy(self):\n            return np.zeros(1)",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "Normal"
  },
  {
    "name": "is_available",
    "category": "feature_engineering",
    "formula": "False",
    "explanation": "",
    "python_code": "def is_available():\n                return False",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "cuda"
  },
  {
    "name": "zero_grad",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "",
    "python_code": "def zero_grad(self):\n                pass",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "Adam"
  },
  {
    "name": "step",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "",
    "python_code": "def step(self):\n                pass",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "Adam"
  },
  {
    "name": "param_groups",
    "category": "feature_engineering",
    "formula": "[{'params': []}]",
    "explanation": "",
    "python_code": "def param_groups(self):\n                return [{'params': []}]",
    "source_file": "core\\features\\deep_rl_trading.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "Adam"
  },
  {
    "name": "__init__",
    "category": "statistical",
    "formula": "",
    "explanation": "Initialize Emerging Markets Features.\n\nArgs:\n    em_rate_differential: Assumed EM vs DM rate differential (default 10%)\n    oil_correlation_window: Window for commodity correlation calc",
    "python_code": "def __init__(\n        self,\n        em_rate_differential: float = 0.10,  # Assumed EM-DM rate differential\n        oil_correlation_window: int = 60,     # Window for oil correlation proxy\n    ):\n        \"\"\"\n        Initialize Emerging Markets Features.\n\n        Args:\n            em_rate_differential: Assumed EM vs DM rate differential (default 10%)\n            oil_correlation_window: Window for commodity correlation calc\n        \"\"\"\n        self.em_rate = em_rate_differential\n        self.oil_window = oil_correlation_window",
    "source_file": "core\\features\\emerging_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "EmergingMarketsFeatures"
  },
  {
    "name": "_brl_carry_factors",
    "category": "microstructure",
    "formula": "+ rate differential | = returns.rolling(d, min_periods=1).mean() | + (self.em_rate / 252)",
    "explanation": "Brazilian Real Carry Features.\n\nBrazil maintains one of highest real interest rates globally:\n- SELIC rate often 10%+ nominal\n- Strong carry trade flows\n- DI (interbank deposit) rate correlation\n\nB3 Exchange characteristics:\n- Quantitative Brokers algorithms\n- High algorithmic trading penetration\n- Spread dynamics research\n\nReferences:\n- B3 Exchange: \"Algorithmic trading research\"\n- ScienceDirect: \"BRL carry decomposition\"",
    "python_code": "def _brl_carry_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Brazilian Real Carry Features.\n\n        Brazil maintains one of highest real interest rates globally:\n        - SELIC rate often 10%+ nominal\n        - Strong carry trade flows\n        - DI (interbank deposit) rate correlation\n\n        B3 Exchange characteristics:\n        - Quantitative Brokers algorithms\n        - High algorithmic trading penetration\n        - Spread dynamics research\n\n        References:\n        - B3 Exchange: \"Algorithmic trading research\"\n        - ScienceDirect: \"BRL carry decomposition\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. High-yield carry proxy: return + rate differential\n        for d in [10, 30]:\n            carry_return = returns.rolling(d, min_periods=1).mean()\n            # High EM rates = larger carry component\n            features[f'EM_BRL_CARRY_{d}d'] = carry_return + (self.em_rate / 252)\n\n        # 2. DI rate proxy: volatility-adjusted momentum\n        # DI rates correlate with BRL strength\n        vol = returns.rolling(20, min_periods=2).std()\n        mom = returns.rolling(20, min_periods=1).mean()\n        features['EM_BRL_DI'] = mom / (vol + 1e-12)\n\n        # 3. Carry unwind risk: sharp reversals in high-carry environment\n        # EM currencies prone to sudden stop episodes\n        cumret = returns.rolling(5, min_periods=1).sum()\n        sudden_stop = np.where(cumret < -3 * vol, 1, 0)\n        features['EM_BRL_UNWIND'] = sudden_stop\n\n        return features",
    "source_file": "core\\features\\emerging_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "EmergingMarketsFeatures"
  },
  {
    "name": "_petrocurrency_factors",
    "category": "volatility",
    "formula": "features",
    "explanation": "Petrocurrency Dynamics (RUB, NOK, CAD, MXN).\n\nOil-exporting countries' currencies correlate with oil prices:\n- RUB: Historically strong oil correlation (pre-2022)\n- NOK: Norwegian krone as petrocurrency\n- CAD: Canadian dollar oil sensitivity\n- MXN: Mexican peso oil income component\n\nWe proxy oil correlation using:\n- Momentum patterns similar to commodity cycles\n- Volatility clustering (commodity style)\n- Trend-following signals\n\nReferences:\n- IMF: \"Commodity currencies and exchange rates\"\n- BIS: \"Oil prices and exchange rates\"",
    "python_code": "def _petrocurrency_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Petrocurrency Dynamics (RUB, NOK, CAD, MXN).\n\n        Oil-exporting countries' currencies correlate with oil prices:\n        - RUB: Historically strong oil correlation (pre-2022)\n        - NOK: Norwegian krone as petrocurrency\n        - CAD: Canadian dollar oil sensitivity\n        - MXN: Mexican peso oil income component\n\n        We proxy oil correlation using:\n        - Momentum patterns similar to commodity cycles\n        - Volatility clustering (commodity style)\n        - Trend-following signals\n\n        References:\n        - IMF: \"Commodity currencies and exchange rates\"\n        - BIS: \"Oil prices and exchange rates\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Commodity momentum proxy: trend-following signal\n        # Petrocurrencies follow commodity super-cycles\n        for d in [20, 60]:\n            trend = close / close.shift(d) - 1\n            features[f'EM_PETRO_MOM_{d}d'] = trend\n\n        # 2. Commodity volatility clustering\n        # Oil price shocks create clustered volatility\n        vol = returns.rolling(20, min_periods=2).std()\n        vol_lag = vol.shift(5)\n        features['EM_PETRO_VOL_CLUST'] = vol / (vol_lag + 1e-12)\n\n        # 3. Oil shock proxy: large price moves\n        # Petrocurrencies react strongly to oil shocks\n        vol_60 = returns.rolling(60, min_periods=10).std()\n        oil_shock = np.where(returns.abs() > 2 * vol_60, 1, 0)\n        features['EM_PETRO_SHOCK'] = oil_shock\n\n        return features",
    "source_file": "core\\features\\emerging_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "EmergingMarketsFeatures"
  },
  {
    "name": "_asian_peg_factors",
    "category": "technical",
    "formula": "features",
    "explanation": "Asian Managed Currency Features (SGD, HKD, CNH).\n\nKey characteristics:\n- SGD: MAS manages via NEER band (undisclosed width)\n- HKD: Hard peg to USD (7.75-7.85 band)\n- CNH: Offshore RMB, managed float\n\nTrading strategies:\n- Band edge detection\n- Mean reversion within bands\n- Intervention anticipation\n\nReferences:\n- MAS: \"Exchange rate management in Singapore\"\n- HKMA: \"Linked exchange rate system\"\n- PBOC: \"CNH market development\"",
    "python_code": "def _asian_peg_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Asian Managed Currency Features (SGD, HKD, CNH).\n\n        Key characteristics:\n        - SGD: MAS manages via NEER band (undisclosed width)\n        - HKD: Hard peg to USD (7.75-7.85 band)\n        - CNH: Offshore RMB, managed float\n\n        Trading strategies:\n        - Band edge detection\n        - Mean reversion within bands\n        - Intervention anticipation\n\n        References:\n        - MAS: \"Exchange rate management in Singapore\"\n        - HKMA: \"Linked exchange rate system\"\n        - PBOC: \"CNH market development\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Band position: where price is relative to recent range\n        # Pegged currencies mean-revert within bands\n        rolling_min = close.rolling(60, min_periods=10).min()\n        rolling_max = close.rolling(60, min_periods=10).max()\n        band_pos = (close - rolling_min) / (rolling_max - rolling_min + 1e-12)\n        features['EM_ASIAN_BAND_POS'] = band_pos\n\n        # 2. Band edge signal: extreme positions trigger mean reversion\n        features['EM_ASIAN_BAND_EDGE'] = np.where(\n            (band_pos < 0.1) | (band_pos > 0.9), 1, 0\n        )\n\n        # 3. Intervention proxy: unusual low volatility after large move\n        vol_5 = returns.rolling(5, min_periods=2).std()\n        vol_20 = returns.rolling(20, min_periods=2).std()\n        vol_ratio = vol_5 / (vol_20 + 1e-12)\n        # Low recent vol after being at band edge = intervention\n        features['EM_ASIAN_INTERV'] = np.where(\n            (band_pos < 0.1) | (band_pos > 0.9),\n            1 - vol_ratio,  # Positive when vol drops\n            0\n        )\n\n        # 4. CNH-CNY spread proxy: offshore premium\n        # Use momentum divergence as proxy for CNH premium\n        mom_5 = returns.rolling(5, min_periods=1).mean()\n        mom_20 = returns.rolling(20, min_periods=1).mean()\n        features['EM_ASIAN_OFFSHORE'] = mom_5 - mom_20\n\n        return features",
    "source_file": "core\\features\\emerging_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "EmergingMarketsFeatures"
  },
  {
    "name": "_em_volatility_factors",
    "category": "volatility",
    "formula": "features",
    "explanation": "Emerging Market Volatility Features.\n\nEM currencies exhibit unique volatility patterns:\n- Higher baseline volatility than DM\n- Fat tails (kurtosis)\n- Volatility asymmetry (larger on depreciation)\n- Contagion during crises\n\nReferences:\n- BIS: \"Volatility patterns in EM currencies\"\n- IMF: \"Currency crisis early warning indicators\"",
    "python_code": "def _em_volatility_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Emerging Market Volatility Features.\n\n        EM currencies exhibit unique volatility patterns:\n        - Higher baseline volatility than DM\n        - Fat tails (kurtosis)\n        - Volatility asymmetry (larger on depreciation)\n        - Contagion during crises\n\n        References:\n        - BIS: \"Volatility patterns in EM currencies\"\n        - IMF: \"Currency crisis early warning indicators\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. EM volatility premium: vol relative to historical\n        vol = returns.rolling(20, min_periods=2).std() * np.sqrt(252)\n        vol_hist = returns.rolling(252, min_periods=60).std() * np.sqrt(252)\n        features['EM_VOL_PREM'] = vol / (vol_hist + 1e-12)\n\n        # 2. Fat tails indicator: excess kurtosis\n        kurt = returns.rolling(60, min_periods=20).apply(\n            lambda x: stats.kurtosis(x, nan_policy='omit') if len(x) > 10 else 0,\n            raw=True\n        )\n        features['EM_FAT_TAILS'] = kurt\n\n        # 3. Volatility asymmetry: depreciation vol vs appreciation vol\n        pos_ret = returns.where(returns > 0, 0)\n        neg_ret = returns.where(returns < 0, 0)\n        vol_pos = pos_ret.rolling(30, min_periods=5).std()\n        vol_neg = neg_ret.abs().rolling(30, min_periods=5).std()\n        features['EM_VOL_ASYM'] = (vol_neg - vol_pos) / (vol_neg + vol_pos + 1e-12)\n\n        # 4. Crisis indicator: simultaneous high vol and depreciation\n        high_vol = vol > vol.rolling(60, min_periods=20).quantile(0.9)\n        depreciation = returns.rolling(5, min_periods=1).sum() < -0.02  # 2% drop\n        features['EM_CRISIS_IND'] = np.where(high_vol & depreciation, 1, 0)\n\n        return features",
    "source_file": "core\\features\\emerging_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "EmergingMarketsFeatures"
  },
  {
    "name": "_commodity_fx_factors",
    "category": "statistical",
    "formula": "features",
    "explanation": "Commodity-Linked Currency Features (AUD, NZD, ZAR, CLP).\n\nCurrencies of commodity exporters:\n- AUD: Iron ore, coal\n- NZD: Dairy, agricultural\n- ZAR: Gold, platinum, coal\n- CLP: Copper\n\nCommon characteristics:\n- Terms of trade sensitivity\n- Risk-on/risk-off correlation\n- Global growth sensitivity\n\nReferences:\n- RBA: \"AUD and commodity prices\"\n- SARB: \"ZAR as a commodity currency\"",
    "python_code": "def _commodity_fx_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Commodity-Linked Currency Features (AUD, NZD, ZAR, CLP).\n\n        Currencies of commodity exporters:\n        - AUD: Iron ore, coal\n        - NZD: Dairy, agricultural\n        - ZAR: Gold, platinum, coal\n        - CLP: Copper\n\n        Common characteristics:\n        - Terms of trade sensitivity\n        - Risk-on/risk-off correlation\n        - Global growth sensitivity\n\n        References:\n        - RBA: \"AUD and commodity prices\"\n        - SARB: \"ZAR as a commodity currency\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Terms of trade proxy: trend strength\n        # Commodity currencies follow global growth trends\n        trend_60 = close / close.shift(60) - 1\n        trend_20 = close / close.shift(20) - 1\n        features['EM_TOT_TREND'] = trend_60\n\n        # 2. Global growth sensitivity: beta to market moves\n        # Rolling beta to market proxy (own momentum as proxy)\n        market_proxy = returns.rolling(60, min_periods=20).mean()\n        cov = returns.rolling(60, min_periods=20).cov(market_proxy)\n        var_mkt = market_proxy.rolling(60, min_periods=20).var()\n        features['EM_GROWTH_BETA'] = cov / (var_mkt + 1e-12)\n\n        # 3. Risk-on indicator: positive when risk appetite high\n        # Use volatility as inverse risk appetite\n        vol = returns.rolling(20, min_periods=2).std()\n        vol_zscore = (vol - vol.rolling(120, min_periods=30).mean()) / (vol.rolling(120, min_periods=30).std() + 1e-12)\n        features['EM_RISK_ON'] = -vol_zscore  # Positive in low vol (risk-on)\n\n        # 4. Commodity cycle position: where in the cycle\n        ma_20 = close.rolling(20, min_periods=1).mean()\n        ma_100 = close.rolling(100, min_periods=20).mean()\n        features['EM_COMM_CYCLE'] = (ma_20 - ma_100) / (ma_100 + 1e-12)\n\n        return features",
    "source_file": "core\\features\\emerging_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "EmergingMarketsFeatures"
  },
  {
    "name": "generate_all",
    "category": "reinforcement_learning",
    "formula": "features",
    "explanation": "Generate all Emerging Markets features.\n\nArgs:\n    df: DataFrame with columns: open, high, low, close, volume\n\nReturns:\n    DataFrame with 20 factor columns",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all Emerging Markets features.\n\n        Args:\n            df: DataFrame with columns: open, high, low, close, volume\n\n        Returns:\n            DataFrame with 20 factor columns\n        \"\"\"\n        # Ensure required columns\n        if 'close' not in df.columns:\n            raise ValueError(\"Missing required column: 'close'\")\n\n        # Fill missing OHLC from close\n        df = df.copy()\n        if 'open' not in df.columns:\n            df['open'] = df['close'].shift(1).fillna(df['close'])\n        if 'high' not in df.columns:\n            df['high'] = df['close']\n        if 'low' not in df.columns:\n            df['low'] = df['close']\n\n        # Generate all factor groups\n        brl = self._brl_carry_factors(df)\n        petro = self._petrocurrency_factors(df)\n        asian = self._asian_peg_factors(df)\n        vol = self._em_volatility_factors(df)\n        commodity = self._commodity_fx_factors(df)\n\n        # Combine all features\n        features = pd.concat([\n            brl, petro, asian, vol, commodity\n        ], axis=1)\n\n        # Clean up\n        features = features.replace([np.inf, -np.inf], np.nan)\n        features = features.fillna(0)\n\n        return features",
    "source_file": "core\\features\\emerging_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "EmergingMarketsFeatures"
  },
  {
    "name": "get_feature_names",
    "category": "feature_engineering",
    "formula": "names",
    "explanation": "Get list of all feature names.",
    "python_code": "def get_feature_names(self) -> List[str]:\n        \"\"\"Get list of all feature names.\"\"\"\n        names = []\n\n        # BRL Carry (4)\n        names.extend(['EM_BRL_CARRY_10d', 'EM_BRL_CARRY_30d', 'EM_BRL_DI',\n                      'EM_BRL_UNWIND'])\n\n        # Petrocurrency (4)\n        names.extend(['EM_PETRO_MOM_20d', 'EM_PETRO_MOM_60d', 'EM_PETRO_VOL_CLUST',\n                      'EM_PETRO_SHOCK'])\n\n        # Asian Pegs (4)\n        names.extend(['EM_ASIAN_BAND_POS', 'EM_ASIAN_BAND_EDGE', 'EM_ASIAN_INTERV',\n                      'EM_ASIAN_OFFSHORE'])\n\n        # EM Volatility (4)\n        names.extend(['EM_VOL_PREM', 'EM_FAT_TAILS', 'EM_VOL_ASYM',\n                      'EM_CRISIS_IND'])\n\n        # Commodity FX (4)\n        names.extend(['EM_TOT_TREND', 'EM_GROWTH_BETA', 'EM_RISK_ON',\n                      'EM_COMM_CYCLE'])\n\n        return names",
    "source_file": "core\\features\\emerging_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "EmergingMarketsFeatures"
  },
  {
    "name": "get_factor_category",
    "category": "reinforcement_learning",
    "formula": "'BRL High-Yield Carry' | 'Petrocurrency Dynamics' | 'Asian Managed Currencies'",
    "explanation": "Get the category of a factor by name.",
    "python_code": "def get_factor_category(self, factor_name: str) -> str:\n        \"\"\"Get the category of a factor by name.\"\"\"\n        if 'BRL' in factor_name:\n            return 'BRL High-Yield Carry'\n        elif 'PETRO' in factor_name:\n            return 'Petrocurrency Dynamics'\n        elif 'ASIAN' in factor_name:\n            return 'Asian Managed Currencies'\n        elif any(x in factor_name for x in ['VOL_PREM', 'FAT_TAILS', 'VOL_ASYM', 'CRISIS']):\n            return 'EM Volatility'\n        elif any(x in factor_name for x in ['TOT', 'GROWTH_BETA', 'RISK_ON', 'COMM_CYCLE']):\n            return 'Commodity FX'\n        return 'Unknown'",
    "source_file": "core\\features\\emerging_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "EmergingMarketsFeatures"
  },
  {
    "name": "generate_emerging_features",
    "category": "reinforcement_learning",
    "formula": "features.generate_all(df)",
    "explanation": "Generate Emerging Markets features.\n\nArgs:\n    df: OHLCV DataFrame\n\nReturns:\n    DataFrame with 20 emerging market factors",
    "python_code": "def generate_emerging_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generate Emerging Markets features.\n\n    Args:\n        df: OHLCV DataFrame\n\n    Returns:\n        DataFrame with 20 emerging market factors\n    \"\"\"\n    features = EmergingMarketsFeatures()\n    return features.generate_all(df)",
    "source_file": "core\\features\\emerging_quant.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, lookback: int = 200):\n        self.lookback = lookback\n        self.prices = []\n        self.volumes = []\n        self.timestamps = []",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "FastTechnicalFeatures"
  },
  {
    "name": "update",
    "category": "feature_engineering",
    "formula": "features.\"\"\"",
    "explanation": "Update with new tick and return features.",
    "python_code": "def update(self, price: float, volume: float = 0.0,\n               timestamp: datetime = None) -> Dict[str, float]:\n        \"\"\"Update with new tick and return features.\"\"\"\n        self.prices.append(price)\n        self.volumes.append(volume)\n        self.timestamps.append(timestamp or datetime.now())\n\n        # Keep only lookback\n        if len(self.prices) > self.lookback:\n            self.prices = self.prices[-self.lookback:]\n            self.volumes = self.volumes[-self.lookback:]\n            self.timestamps = self.timestamps[-self.lookback:]\n\n        return self.compute_features()",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "FastTechnicalFeatures"
  },
  {
    "name": "compute_features",
    "category": "feature_engineering",
    "formula": "{} | features",
    "explanation": "Compute all fast technical features.",
    "python_code": "def compute_features(self) -> Dict[str, float]:\n        \"\"\"Compute all fast technical features.\"\"\"\n        if len(self.prices) < 10:\n            return {}\n\n        prices = np.array(self.prices)\n        volumes = np.array(self.volumes)\n\n        features = {}\n\n        # Returns at different lags\n        for lag in [1, 5, 10, 20]:\n            if len(prices) > lag:\n                ret = (prices[-1] / prices[-lag-1] - 1) * 10000  # bps\n                features[f'return_{lag}'] = ret\n\n        # Volatility estimates\n        if len(prices) > 20:\n            returns = np.diff(np.log(prices[-21:]))\n            features['vol_20'] = np.std(returns) * np.sqrt(252 * 24 * 60)  # Annualized\n\n        if len(prices) > 5:\n            returns = np.diff(np.log(prices[-6:]))\n            features['vol_5'] = np.std(returns) * np.sqrt(252 * 24 * 60)\n\n        # Momentum indicators\n        if len(prices) > 20:\n            features['momentum_20'] = (prices[-1] / prices[-20] - 1) * 10000\n\n        if len(prices) > 10:\n            features['momentum_10'] = (prices[-1] / prices[-10] - 1) * 10000\n\n        # Mean reversion signals\n        for window in [10, 20, 50]:\n            if len(prices) > window:\n                ma = np.mean(prices[-window:])\n                std = np.std(prices[-window:])\n                if std > 0:\n                    features[f'zscore_{window}'] = (prices[-1] - ma) / std\n\n        # Price acceleration\n        if len(prices) > 10:\n            ret_5 = prices[-1] / prices[-6] - 1 if len(prices) > 5 else 0\n            ret_10 = prices[-1] / prices[-11] - 1\n            features['acceleration'] = (ret_5 - ret_10 / 2) * 10000\n\n        # High/Low range position\n        for window in [20, 50]:\n            if len(prices) > window:\n                high = np.max(prices[-window:])\n                low = np.min(prices[-window:])\n                rng = high - low\n                if rng > 0:\n                    features[f'range_position_{window}'] = (prices[-1] - low) / rng\n\n        # Volume features\n        if len(volumes) > 20 and np.sum(volumes) > 0:\n            avg_vol = np.mean(volumes[-20:])\n            if avg_vol > 0:\n                features['volume_ratio'] = volumes[-1] / avg_vol\n\n            # Volume-weighted price\n            vwap = np.sum(prices[-20:] * volumes[-20:]) / np.sum(volumes[-20:])\n            features['vwap_deviation'] = (prices[-1] - vwap) / prices[-1] * 10000\n\n        # Tick direction\n        if len(prices) > 1:\n            features['tick_direction'] = 1 if prices[-1] > prices[-2] else (-1 if prices[-1] < prices[-2] else 0)\n\n        # Consecutive moves\n        if len(prices) > 5:\n            directions = np.sign(np.diff(prices[-5:]))\n            features['consecutive_ups'] = np.sum(directions > 0)\n            features['consecutive_downs'] = np.sum(directions < 0)\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "FastTechnicalFeatures"
  },
  {
    "name": "_lazy_load_alpha101",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Lazy load Alpha101 module.",
    "python_code": "def _lazy_load_alpha101(self):\n        \"\"\"Lazy load Alpha101 module.\"\"\"\n        if self._alpha101 is None:\n            try:\n                from core.features.alpha101 import Alpha101Complete\n                self._alpha101 = Alpha101Complete()\n            except ImportError:\n                logger.warning(\"Alpha101 module not available\")\n                self._alpha101 = None\n        return self._alpha101",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_lazy_load_renaissance",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Lazy load Renaissance signals module.",
    "python_code": "def _lazy_load_renaissance(self):\n        \"\"\"Lazy load Renaissance signals module.\"\"\"\n        if self._renaissance is None:\n            try:\n                from core.features.renaissance import RenaissanceSignalGenerator\n                self._renaissance = RenaissanceSignalGenerator()\n            except ImportError:\n                logger.warning(\"Renaissance signals module not available\")\n                self._renaissance = None\n        return self._renaissance",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_lazy_load_order_flow",
    "category": "microstructure",
    "formula": "",
    "explanation": "Lazy load Order Flow features module.",
    "python_code": "def _lazy_load_order_flow(self):\n        \"\"\"Lazy load Order Flow features module.\"\"\"\n        if self._order_flow is None:\n            try:\n                from core.order_flow_features import OrderFlowFeatures\n                self._order_flow = OrderFlowFeatures()\n            except ImportError:\n                logger.warning(\"Order Flow features module not available\")\n                self._order_flow = None\n        return self._order_flow",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_lazy_load_microstructure",
    "category": "volatility",
    "formula": "",
    "explanation": "Lazy load Microstructure volatility module.",
    "python_code": "def _lazy_load_microstructure(self):\n        \"\"\"Lazy load Microstructure volatility module.\"\"\"\n        if self._microstructure is None:\n            try:\n                from core.microstructure_vol import MicrostructureVolatility\n                self._microstructure = MicrostructureVolatility()\n            except ImportError:\n                logger.warning(\"Microstructure volatility module not available\")\n                self._microstructure = None\n        return self._microstructure",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_lazy_load_alpha191",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Lazy load  Alpha191 module.",
    "python_code": "def _lazy_load_alpha191(self):\n        \"\"\"Lazy load  Alpha191 module.\"\"\"\n        if self._alpha191 is None:\n            try:\n                from core.alpha191_guotaijunan import Alpha191GuotaiJunan\n                self._alpha191 = Alpha191GuotaiJunan()\n            except ImportError:\n                logger.warning(\"Alpha191 module not available\")\n                self._alpha191 = None\n        return self._alpha191",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_lazy_load_har_rv",
    "category": "volatility",
    "formula": "",
    "explanation": "Lazy load HAR-RV volatility module.",
    "python_code": "def _lazy_load_har_rv(self):\n        \"\"\"Lazy load HAR-RV volatility module.\"\"\"\n        if self._har_rv is None:\n            try:\n                from core.har_rv_volatility import HARRVVolatility\n                self._har_rv = HARRVVolatility()\n            except ImportError:\n                logger.warning(\"HAR-RV volatility module not available\")\n                self._har_rv = None\n        return self._har_rv",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_lazy_load_range_vol",
    "category": "volatility",
    "formula": "",
    "explanation": "Lazy load Range-Based Volatility module.\nIncludes: Parkinson (1980), Garman-Klass (1980), Rogers-Satchell (1991), Yang-Zhang (2000)",
    "python_code": "def _lazy_load_range_vol(self):\n        \"\"\"\n        Lazy load Range-Based Volatility module.\n        Includes: Parkinson (1980), Garman-Klass (1980), Rogers-Satchell (1991), Yang-Zhang (2000)\n        \"\"\"\n        if self._range_vol is None:\n            try:\n                from core.range_volatility import RangeVolatility\n                self._range_vol = RangeVolatility()\n            except ImportError:\n                logger.warning(\"Range volatility module not available\")\n                self._range_vol = None\n        return self._range_vol",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_lazy_load_market_impact",
    "category": "microstructure",
    "formula": "",
    "explanation": "Lazy load Market Impact models module.\nIncludes: Kyle (1985), Glosten-Milgrom (1985), Roll (1984), Huang-Stoll (1997)",
    "python_code": "def _lazy_load_market_impact(self):\n        \"\"\"\n        Lazy load Market Impact models module.\n        Includes: Kyle (1985), Glosten-Milgrom (1985), Roll (1984), Huang-Stoll (1997)\n        \"\"\"\n        if self._market_impact is None:\n            try:\n                from core.market_impact_models import MarketImpactFeatures\n                self._market_impact = MarketImpactFeatures()\n            except ImportError:\n                logger.warning(\"Market impact module not available\")\n                self._market_impact = None\n        return self._market_impact",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_lazy_load_kalman",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Lazy load Advanced Kalman Filter module.\nIncludes: EKF, UKF, Adaptive Kalman, Kalman Ensemble\nSource: Bar-Shalom \"Estimation with Applications to Tracking\"",
    "python_code": "def _lazy_load_kalman(self):\n        \"\"\"\n        Lazy load Advanced Kalman Filter module.\n        Includes: EKF, UKF, Adaptive Kalman, Kalman Ensemble\n        Source: Bar-Shalom \"Estimation with Applications to Tracking\"\n        \"\"\"\n        if self._kalman is None:\n            try:\n                from core.advanced_kalman_filters import AdaptiveKalmanFilter\n                self._kalman = AdaptiveKalmanFilter(base_filter='ukf')\n            except ImportError:\n                logger.warning(\"Advanced Kalman filters module not available\")\n                self._kalman = None\n        return self._kalman",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_lazy_load_garch",
    "category": "volatility",
    "formula": "",
    "explanation": "Lazy load GARCH volatility module.\nIncludes: GARCH, EGARCH, GJR-GARCH, Regime-Switching GARCH\nSource: Bollerslev (1986), Nelson (1991)",
    "python_code": "def _lazy_load_garch(self):\n        \"\"\"\n        Lazy load GARCH volatility module.\n        Includes: GARCH, EGARCH, GJR-GARCH, Regime-Switching GARCH\n        Source: Bollerslev (1986), Nelson (1991)\n        \"\"\"\n        if self._garch is None:\n            try:\n                from core.arima_garch_models import VolatilityForecaster\n                self._garch = VolatilityForecaster()\n            except ImportError:\n                logger.warning(\"GARCH module not available\")\n                self._garch = None\n        return self._garch",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_lazy_load_jump_detection",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Lazy load Jump Detection module.\nIncludes: Bipower Variation, BNS Test, Lee-Mykland\nSource: Barndorff-Nielsen & Shephard (2004)",
    "python_code": "def _lazy_load_jump_detection(self):\n        \"\"\"\n        Lazy load Jump Detection module.\n        Includes: Bipower Variation, BNS Test, Lee-Mykland\n        Source: Barndorff-Nielsen & Shephard (2004)\n        \"\"\"\n        if self._jump_detection is None:\n            try:\n                from core.jump_detection import JumpVolatilityModel\n                self._jump_detection = JumpVolatilityModel()\n            except ImportError:\n                logger.warning(\"Jump detection module not available\")\n                self._jump_detection = None\n        return self._jump_detection",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_lazy_load_regime",
    "category": "regime",
    "formula": "",
    "explanation": "Lazy load Regime Detection module.\nIncludes: HMM 3-state, Regime-Scaled Features\nSource: Hamilton (1989)",
    "python_code": "def _lazy_load_regime(self):\n        \"\"\"\n        Lazy load Regime Detection module.\n        Includes: HMM 3-state, Regime-Scaled Features\n        Source: Hamilton (1989)\n        \"\"\"\n        if self._regime is None:\n            try:\n                from core.regime_features import RegimeDependentFeatureEngine\n                self._regime = RegimeDependentFeatureEngine()\n            except ImportError:\n                logger.warning(\"Regime detection module not available\")\n                self._regime = None\n        return self._regime",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_lazy_load_spectral",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Lazy load Spectral/Wavelet Analysis module.\nIncludes: FFT, Welch PSD, DWT, CWT, Hilbert Transform\nSource: Mallat (1989), Oppenheim & Schafer",
    "python_code": "def _lazy_load_spectral(self):\n        \"\"\"\n        Lazy load Spectral/Wavelet Analysis module.\n        Includes: FFT, Welch PSD, DWT, CWT, Hilbert Transform\n        Source: Mallat (1989), Oppenheim & Schafer\n        \"\"\"\n        if self._spectral is None:\n            try:\n                from core.spectral_analysis import SpectralFeatureEngine\n                self._spectral = SpectralFeatureEngine()\n            except ImportError:\n                logger.warning(\"Spectral analysis module not available\")\n                self._spectral = None\n        return self._spectral",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_lazy_load_chinese_hft",
    "category": "microstructure",
    "formula": "",
    "explanation": "Lazy load Chinese HFT Factors module.\nIncludes: Microprice (Stoikov 2018), Smart Money Factor (),\n          Kyle Lambda, Amihud Illiquidity, Integrated OFI (Cont 2014)\nSource: QUANTAXIS, QuantsPlaybook, HftBacktest (Gitee/GitHub)",
    "python_code": "def _lazy_load_chinese_hft(self):\n        \"\"\"\n        Lazy load Chinese HFT Factors module.\n        Includes: Microprice (Stoikov 2018), Smart Money Factor (),\n                  Kyle Lambda, Amihud Illiquidity, Integrated OFI (Cont 2014)\n        Source: QUANTAXIS, QuantsPlaybook, HftBacktest (Gitee/GitHub)\n        \"\"\"\n        if self._chinese_hft is None:\n            try:\n                from core.chinese_hft_factors import ChineseHFTFactors\n                self._chinese_hft = ChineseHFTFactors()\n            except ImportError:\n                logger.warning(\"Chinese HFT factors module not available\")\n                self._chinese_hft = None\n        return self._chinese_hft",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_lazy_load_lob_features",
    "category": "microstructure",
    "formula": "",
    "explanation": "Lazy load LOB Features module.\nIncludes: Lee-Ready Classification (85% accuracy), Book Imbalance,\n          Queue Imbalance, Spread Decomposition, LOB Slope\nSource: HftBacktest, LOBSTER, Academic literature",
    "python_code": "def _lazy_load_lob_features(self):\n        \"\"\"\n        Lazy load LOB Features module.\n        Includes: Lee-Ready Classification (85% accuracy), Book Imbalance,\n                  Queue Imbalance, Spread Decomposition, LOB Slope\n        Source: HftBacktest, LOBSTER, Academic literature\n        \"\"\"\n        if self._lob_features is None:\n            try:\n                from core.lob_features import LOBFeatureEngine\n                self._lob_features = LOBFeatureEngine()\n            except ImportError:\n                logger.warning(\"LOB features module not available\")\n                self._lob_features = None\n        return self._lob_features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_lazy_load_elite_quant",
    "category": "technical",
    "formula": "",
    "explanation": "Lazy load Elite Quant Factors module.\nIncludes: IC/ICIR Engine, PCA Orthogonalization, Elastic Net Combination,\n          Higher-Order Moments (skew, kurtosis, tail risk), Intraday Momentum\nSource: , ,  (Chinese hedge funds)",
    "python_code": "def _lazy_load_elite_quant(self):\n        \"\"\"\n        Lazy load Elite Quant Factors module.\n        Includes: IC/ICIR Engine, PCA Orthogonalization, Elastic Net Combination,\n                  Higher-Order Moments (skew, kurtosis, tail risk), Intraday Momentum\n        Source: , ,  (Chinese hedge funds)\n        \"\"\"\n        if self._elite_quant is None:\n            try:\n                from core.elite_quant_factors import EliteQuantEngine\n                self._elite_quant = EliteQuantEngine()\n            except ImportError:\n                logger.warning(\"Elite quant factors module not available\")\n                self._elite_quant = None\n        return self._elite_quant",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_lazy_load_genetic_mining",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Lazy load Genetic Factor Mining module.\nIncludes: gplearn genetic programming, DEAP evolutionary algorithms\nSource: ,  DFQ",
    "python_code": "def _lazy_load_genetic_mining(self):\n        \"\"\"\n        Lazy load Genetic Factor Mining module.\n        Includes: gplearn genetic programming, DEAP evolutionary algorithms\n        Source: ,  DFQ\n        \"\"\"\n        if self._genetic_mining is None:\n            try:\n                from core.genetic_factor_mining import GeneticFactorEngine\n                self._genetic_mining = GeneticFactorEngine()\n            except ImportError:\n                logger.warning(\"Genetic factor mining module not available\")\n                self._genetic_mining = None\n        return self._genetic_mining",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_lazy_load_tsfresh",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Lazy load TSFresh Automatic Feature Extraction module.\nIncludes: 65,000+ automatic time series features\nSource:  6+",
    "python_code": "def _lazy_load_tsfresh(self):\n        \"\"\"\n        Lazy load TSFresh Automatic Feature Extraction module.\n        Includes: 65,000+ automatic time series features\n        Source:  6+\n        \"\"\"\n        if self._tsfresh is None:\n            try:\n                from core.tsfresh_auto_features import TSFreshFactorEngine\n                self._tsfresh = TSFreshFactorEngine(mode='quick')\n            except ImportError:\n                logger.warning(\"TSFresh module not available\")\n                self._tsfresh = None\n        return self._tsfresh",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "initialize",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Initialize engine with historical data.\n\nArgs:\n    historical_data: DataFrame with columns [timestamp, bid, ask, volume]\n    symbol: Trading symbol",
    "python_code": "def initialize(self, historical_data: pd.DataFrame, symbol: str = \"EURUSD\"):\n        \"\"\"\n        Initialize engine with historical data.\n\n        Args:\n            historical_data: DataFrame with columns [timestamp, bid, ask, volume]\n            symbol: Trading symbol\n        \"\"\"\n        logger.info(f\"Initializing HFT Feature Engine for {symbol}\")\n\n        # Store in buffers\n        if 'bid' in historical_data.columns and 'ask' in historical_data.columns:\n            prices = (historical_data['bid'] + historical_data['ask']) / 2\n        elif 'close' in historical_data.columns:\n            prices = historical_data['close']\n        elif 'price' in historical_data.columns:\n            prices = historical_data['price']\n        else:\n            raise ValueError(\"No price column found (bid/ask, close, or price)\")\n\n        self.price_buffer[symbol] = prices.tolist()[-1000:]\n\n        if 'volume' in historical_data.columns:\n            self.volume_buffer[symbol] = historical_data['volume'].tolist()[-1000:]\n        else:\n            self.volume_buffer[symbol] = [0.0] * len(self.price_buffer[symbol])\n\n        self.tick_buffer[symbol] = historical_data.tail(1000).copy()\n\n        # Pre-compute features on historical data\n        logger.info(\"Pre-computing features on historical data...\")\n\n        # Fast technical\n        for price, vol in zip(self.price_buffer[symbol], self.volume_buffer[symbol]):\n            self.fast_tech.update(price, vol)\n\n        # Build feature names from first computation\n        features = self._compute_all_features(symbol)\n        self.feature_names = sorted(features.keys())\n\n        logger.info(f\"Initialized with {len(self.feature_names)} features\")",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "process_tick",
    "category": "feature_engineering",
    "formula": "features. | all_features",
    "explanation": "Process a single tick and return features.\n\nArgs:\n    symbol: Trading symbol\n    bid: Bid price\n    ask: Ask price\n    volume: Trade volume\n    timestamp: Tick timestamp\n\nReturns:\n    Dict of feature_name -> feature_value",
    "python_code": "def process_tick(self, symbol: str, bid: float, ask: float,\n                     volume: float = 0.0, timestamp: datetime = None) -> Dict[str, float]:\n        \"\"\"\n        Process a single tick and return features.\n\n        Args:\n            symbol: Trading symbol\n            bid: Bid price\n            ask: Ask price\n            volume: Trade volume\n            timestamp: Tick timestamp\n\n        Returns:\n            Dict of feature_name -> feature_value\n        \"\"\"\n        mid = (bid + ask) / 2\n        spread = ask - bid\n\n        # Update buffers\n        if symbol not in self.price_buffer:\n            self.price_buffer[symbol] = []\n            self.volume_buffer[symbol] = []\n\n        self.price_buffer[symbol].append(mid)\n        self.volume_buffer[symbol].append(volume)\n\n        # Keep buffer size manageable\n        if len(self.price_buffer[symbol]) > 1000:\n            self.price_buffer[symbol] = self.price_buffer[symbol][-1000:]\n            self.volume_buffer[symbol] = self.volume_buffer[symbol][-1000:]\n\n        # Update fast technical\n        tech_features = self.fast_tech.update(mid, volume, timestamp)\n\n        # Add spread features\n        tech_features['spread_bps'] = spread / mid * 10000\n        tech_features['mid_price'] = mid\n\n        # Compute full features periodically (every 10 ticks for efficiency)\n        tick_count = len(self.price_buffer[symbol])\n\n        if tick_count % 10 == 0 or symbol not in self.feature_cache:\n            all_features = self._compute_all_features(symbol)\n            all_features.update(tech_features)\n            self.feature_cache[symbol] = all_features\n            self.last_feature_time[symbol] = timestamp or datetime.now()\n        else:\n            # Use cached features with updated tick-level features\n            all_features = self.feature_cache.get(symbol, {}).copy()\n            all_features.update(tech_features)\n\n        return all_features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_all_features",
    "category": "feature_engineering",
    "formula": "features | features",
    "explanation": "Compute all feature groups.",
    "python_code": "def _compute_all_features(self, symbol: str) -> Dict[str, float]:\n        \"\"\"Compute all feature groups.\"\"\"\n        features = {}\n        prices = np.array(self.price_buffer.get(symbol, []))\n        volumes = np.array(self.volume_buffer.get(symbol, []))\n\n        if len(prices) < 20:\n            return features\n\n        # 1. Alpha101 features\n        if self.config.enable_alpha101:\n            alpha_features = self._compute_alpha101_features(prices, volumes)\n            features.update(alpha_features)\n\n        # 2. Renaissance signals\n        if self.config.enable_renaissance:\n            ren_features = self._compute_renaissance_features(prices, volumes)\n            features.update(ren_features)\n\n        # 3. Order flow features\n        if self.config.enable_order_flow:\n            of_features = self._compute_order_flow_features(prices, volumes)\n            features.update(of_features)\n\n        # 4. Microstructure features\n        if self.config.enable_microstructure:\n            micro_features = self._compute_microstructure_features(prices)\n            features.update(micro_features)\n\n        # 5. Cross-asset features (if available)\n        if self.config.enable_cross_asset:\n            cross_features = self._compute_cross_asset_features(symbol)\n            features.update(cross_features)\n\n        # 6.  Alpha191 features\n        if self.config.enable_alpha191:\n            alpha191_features = self._compute_alpha191_features(prices, volumes)\n            features.update(alpha191_features)\n\n        # 7. HAR-RV volatility features\n        if self.config.enable_har_rv:\n            har_rv_features = self._compute_har_rv_features(prices)\n            features.update(har_rv_features)\n\n        # 8. Range-based volatility features (Parkinson, Garman-Klass, Yang-Zhang)\n        if self.config.enable_range_vol:\n            range_vol_features = self._compute_range_vol_features(prices, volumes)\n            features.update(range_vol_features)\n\n        # 9. Market impact features (Kyle, Glosten-Milgrom, Roll)\n        if self.config.enable_market_impact:\n            market_impact_features = self._compute_market_impact_features(prices, volumes)\n            features.update(market_impact_features)\n\n        # 10. Advanced Kalman Filter features (EKF/UKF)\n        if self.config.enable_kalman:\n            kalman_features = self._compute_kalman_features(prices)\n            features.update(kalman_features)\n\n        # 11. GARCH volatility features\n        if self.config.enable_garch:\n            garch_features = self._compute_garch_features(prices)\n            features.update(garch_features)\n\n        # 12. Jump detection features (Bipower Variation)\n        if self.config.enable_jump_detection:\n            jump_features = self._compute_jump_features(prices)\n            features.update(jump_features)\n\n        # 13. Regime features (HMM)\n        if self.config.enable_regime:\n            regime_features = self._compute_regime_features(prices)\n            features.update(reg",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_alpha101_features",
    "category": "alpha_factor",
    "formula": "features | features",
    "explanation": "Compute Alpha101 features.",
    "python_code": "def _compute_alpha101_features(self, prices: np.ndarray,\n                                   volumes: np.ndarray) -> Dict[str, float]:\n        \"\"\"Compute Alpha101 features.\"\"\"\n        features = {}\n\n        alpha101 = self._lazy_load_alpha101()\n        if alpha101 is None:\n            return features\n\n        try:\n            # Create minimal DataFrame for Alpha101\n            df = pd.DataFrame({\n                'open': prices * 0.999,  # Approximate open\n                'high': prices * 1.0002,  # Approximate high\n                'low': prices * 0.9998,   # Approximate low\n                'close': prices,\n                'volume': volumes if len(volumes) == len(prices) else np.ones(len(prices))\n            })\n\n            # Use generate_all_alphas for efficient computation\n            result = alpha101.generate_all_alphas(df)\n\n            # Extract last values for each alpha column\n            for col in result.columns:\n                if col.startswith('alpha') and col not in ['open', 'high', 'low', 'close', 'volume']:\n                    val = result[col].iloc[-1]\n                    if not np.isnan(val) and not np.isinf(val):\n                        features[f'alpha101_{col}'] = float(val)\n\n        except Exception as e:\n            logger.debug(f\"Alpha101 computation error: {e}\")\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_renaissance_features",
    "category": "feature_engineering",
    "formula": "features",
    "explanation": "Compute Renaissance-style signals.",
    "python_code": "def _compute_renaissance_features(self, prices: np.ndarray,\n                                      volumes: np.ndarray) -> Dict[str, float]:\n        \"\"\"Compute Renaissance-style signals.\"\"\"\n        features = {}\n\n        renaissance = self._lazy_load_renaissance()\n        if renaissance is None:\n            # Fallback: compute basic Renaissance-style signals\n            return self._compute_basic_renaissance(prices, volumes)\n\n        try:\n            df = pd.DataFrame({\n                'open': prices,\n                'high': prices,\n                'low': prices,\n                'close': prices,\n                'volume': volumes if len(volumes) == len(prices) else np.ones(len(prices))\n            })\n\n            result = renaissance.generate_all_signals(df)\n\n            # Extract last values\n            for col in result.columns:\n                if col not in ['open', 'high', 'low', 'close', 'volume']:\n                    val = result[col].iloc[-1]\n                    if not np.isnan(val) and not np.isinf(val):\n                        features[f'ren_{col}'] = float(val)\n\n        except Exception as e:\n            logger.debug(f\"Renaissance computation error: {e}\")\n            return self._compute_basic_renaissance(prices, volumes)\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_basic_renaissance",
    "category": "feature_engineering",
    "formula": "features",
    "explanation": "Basic Renaissance-style signals without full module.",
    "python_code": "def _compute_basic_renaissance(self, prices: np.ndarray,\n                                   volumes: np.ndarray) -> Dict[str, float]:\n        \"\"\"Basic Renaissance-style signals without full module.\"\"\"\n        features = {}\n\n        # Trend signals\n        for window in [5, 10, 20, 50]:\n            if len(prices) > window:\n                ma = np.mean(prices[-window:])\n                features[f'ren_trend_{window}'] = (prices[-1] / ma - 1) * 10000\n\n                # MA slope\n                if len(prices) > window + 5:\n                    ma_prev = np.mean(prices[-window-5:-5])\n                    features[f'ren_slope_{window}'] = (ma / ma_prev - 1) * 10000\n\n        # Mean reversion\n        for window in [10, 20, 50]:\n            if len(prices) > window:\n                mean = np.mean(prices[-window:])\n                std = np.std(prices[-window:])\n                if std > 0:\n                    features[f'ren_mr_{window}'] = (prices[-1] - mean) / std\n\n        # Momentum\n        for lag in [5, 10, 20]:\n            if len(prices) > lag:\n                features[f'ren_mom_{lag}'] = (prices[-1] / prices[-lag-1] - 1) * 10000\n\n        # Volatility regime\n        if len(prices) > 20:\n            vol_short = np.std(np.diff(np.log(prices[-10:])))\n            vol_long = np.std(np.diff(np.log(prices[-50:]))) if len(prices) > 50 else vol_short\n            if vol_long > 0:\n                features['ren_vol_regime'] = vol_short / vol_long\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_order_flow_features",
    "category": "microstructure",
    "formula": "features",
    "explanation": "Compute order flow features.",
    "python_code": "def _compute_order_flow_features(self, prices: np.ndarray,\n                                     volumes: np.ndarray) -> Dict[str, float]:\n        \"\"\"Compute order flow features.\"\"\"\n        features = {}\n\n        order_flow = self._lazy_load_order_flow()\n        if order_flow is None:\n            return self._compute_basic_order_flow(prices, volumes)\n\n        try:\n            # Update order flow with recent trades\n            for i in range(-min(20, len(prices)), 0):\n                price = prices[i]\n                vol = volumes[i] if len(volumes) > abs(i) else 1.0\n                # Infer direction from price movement\n                if i > -len(prices):\n                    direction = 1 if prices[i] > prices[i-1] else -1\n                else:\n                    direction = 1\n                order_flow.on_trade(price, vol, direction)\n\n            signals = order_flow.get_signals()\n            for key, val in signals.items():\n                if not np.isnan(val) and not np.isinf(val):\n                    features[f'of_{key}'] = float(val)\n\n        except Exception as e:\n            logger.debug(f\"Order flow computation error: {e}\")\n            return self._compute_basic_order_flow(prices, volumes)\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_basic_order_flow",
    "category": "microstructure",
    "formula": "features | features",
    "explanation": "Basic order flow features without full module.",
    "python_code": "def _compute_basic_order_flow(self, prices: np.ndarray,\n                                  volumes: np.ndarray) -> Dict[str, float]:\n        \"\"\"Basic order flow features without full module.\"\"\"\n        features = {}\n\n        if len(prices) < 10:\n            return features\n\n        # Price direction\n        directions = np.sign(np.diff(prices[-50:]))\n\n        # Order flow imbalance (simple)\n        vol = volumes[-50:] if len(volumes) >= 50 else np.ones(len(directions))\n        if len(vol) > len(directions):\n            vol = vol[-len(directions):]\n\n        buy_vol = np.sum(vol[directions > 0])\n        sell_vol = np.sum(vol[directions < 0])\n        total = buy_vol + sell_vol\n\n        if total > 0:\n            features['of_imbalance'] = (buy_vol - sell_vol) / total\n\n        # Trade intensity\n        features['of_trade_count'] = len(directions)\n\n        # Volume-weighted direction\n        if len(vol) == len(directions):\n            vw_dir = np.sum(directions * vol) / np.sum(vol) if np.sum(vol) > 0 else 0\n            features['of_vw_direction'] = vw_dir\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_microstructure_features",
    "category": "feature_engineering",
    "formula": "features",
    "explanation": "Compute microstructure features.",
    "python_code": "def _compute_microstructure_features(self, prices: np.ndarray) -> Dict[str, float]:\n        \"\"\"Compute microstructure features.\"\"\"\n        features = {}\n\n        micro = self._lazy_load_microstructure()\n        if micro is None:\n            return self._compute_basic_microstructure(prices)\n\n        try:\n            # TSRV\n            tsrv = micro.tsrv(prices)\n            features['micro_tsrv'] = float(tsrv) if not np.isnan(tsrv) else 0.0\n\n            # Noise variance\n            noise_var = micro.estimate_noise_variance(prices)\n            features['micro_noise_var'] = float(noise_var) if not np.isnan(noise_var) else 0.0\n\n            # Signal-to-noise ratio\n            snr = micro.signal_to_noise_ratio(prices)\n            if not np.isinf(snr):\n                features['micro_snr'] = float(snr)\n\n        except Exception as e:\n            logger.debug(f\"Microstructure computation error: {e}\")\n            return self._compute_basic_microstructure(prices)\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_basic_microstructure",
    "category": "feature_engineering",
    "formula": "features | features",
    "explanation": "Basic microstructure features without full module.",
    "python_code": "def _compute_basic_microstructure(self, prices: np.ndarray) -> Dict[str, float]:\n        \"\"\"Basic microstructure features without full module.\"\"\"\n        features = {}\n\n        if len(prices) < 20:\n            return features\n\n        returns = np.diff(np.log(prices))\n\n        # Realized variance\n        features['micro_rv'] = np.sum(returns[-100:]**2) if len(returns) >= 100 else np.sum(returns**2)\n\n        # First-order autocorrelation (noise indicator)\n        if len(returns) > 2:\n            autocov = np.cov(returns[:-1], returns[1:])[0, 1]\n            features['micro_autocov'] = float(autocov) if not np.isnan(autocov) else 0.0\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_cross_asset_features",
    "category": "statistical",
    "formula": "empty as these require external data feeds | features",
    "explanation": "Compute cross-asset correlation features.",
    "python_code": "def _compute_cross_asset_features(self, symbol: str) -> Dict[str, float]:\n        \"\"\"Compute cross-asset correlation features.\"\"\"\n        features = {}\n\n        # Note: In production, this would fetch DXY, VIX, etc.\n        # For now, return empty as these require external data feeds\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_alpha191_features",
    "category": "alpha_factor",
    "formula": "features | features",
    "explanation": "Compute  Alpha191 features.",
    "python_code": "def _compute_alpha191_features(self, prices: np.ndarray,\n                                   volumes: np.ndarray) -> Dict[str, float]:\n        \"\"\"Compute  Alpha191 features.\"\"\"\n        features = {}\n\n        alpha191 = self._lazy_load_alpha191()\n        if alpha191 is None:\n            return features\n\n        try:\n            # Create minimal DataFrame for Alpha191\n            df = pd.DataFrame({\n                'open': prices * 0.999,   # Approximate open\n                'high': prices * 1.0002,  # Approximate high\n                'low': prices * 0.9998,   # Approximate low\n                'close': prices,\n                'volume': volumes if len(volumes) == len(prices) else np.ones(len(prices)),\n                'vwap': prices  # Use close as VWAP approximation\n            })\n\n            # Use generate_all_alphas for efficient computation\n            result = alpha191.generate_all_alphas(df)\n\n            # Extract last values for each alpha column (only include alpha191_* columns)\n            for col in result.columns:\n                if col.startswith('alpha191_'):\n                    val = result[col].iloc[-1]\n                    if not np.isnan(val) and not np.isinf(val):\n                        features[col] = float(val)\n\n        except Exception as e:\n            logger.debug(f\"Alpha191 computation error: {e}\")\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_har_rv_features",
    "category": "volatility",
    "formula": "features | features",
    "explanation": "Compute HAR-RV volatility features.",
    "python_code": "def _compute_har_rv_features(self, prices: np.ndarray) -> Dict[str, float]:\n        \"\"\"Compute HAR-RV volatility features.\"\"\"\n        features = {}\n\n        har_rv = self._lazy_load_har_rv()\n        if har_rv is None:\n            return self._compute_basic_har_rv(prices)\n\n        try:\n            # Compute log returns\n            if len(prices) < 30:\n                return features\n\n            returns = np.diff(np.log(prices))\n            returns_series = pd.Series(returns)\n\n            # Compute RV at different scales\n            rv = har_rv.compute_realized_volatility(returns_series, window=1)\n            if len(rv) > 0 and not np.isnan(rv.iloc[-1]):\n                features['har_rv_daily'] = float(rv.iloc[-1])\n\n            # HAR components\n            components = har_rv.compute_har_components(rv)\n            if components is not None and len(components) > 0:\n                for col in components.columns:\n                    val = components[col].iloc[-1]\n                    if not np.isnan(val) and not np.isinf(val):\n                        features[f'har_{col}'] = float(val)\n\n            # Volatility regime (get last value from Series)\n            regime_series = har_rv.get_volatility_regime(returns_series)\n            if regime_series is not None and len(regime_series) > 0:\n                regime = regime_series.iloc[-1]\n                features['har_regime'] = {'low': 0, 'normal': 1, 'high': 2}.get(regime, 1)\n\n        except Exception as e:\n            logger.debug(f\"HAR-RV computation error: {e}\")\n            return self._compute_basic_har_rv(prices)\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_basic_har_rv",
    "category": "volatility",
    "formula": "features | features",
    "explanation": "Basic HAR-RV features without full module.",
    "python_code": "def _compute_basic_har_rv(self, prices: np.ndarray) -> Dict[str, float]:\n        \"\"\"Basic HAR-RV features without full module.\"\"\"\n        features = {}\n\n        if len(prices) < 30:\n            return features\n\n        returns = np.diff(np.log(prices))\n\n        # Daily RV (sum of squared returns)\n        rv_daily = np.sum(returns[-5:]**2)\n        features['har_rv_daily'] = float(rv_daily) if not np.isnan(rv_daily) else 0.0\n\n        # Weekly RV (average of last 5 daily)\n        if len(returns) >= 25:\n            rv_weekly = np.mean([np.sum(returns[i:i+5]**2) for i in range(-25, -4, 5)])\n            features['har_rv_weekly'] = float(rv_weekly) if not np.isnan(rv_weekly) else 0.0\n\n        # Monthly RV (average of last 22 daily)\n        if len(returns) >= 110:\n            rv_monthly = np.mean([np.sum(returns[i:i+5]**2) for i in range(-110, -4, 5)])\n            features['har_rv_monthly'] = float(rv_monthly) if not np.isnan(rv_monthly) else 0.0\n\n        # Simple regime based on RV percentile\n        if len(returns) >= 50:\n            rv_history = pd.Series([np.sum(returns[i:i+5]**2) for i in range(0, len(returns)-4, 5)])\n            percentile = (rv_history < rv_daily).mean()\n            if percentile < 0.3:\n                features['har_regime'] = 0  # Low vol\n            elif percentile > 0.7:\n                features['har_regime'] = 2  # High vol\n            else:\n                features['har_regime'] = 1  # Normal\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_range_vol_features",
    "category": "volatility",
    "formula": "features | features | features",
    "explanation": "Compute range-based volatility features.\n\nSources:\n- Parkinson (1980): \"The Extreme Value Method for Estimating the Variance of the Rate of Return\"\n- Garman-Klass (1980): \"On the Estimation of Security Price Volatilities from Historical Data\"\n- Rogers-Satchell (1991): \"Estimating Variance from High, Low and Closing Prices\"\n- Yang-Zhang (2000): \"Drift Independent Volatility Estimation\"",
    "python_code": "def _compute_range_vol_features(self, prices: np.ndarray,\n                                    volumes: np.ndarray) -> Dict[str, float]:\n        \"\"\"\n        Compute range-based volatility features.\n\n        Sources:\n        - Parkinson (1980): \"The Extreme Value Method for Estimating the Variance of the Rate of Return\"\n        - Garman-Klass (1980): \"On the Estimation of Security Price Volatilities from Historical Data\"\n        - Rogers-Satchell (1991): \"Estimating Variance from High, Low and Closing Prices\"\n        - Yang-Zhang (2000): \"Drift Independent Volatility Estimation\"\n        \"\"\"\n        features = {}\n\n        range_vol = self._lazy_load_range_vol()\n        if range_vol is None:\n            return self._compute_basic_range_vol(prices)\n\n        try:\n            # Need OHLC - approximate from mid prices\n            if len(prices) < 30:\n                return features\n\n            # Create approximate OHLC (for tick data)\n            window = 5  # Aggregate 5 ticks into one \"bar\"\n            n_bars = len(prices) // window\n\n            if n_bars < 10:\n                return features\n\n            opens = []\n            highs = []\n            lows = []\n            closes = []\n\n            for i in range(n_bars):\n                start_idx = i * window\n                end_idx = start_idx + window\n                bar_prices = prices[start_idx:end_idx]\n                opens.append(bar_prices[0])\n                highs.append(np.max(bar_prices))\n                lows.append(np.min(bar_prices))\n                closes.append(bar_prices[-1])\n\n            o = pd.Series(opens)\n            h = pd.Series(highs)\n            l = pd.Series(lows)\n            c = pd.Series(closes)\n\n            # Compute volatility estimators\n            vol_window = min(20, len(o) - 1)\n\n            # Parkinson (1980) - uses only high/low\n            parkinson = range_vol.parkinson(h, l, window=vol_window)\n            if len(parkinson) > 0 and not np.isnan(parkinson.iloc[-1]):\n                features['range_vol_parkinson'] = float(parkinson.iloc[-1])\n\n            # Garman-Klass (1980) - uses OHLC\n            gk = range_vol.garman_klass(o, h, l, c, window=vol_window)\n            if len(gk) > 0 and not np.isnan(gk.iloc[-1]):\n                features['range_vol_gk'] = float(gk.iloc[-1])\n\n            # Rogers-Satchell (1991) - zero-drift estimator\n            rs = range_vol.rogers_satchell(o, h, l, c, window=vol_window)\n            if len(rs) > 0 and not np.isnan(rs.iloc[-1]):\n                features['range_vol_rs'] = float(rs.iloc[-1])\n\n            # Yang-Zhang (2000) - handles overnight jumps\n            yz = range_vol.yang_zhang(o, h, l, c, window=vol_window)\n            if len(yz) > 0 and not np.isnan(yz.iloc[-1]):\n                features['range_vol_yz'] = float(yz.iloc[-1])\n\n            # Ensemble volatility\n            ensemble = range_vol.ensemble(o, h, l, c, window=vol_window)\n            if len(ensemble) > 0 and not np.isnan(ensemble.iloc[-1]):\n                features['r",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_basic_range_vol",
    "category": "volatility",
    "formula": "features | features | features",
    "explanation": "Basic range volatility features without full module.",
    "python_code": "def _compute_basic_range_vol(self, prices: np.ndarray) -> Dict[str, float]:\n        \"\"\"Basic range volatility features without full module.\"\"\"\n        features = {}\n\n        if len(prices) < 30:\n            return features\n\n        # Simple Parkinson-style from tick range\n        window = 5\n        n_bars = len(prices) // window\n\n        if n_bars < 10:\n            return features\n\n        # Compute high-low range volatility\n        hl_vols = []\n        for i in range(n_bars):\n            bar = prices[i*window:(i+1)*window]\n            hl_range = np.log(np.max(bar) / np.min(bar))\n            # Parkinson constant: 1 / (4 * ln(2))\n            hl_vols.append(hl_range**2 / (4 * np.log(2)))\n\n        if hl_vols:\n            features['range_vol_parkinson'] = np.sqrt(np.mean(hl_vols[-20:])) * np.sqrt(252 * 24 * 12)\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_market_impact_features",
    "category": "microstructure",
    "formula": "features | features",
    "explanation": "Compute market impact features.\n\nSources:\n- Kyle (1985): \"Continuous Auctions and Insider Trading\" - lambda estimation\n- Glosten-Milgrom (1985): \"Bid, Ask and Transaction Prices\" - adverse selection\n- Roll (1984): \"A Simple Implicit Measure of the Effective Bid-Ask Spread\"\n- Huang-Stoll (1997): \"The Components of the Bid-Ask Spread\"",
    "python_code": "def _compute_market_impact_features(self, prices: np.ndarray,\n                                        volumes: np.ndarray) -> Dict[str, float]:\n        \"\"\"\n        Compute market impact features.\n\n        Sources:\n        - Kyle (1985): \"Continuous Auctions and Insider Trading\" - lambda estimation\n        - Glosten-Milgrom (1985): \"Bid, Ask and Transaction Prices\" - adverse selection\n        - Roll (1984): \"A Simple Implicit Measure of the Effective Bid-Ask Spread\"\n        - Huang-Stoll (1997): \"The Components of the Bid-Ask Spread\"\n        \"\"\"\n        features = {}\n\n        market_impact = self._lazy_load_market_impact()\n        if market_impact is None:\n            return self._compute_basic_market_impact(prices, volumes)\n\n        try:\n            if len(prices) < 30:\n                return features\n\n            prices_series = pd.Series(prices)\n            volumes_series = pd.Series(volumes) if len(volumes) == len(prices) else pd.Series(np.ones(len(prices)))\n\n            # Compute all features\n            result = market_impact.compute_all_features(prices_series, volumes_series)\n\n            # Extract features\n            for key, val in result.items():\n                if isinstance(val, (pd.Series, np.ndarray)):\n                    if len(val) > 0:\n                        last_val = val.iloc[-1] if hasattr(val, 'iloc') else val[-1]\n                        if not np.isnan(last_val) and not np.isinf(last_val):\n                            features[f'mi_{key}'] = float(last_val)\n                elif isinstance(val, (int, float)):\n                    if not np.isnan(val) and not np.isinf(val):\n                        features[f'mi_{key}'] = float(val)\n\n        except Exception as e:\n            logger.debug(f\"Market impact computation error: {e}\")\n            return self._compute_basic_market_impact(prices, volumes)\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_basic_market_impact",
    "category": "microstructure",
    "formula": "features | per dollar volume) | features",
    "explanation": "Basic market impact features without full module.",
    "python_code": "def _compute_basic_market_impact(self, prices: np.ndarray,\n                                     volumes: np.ndarray) -> Dict[str, float]:\n        \"\"\"Basic market impact features without full module.\"\"\"\n        features = {}\n\n        if len(prices) < 30:\n            return features\n\n        # Roll (1984) effective spread estimate\n        # Roll spread = 2 * sqrt(-cov(r_t, r_{t-1}))\n        returns = np.diff(np.log(prices))\n        if len(returns) > 2:\n            autocov = np.cov(returns[:-1], returns[1:])[0, 1]\n            if autocov < 0:\n                features['mi_roll_spread'] = 2 * np.sqrt(-autocov)\n\n        # Simple Kyle lambda proxy\n        # lambda  |P| / V (price impact per unit volume)\n        if len(volumes) == len(prices) and np.sum(volumes) > 0:\n            price_changes = np.abs(np.diff(prices))\n            vol = volumes[1:]\n            vol_mask = vol > 0\n            if np.sum(vol_mask) > 0:\n                lambda_est = np.mean(price_changes[vol_mask] / vol[vol_mask])\n                features['mi_kyle_lambda'] = float(lambda_est) if not np.isnan(lambda_est) else 0.0\n\n        # Amihud illiquidity\n        # ILLIQ = |r| / V (return per dollar volume)\n        if len(volumes) == len(prices) and np.sum(volumes) > 0:\n            abs_returns = np.abs(returns)\n            dollar_vol = volumes[1:] * prices[1:]\n            vol_mask = dollar_vol > 0\n            if np.sum(vol_mask) > 0:\n                illiq = np.mean(abs_returns[vol_mask] / dollar_vol[vol_mask])\n                features['mi_amihud_illiq'] = float(illiq) * 1e6 if not np.isnan(illiq) else 0.0\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_kalman_features",
    "category": "technical",
    "formula": "Price = TrueValue + MicrostructureNoise | features | features",
    "explanation": "Compute Advanced Kalman Filter features.\n\nSources:\n- Bar-Shalom \"Estimation with Applications to Tracking\"\n- Julier & Uhlmann (1997) \"UKF for Nonlinear Estimation\"\n\nWhy Renaissance Uses This:\n- Price = TrueValue + MicrostructureNoise\n- Filters out bid-ask bounce\n- Estimates latent price velocity (momentum)",
    "python_code": "def _compute_kalman_features(self, prices: np.ndarray) -> Dict[str, float]:\n        \"\"\"\n        Compute Advanced Kalman Filter features.\n\n        Sources:\n        - Bar-Shalom \"Estimation with Applications to Tracking\"\n        - Julier & Uhlmann (1997) \"UKF for Nonlinear Estimation\"\n\n        Why Renaissance Uses This:\n        - Price = TrueValue + MicrostructureNoise\n        - Filters out bid-ask bounce\n        - Estimates latent price velocity (momentum)\n        \"\"\"\n        features = {}\n\n        kalman = self._lazy_load_kalman()\n        if kalman is None:\n            return self._compute_basic_kalman(prices)\n\n        try:\n            if len(prices) < 30:\n                return features\n\n            # Update Kalman filter with recent prices\n            for price in prices[-50:]:\n                state = kalman.update(float(price))\n\n            # Extract features\n            features['kalman_price'] = kalman.get_filtered_price()\n            features['kalman_velocity'] = kalman.get_velocity()\n\n            # Price deviation from Kalman estimate (mean reversion signal)\n            deviation = prices[-1] - features['kalman_price']\n            features['kalman_deviation'] = deviation * 10000  # In bps\n\n            # Regime indicator from innovation sequence\n            if hasattr(kalman, 'get_regime_indicator'):\n                features['kalman_regime'] = kalman.get_regime_indicator()\n\n        except Exception as e:\n            logger.debug(f\"Kalman computation error: {e}\")\n            return self._compute_basic_kalman(prices)\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_basic_kalman",
    "category": "filtering",
    "formula": "features | features",
    "explanation": "Basic Kalman-like features without full module.",
    "python_code": "def _compute_basic_kalman(self, prices: np.ndarray) -> Dict[str, float]:\n        \"\"\"Basic Kalman-like features without full module.\"\"\"\n        features = {}\n\n        if len(prices) < 20:\n            return features\n\n        # Simple exponential smoothing as Kalman proxy\n        alpha = 0.1\n        filtered = prices[0]\n        for p in prices[1:]:\n            filtered = alpha * p + (1 - alpha) * filtered\n\n        features['kalman_price'] = filtered\n        features['kalman_deviation'] = (prices[-1] - filtered) * 10000\n\n        # Velocity from differenced filtered price\n        if len(prices) > 5:\n            velocity = (filtered - prices[-5]) / 5\n            features['kalman_velocity'] = velocity * 10000\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_garch_features",
    "category": "volatility",
    "formula": "features",
    "explanation": "Compute GARCH volatility features.\n\nSources:\n- Bollerslev (1986) \"GARCH\"\n- Nelson (1991) \"EGARCH\"\n\nWhy Renaissance Uses This:\n- Volatility clustering (high vol follows high vol)\n- Position sizing with Kelly criterion\n- Leverage effect (neg returns increase vol more)",
    "python_code": "def _compute_garch_features(self, prices: np.ndarray) -> Dict[str, float]:\n        \"\"\"\n        Compute GARCH volatility features.\n\n        Sources:\n        - Bollerslev (1986) \"GARCH\"\n        - Nelson (1991) \"EGARCH\"\n\n        Why Renaissance Uses This:\n        - Volatility clustering (high vol follows high vol)\n        - Position sizing with Kelly criterion\n        - Leverage effect (neg returns increase vol more)\n        \"\"\"\n        features = {}\n\n        if len(prices) < 50:\n            return self._compute_basic_garch(prices)\n\n        garch = self._lazy_load_garch()\n        if garch is None:\n            return self._compute_basic_garch(prices)\n\n        try:\n            # Compute returns\n            returns = np.diff(np.log(prices))\n\n            # Fit if not already (expensive, so cache)\n            if not garch.fitted:\n                garch.fit(returns)\n\n            # Get current volatility\n            features['garch_vol'] = garch.get_current_volatility() * np.sqrt(252 * 24 * 60)\n\n            # Volatility forecast\n            if garch.fitted:\n                forecast = garch.forecast(horizon=5)\n                features['garch_vol_forecast'] = float(forecast[-1]) * np.sqrt(252 * 24 * 60)\n\n        except Exception as e:\n            logger.debug(f\"GARCH computation error: {e}\")\n            return self._compute_basic_garch(prices)\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_basic_garch",
    "category": "volatility",
    "formula": "features | features",
    "explanation": "Basic GARCH-like features without full module.",
    "python_code": "def _compute_basic_garch(self, prices: np.ndarray) -> Dict[str, float]:\n        \"\"\"Basic GARCH-like features without full module.\"\"\"\n        features = {}\n\n        if len(prices) < 20:\n            return features\n\n        returns = np.diff(np.log(prices))\n\n        # Simple EWMA volatility as GARCH proxy\n        alpha = 0.06  # Decay factor\n        ewma_var = returns[0]**2\n        for r in returns[1:]:\n            ewma_var = alpha * r**2 + (1 - alpha) * ewma_var\n\n        features['garch_vol'] = np.sqrt(ewma_var) * np.sqrt(252 * 24 * 60)\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_jump_features",
    "category": "volatility",
    "formula": "features",
    "explanation": "Compute Jump Detection features using Bipower Variation.\n\nSources:\n- Barndorff-Nielsen & Shephard (2004) \"Power and Multipower Variations\"\n\nWhy Renaissance Uses This:\n- Distinguish news events from continuous trading\n- GARCH fails on jumps\n- Adjust position sizing during jump periods",
    "python_code": "def _compute_jump_features(self, prices: np.ndarray) -> Dict[str, float]:\n        \"\"\"\n        Compute Jump Detection features using Bipower Variation.\n\n        Sources:\n        - Barndorff-Nielsen & Shephard (2004) \"Power and Multipower Variations\"\n\n        Why Renaissance Uses This:\n        - Distinguish news events from continuous trading\n        - GARCH fails on jumps\n        - Adjust position sizing during jump periods\n        \"\"\"\n        features = {}\n\n        if len(prices) < 50:\n            return self._compute_basic_jump(prices)\n\n        jump_model = self._lazy_load_jump_detection()\n        if jump_model is None:\n            return self._compute_basic_jump(prices)\n\n        try:\n            returns = np.diff(np.log(prices))\n            returns_series = pd.Series(returns)\n\n            # Compute decomposition\n            decomp = jump_model.decompose_volatility(returns_series)\n\n            # Latest values\n            features['jump_rv'] = float(decomp.realized_variance[-1])\n            features['jump_bpv'] = float(decomp.bipower_variance[-1])\n            features['jump_var'] = float(decomp.jump_variance[-1])\n            features['jump_ratio'] = float(decomp.relative_jump[-1])\n            features['jump_indicator'] = float(decomp.jump_indicator[-1])\n\n            # Continuous vs jump vol\n            features['continuous_vol'] = np.sqrt(float(decomp.bipower_variance[-1]))\n            features['jump_vol'] = np.sqrt(float(decomp.jump_variance[-1]))\n\n        except Exception as e:\n            logger.debug(f\"Jump detection error: {e}\")\n            return self._compute_basic_jump(prices)\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_basic_jump",
    "category": "feature_engineering",
    "formula": "features | features",
    "explanation": "Basic jump detection without full module.",
    "python_code": "def _compute_basic_jump(self, prices: np.ndarray) -> Dict[str, float]:\n        \"\"\"Basic jump detection without full module.\"\"\"\n        features = {}\n\n        if len(prices) < 20:\n            return features\n\n        returns = np.diff(np.log(prices))\n\n        # Simple threshold-based jump detection\n        mad = np.median(np.abs(returns - np.median(returns)))\n        sigma = mad * 1.4826\n        threshold = 3 * sigma\n\n        jumps = np.abs(returns) > threshold\n        features['jump_indicator'] = float(jumps[-1]) if len(jumps) > 0 else 0.0\n        features['jump_ratio'] = float(np.sum(jumps)) / len(jumps)\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_regime_features",
    "category": "regime",
    "formula": "features",
    "explanation": "Compute Regime-Dependent features using HMM.\n\nSources:\n- Hamilton (1989) \"Markov Switching Models\"\n\nWhy Renaissance Uses This:\n- Different strategies for different regimes\n- Position sizing based on regime\n- Avoid trading during regime transitions",
    "python_code": "def _compute_regime_features(self, prices: np.ndarray) -> Dict[str, float]:\n        \"\"\"\n        Compute Regime-Dependent features using HMM.\n\n        Sources:\n        - Hamilton (1989) \"Markov Switching Models\"\n\n        Why Renaissance Uses This:\n        - Different strategies for different regimes\n        - Position sizing based on regime\n        - Avoid trading during regime transitions\n        \"\"\"\n        features = {}\n\n        if len(prices) < 50:\n            return self._compute_basic_regime(prices)\n\n        regime_engine = self._lazy_load_regime()\n        if regime_engine is None:\n            return self._compute_basic_regime(prices)\n\n        try:\n            returns = np.diff(np.log(prices))\n\n            # Fit if not already\n            if not regime_engine.fitted and len(returns) > 100:\n                regime_engine.fit(returns)\n\n            # Detect current regime\n            state = regime_engine.detect_regime(returns)\n\n            features['regime'] = float(state.regime)\n            features['regime_duration'] = float(state.duration)\n            features['regime_transition_prob'] = float(state.transition_prob)\n            features['regime_prob_low'] = float(state.probability[0])\n            features['regime_prob_normal'] = float(state.probability[1])\n            features['regime_prob_high'] = float(state.probability[2])\n\n            # Position and stop multipliers\n            features['regime_position_mult'] = regime_engine.config.position_mult.get(state.regime, 1.0)\n            features['regime_stop_mult'] = regime_engine.config.stop_mult.get(state.regime, 1.0)\n\n        except Exception as e:\n            logger.debug(f\"Regime detection error: {e}\")\n            return self._compute_basic_regime(prices)\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_basic_regime",
    "category": "regime",
    "formula": "features | features",
    "explanation": "Basic regime detection without full module.",
    "python_code": "def _compute_basic_regime(self, prices: np.ndarray) -> Dict[str, float]:\n        \"\"\"Basic regime detection without full module.\"\"\"\n        features = {}\n\n        if len(prices) < 20:\n            return features\n\n        # Simple volatility-based regime\n        returns = np.diff(np.log(prices))\n        vol = np.std(returns[-20:])\n\n        if vol < 0.0002:\n            regime = 0  # Low\n        elif vol > 0.0006:\n            regime = 2  # High\n        else:\n            regime = 1  # Normal\n\n        features['regime'] = float(regime)\n        features['regime_position_mult'] = [1.5, 1.0, 0.5][regime]\n        features['regime_stop_mult'] = [1.0, 1.5, 2.5][regime]\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_spectral_features",
    "category": "filtering",
    "formula": "features",
    "explanation": "Compute Spectral/Wavelet Analysis features.\n\nSources:\n- Mallat (1989) \"Wavelet Decomposition\"\n- Oppenheim & Schafer \"Signal Processing\"\n\nWhy Renaissance Uses This:\n- Identify dominant market cycles\n- Filter noise from signal\n- Detect phase shifts",
    "python_code": "def _compute_spectral_features(self, prices: np.ndarray) -> Dict[str, float]:\n        \"\"\"\n        Compute Spectral/Wavelet Analysis features.\n\n        Sources:\n        - Mallat (1989) \"Wavelet Decomposition\"\n        - Oppenheim & Schafer \"Signal Processing\"\n\n        Why Renaissance Uses This:\n        - Identify dominant market cycles\n        - Filter noise from signal\n        - Detect phase shifts\n        \"\"\"\n        features = {}\n\n        if len(prices) < 100:\n            return self._compute_basic_spectral(prices)\n\n        spectral = self._lazy_load_spectral()\n        if spectral is None:\n            return self._compute_basic_spectral(prices)\n\n        try:\n            prices_series = pd.Series(prices)\n            result = spectral.compute_features(prices_series, window=min(100, len(prices) - 10))\n\n            # Get last row\n            for col in result.columns:\n                val = result[col].iloc[-1]\n                if not np.isnan(val) and not np.isinf(val):\n                    features[f'spectral_{col}'] = float(val)\n\n        except Exception as e:\n            logger.debug(f\"Spectral analysis error: {e}\")\n            return self._compute_basic_spectral(prices)\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_basic_spectral",
    "category": "feature_engineering",
    "formula": "features | features",
    "explanation": "Basic spectral features without full module.",
    "python_code": "def _compute_basic_spectral(self, prices: np.ndarray) -> Dict[str, float]:\n        \"\"\"Basic spectral features without full module.\"\"\"\n        features = {}\n\n        if len(prices) < 50:\n            return features\n\n        returns = np.diff(np.log(prices))\n\n        # Simple FFT-based dominant frequency\n        from scipy.fft import fft, fftfreq\n\n        spectrum = np.abs(fft(returns))**2\n        freqs = fftfreq(len(returns))\n\n        # Positive frequencies only\n        pos_mask = freqs > 0\n        if np.sum(pos_mask) > 0:\n            pos_freqs = freqs[pos_mask]\n            pos_power = spectrum[pos_mask]\n\n            dominant_idx = np.argmax(pos_power)\n            dominant_freq = pos_freqs[dominant_idx]\n            dominant_period = 1 / dominant_freq if dominant_freq > 0 else len(returns)\n\n            features['spectral_dominant_period'] = dominant_period\n\n            # Spectral entropy\n            power_norm = pos_power / (pos_power.sum() + 1e-10)\n            entropy = -np.sum(power_norm * np.log(power_norm + 1e-10))\n            features['spectral_entropy'] = entropy / np.log(len(power_norm))\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_chinese_hft_features",
    "category": "microstructure",
    "formula": "features",
    "explanation": "Compute Chinese HFT factors.\n\nSources:\n- Stoikov (2018) \"Microprice\"\n-  \"Smart Money Factor 2.0\"\n- QUANTAXIS, QuantsPlaybook (Gitee)\n- Cont et al. (2014) \"Integrated OFI\"\n\nIncludes: Microprice, Smart Money, Kyle Lambda, Amihud Illiquidity",
    "python_code": "def _compute_chinese_hft_features(self, prices: np.ndarray,\n                                      volumes: np.ndarray) -> Dict[str, float]:\n        \"\"\"\n        Compute Chinese HFT factors.\n\n        Sources:\n        - Stoikov (2018) \"Microprice\"\n        -  \"Smart Money Factor 2.0\"\n        - QUANTAXIS, QuantsPlaybook (Gitee)\n        - Cont et al. (2014) \"Integrated OFI\"\n\n        Includes: Microprice, Smart Money, Kyle Lambda, Amihud Illiquidity\n        \"\"\"\n        features = {}\n\n        if len(prices) < 30:\n            return self._compute_basic_chinese_hft(prices, volumes)\n\n        chinese_hft = self._lazy_load_chinese_hft()\n        if chinese_hft is None:\n            return self._compute_basic_chinese_hft(prices, volumes)\n\n        try:\n            # Create DataFrame for Chinese HFT\n            df = pd.DataFrame({\n                'close': prices,\n                'volume': volumes if len(volumes) == len(prices) else np.ones(len(prices)),\n                'high': prices * 1.0002,\n                'low': prices * 0.9998,\n                'bid': prices * 0.9999,\n                'ask': prices * 1.0001,\n                'bid_size': np.ones(len(prices)) * 100,\n                'ask_size': np.ones(len(prices)) * 100\n            })\n\n            result = chinese_hft.compute_all_features(df)\n\n            # Extract last values\n            for col in result.columns:\n                if col not in ['close', 'volume', 'high', 'low', 'bid', 'ask', 'bid_size', 'ask_size']:\n                    val = result[col].iloc[-1]\n                    if not np.isnan(val) and not np.isinf(val):\n                        features[f'cn_{col}'] = float(val)\n\n        except Exception as e:\n            logger.debug(f\"Chinese HFT computation error: {e}\")\n            return self._compute_basic_chinese_hft(prices, volumes)\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_basic_chinese_hft",
    "category": "feature_engineering",
    "formula": "features | features",
    "explanation": "Basic Chinese HFT features without full module.",
    "python_code": "def _compute_basic_chinese_hft(self, prices: np.ndarray,\n                                   volumes: np.ndarray) -> Dict[str, float]:\n        \"\"\"Basic Chinese HFT features without full module.\"\"\"\n        features = {}\n\n        if len(prices) < 20:\n            return features\n\n        returns = np.diff(np.log(prices))\n\n        # Microprice approximation (using equal bid/ask sizes)\n        mid = prices[-1]\n        spread = mid * 0.0002  # Assume 2 pip spread\n        features['cn_microprice'] = mid  # Equal to mid when sizes equal\n\n        # Kyle Lambda approximation\n        if len(volumes) == len(prices) and np.sum(volumes) > 0:\n            price_changes = np.abs(np.diff(prices))\n            vol = volumes[1:]\n            vol_mask = vol > 0\n            if np.sum(vol_mask) > 0:\n                lambda_est = np.mean(price_changes[vol_mask] / vol[vol_mask])\n                features['cn_kyle_lambda'] = float(lambda_est) if not np.isnan(lambda_est) else 0.0\n\n        # Amihud illiquidity\n        if len(volumes) == len(prices) and np.sum(volumes) > 0:\n            abs_returns = np.abs(returns)\n            dollar_vol = volumes[1:] * prices[1:]\n            vol_mask = dollar_vol > 0\n            if np.sum(vol_mask) > 0:\n                illiq = np.mean(abs_returns[vol_mask] / dollar_vol[vol_mask])\n                features['cn_amihud_illiq'] = float(illiq) * 1e6 if not np.isnan(illiq) else 0.0\n\n        # Smart money factor approximation\n        if len(volumes) == len(prices):\n            signed_vol = np.sign(returns) * volumes[1:] * np.abs(returns)\n            vol_ret = volumes[1:] * np.abs(returns)\n            if np.sum(vol_ret) > 0:\n                smart_money = np.sum(signed_vol) / np.sum(vol_ret)\n                features['cn_smart_money'] = float(smart_money) if not np.isnan(smart_money) else 0.0\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_lob_features",
    "category": "microstructure",
    "formula": "features",
    "explanation": "Compute Limit Order Book features.\n\nSources:\n- Lee & Ready (1991) \"Trade Direction Classification\"\n- HftBacktest (GitHub)\n- LOBSTER dataset methodology\n\nIncludes: Lee-Ready, Book Imbalance, Queue Imbalance, Spread Decomposition",
    "python_code": "def _compute_lob_features(self, prices: np.ndarray,\n                              volumes: np.ndarray) -> Dict[str, float]:\n        \"\"\"\n        Compute Limit Order Book features.\n\n        Sources:\n        - Lee & Ready (1991) \"Trade Direction Classification\"\n        - HftBacktest (GitHub)\n        - LOBSTER dataset methodology\n\n        Includes: Lee-Ready, Book Imbalance, Queue Imbalance, Spread Decomposition\n        \"\"\"\n        features = {}\n\n        if len(prices) < 30:\n            return self._compute_basic_lob(prices, volumes)\n\n        lob_features = self._lazy_load_lob_features()\n        if lob_features is None:\n            return self._compute_basic_lob(prices, volumes)\n\n        try:\n            # Create DataFrame for LOB\n            df = pd.DataFrame({\n                'trade_price': prices,\n                'volume': volumes if len(volumes) == len(prices) else np.ones(len(prices)),\n                'bid': prices * 0.9999,\n                'ask': prices * 1.0001,\n                'bid_size': np.ones(len(prices)) * 100,\n                'ask_size': np.ones(len(prices)) * 100\n            })\n\n            result = lob_features.compute_all_features(df)\n\n            # Extract last values\n            for col in result.columns:\n                if col not in ['trade_price', 'volume', 'bid', 'ask', 'bid_size', 'ask_size']:\n                    val = result[col].iloc[-1]\n                    if not np.isnan(val) and not np.isinf(val):\n                        features[f'lob_{col}'] = float(val)\n\n        except Exception as e:\n            logger.debug(f\"LOB features computation error: {e}\")\n            return self._compute_basic_lob(prices, volumes)\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_basic_lob",
    "category": "feature_engineering",
    "formula": "features | features",
    "explanation": "Basic LOB features without full module.",
    "python_code": "def _compute_basic_lob(self, prices: np.ndarray,\n                           volumes: np.ndarray) -> Dict[str, float]:\n        \"\"\"Basic LOB features without full module.\"\"\"\n        features = {}\n\n        if len(prices) < 20:\n            return features\n\n        # Simple Lee-Ready direction inference\n        price_changes = np.diff(prices)\n        directions = np.sign(price_changes)\n\n        # Buy/sell pressure\n        if len(directions) > 0:\n            buy_pressure = np.sum(directions > 0) / len(directions)\n            features['lob_buy_pressure'] = float(buy_pressure)\n\n        # Book imbalance approximation (assume equal sizes)\n        features['lob_book_imbalance'] = 0.0  # Equal sizes = no imbalance\n\n        # Spread approximation\n        spread_bps = 0.02  # 2 pip approximation\n        features['lob_spread_bps'] = spread_bps * 100\n\n        # Roll spread estimate\n        returns = np.diff(np.log(prices))\n        if len(returns) > 2:\n            autocov = np.cov(returns[:-1], returns[1:])[0, 1]\n            if autocov < 0:\n                features['lob_roll_spread'] = 2 * np.sqrt(-autocov)\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_elite_quant_features",
    "category": "technical",
    "formula": "features",
    "explanation": "Compute Elite Quant factors from top Chinese funds.\n\nSources:\n-  (Huanfang Quant)\n-  (Nine Kun Investment)\n-  (Minghui Investment)\n\nIncludes: IC/ICIR evaluation, PCA orthogonalization, Higher-order moments,\n          Intraday momentum, Adaptive factor weighting",
    "python_code": "def _compute_elite_quant_features(self, prices: np.ndarray,\n                                      volumes: np.ndarray) -> Dict[str, float]:\n        \"\"\"\n        Compute Elite Quant factors from top Chinese funds.\n\n        Sources:\n        -  (Huanfang Quant)\n        -  (Nine Kun Investment)\n        -  (Minghui Investment)\n\n        Includes: IC/ICIR evaluation, PCA orthogonalization, Higher-order moments,\n                  Intraday momentum, Adaptive factor weighting\n        \"\"\"\n        features = {}\n\n        if len(prices) < 50:\n            return self._compute_basic_elite_quant(prices, volumes)\n\n        elite_quant = self._lazy_load_elite_quant()\n        if elite_quant is None:\n            return self._compute_basic_elite_quant(prices, volumes)\n\n        try:\n            # Create DataFrame for Elite Quant\n            df = pd.DataFrame({\n                'close': prices,\n                'volume': volumes if len(volumes) == len(prices) else np.ones(len(prices)),\n                'high': prices * 1.0002,\n                'low': prices * 0.9998,\n                'open': np.roll(prices, 1)\n            })\n            df['open'].iloc[0] = df['close'].iloc[0]\n\n            result = elite_quant.compute_all_features(df)\n\n            # Extract last values\n            for col in result.columns:\n                if col not in ['close', 'volume', 'high', 'low', 'open']:\n                    val = result[col].iloc[-1]\n                    if not np.isnan(val) and not np.isinf(val):\n                        features[f'elite_{col}'] = float(val)\n\n        except Exception as e:\n            logger.debug(f\"Elite quant computation error: {e}\")\n            return self._compute_basic_elite_quant(prices, volumes)\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_basic_elite_quant",
    "category": "feature_engineering",
    "formula": "features | features",
    "explanation": "Basic elite quant features without full module.",
    "python_code": "def _compute_basic_elite_quant(self, prices: np.ndarray,\n                                   volumes: np.ndarray) -> Dict[str, float]:\n        \"\"\"Basic elite quant features without full module.\"\"\"\n        features = {}\n\n        if len(prices) < 30:\n            return features\n\n        returns = np.diff(np.log(prices))\n\n        # Higher-order moments\n        if len(returns) >= 20:\n            # Skewness\n            mean_ret = np.mean(returns[-20:])\n            std_ret = np.std(returns[-20:])\n            if std_ret > 0:\n                skew = np.mean(((returns[-20:] - mean_ret) / std_ret) ** 3)\n                features['elite_skewness'] = float(skew) if not np.isnan(skew) else 0.0\n\n            # Kurtosis\n            if std_ret > 0:\n                kurt = np.mean(((returns[-20:] - mean_ret) / std_ret) ** 4) - 3\n                features['elite_kurtosis'] = float(kurt) if not np.isnan(kurt) else 0.0\n\n        # Tail risk (CVaR approximation)\n        if len(returns) >= 50:\n            sorted_returns = np.sort(returns[-50:])\n            var_5 = np.percentile(sorted_returns, 5)\n            cvar = np.mean(sorted_returns[sorted_returns <= var_5]) if np.sum(sorted_returns <= var_5) > 0 else var_5\n            features['elite_cvar_5'] = float(cvar) if not np.isnan(cvar) else 0.0\n\n        # Intraday momentum (first half vs second half)\n        if len(returns) >= 20:\n            half = len(returns) // 2\n            first_half_ret = np.sum(returns[:half])\n            second_half_ret = np.sum(returns[half:])\n            features['elite_intraday_mom'] = float(first_half_ret - second_half_ret)\n\n        # Cross-sectional momentum proxy (deviation from rolling mean)\n        if len(returns) >= 20:\n            rolling_mean = np.mean(returns[-20:])\n            current_ret = returns[-1]\n            features['elite_cs_momentum'] = float(current_ret - rolling_mean) if not np.isnan(current_ret) else 0.0\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_genetic_mining_features",
    "category": "reinforcement_learning",
    "formula": "features | features",
    "explanation": "Compute Genetic Factor Mining features.\nSource: ,  DFQ",
    "python_code": "def _compute_genetic_mining_features(self, prices: np.ndarray,\n                                         volumes: np.ndarray) -> Dict[str, float]:\n        \"\"\"\n        Compute Genetic Factor Mining features.\n        Source: ,  DFQ\n        \"\"\"\n        features = {}\n\n        if len(prices) < 20:\n            return features\n\n        genetic_mining = self._lazy_load_genetic_mining()\n\n        if genetic_mining is not None:\n            try:\n                df = pd.DataFrame({\n                    'close': prices,\n                    'volume': volumes if len(volumes) == len(prices) else np.zeros_like(prices)\n                })\n                df['open'] = np.roll(prices, 1)\n                df['open'][0] = prices[0]\n                df['high'] = np.maximum(prices, df['open'])\n                df['low'] = np.minimum(prices, df['open'])\n\n                result = genetic_mining.compute_all_features(df)\n\n                if isinstance(result, pd.DataFrame) and not result.empty:\n                    for col in result.columns:\n                        val = result[col].iloc[-1]\n                        if not np.isnan(val) and not np.isinf(val):\n                            features[f'gp_{col}'] = float(val)\n            except Exception as e:\n                logger.debug(f\"Genetic mining feature extraction failed: {e}\")\n        else:\n            # Fallback: Basic genetic-inspired features\n            returns = np.diff(np.log(prices + 1e-10))\n            if len(returns) >= 10:\n                features['gp_momentum_10'] = float(np.sum(returns[-10:]))\n                features['gp_volatility_10'] = float(np.std(returns[-10:]))\n                features['gp_range_ratio'] = float(\n                    (np.max(prices[-10:]) - np.min(prices[-10:])) / (np.mean(prices[-10:]) + 1e-10)\n                )\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_compute_tsfresh_features",
    "category": "feature_engineering",
    "formula": "features | features",
    "explanation": "Compute TSFresh automatic features.\nSource:  6+",
    "python_code": "def _compute_tsfresh_features(self, prices: np.ndarray,\n                                  volumes: np.ndarray) -> Dict[str, float]:\n        \"\"\"\n        Compute TSFresh automatic features.\n        Source:  6+\n        \"\"\"\n        features = {}\n\n        if len(prices) < 20:\n            return features\n\n        tsfresh = self._lazy_load_tsfresh()\n\n        if tsfresh is not None:\n            try:\n                df = pd.DataFrame({\n                    'close': prices,\n                    'volume': volumes if len(volumes) == len(prices) else np.zeros_like(prices)\n                })\n                df['open'] = np.roll(prices, 1)\n                df['open'][0] = prices[0]\n                df['high'] = np.maximum(prices, df['open'])\n                df['low'] = np.minimum(prices, df['open'])\n\n                result = tsfresh.compute_all_features(df)\n\n                if isinstance(result, pd.DataFrame) and not result.empty:\n                    for col in result.columns:\n                        val = result[col].iloc[-1]\n                        if not np.isnan(val) and not np.isinf(val):\n                            features[f'ts_{col}'] = float(val)\n            except Exception as e:\n                logger.debug(f\"TSFresh feature extraction failed: {e}\")\n        else:\n            # Fallback: Basic rolling features\n            for window in [5, 10, 20]:\n                if len(prices) >= window:\n                    features[f'ts_mean_{window}'] = float(np.mean(prices[-window:]))\n                    features[f'ts_std_{window}'] = float(np.std(prices[-window:]))\n                    features[f'ts_min_{window}'] = float(np.min(prices[-window:]))\n                    features[f'ts_max_{window}'] = float(np.max(prices[-window:]))\n                    features[f'ts_median_{window}'] = float(np.median(prices[-window:]))\n\n        return features",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "_normalize_features",
    "category": "statistical",
    "formula": "normalized",
    "explanation": "Normalize features using z-score.",
    "python_code": "def _normalize_features(self, features: Dict[str, float],\n                           prices: np.ndarray) -> Dict[str, float]:\n        \"\"\"Normalize features using z-score.\"\"\"\n        # Simple clipping for extreme values\n        normalized = {}\n        for key, val in features.items():\n            if np.isnan(val) or np.isinf(val):\n                normalized[key] = 0.0\n            else:\n                # Clip to [-10, 10] for z-scores, [-1, 1] for ratios\n                if 'zscore' in key or 'mr_' in key:\n                    normalized[key] = np.clip(val, -10, 10)\n                elif 'ratio' in key or 'position' in key:\n                    normalized[key] = np.clip(val, -1, 1)\n                else:\n                    normalized[key] = np.clip(val, -1000, 1000)\n\n        return normalized",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "get_feature_vector",
    "category": "deep_learning",
    "formula": "vector",
    "explanation": "Convert feature dict to numpy array in consistent order.",
    "python_code": "def get_feature_vector(self, features: Dict[str, float]) -> np.ndarray:\n        \"\"\"Convert feature dict to numpy array in consistent order.\"\"\"\n        if not self.feature_names:\n            self.feature_names = sorted(features.keys())\n\n        vector = np.zeros(len(self.feature_names))\n        for i, name in enumerate(self.feature_names):\n            vector[i] = features.get(name, 0.0)\n\n        return vector",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "get_feature_dataframe",
    "category": "deep_learning",
    "formula": "pd.DataFrame([features])",
    "explanation": "Convert feature dict to single-row DataFrame.",
    "python_code": "def get_feature_dataframe(self, features: Dict[str, float]) -> pd.DataFrame:\n        \"\"\"Convert feature dict to single-row DataFrame.\"\"\"\n        return pd.DataFrame([features])",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "batch_process",
    "category": "feature_engineering",
    "formula": "result",
    "explanation": "Process entire DataFrame and add all features.\n\nArgs:\n    data: DataFrame with price data\n    symbol: Trading symbol\n\nReturns:\n    DataFrame with all features added",
    "python_code": "def batch_process(self, data: pd.DataFrame, symbol: str = \"EURUSD\") -> pd.DataFrame:\n        \"\"\"\n        Process entire DataFrame and add all features.\n\n        Args:\n            data: DataFrame with price data\n            symbol: Trading symbol\n\n        Returns:\n            DataFrame with all features added\n        \"\"\"\n        logger.info(f\"Batch processing {len(data)} rows for {symbol}\")\n\n        # Initialize with data\n        self.initialize(data.head(100), symbol)\n\n        # Process each row\n        all_features = []\n\n        for idx, row in data.iterrows():\n            if 'bid' in row and 'ask' in row:\n                bid, ask = row['bid'], row['ask']\n            elif 'close' in row:\n                mid = row['close']\n                bid = mid - 0.00005  # Assume 1 pip spread\n                ask = mid + 0.00005\n            else:\n                continue\n\n            volume = row.get('volume', 0.0)\n            timestamp = row.get('timestamp', None)\n\n            features = self.process_tick(symbol, bid, ask, volume, timestamp)\n            features['timestamp'] = timestamp\n            all_features.append(features)\n\n        result = pd.DataFrame(all_features)\n\n        # Merge with original data\n        if 'timestamp' in data.columns:\n            result = result.set_index('timestamp')\n            data = data.set_index('timestamp')\n            result = pd.concat([data, result], axis=1)\n            result = result.reset_index()\n\n        logger.info(f\"Generated {len(result.columns)} total columns\")\n\n        return result",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HFTFeatureEngine"
  },
  {
    "name": "fit",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Fit selector to training data.",
    "python_code": "def fit(self, X: pd.DataFrame) -> 'FeatureSelector':\n        \"\"\"Fit selector to training data.\"\"\"\n        # Remove low variance\n        variances = X.var()\n        high_variance = variances[variances > self.min_variance].index.tolist()\n\n        # Remove highly correlated\n        X_filtered = X[high_variance]\n        corr_matrix = X_filtered.corr().abs()\n\n        # Upper triangle\n        upper = corr_matrix.where(\n            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n        )\n\n        # Find features to drop\n        to_drop = [col for col in upper.columns\n                   if any(upper[col] > self.max_correlation)]\n\n        self.selected_features = [f for f in high_variance if f not in to_drop]\n\n        logger.info(f\"Selected {len(self.selected_features)} features \"\n                   f\"from {len(X.columns)}\")\n\n        return self",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "FeatureSelector"
  },
  {
    "name": "transform",
    "category": "feature_engineering",
    "formula": "X[available]",
    "explanation": "Transform data to selected features only.",
    "python_code": "def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Transform data to selected features only.\"\"\"\n        available = [f for f in self.selected_features if f in X.columns]\n        return X[available]",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "FeatureSelector"
  },
  {
    "name": "fit_transform",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Fit and transform in one step.",
    "python_code": "def fit_transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Fit and transform in one step.\"\"\"\n        return self.fit(X).transform(X)",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "FeatureSelector"
  },
  {
    "name": "create_hft_feature_engine",
    "category": "reinforcement_learning",
    "formula": "HFTFeatureEngine(config)",
    "explanation": "Factory function to create feature engine.",
    "python_code": "def create_hft_feature_engine(config: FeatureConfig = None) -> HFTFeatureEngine:\n    \"\"\"Factory function to create feature engine.\"\"\"\n    return HFTFeatureEngine(config)",
    "source_file": "core\\features\\engine.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "volatility",
    "formula": "",
    "explanation": "Initialize Europe Quant Features.\n\nArgs:\n    hurst_exponent: Hurst parameter for rough volatility (default 0.1)\n    jump_threshold: Z-score threshold for jump detection (default 3.0)",
    "python_code": "def __init__(\n        self,\n        hurst_exponent: float = 0.1,  # Rough volatility Hurst parameter\n        jump_threshold: float = 3.0,   # Z-score for jump detection\n    ):\n        \"\"\"\n        Initialize Europe Quant Features.\n\n        Args:\n            hurst_exponent: Hurst parameter for rough volatility (default 0.1)\n            jump_threshold: Z-score threshold for jump detection (default 3.0)\n        \"\"\"\n        self.hurst = hurst_exponent\n        self.jump_thresh = jump_threshold",
    "source_file": "core\\features\\europe_quant.py",
    "academic_reference": null,
    "class_name": "EuropeQuantFeatures"
  },
  {
    "name": "_heston_factors",
    "category": "volatility",
    "formula": "dv_t = kappa * (theta - v_t) * dt + sigma * sqrt(v_t) * dW_t | features",
    "explanation": "Heston Stochastic Volatility Features.\n\nThe Heston (1993) model assumes volatility follows:\ndv_t = kappa * (theta - v_t) * dt + sigma * sqrt(v_t) * dW_t\n\nWe extract features that proxy the state variables:\n- Current variance (v_t)\n- Mean reversion speed (kappa)\n- Long-run variance (theta)\n- Vol of vol (sigma)\n\nReferences:\n- Heston (1993): \"A Closed-Form Solution for Options with\n  Stochastic Volatility\"\n- ETH Zurich: \"Numerical methods for Heston model\"",
    "python_code": "def _heston_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Heston Stochastic Volatility Features.\n\n        The Heston (1993) model assumes volatility follows:\n        dv_t = kappa * (theta - v_t) * dt + sigma * sqrt(v_t) * dW_t\n\n        We extract features that proxy the state variables:\n        - Current variance (v_t)\n        - Mean reversion speed (kappa)\n        - Long-run variance (theta)\n        - Vol of vol (sigma)\n\n        References:\n        - Heston (1993): \"A Closed-Form Solution for Options with\n          Stochastic Volatility\"\n        - ETH Zurich: \"Numerical methods for Heston model\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Instantaneous variance proxy (v_t)\n        # Use realized variance as proxy\n        realized_var = (returns ** 2).rolling(20, min_periods=2).mean() * 252\n        features['EUR_HESTON_VAR'] = realized_var\n\n        # 2. Variance mean reversion signal\n        # How far current var is from long-run level\n        long_run_var = (returns ** 2).rolling(120, min_periods=20).mean() * 252\n        features['EUR_HESTON_MR'] = (long_run_var - realized_var) / (long_run_var + 1e-12)\n\n        # 3. Volatility of volatility (vol-of-vol)\n        vol = returns.rolling(20, min_periods=2).std() * np.sqrt(252)\n        vol_of_vol = vol.rolling(20, min_periods=5).std()\n        features['EUR_HESTON_VVOL'] = vol_of_vol\n\n        return features",
    "source_file": "core\\features\\europe_quant.py",
    "academic_reference": null,
    "class_name": "EuropeQuantFeatures"
  },
  {
    "name": "_sabr_factors",
    "category": "alpha_factor",
    "formula": "pd.Series(result, index=ret.index) | features",
    "explanation": "SABR Model Features (Stochastic Alpha Beta Rho).\n\nSABR is the industry standard for FX options smile:\n- Alpha: ATM volatility level\n- Beta: CEV exponent (backbone)\n- Rho: Vol-spot correlation\n- Nu: Vol of vol\n\nWe proxy these parameters from historical data.\n\nReferences:\n- Hagan et al (2002): \"Managing Smile Risk\"\n- Paris-Dauphine: \"SABR calibration for FX\"",
    "python_code": "def _sabr_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        SABR Model Features (Stochastic Alpha Beta Rho).\n\n        SABR is the industry standard for FX options smile:\n        - Alpha: ATM volatility level\n        - Beta: CEV exponent (backbone)\n        - Rho: Vol-spot correlation\n        - Nu: Vol of vol\n\n        We proxy these parameters from historical data.\n\n        References:\n        - Hagan et al (2002): \"Managing Smile Risk\"\n        - Paris-Dauphine: \"SABR calibration for FX\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Alpha proxy: ATM implied vol estimate\n        # Use rolling std as proxy\n        alpha_proxy = returns.rolling(20, min_periods=2).std() * np.sqrt(252)\n        features['EUR_SABR_ALPHA'] = alpha_proxy\n\n        # 2. Rho proxy: volatility-spot correlation\n        # Correlation between returns and vol changes\n        vol = returns.rolling(10, min_periods=2).std()\n        vol_change = vol.pct_change()\n\n        def rolling_corr(ret, vol_chg, window):\n            result = np.empty(len(ret))\n            result[:] = np.nan\n            for i in range(window, len(ret)):\n                r = ret.iloc[i-window:i]\n                v = vol_chg.iloc[i-window:i]\n                valid = ~(r.isna() | v.isna())\n                if valid.sum() > 2:\n                    result[i] = np.corrcoef(r[valid], v[valid])[0, 1]\n            return pd.Series(result, index=ret.index)\n\n        rho_proxy = rolling_corr(returns, vol_change, 30)\n        features['EUR_SABR_RHO'] = rho_proxy.fillna(0)\n\n        # 3. Smile skew proxy: asymmetry in returns\n        skew = returns.rolling(30, min_periods=10).skew()\n        features['EUR_SABR_SKEW'] = skew\n\n        return features",
    "source_file": "core\\features\\europe_quant.py",
    "academic_reference": null,
    "class_name": "EuropeQuantFeatures"
  },
  {
    "name": "_rough_vol_factors",
    "category": "volatility",
    "formula": "pd.Series(result, index=series.index) | features",
    "explanation": "Rough Volatility Features.\n\nGatheral et al (2018) \"Volatility is Rough\" showed that:\n- Log-volatility behaves like fractional Brownian motion\n- Hurst exponent H ~ 0.1 (very rough)\n- Better captures vol dynamics than classical models\n\nWe implement simplified rough vol proxies:\n- Roughness of vol path\n- Fractional differencing\n- Memory parameter estimation\n\nReferences:\n- Gatheral et al (2018): \"Volatility is Rough\"\n- Bayer et al (2016): \"Pricing under rough volatility\"",
    "python_code": "def _rough_vol_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Rough Volatility Features.\n\n        Gatheral et al (2018) \"Volatility is Rough\" showed that:\n        - Log-volatility behaves like fractional Brownian motion\n        - Hurst exponent H ~ 0.1 (very rough)\n        - Better captures vol dynamics than classical models\n\n        We implement simplified rough vol proxies:\n        - Roughness of vol path\n        - Fractional differencing\n        - Memory parameter estimation\n\n        References:\n        - Gatheral et al (2018): \"Volatility is Rough\"\n        - Bayer et al (2016): \"Pricing under rough volatility\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Log-volatility roughness: variation of log-vol\n        vol = returns.rolling(5, min_periods=2).std()\n        log_vol = np.log(vol + 1e-12)\n        # Roughness = high variation at short lags\n        roughness = log_vol.diff().abs().rolling(20, min_periods=5).mean()\n        features['EUR_ROUGH_VAR'] = roughness\n\n        # 2. Fractional differencing proxy\n        # Using H ~ 0.1, fractional diff = (1-L)^H where L is lag operator\n        # Simplified: weighted sum of lagged log-vols\n        weights = np.array([(-1)**k * special.binom(self.hurst, k)\n                           for k in range(10)])\n        weights = weights / np.sum(np.abs(weights))\n\n        def frac_diff(series, w):\n            result = np.zeros(len(series))\n            for i in range(len(w), len(series)):\n                result[i] = np.sum(w * series.iloc[i-len(w):i].values[::-1])\n            return pd.Series(result, index=series.index)\n\n        frac_log_vol = frac_diff(log_vol.fillna(0), weights)\n        features['EUR_ROUGH_FRAC'] = frac_log_vol\n\n        # 3. Memory parameter proxy: autocorrelation decay\n        # Rough vol has slowly decaying autocorrelation\n        log_vol_diff = log_vol.diff().fillna(0)\n        ac_lag1 = log_vol_diff.rolling(30).apply(\n            lambda x: x.autocorr() if len(x) > 5 else 0, raw=False\n        )\n        features['EUR_ROUGH_MEM'] = ac_lag1.fillna(0)\n\n        return features",
    "source_file": "core\\features\\europe_quant.py",
    "academic_reference": null,
    "class_name": "EuropeQuantFeatures"
  },
  {
    "name": "_jump_factors",
    "category": "reinforcement_learning",
    "formula": "dS = mu*S*dt + sigma*S*dW + J*S*dN | features",
    "explanation": "Jump-Diffusion Detection Features.\n\nMerton (1976) jump-diffusion model adds jumps to GBM:\ndS = mu*S*dt + sigma*S*dW + J*S*dN\n\nWe detect jumps using:\n- Bipower variation (separates jumps from diffusion)\n- Z-score extreme returns\n- Jump intensity estimation\n\nReferences:\n- Merton (1976): \"Option Pricing with Discontinuous Returns\"\n- Barndorff-Nielsen & Shephard (2004): \"Power and Bipower Variation\"",
    "python_code": "def _jump_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Jump-Diffusion Detection Features.\n\n        Merton (1976) jump-diffusion model adds jumps to GBM:\n        dS = mu*S*dt + sigma*S*dW + J*S*dN\n\n        We detect jumps using:\n        - Bipower variation (separates jumps from diffusion)\n        - Z-score extreme returns\n        - Jump intensity estimation\n\n        References:\n        - Merton (1976): \"Option Pricing with Discontinuous Returns\"\n        - Barndorff-Nielsen & Shephard (2004): \"Power and Bipower Variation\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Bipower variation (robust to jumps)\n        # BV = (pi/2) * sum(|r_t| * |r_{t-1}|)\n        abs_ret = returns.abs()\n        bipower = (np.pi / 2) * (abs_ret * abs_ret.shift(1)).rolling(20, min_periods=5).mean()\n        realized_var = (returns ** 2).rolling(20, min_periods=5).mean()\n\n        # Relative jump variation: (RV - BV) / RV\n        features['EUR_JUMP_VAR'] = (realized_var - bipower) / (realized_var + 1e-12)\n\n        # 2. Jump indicator: large returns relative to local vol\n        vol_local = returns.rolling(50, min_periods=10).std()\n        zscore = returns / (vol_local + 1e-12)\n        features['EUR_JUMP_IND'] = np.where(np.abs(zscore) > self.jump_thresh, 1, 0)\n\n        # 3. Jump intensity: frequency of jumps\n        jump_count = features['EUR_JUMP_IND'].rolling(50, min_periods=10).sum()\n        features['EUR_JUMP_INT'] = jump_count / 50  # Jumps per period\n\n        return features",
    "source_file": "core\\features\\europe_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "EuropeQuantFeatures"
  },
  {
    "name": "_ecb_factors",
    "category": "volatility",
    "formula": "features",
    "explanation": "ECB Policy Impact Features.\n\nEuropean Central Bank policy significantly impacts EUR pairs:\n- Interest rate decisions\n- QE announcements\n- Forward guidance\n\nWe proxy policy impact using:\n- Volatility around ECB meeting days (proxied by calendar)\n- Trend breaks (policy shifts)\n- Currency strength relative to DXY proxy\n\nReferences:\n- Deutsche Bundesbank research\n- Frankfurt School: \"ECB communication and forex markets\"",
    "python_code": "def _ecb_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        ECB Policy Impact Features.\n\n        European Central Bank policy significantly impacts EUR pairs:\n        - Interest rate decisions\n        - QE announcements\n        - Forward guidance\n\n        We proxy policy impact using:\n        - Volatility around ECB meeting days (proxied by calendar)\n        - Trend breaks (policy shifts)\n        - Currency strength relative to DXY proxy\n\n        References:\n        - Deutsche Bundesbank research\n        - Frankfurt School: \"ECB communication and forex markets\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Policy uncertainty proxy: vol regime\n        vol_short = returns.rolling(5, min_periods=2).std()\n        vol_long = returns.rolling(60, min_periods=10).std()\n        features['EUR_ECB_UNC'] = vol_short / (vol_long + 1e-12)\n\n        # 2. Trend break detector: potential policy shift signal\n        # Uses structural break detection via regime change\n        ma_short = close.rolling(10, min_periods=1).mean()\n        ma_long = close.rolling(50, min_periods=1).mean()\n        trend_diff = (ma_short - ma_long) / (ma_long + 1e-12)\n        trend_accel = trend_diff - trend_diff.shift(10)\n        features['EUR_ECB_BREAK'] = trend_accel\n\n        # 3. EUR strength indicator: momentum as policy response proxy\n        features['EUR_ECB_STR'] = returns.rolling(20, min_periods=1).sum()\n\n        return features",
    "source_file": "core\\features\\europe_quant.py",
    "academic_reference": null,
    "class_name": "EuropeQuantFeatures"
  },
  {
    "name": "generate_all",
    "category": "reinforcement_learning",
    "formula": "features",
    "explanation": "Generate all Europe Quant features.\n\nArgs:\n    df: DataFrame with columns: open, high, low, close, volume\n\nReturns:\n    DataFrame with 15 factor columns",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all Europe Quant features.\n\n        Args:\n            df: DataFrame with columns: open, high, low, close, volume\n\n        Returns:\n            DataFrame with 15 factor columns\n        \"\"\"\n        # Ensure required columns\n        if 'close' not in df.columns:\n            raise ValueError(\"Missing required column: 'close'\")\n\n        # Fill missing OHLC from close\n        df = df.copy()\n        if 'open' not in df.columns:\n            df['open'] = df['close'].shift(1).fillna(df['close'])\n        if 'high' not in df.columns:\n            df['high'] = df['close']\n        if 'low' not in df.columns:\n            df['low'] = df['close']\n\n        # Generate all factor groups\n        heston = self._heston_factors(df)\n        sabr = self._sabr_factors(df)\n        rough = self._rough_vol_factors(df)\n        jumps = self._jump_factors(df)\n        ecb = self._ecb_factors(df)\n\n        # Combine all features\n        features = pd.concat([\n            heston, sabr, rough, jumps, ecb\n        ], axis=1)\n\n        # Clean up\n        features = features.replace([np.inf, -np.inf], np.nan)\n        features = features.fillna(0)\n\n        return features",
    "source_file": "core\\features\\europe_quant.py",
    "academic_reference": null,
    "class_name": "EuropeQuantFeatures"
  },
  {
    "name": "get_feature_names",
    "category": "feature_engineering",
    "formula": "names",
    "explanation": "Get list of all feature names.",
    "python_code": "def get_feature_names(self) -> List[str]:\n        \"\"\"Get list of all feature names.\"\"\"\n        names = []\n\n        # Heston (3)\n        names.extend(['EUR_HESTON_VAR', 'EUR_HESTON_MR', 'EUR_HESTON_VVOL'])\n\n        # SABR (3)\n        names.extend(['EUR_SABR_ALPHA', 'EUR_SABR_RHO', 'EUR_SABR_SKEW'])\n\n        # Rough Vol (3)\n        names.extend(['EUR_ROUGH_VAR', 'EUR_ROUGH_FRAC', 'EUR_ROUGH_MEM'])\n\n        # Jumps (3)\n        names.extend(['EUR_JUMP_VAR', 'EUR_JUMP_IND', 'EUR_JUMP_INT'])\n\n        # ECB (3)\n        names.extend(['EUR_ECB_UNC', 'EUR_ECB_BREAK', 'EUR_ECB_STR'])\n\n        return names",
    "source_file": "core\\features\\europe_quant.py",
    "academic_reference": null,
    "class_name": "EuropeQuantFeatures"
  },
  {
    "name": "get_factor_category",
    "category": "reinforcement_learning",
    "formula": "'Heston Stochastic Volatility' | 'SABR Smile Model' | 'Rough Volatility'",
    "explanation": "Get the category of a factor by name.",
    "python_code": "def get_factor_category(self, factor_name: str) -> str:\n        \"\"\"Get the category of a factor by name.\"\"\"\n        if 'HESTON' in factor_name:\n            return 'Heston Stochastic Volatility'\n        elif 'SABR' in factor_name:\n            return 'SABR Smile Model'\n        elif 'ROUGH' in factor_name:\n            return 'Rough Volatility'\n        elif 'JUMP' in factor_name:\n            return 'Jump Detection'\n        elif 'ECB' in factor_name:\n            return 'ECB Policy'\n        return 'Unknown'",
    "source_file": "core\\features\\europe_quant.py",
    "academic_reference": null,
    "class_name": "EuropeQuantFeatures"
  },
  {
    "name": "generate_europe_features",
    "category": "reinforcement_learning",
    "formula": "features.generate_all(df)",
    "explanation": "Generate European Academic features.\n\nArgs:\n    df: OHLCV DataFrame\n\nReturns:\n    DataFrame with 15 European quant factors",
    "python_code": "def generate_europe_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generate European Academic features.\n\n    Args:\n        df: OHLCV DataFrame\n\n    Returns:\n        DataFrame with 15 European quant factors\n    \"\"\"\n    features = EuropeQuantFeatures()\n    return features.generate_all(df)",
    "source_file": "core\\features\\europe_quant.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "rolling_corr",
    "category": "statistical",
    "formula": "pd.Series(result, index=ret.index)",
    "explanation": "",
    "python_code": "def rolling_corr(ret, vol_chg, window):\n            result = np.empty(len(ret))\n            result[:] = np.nan\n            for i in range(window, len(ret)):\n                r = ret.iloc[i-window:i]\n                v = vol_chg.iloc[i-window:i]\n                valid = ~(r.isna() | v.isna())\n                if valid.sum() > 2:\n                    result[i] = np.corrcoef(r[valid], v[valid])[0, 1]\n            return pd.Series(result, index=ret.index)",
    "source_file": "core\\features\\europe_quant.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "frac_diff",
    "category": "feature_engineering",
    "formula": "pd.Series(result, index=series.index)",
    "explanation": "",
    "python_code": "def frac_diff(series, w):\n            result = np.zeros(len(series))\n            for i in range(len(w), len(series)):\n                result[i] = np.sum(w * series.iloc[i-len(w):i].values[::-1])\n            return pd.Series(result, index=series.index)",
    "source_file": "core\\features\\europe_quant.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(\n        self,\n        enable_kalman: bool = True,\n        enable_garch: bool = True,\n        enable_chinese_hft: bool = True,\n        enable_attention: bool = True,\n        enable_market_impact: bool = True,\n        enable_order_flow: bool = True,\n        enable_jump: bool = True,\n        enable_har_rv: bool = True,\n        enable_alpha191: bool = True,\n        enable_lob: bool = True,\n        enable_regime: bool = True,\n        enable_range_vol: bool = True,\n        enable_elite: bool = True,\n        enable_mytt: bool = True,\n        verbose: bool = False,\n    ):\n        self.enable_flags = {\n            'kalman': enable_kalman,\n            'garch': enable_garch,\n            'chinese_hft': enable_chinese_hft,\n            'attention': enable_attention,\n            'market_impact': enable_market_impact,\n            'order_flow': enable_order_flow,\n            'jump': enable_jump,\n            'har_rv': enable_har_rv,\n            'alpha191': enable_alpha191,\n            'lob': enable_lob,\n            'regime': enable_regime,\n            'range_vol': enable_range_vol,\n            'elite': enable_elite,\n            'mytt': enable_mytt,\n        }\n        self.verbose = verbose\n        self._generators = {}",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_log",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "",
    "python_code": "def _log(self, msg: str):\n        if self.verbose:\n            print(f\"[ExperimentalEngine] {msg}\")",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_get_generator",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Lazy load experimental generators.",
    "python_code": "def _get_generator(self, name: str):\n        \"\"\"Lazy load experimental generators.\"\"\"\n        if name not in self._generators:\n            try:\n                if name == 'kalman':\n                    from core._experimental.advanced_kalman_filters import KalmanFilterFeatures\n                    self._generators[name] = KalmanFilterFeatures()\n                elif name == 'garch':\n                    from core._experimental.arima_garch_models import GARCHFeatures\n                    self._generators[name] = GARCHFeatures()\n                elif name == 'chinese_hft':\n                    from core._experimental.chinese_hft_factors import ChineseHFTFactorEngine\n                    self._generators[name] = ChineseHFTFactorEngine()\n                elif name == 'attention':\n                    from core._experimental.attention_factors import AttentionFactorEngine\n                    self._generators[name] = AttentionFactorEngine()\n                elif name == 'market_impact':\n                    from core._experimental.market_impact_models import MarketImpactFeatures\n                    self._generators[name] = MarketImpactFeatures()\n                elif name == 'order_flow':\n                    from core._experimental.order_flow_features import OrderFlowEngine\n                    self._generators[name] = OrderFlowEngine()\n                elif name == 'jump':\n                    from core._experimental.jump_detection import JumpDetectionFeatures\n                    self._generators[name] = JumpDetectionFeatures()\n                elif name == 'har_rv':\n                    from core._experimental.har_rv_volatility import HARRVFeatures\n                    self._generators[name] = HARRVFeatures()\n                elif name == 'alpha191':\n                    from core._experimental.alpha191_guotaijunan import Alpha191GuotaiJunan\n                    self._generators[name] = Alpha191GuotaiJunan()\n                elif name == 'lob':\n                    from core._experimental.lob_features import LOBFeatures\n                    self._generators[name] = LOBFeatures()\n                elif name == 'regime':\n                    from core._experimental.regime_features import RegimeFeatures\n                    self._generators[name] = RegimeFeatures()\n                elif name == 'range_vol':\n                    from core._experimental.range_volatility import RangeVolatilityFeatures\n                    self._generators[name] = RangeVolatilityFeatures()\n                elif name == 'elite':\n                    from core._experimental.elite_quant_factors import EliteQuantFactors\n                    self._generators[name] = EliteQuantFactors()\n            except ImportError as e:\n                logger.warning(f\"Could not import {name}: {e}\")\n                self._generators[name] = None\n            except Exception as e:\n                logger.warning(f\"Error loading {name}: {e}\")\n                self._generators[name] = None\n\n        return self._generators.get(name)",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "generate_all",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Generate all experimental features.\n\nArgs:\n    df: DataFrame with OHLCV data\n\nReturns:\n    DataFrame with experimental features",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all experimental features.\n\n        Args:\n            df: DataFrame with OHLCV data\n\n        Returns:\n            DataFrame with experimental features\n        \"\"\"\n        feature_sets = []\n        feature_counts = {}\n\n        # Ensure required columns\n        df = self._prepare_dataframe(df)\n\n        # 1. Kalman Filters\n        if self.enable_flags['kalman']:\n            self._log(\"Generating Kalman Filter features...\")\n            features = self._generate_kalman(df)\n            if features is not None and len(features.columns) > 0:\n                feature_sets.append(features)\n                feature_counts['kalman'] = len(features.columns)\n\n        # 2. GARCH Models\n        if self.enable_flags['garch']:\n            self._log(\"Generating GARCH features...\")\n            features = self._generate_garch(df)\n            if features is not None and len(features.columns) > 0:\n                feature_sets.append(features)\n                feature_counts['garch'] = len(features.columns)\n\n        # 3. Chinese HFT Factors\n        if self.enable_flags['chinese_hft']:\n            self._log(\"Generating Chinese HFT features...\")\n            features = self._generate_chinese_hft(df)\n            if features is not None and len(features.columns) > 0:\n                feature_sets.append(features)\n                feature_counts['chinese_hft'] = len(features.columns)\n\n        # 4. Attention Factors\n        if self.enable_flags['attention']:\n            self._log(\"Generating Attention features...\")\n            features = self._generate_attention(df)\n            if features is not None and len(features.columns) > 0:\n                feature_sets.append(features)\n                feature_counts['attention'] = len(features.columns)\n\n        # 5. Market Impact\n        if self.enable_flags['market_impact']:\n            self._log(\"Generating Market Impact features...\")\n            features = self._generate_market_impact(df)\n            if features is not None and len(features.columns) > 0:\n                feature_sets.append(features)\n                feature_counts['market_impact'] = len(features.columns)\n\n        # 6. Order Flow\n        if self.enable_flags['order_flow']:\n            self._log(\"Generating Order Flow features...\")\n            features = self._generate_order_flow(df)\n            if features is not None and len(features.columns) > 0:\n                feature_sets.append(features)\n                feature_counts['order_flow'] = len(features.columns)\n\n        # 7. Jump Detection\n        if self.enable_flags['jump']:\n            self._log(\"Generating Jump Detection features...\")\n            features = self._generate_jump(df)\n            if features is not None and len(features.columns) > 0:\n                feature_sets.append(features)\n                feature_counts['jump'] = len(features.columns)\n\n        # 8. HAR-RV\n        if self.enable_flags['har_rv']:\n            self._lo",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_prepare_dataframe",
    "category": "feature_engineering",
    "formula": "ohlcv",
    "explanation": "Prepare DataFrame with required columns.",
    "python_code": "def _prepare_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Prepare DataFrame with required columns.\"\"\"\n        ohlcv = pd.DataFrame(index=df.index)\n\n        ohlcv['close'] = df['close'].values\n        ohlcv['open'] = df.get('open', df['close'].shift(1).fillna(df['close'])).values\n        ohlcv['high'] = df.get('high', df['close']).values\n        ohlcv['low'] = df.get('low', df['close']).values\n        ohlcv['volume'] = df.get('volume', pd.Series(1, index=df.index)).values\n        ohlcv['returns'] = ohlcv['close'].pct_change().fillna(0)\n\n        return ohlcv",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_generate_kalman",
    "category": "filtering",
    "formula": "features",
    "explanation": "Generate Kalman filter features.",
    "python_code": "def _generate_kalman(self, df: pd.DataFrame) -> Optional[pd.DataFrame]:\n        \"\"\"Generate Kalman filter features.\"\"\"\n        try:\n            gen = self._get_generator('kalman')\n            if gen is not None:\n                features = gen.generate_all(df)\n                features.columns = [f'EXP_KAL_{c}' for c in features.columns]\n                return features\n        except Exception as e:\n            logger.warning(f\"Kalman generation failed: {e}\")\n\n        # Fallback: simple Kalman-like filter\n        return self._kalman_fallback(df)",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_kalman_fallback",
    "category": "filtering",
    "formula": "result",
    "explanation": "Fallback Kalman-like features.",
    "python_code": "def _kalman_fallback(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Fallback Kalman-like features.\"\"\"\n        result = pd.DataFrame(index=df.index)\n        close = df['close']\n\n        # Simple exponential smoothing as Kalman approximation\n        for alpha in [0.1, 0.3, 0.5]:\n            smoothed = close.ewm(alpha=alpha).mean()\n            result[f'EXP_KAL_smooth_{int(alpha*10)}'] = smoothed\n            result[f'EXP_KAL_error_{int(alpha*10)}'] = close - smoothed\n\n        # Trend estimation\n        result['EXP_KAL_trend'] = close.diff().ewm(span=20).mean()\n        result['EXP_KAL_trend_var'] = close.diff().ewm(span=20).var()\n\n        return result",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_generate_garch",
    "category": "volatility",
    "formula": "features",
    "explanation": "Generate GARCH features.",
    "python_code": "def _generate_garch(self, df: pd.DataFrame) -> Optional[pd.DataFrame]:\n        \"\"\"Generate GARCH features.\"\"\"\n        try:\n            gen = self._get_generator('garch')\n            if gen is not None:\n                features = gen.generate_all(df)\n                features.columns = [f'EXP_GARCH_{c}' for c in features.columns]\n                return features\n        except Exception as e:\n            logger.warning(f\"GARCH generation failed: {e}\")\n\n        # Fallback: GARCH-like volatility\n        return self._garch_fallback(df)",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_garch_fallback",
    "category": "volatility",
    "formula": "result",
    "explanation": "Fallback GARCH-like features.",
    "python_code": "def _garch_fallback(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Fallback GARCH-like features.\"\"\"\n        result = pd.DataFrame(index=df.index)\n        returns = df['close'].pct_change().fillna(0)\n\n        # EWMA volatility (GARCH-like)\n        for span in [10, 20, 60]:\n            vol = returns.ewm(span=span).std()\n            result[f'EXP_GARCH_vol_{span}'] = vol\n\n        # Volatility of volatility\n        result['EXP_GARCH_vol_of_vol'] = result['EXP_GARCH_vol_20'].rolling(10).std()\n\n        # Asymmetric volatility (leverage effect)\n        neg_returns = returns.where(returns < 0, 0)\n        result['EXP_GARCH_neg_vol'] = neg_returns.ewm(span=20).std()\n\n        return result",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_generate_chinese_hft",
    "category": "feature_engineering",
    "formula": "features",
    "explanation": "Generate Chinese HFT features.",
    "python_code": "def _generate_chinese_hft(self, df: pd.DataFrame) -> Optional[pd.DataFrame]:\n        \"\"\"Generate Chinese HFT features.\"\"\"\n        try:\n            gen = self._get_generator('chinese_hft')\n            if gen is not None:\n                features = gen.generate_all_factors(df)\n                features.columns = [f'EXP_CNHFT_{c}' for c in features.columns]\n                return features\n        except Exception as e:\n            logger.warning(f\"Chinese HFT generation failed: {e}\")\n\n        return self._chinese_hft_fallback(df)",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_chinese_hft_fallback",
    "category": "feature_engineering",
    "formula": "result",
    "explanation": "Fallback Chinese HFT-like features.",
    "python_code": "def _chinese_hft_fallback(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Fallback Chinese HFT-like features.\"\"\"\n        result = pd.DataFrame(index=df.index)\n        close = df['close']\n        high = df['high']\n        low = df['low']\n        volume = df['volume']\n\n        # Microprice\n        result['EXP_CNHFT_microprice'] = (high + low + close) / 3\n\n        # Volume clock\n        vol_ma = volume.rolling(20).mean()\n        result['EXP_CNHFT_vol_clock'] = volume / (vol_ma + 1e-10)\n\n        # Smart money indicator\n        range_ = high - low + 1e-10\n        result['EXP_CNHFT_smart_money'] = (close - low) / range_ - (high - close) / range_\n\n        # Kyle lambda approximation\n        returns = close.pct_change().fillna(0)\n        vol_change = volume.pct_change().fillna(0)\n        result['EXP_CNHFT_kyle_lambda'] = returns.rolling(20).std() / (vol_change.rolling(20).std() + 1e-10)\n\n        return result",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_generate_attention",
    "category": "deep_learning",
    "formula": "features",
    "explanation": "Generate attention-based features.",
    "python_code": "def _generate_attention(self, df: pd.DataFrame) -> Optional[pd.DataFrame]:\n        \"\"\"Generate attention-based features.\"\"\"\n        try:\n            gen = self._get_generator('attention')\n            if gen is not None:\n                features = gen.generate_all(df)\n                features.columns = [f'EXP_ATT_{c}' for c in features.columns]\n                return features\n        except Exception as e:\n            logger.warning(f\"Attention generation failed: {e}\")\n\n        return self._attention_fallback(df)",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_attention_fallback",
    "category": "deep_learning",
    "formula": "result",
    "explanation": "Fallback attention-like features.",
    "python_code": "def _attention_fallback(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Fallback attention-like features.\"\"\"\n        result = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change().fillna(0)\n\n        # Self-attention proxy (correlation with lagged self)\n        for lag in [1, 5, 10]:\n            result[f'EXP_ATT_self_corr_{lag}'] = returns.rolling(20).corr(returns.shift(lag))\n\n        # Cross-window attention\n        short_mean = close.rolling(5).mean()\n        long_mean = close.rolling(20).mean()\n        result['EXP_ATT_cross_window'] = short_mean / (long_mean + 1e-10) - 1\n\n        return result",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_generate_market_impact",
    "category": "microstructure",
    "formula": "features",
    "explanation": "Generate market impact features.",
    "python_code": "def _generate_market_impact(self, df: pd.DataFrame) -> Optional[pd.DataFrame]:\n        \"\"\"Generate market impact features.\"\"\"\n        try:\n            gen = self._get_generator('market_impact')\n            if gen is not None:\n                features = gen.generate_all(df)\n                features.columns = [f'EXP_MI_{c}' for c in features.columns]\n                return features\n        except Exception as e:\n            logger.warning(f\"Market impact generation failed: {e}\")\n\n        return self._market_impact_fallback(df)",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_market_impact_fallback",
    "category": "microstructure",
    "formula": "result",
    "explanation": "Fallback market impact features.",
    "python_code": "def _market_impact_fallback(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Fallback market impact features.\"\"\"\n        result = pd.DataFrame(index=df.index)\n        close = df['close']\n        volume = df['volume']\n        returns = close.pct_change().fillna(0)\n\n        # Kyle lambda (price impact per unit volume)\n        vol_ma = volume.rolling(20).mean()\n        result['EXP_MI_kyle_lambda'] = returns.abs() / (volume / (vol_ma + 1e-10) + 1e-10)\n\n        # Amihud illiquidity\n        result['EXP_MI_amihud'] = returns.abs() / (volume + 1e-10)\n        result['EXP_MI_amihud_20'] = result['EXP_MI_amihud'].rolling(20).mean()\n\n        return result",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_generate_order_flow",
    "category": "microstructure",
    "formula": "features",
    "explanation": "Generate order flow features.",
    "python_code": "def _generate_order_flow(self, df: pd.DataFrame) -> Optional[pd.DataFrame]:\n        \"\"\"Generate order flow features.\"\"\"\n        try:\n            gen = self._get_generator('order_flow')\n            if gen is not None:\n                features = gen.generate_all(df)\n                features.columns = [f'EXP_OF_{c}' for c in features.columns]\n                return features\n        except Exception as e:\n            logger.warning(f\"Order flow generation failed: {e}\")\n\n        return self._order_flow_fallback(df)",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_order_flow_fallback",
    "category": "microstructure",
    "formula": "result",
    "explanation": "Fallback order flow features.",
    "python_code": "def _order_flow_fallback(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Fallback order flow features.\"\"\"\n        result = pd.DataFrame(index=df.index)\n        close = df['close']\n        high = df['high']\n        low = df['low']\n        volume = df['volume']\n        returns = close.pct_change().fillna(0)\n\n        # Order Flow Imbalance (OFI)\n        buy_vol = volume.where(returns > 0, 0)\n        sell_vol = volume.where(returns < 0, 0)\n        result['EXP_OF_ofi'] = (buy_vol - sell_vol) / (buy_vol + sell_vol + 1e-10)\n        result['EXP_OF_ofi_5'] = result['EXP_OF_ofi'].rolling(5).mean()\n        result['EXP_OF_ofi_20'] = result['EXP_OF_ofi'].rolling(20).mean()\n\n        # Trade direction (tick rule)\n        tick_dir = np.sign(close.diff())\n        result['EXP_OF_tick_dir'] = tick_dir.rolling(10).sum() / 10\n\n        return result",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_generate_jump",
    "category": "feature_engineering",
    "formula": "features",
    "explanation": "Generate jump detection features.",
    "python_code": "def _generate_jump(self, df: pd.DataFrame) -> Optional[pd.DataFrame]:\n        \"\"\"Generate jump detection features.\"\"\"\n        try:\n            gen = self._get_generator('jump')\n            if gen is not None:\n                features = gen.generate_all(df)\n                features.columns = [f'EXP_JUMP_{c}' for c in features.columns]\n                return features\n        except Exception as e:\n            logger.warning(f\"Jump detection failed: {e}\")\n\n        return self._jump_fallback(df)",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_jump_fallback",
    "category": "feature_engineering",
    "formula": "result",
    "explanation": "Fallback jump detection features.",
    "python_code": "def _jump_fallback(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Fallback jump detection features.\"\"\"\n        result = pd.DataFrame(index=df.index)\n        returns = df['close'].pct_change().fillna(0)\n\n        # Z-score of returns\n        mu = returns.rolling(20).mean()\n        sigma = returns.rolling(20).std()\n        z_score = (returns - mu) / (sigma + 1e-10)\n        result['EXP_JUMP_zscore'] = z_score\n\n        # Jump indicator\n        result['EXP_JUMP_indicator'] = (z_score.abs() > 3).astype(float)\n        result['EXP_JUMP_count'] = result['EXP_JUMP_indicator'].rolling(20).sum()\n\n        return result",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_generate_har_rv",
    "category": "volatility",
    "formula": "features",
    "explanation": "Generate HAR-RV features.",
    "python_code": "def _generate_har_rv(self, df: pd.DataFrame) -> Optional[pd.DataFrame]:\n        \"\"\"Generate HAR-RV features.\"\"\"\n        try:\n            gen = self._get_generator('har_rv')\n            if gen is not None:\n                features = gen.generate_all(df)\n                features.columns = [f'EXP_HARRV_{c}' for c in features.columns]\n                return features\n        except Exception as e:\n            logger.warning(f\"HAR-RV generation failed: {e}\")\n\n        return self._har_rv_fallback(df)",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_har_rv_fallback",
    "category": "volatility",
    "formula": "result",
    "explanation": "Fallback HAR-RV features.",
    "python_code": "def _har_rv_fallback(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Fallback HAR-RV features.\"\"\"\n        result = pd.DataFrame(index=df.index)\n        returns = df['close'].pct_change().fillna(0)\n\n        # Realized variance at different horizons\n        rv_d = returns ** 2  # Daily component\n        rv_w = rv_d.rolling(5).mean()  # Weekly component\n        rv_m = rv_d.rolling(20).mean()  # Monthly component\n\n        result['EXP_HARRV_daily'] = rv_d\n        result['EXP_HARRV_weekly'] = rv_w\n        result['EXP_HARRV_monthly'] = rv_m\n\n        return result",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_generate_alpha191",
    "category": "alpha_factor",
    "formula": "features",
    "explanation": "Generate Alpha191 features.",
    "python_code": "def _generate_alpha191(self, df: pd.DataFrame) -> Optional[pd.DataFrame]:\n        \"\"\"Generate Alpha191 features.\"\"\"\n        try:\n            gen = self._get_generator('alpha191')\n            if gen is not None:\n                features = gen.generate_all_alphas(df)\n                # Take top 30\n                features = features.iloc[:, :30]\n                features.columns = [f'EXP_A191_{c}' for c in features.columns]\n                return features\n        except Exception as e:\n            logger.warning(f\"Alpha191 generation failed: {e}\")\n\n        return None",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_generate_lob",
    "category": "feature_engineering",
    "formula": "features",
    "explanation": "Generate LOB features.",
    "python_code": "def _generate_lob(self, df: pd.DataFrame) -> Optional[pd.DataFrame]:\n        \"\"\"Generate LOB features.\"\"\"\n        try:\n            gen = self._get_generator('lob')\n            if gen is not None:\n                features = gen.generate_all(df)\n                features.columns = [f'EXP_LOB_{c}' for c in features.columns]\n                return features\n        except Exception as e:\n            logger.warning(f\"LOB generation failed: {e}\")\n\n        return self._lob_fallback(df)",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_lob_fallback",
    "category": "feature_engineering",
    "formula": "result",
    "explanation": "Fallback LOB-like features.",
    "python_code": "def _lob_fallback(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Fallback LOB-like features.\"\"\"\n        result = pd.DataFrame(index=df.index)\n        close = df['close']\n        high = df['high']\n        low = df['low']\n\n        # Price level imbalance\n        range_ = high - low + 1e-10\n        result['EXP_LOB_upper_ratio'] = (high - close) / range_\n        result['EXP_LOB_lower_ratio'] = (close - low) / range_\n        result['EXP_LOB_imbalance'] = result['EXP_LOB_lower_ratio'] - result['EXP_LOB_upper_ratio']\n\n        return result",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_generate_regime",
    "category": "regime",
    "formula": "features",
    "explanation": "Generate regime features.",
    "python_code": "def _generate_regime(self, df: pd.DataFrame) -> Optional[pd.DataFrame]:\n        \"\"\"Generate regime features.\"\"\"\n        try:\n            gen = self._get_generator('regime')\n            if gen is not None:\n                features = gen.generate_all(df)\n                features.columns = [f'EXP_REG_{c}' for c in features.columns]\n                return features\n        except Exception as e:\n            logger.warning(f\"Regime generation failed: {e}\")\n\n        return self._regime_fallback(df)",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_regime_fallback",
    "category": "regime",
    "formula": "result",
    "explanation": "Fallback regime features.",
    "python_code": "def _regime_fallback(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Fallback regime features.\"\"\"\n        result = pd.DataFrame(index=df.index)\n        returns = df['close'].pct_change().fillna(0)\n\n        # Simple regime based on volatility\n        vol = returns.rolling(20).std()\n        vol_mean = vol.rolling(60).mean()\n        result['EXP_REG_vol_regime'] = np.where(vol > vol_mean, 1, 0)\n\n        # Trend regime\n        ma_short = df['close'].rolling(10).mean()\n        ma_long = df['close'].rolling(50).mean()\n        result['EXP_REG_trend_regime'] = np.where(ma_short > ma_long, 1, 0)\n\n        return result",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_generate_range_vol",
    "category": "volatility",
    "formula": "features",
    "explanation": "Generate range volatility features.",
    "python_code": "def _generate_range_vol(self, df: pd.DataFrame) -> Optional[pd.DataFrame]:\n        \"\"\"Generate range volatility features.\"\"\"\n        try:\n            gen = self._get_generator('range_vol')\n            if gen is not None:\n                features = gen.generate_all(df)\n                features.columns = [f'EXP_RV_{c}' for c in features.columns]\n                return features\n        except Exception as e:\n            logger.warning(f\"Range volatility generation failed: {e}\")\n\n        return self._range_vol_fallback(df)",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_range_vol_fallback",
    "category": "volatility",
    "formula": "result",
    "explanation": "Fallback range volatility features.",
    "python_code": "def _range_vol_fallback(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Fallback range volatility features.\"\"\"\n        result = pd.DataFrame(index=df.index)\n        high = df['high']\n        low = df['low']\n        close = df['close']\n        open_ = df['open']\n\n        # Parkinson volatility\n        log_hl = np.log(high / low)\n        result['EXP_RV_parkinson'] = np.sqrt((log_hl ** 2).rolling(20).mean() / (4 * np.log(2)))\n\n        # Garman-Klass volatility\n        log_oc = np.log(close / open_)\n        gk = 0.5 * (log_hl ** 2) - (2 * np.log(2) - 1) * (log_oc ** 2)\n        result['EXP_RV_garman_klass'] = np.sqrt(gk.rolling(20).mean())\n\n        return result",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_generate_elite",
    "category": "feature_engineering",
    "formula": "features",
    "explanation": "Generate elite quant features.",
    "python_code": "def _generate_elite(self, df: pd.DataFrame) -> Optional[pd.DataFrame]:\n        \"\"\"Generate elite quant features.\"\"\"\n        try:\n            gen = self._get_generator('elite')\n            if gen is not None:\n                features = gen.generate_all(df)\n                features.columns = [f'EXP_ELITE_{c}' for c in features.columns]\n                return features\n        except Exception as e:\n            logger.warning(f\"Elite quant generation failed: {e}\")\n\n        return None",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_generate_mytt",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Generate MyTT () Chinese technical indicator features.\n\nMyTT is a popular Chinese technical analysis library implementing\nclassic indicators from / trading platforms.",
    "python_code": "def _generate_mytt(self, df: pd.DataFrame) -> Optional[pd.DataFrame]:\n        \"\"\"\n        Generate MyTT () Chinese technical indicator features.\n\n        MyTT is a popular Chinese technical analysis library implementing\n        classic indicators from / trading platforms.\n        \"\"\"\n        try:\n            # Try to import MyTT from libs/china_formulas/\n            import sys\n            from pathlib import Path\n            mytt_path = Path(__file__).parent.parent.parent / 'libs' / 'china_formulas'\n            if str(mytt_path) not in sys.path:\n                sys.path.insert(0, str(mytt_path))\n\n            from MyTT import (\n                MACD, KDJ, RSI, BOLL, WR, BIAS, DMI, TRIX, VR, EMV,\n                DPO, BRAR, DMA, MTM, ROC, CCI, ATR, BBI, PSY, TAQ,\n                MA, EMA, HHV, LLV, STD\n            )\n\n            result = pd.DataFrame(index=df.index)\n            close = df['close'].values\n            high = df['high'].values\n            low = df['low'].values\n            open_ = df['open'].values\n            volume = df['volume'].values\n\n            # MACD\n            try:\n                dif, dea, macd = MACD(close)\n                result['EXP_MYTT_MACD_DIF'] = dif\n                result['EXP_MYTT_MACD_DEA'] = dea\n                result['EXP_MYTT_MACD_HIST'] = macd\n            except:\n                pass\n\n            # KDJ\n            try:\n                k, d, j = KDJ(close, high, low)\n                result['EXP_MYTT_KDJ_K'] = k\n                result['EXP_MYTT_KDJ_D'] = d\n                result['EXP_MYTT_KDJ_J'] = j\n            except:\n                pass\n\n            # RSI at different periods\n            for period in [6, 12, 24]:\n                try:\n                    rsi = RSI(close, period)\n                    result[f'EXP_MYTT_RSI_{period}'] = rsi\n                except:\n                    pass\n\n            # Bollinger Bands\n            try:\n                upper, mid, lower = BOLL(close)\n                result['EXP_MYTT_BOLL_UPPER'] = upper\n                result['EXP_MYTT_BOLL_MID'] = mid\n                result['EXP_MYTT_BOLL_LOWER'] = lower\n                # Normalized position within bands\n                result['EXP_MYTT_BOLL_POS'] = (close - lower) / (upper - lower + 1e-10)\n            except:\n                pass\n\n            # Williams %R\n            try:\n                wr, wr1 = WR(close, high, low)\n                result['EXP_MYTT_WR'] = wr\n                result['EXP_MYTT_WR1'] = wr1\n            except:\n                pass\n\n            # BIAS\n            try:\n                bias1, bias2, bias3 = BIAS(close)\n                result['EXP_MYTT_BIAS1'] = bias1\n                result['EXP_MYTT_BIAS2'] = bias2\n                result['EXP_MYTT_BIAS3'] = bias3\n            except:\n                pass\n\n            # DMI\n            try:\n                pdi, mdi, adx, adxr = DMI(close, high, low)\n                result['EXP_MYTT_DMI_PDI'] = pdi\n                result['EXP_MYTT_DMI_MDI'] = mdi\n                ",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "_mytt_fallback",
    "category": "feature_engineering",
    "formula": "result",
    "explanation": "Fallback MyTT-like features when MyTT module unavailable.",
    "python_code": "def _mytt_fallback(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Fallback MyTT-like features when MyTT module unavailable.\"\"\"\n        result = pd.DataFrame(index=df.index)\n        close = df['close']\n        high = df['high']\n        low = df['low']\n\n        # Simple MACD\n        ema12 = close.ewm(span=12).mean()\n        ema26 = close.ewm(span=26).mean()\n        dif = ema12 - ema26\n        dea = dif.ewm(span=9).mean()\n        result['EXP_MYTT_MACD_DIF'] = dif\n        result['EXP_MYTT_MACD_DEA'] = dea\n        result['EXP_MYTT_MACD_HIST'] = (dif - dea) * 2\n\n        # Simple RSI\n        delta = close.diff()\n        gain = delta.where(delta > 0, 0).rolling(14).mean()\n        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n        rs = gain / (loss + 1e-10)\n        result['EXP_MYTT_RSI_14'] = 100 - (100 / (1 + rs))\n\n        # Bollinger Bands\n        sma20 = close.rolling(20).mean()\n        std20 = close.rolling(20).std()\n        result['EXP_MYTT_BOLL_UPPER'] = sma20 + 2 * std20\n        result['EXP_MYTT_BOLL_MID'] = sma20\n        result['EXP_MYTT_BOLL_LOWER'] = sma20 - 2 * std20\n\n        # ATR\n        tr = pd.concat([\n            high - low,\n            (high - close.shift()).abs(),\n            (low - close.shift()).abs()\n        ], axis=1).max(axis=1)\n        result['EXP_MYTT_ATR'] = tr.rolling(14).mean()\n\n        # Williams %R\n        hh = high.rolling(14).max()\n        ll = low.rolling(14).min()\n        result['EXP_MYTT_WR'] = -100 * (hh - close) / (hh - ll + 1e-10)\n\n        return result",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "ExperimentalFeatureEngine"
  },
  {
    "name": "generate_experimental_features",
    "category": "deep_learning",
    "formula": "engine.generate_all(df)",
    "explanation": "Convenience function to generate all experimental features.",
    "python_code": "def generate_experimental_features(df: pd.DataFrame, verbose: bool = False) -> pd.DataFrame:\n    \"\"\"Convenience function to generate all experimental features.\"\"\"\n    engine = ExperimentalFeatureEngine(verbose=verbose)\n    return engine.generate_all(df)",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": null
  },
  {
    "name": "create_experimental_engine",
    "category": "reinforcement_learning",
    "formula": "engine",
    "explanation": "Factory function to create experimental feature engine.\n\nArgs:\n    config: Experimental config (uses defaults if None)\n    verbose: Enable verbose logging\n\nReturns:\n    ExperimentalFeatureEngine instance",
    "python_code": "def create_experimental_engine(\n    config: Optional[ExperimentalConfig] = None,\n    verbose: bool = False\n) -> ExperimentalFeatureEngine:\n    \"\"\"\n    Factory function to create experimental feature engine.\n\n    Args:\n        config: Experimental config (uses defaults if None)\n        verbose: Enable verbose logging\n\n    Returns:\n        ExperimentalFeatureEngine instance\n    \"\"\"\n    if config is None:\n        config = ExperimentalConfig(verbose=verbose)\n\n    engine = ExperimentalFeatureEngine(verbose=config.verbose)\n\n    # Apply config settings\n    engine.enable_kalman = config.enable_kalman\n    engine.enable_garch = config.enable_garch\n    engine.enable_chinese_hft = config.enable_chinese_hft\n    engine.enable_attention = config.enable_attention\n    engine.enable_market_impact = config.enable_market_impact\n    engine.enable_order_flow = config.enable_order_flow\n    engine.enable_jump_detection = config.enable_jump_detection\n    engine.enable_har_rv = config.enable_har_rv\n    engine.enable_alpha191 = config.enable_alpha191\n    engine.enable_lob = config.enable_lob\n    engine.enable_regime = config.enable_regime\n    engine.enable_range_vol = config.enable_range_vol\n    engine.enable_elite = config.enable_elite\n    engine.enable_mytt = config.enable_mytt\n\n    return engine",
    "source_file": "core\\features\\experimental_engine.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self):\n        self.windows = [5, 10, 20, 50, 100, 200]\n        self.return_lags = [1, 2, 3, 5, 10, 20, 50]",
    "source_file": "core\\features\\fast_engine.py",
    "academic_reference": null,
    "class_name": "FastFeatureEngine"
  },
  {
    "name": "process",
    "category": "feature_engineering",
    "formula": "result",
    "explanation": "Generate all features for a dataframe.\n\nArgs:\n    df: DataFrame with columns: timestamp, bid, ask (or close)\n    symbol: Optional symbol name\n\nReturns:\n    DataFrame with all features added",
    "python_code": "def process(self, df: pd.DataFrame, symbol: str = None) -> pd.DataFrame:\n        \"\"\"\n        Generate all features for a dataframe.\n\n        Args:\n            df: DataFrame with columns: timestamp, bid, ask (or close)\n            symbol: Optional symbol name\n\n        Returns:\n            DataFrame with all features added\n        \"\"\"\n        result = df.copy()\n\n        # Ensure we have mid price\n        if 'bid' in result.columns and 'ask' in result.columns:\n            result['mid'] = (result['bid'] + result['ask']) / 2\n            result['spread'] = result['ask'] - result['bid']\n            result['spread_bps'] = result['spread'] / result['mid'] * 10000\n        elif 'close' in result.columns:\n            result['mid'] = result['close']\n            result['spread'] = 0.0\n            result['spread_bps'] = 0.0\n        else:\n            raise ValueError(\"Need bid/ask or close columns\")\n\n        # Returns features\n        self._add_returns(result)\n\n        # Volatility features\n        self._add_volatility(result)\n\n        # Spread/microstructure features\n        self._add_microstructure(result)\n\n        # Technical indicators\n        self._add_technical(result)\n\n        # Statistical features\n        self._add_statistical(result)\n\n        # Cross-sectional features\n        self._add_cross_sectional(result)\n\n        # Drop intermediate columns\n        result = result.drop(columns=['mid'], errors='ignore')\n\n        # Count features\n        exclude = ['timestamp', 'bid', 'ask', 'volume', 'close', 'open', 'high', 'low', 'spread']\n        feature_cols = [c for c in result.columns\n                       if not c.startswith('target_')\n                       and c not in exclude]\n\n        logger.info(f\"Generated {len(feature_cols)} features for {len(result):,} samples\")\n\n        return result",
    "source_file": "core\\features\\fast_engine.py",
    "academic_reference": null,
    "class_name": "FastFeatureEngine"
  },
  {
    "name": "_add_returns",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Add return-based features.",
    "python_code": "def _add_returns(self, df: pd.DataFrame):\n        \"\"\"Add return-based features.\"\"\"\n        mid = df['mid']\n\n        # Log returns at various lags\n        for lag in self.return_lags:\n            df[f'ret_{lag}'] = np.log(mid / mid.shift(lag)) * 10000  # bps\n\n        # Cumulative returns\n        df['ret_cum_5'] = df['ret_1'].rolling(5).sum()\n        df['ret_cum_10'] = df['ret_1'].rolling(10).sum()\n        df['ret_cum_20'] = df['ret_1'].rolling(20).sum()\n        df['ret_cum_50'] = df['ret_1'].rolling(50).sum()\n\n        # Return acceleration\n        df['ret_accel'] = df['ret_1'] - df['ret_1'].shift(1)\n        df['ret_accel_5'] = df['ret_5'] - df['ret_5'].shift(5)",
    "source_file": "core\\features\\fast_engine.py",
    "academic_reference": null,
    "class_name": "FastFeatureEngine"
  },
  {
    "name": "_add_volatility",
    "category": "volatility",
    "formula": "",
    "explanation": "Add volatility features.",
    "python_code": "def _add_volatility(self, df: pd.DataFrame):\n        \"\"\"Add volatility features.\"\"\"\n        ret = df['ret_1']\n\n        for w in self.windows:\n            # Standard deviation\n            df[f'vol_{w}'] = ret.rolling(w).std()\n\n            # Realized variance\n            df[f'rvar_{w}'] = (ret ** 2).rolling(w).sum()\n\n            # High-low range if available\n            if 'high' in df.columns and 'low' in df.columns:\n                df[f'range_{w}'] = (df['high'].rolling(w).max() -\n                                   df['low'].rolling(w).min()) / df['mid'] * 10000\n\n        # Volatility ratios\n        df['vol_ratio_5_20'] = df['vol_5'] / df['vol_20'].replace(0, np.nan)\n        df['vol_ratio_10_50'] = df['vol_10'] / df['vol_50'].replace(0, np.nan)\n        df['vol_ratio_20_100'] = df['vol_20'] / df['vol_100'].replace(0, np.nan)\n\n        # Volatility z-score\n        vol_mean = df['vol_20'].rolling(100).mean()\n        vol_std = df['vol_20'].rolling(100).std()\n        df['vol_zscore'] = (df['vol_20'] - vol_mean) / vol_std.replace(0, np.nan)",
    "source_file": "core\\features\\fast_engine.py",
    "academic_reference": null,
    "class_name": "FastFeatureEngine"
  },
  {
    "name": "_add_microstructure",
    "category": "microstructure",
    "formula": "",
    "explanation": "Add microstructure/spread features.",
    "python_code": "def _add_microstructure(self, df: pd.DataFrame):\n        \"\"\"Add microstructure/spread features.\"\"\"\n        spread = df['spread_bps']\n\n        # Rolling spread stats\n        for w in [10, 20, 50, 100]:\n            df[f'spread_mean_{w}'] = spread.rolling(w).mean()\n            df[f'spread_std_{w}'] = spread.rolling(w).std()\n\n        # Spread z-score\n        df['spread_zscore'] = ((spread - df['spread_mean_50']) /\n                              df['spread_std_50'].replace(0, np.nan))\n\n        # Spread change\n        df['spread_change'] = spread - spread.shift(1)\n        df['spread_change_pct'] = df['spread_change'] / spread.shift(1).replace(0, np.nan)",
    "source_file": "core\\features\\fast_engine.py",
    "academic_reference": null,
    "class_name": "FastFeatureEngine"
  },
  {
    "name": "_add_technical",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Add technical indicator features.",
    "python_code": "def _add_technical(self, df: pd.DataFrame):\n        \"\"\"Add technical indicator features.\"\"\"\n        mid = df['mid']\n\n        # Moving averages\n        for w in self.windows:\n            df[f'sma_{w}'] = mid.rolling(w).mean()\n            df[f'ema_{w}'] = mid.ewm(span=w, adjust=False).mean()\n\n        # MA crossovers (as difference)\n        df['ma_cross_5_20'] = (df['sma_5'] - df['sma_20']) / df['sma_20'] * 10000\n        df['ma_cross_10_50'] = (df['sma_10'] - df['sma_50']) / df['sma_50'] * 10000\n        df['ma_cross_20_100'] = (df['sma_20'] - df['sma_100']) / df['sma_100'] * 10000\n\n        # Price position relative to MA\n        for w in [20, 50, 100]:\n            df[f'price_vs_sma_{w}'] = (mid - df[f'sma_{w}']) / df[f'sma_{w}'] * 10000\n\n        # RSI approximation (simple momentum)\n        delta = mid.diff()\n        gain = delta.where(delta > 0, 0)\n        loss = (-delta).where(delta < 0, 0)\n\n        for w in [10, 20, 50]:\n            avg_gain = gain.rolling(w).mean()\n            avg_loss = loss.rolling(w).mean()\n            rs = avg_gain / avg_loss.replace(0, np.nan)\n            df[f'rsi_{w}'] = 100 - (100 / (1 + rs))\n\n        # Bollinger bands position\n        for w in [20, 50]:\n            bb_mid = df[f'sma_{w}']\n            bb_std = mid.rolling(w).std()\n            df[f'bb_pos_{w}'] = (mid - bb_mid) / (2 * bb_std).replace(0, np.nan)\n\n        # MACD-like\n        df['macd'] = df['ema_10'] - df['ema_20']\n        df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()\n        df['macd_hist'] = df['macd'] - df['macd_signal']\n\n        # Drop intermediate MA columns\n        for w in self.windows:\n            df.drop(columns=[f'sma_{w}', f'ema_{w}'], inplace=True, errors='ignore')",
    "source_file": "core\\features\\fast_engine.py",
    "academic_reference": null,
    "class_name": "FastFeatureEngine"
  },
  {
    "name": "_add_statistical",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Add statistical moment features.",
    "python_code": "def _add_statistical(self, df: pd.DataFrame):\n        \"\"\"Add statistical moment features.\"\"\"\n        ret = df['ret_1']\n\n        for w in [20, 50, 100]:\n            # Skewness\n            mean = ret.rolling(w).mean()\n            std = ret.rolling(w).std()\n            skew = ((ret - mean) ** 3).rolling(w).mean() / (std ** 3).replace(0, np.nan)\n            df[f'skew_{w}'] = skew\n\n            # Kurtosis (excess)\n            kurt = ((ret - mean) ** 4).rolling(w).mean() / (std ** 4).replace(0, np.nan) - 3\n            df[f'kurt_{w}'] = kurt\n\n        # Autocorrelation\n        for lag in [1, 5, 10]:\n            df[f'autocorr_{lag}'] = ret.rolling(50).apply(\n                lambda x: pd.Series(x).autocorr(lag=lag) if len(x) > lag else np.nan,\n                raw=False\n            )",
    "source_file": "core\\features\\fast_engine.py",
    "academic_reference": null,
    "class_name": "FastFeatureEngine"
  },
  {
    "name": "_add_cross_sectional",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Add cross-sectional/relative features.",
    "python_code": "def _add_cross_sectional(self, df: pd.DataFrame):\n        \"\"\"Add cross-sectional/relative features.\"\"\"\n        mid = df['mid']\n        ret = df['ret_1']\n\n        # Z-scores of returns\n        for w in [20, 50, 100]:\n            mean = ret.rolling(w).mean()\n            std = ret.rolling(w).std()\n            df[f'ret_zscore_{w}'] = (ret - mean) / std.replace(0, np.nan)\n\n        # Price percentile in range\n        for w in [50, 100, 200]:\n            roll_min = mid.rolling(w).min()\n            roll_max = mid.rolling(w).max()\n            df[f'price_pctl_{w}'] = (mid - roll_min) / (roll_max - roll_min).replace(0, np.nan)\n\n        # Momentum percentile\n        for w in [20, 50]:\n            ret_rank = ret.rolling(w).rank(pct=True)\n            df[f'mom_pctl_{w}'] = ret_rank",
    "source_file": "core\\features\\fast_engine.py",
    "academic_reference": null,
    "class_name": "FastFeatureEngine"
  },
  {
    "name": "create_targets",
    "category": "machine_learning",
    "formula": "result",
    "explanation": "Create prediction targets.\n\nArgs:\n    df: DataFrame with mid price\n    horizons: List of forward horizons in ticks\n\nReturns:\n    DataFrame with target columns added",
    "python_code": "def create_targets(df: pd.DataFrame, horizons: List[int] = [1, 5, 10, 20]) -> pd.DataFrame:\n    \"\"\"\n    Create prediction targets.\n\n    Args:\n        df: DataFrame with mid price\n        horizons: List of forward horizons in ticks\n\n    Returns:\n        DataFrame with target columns added\n    \"\"\"\n    result = df.copy()\n\n    # Get mid price\n    if 'mid' in result.columns:\n        mid = result['mid']\n    elif 'bid' in result.columns and 'ask' in result.columns:\n        mid = (result['bid'] + result['ask']) / 2\n    elif 'close' in result.columns:\n        mid = result['close']\n    else:\n        raise ValueError(\"Need price columns\")\n\n    for h in horizons:\n        future = mid.shift(-h)\n        returns = (np.log(future) - np.log(mid)) * 10000  # bps\n        result[f'target_return_{h}'] = returns\n        result[f'target_direction_{h}'] = (returns > 0).astype(int)\n\n    return result",
    "source_file": "core\\features\\fast_engine.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "batch_process_symbol",
    "category": "feature_engineering",
    "formula": "with_targets",
    "explanation": "Load and process a symbol's data.\n\nArgs:\n    symbol: Currency pair symbol\n    data_dir: Directory with CSV files\n\nReturns:\n    Processed DataFrame or None if failed",
    "python_code": "def batch_process_symbol(symbol: str, data_dir: str = \"data/dukascopy\") -> Optional[pd.DataFrame]:\n    \"\"\"\n    Load and process a symbol's data.\n\n    Args:\n        symbol: Currency pair symbol\n        data_dir: Directory with CSV files\n\n    Returns:\n        Processed DataFrame or None if failed\n    \"\"\"\n    from pathlib import Path\n\n    data_path = Path(data_dir)\n    files = sorted(data_path.glob(f\"{symbol}_*.csv\"))\n\n    if not files:\n        logger.warning(f\"No data files for {symbol}\")\n        return None\n\n    # Load all files\n    frames = []\n    for f in files:\n        try:\n            df = pd.read_csv(f)\n            df.columns = [c.lower().strip() for c in df.columns]\n\n            # Handle timestamp\n            for col in ['timestamp', 'time', 'datetime']:\n                if col in df.columns:\n                    df['timestamp'] = pd.to_datetime(df[col])\n                    if col != 'timestamp':\n                        df = df.drop(columns=[col])\n                    break\n\n            frames.append(df)\n        except Exception as e:\n            logger.warning(f\"Error loading {f}: {e}\")\n\n    if not frames:\n        return None\n\n    # Combine and sort\n    combined = pd.concat(frames, ignore_index=True)\n    combined = combined.sort_values('timestamp').reset_index(drop=True)\n\n    logger.info(f\"Loaded {len(combined):,} ticks for {symbol}\")\n\n    # Generate features\n    engine = FastFeatureEngine()\n    featured = engine.process(combined, symbol)\n\n    # Create targets\n    with_targets = create_targets(featured)\n\n    # Drop rows with NaN in targets\n    target_cols = [c for c in with_targets.columns if c.startswith('target_')]\n    with_targets = with_targets.dropna(subset=target_cols)\n\n    logger.info(f\"Final dataset: {len(with_targets):,} samples\")\n\n    return with_targets",
    "source_file": "core\\features\\fast_engine.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Initialize with OHLCV DataFrame.\n\nArgs:\n    df: DataFrame with columns ['open', 'high', 'low', 'close', 'volume']\n        Volume can be tick volume for forex.",
    "python_code": "def __init__(self, df: pd.DataFrame):\n        \"\"\"\n        Initialize with OHLCV DataFrame.\n\n        Args:\n            df: DataFrame with columns ['open', 'high', 'low', 'close', 'volume']\n                Volume can be tick volume for forex.\n        \"\"\"\n        self.open = df['open'] if 'open' in df.columns else df['close']\n        self.high = df['high'] if 'high' in df.columns else df['close']\n        self.low = df['low'] if 'low' in df.columns else df['close']\n        self.close = df['close']\n        self.volume = df['volume'] if 'volume' in df.columns else pd.Series(1, index=df.index)\n        self.index = df.index",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "_delay",
    "category": "alpha_factor",
    "formula": "series.shift(n)",
    "explanation": "DELAY(X, N) = X shifted by N periods.",
    "python_code": "def _delay(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"DELAY(X, N) = X shifted by N periods.\"\"\"\n        return series.shift(n)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "_delta",
    "category": "alpha_factor",
    "formula": "series - series.shift(n)",
    "explanation": "DELTA(X, N) = X - DELAY(X, N).",
    "python_code": "def _delta(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"DELTA(X, N) = X - DELAY(X, N).\"\"\"\n        return series - series.shift(n)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "_rank",
    "category": "alpha_factor",
    "formula": "series.rank(pct=True)",
    "explanation": "RANK(X) = Cross-sectional rank (for single asset, use pct_rank).",
    "python_code": "def _rank(series: pd.Series) -> pd.Series:\n        \"\"\"RANK(X) = Cross-sectional rank (for single asset, use pct_rank).\"\"\"\n        return series.rank(pct=True)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "_ts_rank",
    "category": "alpha_factor",
    "formula": "series.rolling(n).apply(lambda x: stats.rankdata(x)[-1] / len(x), raw=True)",
    "explanation": "TSRANK(X, N) = Time-series rank over N periods.",
    "python_code": "def _ts_rank(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"TSRANK(X, N) = Time-series rank over N periods.\"\"\"\n        return series.rolling(n).apply(lambda x: stats.rankdata(x)[-1] / len(x), raw=True)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "_ts_max",
    "category": "alpha_factor",
    "formula": "series.rolling(n).max()",
    "explanation": "TSMAX(X, N) = Maximum over N periods.",
    "python_code": "def _ts_max(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"TSMAX(X, N) = Maximum over N periods.\"\"\"\n        return series.rolling(n).max()",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "_ts_min",
    "category": "alpha_factor",
    "formula": "series.rolling(n).min()",
    "explanation": "TSMIN(X, N) = Minimum over N periods.",
    "python_code": "def _ts_min(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"TSMIN(X, N) = Minimum over N periods.\"\"\"\n        return series.rolling(n).min()",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "_ts_argmax",
    "category": "alpha_factor",
    "formula": "series.rolling(n).apply(lambda x: np.argmax(x) + 1, raw=True)",
    "explanation": "TSARGMAX(X, N) = Index of maximum in last N periods.",
    "python_code": "def _ts_argmax(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"TSARGMAX(X, N) = Index of maximum in last N periods.\"\"\"\n        return series.rolling(n).apply(lambda x: np.argmax(x) + 1, raw=True)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "_ts_argmin",
    "category": "alpha_factor",
    "formula": "series.rolling(n).apply(lambda x: np.argmin(x) + 1, raw=True)",
    "explanation": "TSARGMIN(X, N) = Index of minimum in last N periods.",
    "python_code": "def _ts_argmin(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"TSARGMIN(X, N) = Index of minimum in last N periods.\"\"\"\n        return series.rolling(n).apply(lambda x: np.argmin(x) + 1, raw=True)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "_sma",
    "category": "alpha_factor",
    "formula": "series.ewm(alpha=m/n, adjust=False).mean()",
    "explanation": "SMA(X, N, M) = Exponential moving average.",
    "python_code": "def _sma(series: pd.Series, n: int, m: int = 1) -> pd.Series:\n        \"\"\"SMA(X, N, M) = Exponential moving average.\"\"\"\n        return series.ewm(alpha=m/n, adjust=False).mean()",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "_wma",
    "category": "alpha_factor",
    "formula": "series.rolling(n).apply(lambda x: np.dot(x, weights) / weights.sum(), raw=True)",
    "explanation": "WMA(X, N) = Weighted moving average.",
    "python_code": "def _wma(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"WMA(X, N) = Weighted moving average.\"\"\"\n        weights = np.arange(1, n + 1)\n        return series.rolling(n).apply(lambda x: np.dot(x, weights) / weights.sum(), raw=True)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "_decay_linear",
    "category": "alpha_factor",
    "formula": "series.rolling(n).apply(lambda x: np.dot(x, weights), raw=True)",
    "explanation": "DECAYLINEAR(X, N) = Linearly decaying weighted average.",
    "python_code": "def _decay_linear(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"DECAYLINEAR(X, N) = Linearly decaying weighted average.\"\"\"\n        weights = np.arange(n, 0, -1).astype(float)\n        weights = weights / weights.sum()\n        return series.rolling(n).apply(lambda x: np.dot(x, weights), raw=True)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "_corr",
    "category": "alpha_factor",
    "formula": "x.rolling(n).corr(y)",
    "explanation": "CORR(X, Y, N) = Rolling correlation.",
    "python_code": "def _corr(x: pd.Series, y: pd.Series, n: int) -> pd.Series:\n        \"\"\"CORR(X, Y, N) = Rolling correlation.\"\"\"\n        return x.rolling(n).corr(y)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "_cov",
    "category": "alpha_factor",
    "formula": "x.rolling(n).cov(y)",
    "explanation": "COV(X, Y, N) = Rolling covariance.",
    "python_code": "def _cov(x: pd.Series, y: pd.Series, n: int) -> pd.Series:\n        \"\"\"COV(X, Y, N) = Rolling covariance.\"\"\"\n        return x.rolling(n).cov(y)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "_count",
    "category": "alpha_factor",
    "formula": "condition.astype(int).rolling(n).sum()",
    "explanation": "COUNT(COND, N) = Count of True in last N periods.",
    "python_code": "def _count(condition: pd.Series, n: int) -> pd.Series:\n        \"\"\"COUNT(COND, N) = Count of True in last N periods.\"\"\"\n        return condition.astype(int).rolling(n).sum()",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "_sum",
    "category": "alpha_factor",
    "formula": "series.rolling(n).sum()",
    "explanation": "SUM(X, N) = Rolling sum.",
    "python_code": "def _sum(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"SUM(X, N) = Rolling sum.\"\"\"\n        return series.rolling(n).sum()",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "_mean",
    "category": "alpha_factor",
    "formula": "series.rolling(n).mean()",
    "explanation": "MEAN(X, N) = Rolling mean.",
    "python_code": "def _mean(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"MEAN(X, N) = Rolling mean.\"\"\"\n        return series.rolling(n).mean()",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "_std",
    "category": "alpha_factor",
    "formula": "series.rolling(n).std()",
    "explanation": "STD(X, N) = Rolling standard deviation.",
    "python_code": "def _std(series: pd.Series, n: int) -> pd.Series:\n        \"\"\"STD(X, N) = Rolling standard deviation.\"\"\"\n        return series.rolling(n).std()",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "alpha_001",
    "category": "alpha_factor",
    "formula": "-1 * CORR(RANK(DELTA(LOG(VOLUME),1)), RANK((CLOSE-OPEN)/OPEN), 6) | -1 * self._corr(x, y, 6)",
    "explanation": "Alpha001: Volume-Price Correlation\nFormula: -1 * CORR(RANK(DELTA(LOG(VOLUME),1)), RANK((CLOSE-OPEN)/OPEN), 6)\n\nForex Applicability: MEDIUM (volume is tick volume)",
    "python_code": "def alpha_001(self) -> pd.Series:\n        \"\"\"\n        Alpha001: Volume-Price Correlation\n        Formula: -1 * CORR(RANK(DELTA(LOG(VOLUME),1)), RANK((CLOSE-OPEN)/OPEN), 6)\n\n        Forex Applicability: MEDIUM (volume is tick volume)\n        \"\"\"\n        x = self._rank(self._delta(np.log(self.volume + 1), 1))\n        y = self._rank((self.close - self.open) / (self.open + 1e-10))\n        return -1 * self._corr(x, y, 6)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "alpha_002",
    "category": "alpha_factor",
    "formula": "-1 * DELTA(((CLOSE-LOW)-(HIGH-CLOSE))/(HIGH-LOW), 1) | -1 * self._delta(inner, 1)",
    "explanation": "Alpha002: Price Position Change\nFormula: -1 * DELTA(((CLOSE-LOW)-(HIGH-CLOSE))/(HIGH-LOW), 1)\n\nForex Applicability: HIGH (pure price-based)",
    "python_code": "def alpha_002(self) -> pd.Series:\n        \"\"\"\n        Alpha002: Price Position Change\n        Formula: -1 * DELTA(((CLOSE-LOW)-(HIGH-CLOSE))/(HIGH-LOW), 1)\n\n        Forex Applicability: HIGH (pure price-based)\n        \"\"\"\n        inner = ((self.close - self.low) - (self.high - self.close)) / (self.high - self.low + 1e-10)\n        return -1 * self._delta(inner, 1)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "alpha_014",
    "category": "alpha_factor",
    "formula": "CLOSE - DELAY(CLOSE, 5)",
    "explanation": "Alpha014: 5-Day Momentum\nFormula: CLOSE - DELAY(CLOSE, 5)\n\nForex Applicability: HIGH",
    "python_code": "def alpha_014(self) -> pd.Series:\n        \"\"\"\n        Alpha014: 5-Day Momentum\n        Formula: CLOSE - DELAY(CLOSE, 5)\n\n        Forex Applicability: HIGH\n        \"\"\"\n        return self.close - self._delay(self.close, 5)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "alpha_018",
    "category": "alpha_factor",
    "formula": "CLOSE / DELAY(CLOSE, 5)",
    "explanation": "Alpha018: 5-Day Price Ratio\nFormula: CLOSE / DELAY(CLOSE, 5)\n\nForex Applicability: HIGH",
    "python_code": "def alpha_018(self) -> pd.Series:\n        \"\"\"\n        Alpha018: 5-Day Price Ratio\n        Formula: CLOSE / DELAY(CLOSE, 5)\n\n        Forex Applicability: HIGH\n        \"\"\"\n        return self.close / (self._delay(self.close, 5) + 1e-10)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "alpha_020",
    "category": "alpha_factor",
    "formula": "(CLOSE - DELAY(CLOSE, 6)) / DELAY(CLOSE, 6) * 100 | (self.close - delayed) / (delayed + 1e-10) * 100",
    "explanation": "Alpha020: 6-Day Percentage Return\nFormula: (CLOSE - DELAY(CLOSE, 6)) / DELAY(CLOSE, 6) * 100\n\nForex Applicability: HIGH",
    "python_code": "def alpha_020(self) -> pd.Series:\n        \"\"\"\n        Alpha020: 6-Day Percentage Return\n        Formula: (CLOSE - DELAY(CLOSE, 6)) / DELAY(CLOSE, 6) * 100\n\n        Forex Applicability: HIGH\n        \"\"\"\n        delayed = self._delay(self.close, 6)\n        return (self.close - delayed) / (delayed + 1e-10) * 100",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "alpha_031",
    "category": "alpha_factor",
    "formula": "(CLOSE - MEAN(CLOSE, 12)) / MEAN(CLOSE, 12) * 100 | (self.close - ma) / (ma + 1e-10) * 100",
    "explanation": "Alpha031: Price Deviation from MA\nFormula: (CLOSE - MEAN(CLOSE, 12)) / MEAN(CLOSE, 12) * 100\n\nForex Applicability: HIGH",
    "python_code": "def alpha_031(self) -> pd.Series:\n        \"\"\"\n        Alpha031: Price Deviation from MA\n        Formula: (CLOSE - MEAN(CLOSE, 12)) / MEAN(CLOSE, 12) * 100\n\n        Forex Applicability: HIGH\n        \"\"\"\n        ma = self._mean(self.close, 12)\n        return (self.close - ma) / (ma + 1e-10) * 100",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "alpha_046",
    "category": "alpha_factor",
    "formula": "(MA(CLOSE,3) + MA(CLOSE,6) + MA(CLOSE,12) + MA(CLOSE,24)) / 4 / CLOSE | (ma3 + ma6 + ma12 + ma24) / 4 / (self.close + 1e-10)",
    "explanation": "Alpha046: Multi-MA Average Ratio\nFormula: (MA(CLOSE,3) + MA(CLOSE,6) + MA(CLOSE,12) + MA(CLOSE,24)) / 4 / CLOSE\n\nForex Applicability: HIGH",
    "python_code": "def alpha_046(self) -> pd.Series:\n        \"\"\"\n        Alpha046: Multi-MA Average Ratio\n        Formula: (MA(CLOSE,3) + MA(CLOSE,6) + MA(CLOSE,12) + MA(CLOSE,24)) / 4 / CLOSE\n\n        Forex Applicability: HIGH\n        \"\"\"\n        ma3 = self._mean(self.close, 3)\n        ma6 = self._mean(self.close, 6)\n        ma12 = self._mean(self.close, 12)\n        ma24 = self._mean(self.close, 24)\n        return (ma3 + ma6 + ma12 + ma24) / 4 / (self.close + 1e-10)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "alpha_053",
    "category": "alpha_factor",
    "formula": "COUNT(CLOSE > DELAY(CLOSE, 1), 12) / 12 * 100",
    "explanation": "Alpha053: Win Rate\nFormula: COUNT(CLOSE > DELAY(CLOSE, 1), 12) / 12 * 100\n\nForex Applicability: HIGH",
    "python_code": "def alpha_053(self) -> pd.Series:\n        \"\"\"\n        Alpha053: Win Rate\n        Formula: COUNT(CLOSE > DELAY(CLOSE, 1), 12) / 12 * 100\n\n        Forex Applicability: HIGH\n        \"\"\"\n        condition = self.close > self._delay(self.close, 1)\n        return self._count(condition, 12) / 12 * 100",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "alpha_054",
    "category": "alpha_factor",
    "formula": "(-1 * RANK((STD(ABS(CLOSE - OPEN)) + (CLOSE - OPEN)) + CORR(CLOSE, OPEN, 10))) | -1 * self._rank(part1 + part2 + part3)",
    "explanation": "Alpha054: Price Dispersion\nFormula: (-1 * RANK((STD(ABS(CLOSE - OPEN)) + (CLOSE - OPEN)) + CORR(CLOSE, OPEN, 10)))\n\nForex Applicability: HIGH",
    "python_code": "def alpha_054(self) -> pd.Series:\n        \"\"\"\n        Alpha054: Price Dispersion\n        Formula: (-1 * RANK((STD(ABS(CLOSE - OPEN)) + (CLOSE - OPEN)) + CORR(CLOSE, OPEN, 10)))\n\n        Forex Applicability: HIGH\n        \"\"\"\n        part1 = self._std(np.abs(self.close - self.open), 10)\n        part2 = self.close - self.open\n        part3 = self._corr(self.close, self.open, 10)\n        return -1 * self._rank(part1 + part2 + part3)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "alpha_060",
    "category": "alpha_factor",
    "formula": "-1 * RANK(((CLOSE - LOW) - (HIGH - CLOSE)) / (HIGH - LOW) * VOLUME) | -1 * self._rank(inner * self.volume)",
    "explanation": "Alpha060: Williams %R Variant\nFormula: -1 * RANK(((CLOSE - LOW) - (HIGH - CLOSE)) / (HIGH - LOW) * VOLUME)\n\nForex Applicability: MEDIUM (uses volume)",
    "python_code": "def alpha_060(self) -> pd.Series:\n        \"\"\"\n        Alpha060: Williams %R Variant\n        Formula: -1 * RANK(((CLOSE - LOW) - (HIGH - CLOSE)) / (HIGH - LOW) * VOLUME)\n\n        Forex Applicability: MEDIUM (uses volume)\n        \"\"\"\n        inner = ((self.close - self.low) - (self.high - self.close)) / (self.high - self.low + 1e-10)\n        return -1 * self._rank(inner * self.volume)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "alpha_101",
    "category": "alpha_factor",
    "formula": "(CLOSE - OPEN) / ((HIGH - LOW) + 0.001) | (self.close - self.open) / (self.high - self.low + 0.001)",
    "explanation": "Alpha101: Normalized Price Position\nFormula: (CLOSE - OPEN) / ((HIGH - LOW) + 0.001)\n\nForex Applicability: HIGH (Williams %R style)",
    "python_code": "def alpha_101(self) -> pd.Series:\n        \"\"\"\n        Alpha101: Normalized Price Position\n        Formula: (CLOSE - OPEN) / ((HIGH - LOW) + 0.001)\n\n        Forex Applicability: HIGH (Williams %R style)\n        \"\"\"\n        return (self.close - self.open) / (self.high - self.low + 0.001)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "alpha_126",
    "category": "alpha_factor",
    "formula": "(CLOSE + HIGH + LOW) / 3 | (self.close + self.high + self.low) / 3",
    "explanation": "Alpha126: Typical Price\nFormula: (CLOSE + HIGH + LOW) / 3\n\nForex Applicability: HIGH",
    "python_code": "def alpha_126(self) -> pd.Series:\n        \"\"\"\n        Alpha126: Typical Price\n        Formula: (CLOSE + HIGH + LOW) / 3\n\n        Forex Applicability: HIGH\n        \"\"\"\n        return (self.close + self.high + self.low) / 3",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "alpha_130",
    "category": "alpha_factor",
    "formula": "RANK(DECAYLINEAR(((HIGH + LOW) / 2 - (DELAY(HIGH) + DELAY(LOW)) / 2) * VWAP, 5))",
    "explanation": "Alpha130: Volume-Weighted Returns\nFormula: RANK(DECAYLINEAR(((HIGH + LOW) / 2 - (DELAY(HIGH) + DELAY(LOW)) / 2) * VWAP, 5))\n\nForex Applicability: MEDIUM (uses volume)",
    "python_code": "def alpha_130(self) -> pd.Series:\n        \"\"\"\n        Alpha130: Volume-Weighted Returns\n        Formula: RANK(DECAYLINEAR(((HIGH + LOW) / 2 - (DELAY(HIGH) + DELAY(LOW)) / 2) * VWAP, 5))\n\n        Forex Applicability: MEDIUM (uses volume)\n        \"\"\"\n        mid = (self.high + self.low) / 2\n        delayed_mid = (self._delay(self.high, 1) + self._delay(self.low, 1)) / 2\n        vwap = (self.close * self.volume).rolling(20).sum() / self.volume.rolling(20).sum()\n\n        inner = (mid - delayed_mid) * vwap\n        return self._rank(self._decay_linear(inner, 5))",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "alpha_160",
    "category": "alpha_factor",
    "formula": "SMA(IF(CLOSE <= DELAY(CLOSE), STD(CLOSE, 20), 0), 20, 1) | pd.Series(conditional_vol, index=self.index).rolling(20).mean()",
    "explanation": "Alpha160: Conditional Volatility\nFormula: SMA(IF(CLOSE <= DELAY(CLOSE), STD(CLOSE, 20), 0), 20, 1)\n\nForex Applicability: HIGH",
    "python_code": "def alpha_160(self) -> pd.Series:\n        \"\"\"\n        Alpha160: Conditional Volatility\n        Formula: SMA(IF(CLOSE <= DELAY(CLOSE), STD(CLOSE, 20), 0), 20, 1)\n\n        Forex Applicability: HIGH\n        \"\"\"\n        condition = self.close <= self._delay(self.close, 1)\n        vol = self._std(self.close, 20)\n        conditional_vol = np.where(condition, vol, 0)\n        return pd.Series(conditional_vol, index=self.index).rolling(20).mean()",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "alpha_188",
    "category": "alpha_factor",
    "formula": "((HIGH - LOW - SMA(HIGH - LOW, 11, 2)) / SMA(HIGH - LOW, 11, 2)) * 100 | (hl_range - sma_range) / (sma_range + 1e-10) * 100",
    "explanation": "Alpha188: Normalized Range Expansion\nFormula: ((HIGH - LOW - SMA(HIGH - LOW, 11, 2)) / SMA(HIGH - LOW, 11, 2)) * 100\n\nForex Applicability: HIGH",
    "python_code": "def alpha_188(self) -> pd.Series:\n        \"\"\"\n        Alpha188: Normalized Range Expansion\n        Formula: ((HIGH - LOW - SMA(HIGH - LOW, 11, 2)) / SMA(HIGH - LOW, 11, 2)) * 100\n\n        Forex Applicability: HIGH\n        \"\"\"\n        hl_range = self.high - self.low\n        sma_range = self._sma(hl_range, 11, 2)\n        return (hl_range - sma_range) / (sma_range + 1e-10) * 100",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "alpha_191",
    "category": "alpha_factor",
    "formula": "(CORR(MEAN(VOLUME, 20), LOW, 5) + (HIGH + LOW) / 2) - CLOSE | corr + mid - self.close",
    "explanation": "Alpha191: Volume-Price Momentum\nFormula: (CORR(MEAN(VOLUME, 20), LOW, 5) + (HIGH + LOW) / 2) - CLOSE\n\nForex Applicability: MEDIUM (uses volume)",
    "python_code": "def alpha_191(self) -> pd.Series:\n        \"\"\"\n        Alpha191: Volume-Price Momentum\n        Formula: (CORR(MEAN(VOLUME, 20), LOW, 5) + (HIGH + LOW) / 2) - CLOSE\n\n        Forex Applicability: MEDIUM (uses volume)\n        \"\"\"\n        vol_ma = self._mean(self.volume, 20)\n        corr = self._corr(vol_ma, self.low, 5)\n        mid = (self.high + self.low) / 2\n        return corr + mid - self.close",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "compute_all",
    "category": "alpha_factor",
    "formula": "factors.replace([np.inf, -np.inf], np.nan).fillna(0)",
    "explanation": "Compute all forex-applicable Alpha191 factors.\n\nReturns:\n    DataFrame with all factor columns",
    "python_code": "def compute_all(self) -> pd.DataFrame:\n        \"\"\"\n        Compute all forex-applicable Alpha191 factors.\n\n        Returns:\n            DataFrame with all factor columns\n        \"\"\"\n        factors = pd.DataFrame(index=self.index)\n\n        # High applicability factors\n        factors['alpha191_001'] = self.alpha_001()\n        factors['alpha191_002'] = self.alpha_002()\n        factors['alpha191_014'] = self.alpha_014()\n        factors['alpha191_018'] = self.alpha_018()\n        factors['alpha191_020'] = self.alpha_020()\n        factors['alpha191_031'] = self.alpha_031()\n        factors['alpha191_046'] = self.alpha_046()\n        factors['alpha191_053'] = self.alpha_053()\n        factors['alpha191_054'] = self.alpha_054()\n        factors['alpha191_060'] = self.alpha_060()\n        factors['alpha191_101'] = self.alpha_101()\n        factors['alpha191_126'] = self.alpha_126()\n        factors['alpha191_130'] = self.alpha_130()\n        factors['alpha191_160'] = self.alpha_160()\n        factors['alpha191_188'] = self.alpha_188()\n        factors['alpha191_191'] = self.alpha_191()\n\n        return factors.replace([np.inf, -np.inf], np.nan).fillna(0)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191ForexFactors"
  },
  {
    "name": "calculate_s_indicator",
    "category": "technical",
    "formula": "S_t = |R_t| / sqrt(V_t) | S_t = |R_t| / sqrt(V_t) | S = large price move relative to volume (informed trade)",
    "explanation": "Calculate S-indicator per observation.\n\nFormula:\n    S_t = |R_t| / sqrt(V_t)\n\nInterpretation:\n    High S = large price move relative to volume (informed trade)\n    Low S = small price move for volume (noise trade)",
    "python_code": "def calculate_s_indicator(self, returns: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"\n        Calculate S-indicator per observation.\n\n        Formula:\n            S_t = |R_t| / sqrt(V_t)\n\n        Interpretation:\n            High S = large price move relative to volume (informed trade)\n            Low S = small price move for volume (noise trade)\n        \"\"\"\n        return np.abs(returns) / (np.sqrt(volume) + 1e-10)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "SmartMoneyFactor2"
  },
  {
    "name": "identify_smart_trades",
    "category": "technical",
    "formula": "True = smart money trade | result[-1]  # Return last value for rolling | s_indicator > s_threshold",
    "explanation": "Identify \"smart money\" trades using S-indicator ranking.\n\nReturns:\n    Boolean series where True = smart money trade",
    "python_code": "def identify_smart_trades(self, df: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Identify \"smart money\" trades using S-indicator ranking.\n\n        Returns:\n            Boolean series where True = smart money trade\n        \"\"\"\n        returns = df['close'].pct_change()\n        volume = df['volume'] if 'volume' in df.columns else pd.Series(1, index=df.index)\n\n        s_indicator = self.calculate_s_indicator(returns, volume)\n\n        # Rolling identification\n        def identify_window(window_data):\n            s_vals = window_data['s'].values\n            vol_vals = window_data['volume'].values\n\n            # Sort by S descending\n            sorted_idx = np.argsort(s_vals)[::-1]\n\n            # Cumulative volume\n            sorted_vol = vol_vals[sorted_idx]\n            cum_vol = np.cumsum(sorted_vol)\n            total_vol = cum_vol[-1]\n\n            # Mark top 20% by cumulative volume as smart\n            is_smart = cum_vol <= total_vol * self.smart_pct\n\n            # Map back to original order\n            result = np.zeros(len(s_vals), dtype=bool)\n            for i, idx in enumerate(sorted_idx):\n                result[idx] = is_smart[i]\n\n            return result[-1]  # Return last value for rolling\n\n        temp_df = pd.DataFrame({\n            's': s_indicator,\n            'volume': volume\n        })\n\n        # Simplified: use quantile threshold\n        s_threshold = s_indicator.rolling(self.lookback).quantile(1 - self.smart_pct)\n        return s_indicator > s_threshold",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "SmartMoneyFactor2"
  },
  {
    "name": "calculate_factor",
    "category": "execution",
    "formula": "Q = VWAP_smart / VWAP_all | Q = VWAP_smart / VWAP_all | 1.0",
    "explanation": "Calculate Smart Money Factor Q.\n\nFormula:\n    Q = VWAP_smart / VWAP_all\n\nSignal:\n    Q > 1: Smart money selling (bearish)\n    Q < 1: Smart money buying (bullish)",
    "python_code": "def calculate_factor(self, df: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Calculate Smart Money Factor Q.\n\n        Formula:\n            Q = VWAP_smart / VWAP_all\n\n        Signal:\n            Q > 1: Smart money selling (bearish)\n            Q < 1: Smart money buying (bullish)\n        \"\"\"\n        close = df['close']\n        volume = df['volume'] if 'volume' in df.columns else pd.Series(1, index=df.index)\n\n        returns = close.pct_change()\n        s_indicator = self.calculate_s_indicator(returns, volume)\n\n        # Rolling calculation\n        def calc_q(window_idx):\n            if len(window_idx) < 5:\n                return 1.0\n\n            window_df = df.loc[window_idx]\n            s_vals = s_indicator.loc[window_idx].values\n\n            # Threshold for smart money\n            threshold = np.percentile(s_vals[~np.isnan(s_vals)], (1 - self.smart_pct) * 100)\n            is_smart = s_vals >= threshold\n\n            if is_smart.sum() == 0:\n                return 1.0\n\n            prices = window_df['close'].values\n            vols = window_df['volume'].values if 'volume' in window_df.columns else np.ones(len(prices))\n\n            # VWAP calculations\n            smart_vwap = np.sum(prices[is_smart] * vols[is_smart]) / (np.sum(vols[is_smart]) + 1e-10)\n            all_vwap = np.sum(prices * vols) / (np.sum(vols) + 1e-10)\n\n            return smart_vwap / (all_vwap + 1e-10)\n\n        # Use rolling apply\n        q_values = []\n        for i in range(len(df)):\n            start_idx = max(0, i - self.lookback + 1)\n            window_idx = df.index[start_idx:i+1]\n            q = calc_q(window_idx)\n            q_values.append(q)\n\n        q_series = pd.Series(q_values, index=df.index)\n\n        # Convert to signal: negative Q means smart money bullish\n        signal = 1 - q_series  # Higher = more bullish\n\n        return signal",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "SmartMoneyFactor2"
  },
  {
    "name": "calculate_enhanced_factor",
    "category": "technical",
    "formula": "result.replace([np.inf, -np.inf], np.nan).fillna(0)",
    "explanation": "Calculate enhanced Smart Money Factor with additional signals.\n\nReturns:\n    DataFrame with multiple smart money indicators",
    "python_code": "def calculate_enhanced_factor(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Calculate enhanced Smart Money Factor with additional signals.\n\n        Returns:\n            DataFrame with multiple smart money indicators\n        \"\"\"\n        result = pd.DataFrame(index=df.index)\n\n        # Base factor\n        result['smart_money_q'] = self.calculate_factor(df)\n\n        # Smart trade identification\n        result['is_smart_trade'] = self.identify_smart_trades(df).astype(int)\n\n        # S-indicator z-score\n        returns = df['close'].pct_change()\n        volume = df['volume'] if 'volume' in df.columns else pd.Series(1, index=df.index)\n        s_indicator = self.calculate_s_indicator(returns, volume)\n\n        s_mean = s_indicator.rolling(50).mean()\n        s_std = s_indicator.rolling(50).std()\n        result['s_indicator_zscore'] = (s_indicator - s_mean) / (s_std + 1e-10)\n\n        # Smart money momentum\n        result['smart_money_momentum'] = result['smart_money_q'].diff(5)\n\n        return result.replace([np.inf, -np.inf], np.nan).fillna(0)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "SmartMoneyFactor2"
  },
  {
    "name": "bulk_volume_classification",
    "category": "microstructure",
    "formula": "Z = (P_close - P_open) / \n    V_buy = V  (Z)\n    V_sell = V  (1 - (Z)) | Z = (P_close - P_open) /  | V_buy = V  (Z)",
    "explanation": "Bulk Volume Classification (BVC) method.\n\nCitation: Easley et al. (2012), Section 2.2\n\nFormula:\n    Z = (P_close - P_open) / \n    V_buy = V  (Z)\n    V_sell = V  (1 - (Z))\n\nWhere  is the standard normal CDF.",
    "python_code": "def bulk_volume_classification(self,\n                                    prices: pd.Series,\n                                    volumes: pd.Series) -> Tuple[pd.Series, pd.Series]:\n        \"\"\"\n        Bulk Volume Classification (BVC) method.\n\n        Citation: Easley et al. (2012), Section 2.2\n\n        Formula:\n            Z = (P_close - P_open) / \n            V_buy = V  (Z)\n            V_sell = V  (1 - (Z))\n\n        Where  is the standard normal CDF.\n        \"\"\"\n        # Calculate price changes\n        price_change = prices.diff()\n\n        # Estimate volatility\n        sigma = price_change.rolling(20).std()\n        sigma = sigma.replace(0, np.nan).fillna(price_change.std())\n\n        # Z-score\n        z_score = price_change / (sigma + 1e-10)\n\n        # CDF classification\n        buy_prob = stats.norm.cdf(z_score)\n\n        v_buy = volumes * buy_prob\n        v_sell = volumes * (1 - buy_prob)\n\n        return v_buy, v_sell",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "VPINEnhanced"
  },
  {
    "name": "calculate_vpin",
    "category": "volatility",
    "formula": "vpin",
    "explanation": "Calculate VPIN time series.\n\nHigh VPIN indicates:\n    - High probability of informed trading\n    - Expected volatility increase\n    - Toxic order flow (adverse selection risk)",
    "python_code": "def calculate_vpin(self,\n                       prices: pd.Series,\n                       volumes: pd.Series) -> pd.Series:\n        \"\"\"\n        Calculate VPIN time series.\n\n        High VPIN indicates:\n            - High probability of informed trading\n            - Expected volatility increase\n            - Toxic order flow (adverse selection risk)\n        \"\"\"\n        v_buy, v_sell = self.bulk_volume_classification(prices, volumes)\n\n        # Order imbalance\n        total_vol = v_buy + v_sell + 1e-10\n        order_imbalance = np.abs(v_buy - v_sell) / total_vol\n\n        # Rolling VPIN\n        vpin = order_imbalance.rolling(self.n_buckets, min_periods=10).mean()\n\n        return vpin",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "VPINEnhanced"
  },
  {
    "name": "calculate_vpin_cdf",
    "category": "microstructure",
    "formula": "vpin_cdf",
    "explanation": "Calculate VPIN CDF (percentile rank).\n\nVPIN CDF > 0.9 = Extreme toxicity, expect flash crash\n\nSource: Easley et al. (2012), Section 4",
    "python_code": "def calculate_vpin_cdf(self,\n                          prices: pd.Series,\n                          volumes: pd.Series,\n                          window: int = 252) -> pd.Series:\n        \"\"\"\n        Calculate VPIN CDF (percentile rank).\n\n        VPIN CDF > 0.9 = Extreme toxicity, expect flash crash\n\n        Source: Easley et al. (2012), Section 4\n        \"\"\"\n        vpin = self.calculate_vpin(prices, volumes)\n        vpin_cdf = vpin.rolling(window).rank(pct=True)\n        return vpin_cdf",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "VPINEnhanced"
  },
  {
    "name": "calculate_all_signals",
    "category": "microstructure",
    "formula": "result.replace([np.inf, -np.inf], np.nan).fillna(0)",
    "explanation": "Calculate all VPIN-related signals.",
    "python_code": "def calculate_all_signals(self,\n                              df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Calculate all VPIN-related signals.\n        \"\"\"\n        result = pd.DataFrame(index=df.index)\n\n        prices = df['close']\n        volumes = df['volume'] if 'volume' in df.columns else pd.Series(1, index=df.index)\n\n        # Base VPIN\n        result['vpin'] = self.calculate_vpin(prices, volumes)\n\n        # VPIN CDF\n        result['vpin_cdf'] = self.calculate_vpin_cdf(prices, volumes)\n\n        # Toxicity signal (binary)\n        result['toxicity_high'] = (result['vpin_cdf'] > 0.9).astype(int)\n        result['toxicity_low'] = (result['vpin_cdf'] < 0.1).astype(int)\n\n        # VPIN momentum\n        result['vpin_momentum'] = result['vpin'].diff(5)\n\n        # VPIN z-score\n        vpin_mean = result['vpin'].rolling(100).mean()\n        vpin_std = result['vpin'].rolling(100).std()\n        result['vpin_zscore'] = (result['vpin'] - vpin_mean) / (vpin_std + 1e-10)\n\n        return result.replace([np.inf, -np.inf], np.nan).fillna(0)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "VPINEnhanced"
  },
  {
    "name": "calculate_single_level_ofi",
    "category": "microstructure",
    "formula": "ofi.fillna(0)",
    "explanation": "Calculate OFI for a single price level.\n\nCitation: Cont et al. (2014), Equation (2)",
    "python_code": "def calculate_single_level_ofi(self,\n                                   bid_price: pd.Series,\n                                   bid_size: pd.Series,\n                                   ask_price: pd.Series,\n                                   ask_size: pd.Series) -> pd.Series:\n        \"\"\"\n        Calculate OFI for a single price level.\n\n        Citation: Cont et al. (2014), Equation (2)\n        \"\"\"\n        # Price changes\n        delta_bid = bid_price.diff()\n        delta_ask = ask_price.diff()\n\n        # Bid side contribution\n        ofi_bid = np.where(delta_bid > 0, bid_size,\n                  np.where(delta_bid < 0, -bid_size.shift(1),\n                          bid_size - bid_size.shift(1)))\n\n        # Ask side contribution (negated)\n        ofi_ask = np.where(delta_ask < 0, -ask_size,\n                  np.where(delta_ask > 0, ask_size.shift(1),\n                          -(ask_size - ask_size.shift(1))))\n\n        ofi = pd.Series(ofi_bid + ofi_ask, index=bid_price.index)\n\n        return ofi.fillna(0)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "IntegratedOFI"
  },
  {
    "name": "calculate_integrated_ofi_simple",
    "category": "microstructure",
    "formula": "normalized_ofi.fillna(0)",
    "explanation": "Calculate integrated OFI from simple OHLCV data.\n\nApproximation when L2 data not available.",
    "python_code": "def calculate_integrated_ofi_simple(self,\n                                        df: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Calculate integrated OFI from simple OHLCV data.\n\n        Approximation when L2 data not available.\n        \"\"\"\n        returns = df['close'].pct_change()\n        volume = df['volume'] if 'volume' in df.columns else pd.Series(1, index=df.index)\n\n        # Sign volume based on price direction\n        signed_volume = np.sign(returns) * volume\n\n        # Cumulative OFI\n        ofi = signed_volume.rolling(20).sum()\n\n        # Normalize by average volume\n        avg_vol = volume.rolling(100).mean()\n        normalized_ofi = ofi / (avg_vol + 1e-10)\n\n        return normalized_ofi.fillna(0)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "IntegratedOFI"
  },
  {
    "name": "calculate_ofi_price_impact",
    "category": "microstructure",
    "formula": "P_t =   OFI_t + _t | ofi, lambda_coef",
    "explanation": "Calculate OFI and estimate price impact coefficient (lambda).\n\nModel: P_t =   OFI_t + _t\n\nCitation: Cont et al. (2014), Section 3\n\nReturns:\n    (ofi_series, lambda_series)",
    "python_code": "def calculate_ofi_price_impact(self,\n                                   df: pd.DataFrame,\n                                   window: int = 50) -> Tuple[pd.Series, pd.Series]:\n        \"\"\"\n        Calculate OFI and estimate price impact coefficient (lambda).\n\n        Model: P_t =   OFI_t + _t\n\n        Citation: Cont et al. (2014), Section 3\n\n        Returns:\n            (ofi_series, lambda_series)\n        \"\"\"\n        returns = df['close'].pct_change() * 10000  # bps\n        ofi = self.calculate_integrated_ofi_simple(df)\n\n        # Rolling regression for lambda\n        cov = returns.rolling(window).cov(ofi)\n        var = ofi.rolling(window).var()\n\n        lambda_coef = cov / (var + 1e-10)\n\n        return ofi, lambda_coef",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "IntegratedOFI"
  },
  {
    "name": "calculate_all_ofi_features",
    "category": "microstructure",
    "formula": "result.replace([np.inf, -np.inf], np.nan).fillna(0)",
    "explanation": "Calculate all OFI-related features.",
    "python_code": "def calculate_all_ofi_features(self,\n                                    df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Calculate all OFI-related features.\n        \"\"\"\n        result = pd.DataFrame(index=df.index)\n\n        # Base OFI\n        ofi, lambda_coef = self.calculate_ofi_price_impact(df)\n        result['ofi'] = ofi\n        result['ofi_lambda'] = lambda_coef\n\n        # OFI momentum\n        result['ofi_momentum_5'] = ofi.diff(5)\n        result['ofi_momentum_20'] = ofi.diff(20)\n\n        # OFI z-score\n        ofi_mean = ofi.rolling(100).mean()\n        ofi_std = ofi.rolling(100).std()\n        result['ofi_zscore'] = (ofi - ofi_mean) / (ofi_std + 1e-10)\n\n        # OFI signal (smoothed)\n        result['ofi_signal'] = ofi.rolling(5).mean()\n\n        # OFI regime (positive/negative accumulation)\n        result['ofi_regime'] = np.sign(ofi.rolling(20).mean())\n\n        return result.replace([np.inf, -np.inf], np.nan).fillna(0)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "IntegratedOFI"
  },
  {
    "name": "parkinson",
    "category": "volatility",
    "formula": " = (1/4ln2)  [ln(H/L)] | vol",
    "explanation": "Parkinson (1980) volatility estimator.\n\nCitation: [A8]\n\nFormula:\n     = (1/4ln2)  [ln(H/L)]\n\nEfficiency: 5x more efficient than close-to-close",
    "python_code": "def parkinson(self, high: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"\n        Parkinson (1980) volatility estimator.\n\n        Citation: [A8]\n\n        Formula:\n             = (1/4ln2)  [ln(H/L)]\n\n        Efficiency: 5x more efficient than close-to-close\n        \"\"\"\n        log_hl = np.log(high / low)\n        variance = (1 / (4 * np.log(2))) * log_hl ** 2\n\n        vol = np.sqrt(variance.rolling(self.window).mean()) * self.annualization_factor\n        return vol",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "VolatilityEstimators"
  },
  {
    "name": "garman_klass",
    "category": "volatility",
    "formula": " = 0.5  [ln(H/L)] - (2ln2 - 1)  [ln(C/O)] | vol",
    "explanation": "Garman-Klass (1980) volatility estimator.\n\nCitation: [A6]\n\nFormula:\n     = 0.5  [ln(H/L)] - (2ln2 - 1)  [ln(C/O)]\n\nEfficiency: 8x more efficient than close-to-close",
    "python_code": "def garman_klass(self,\n                     open_p: pd.Series,\n                     high: pd.Series,\n                     low: pd.Series,\n                     close: pd.Series) -> pd.Series:\n        \"\"\"\n        Garman-Klass (1980) volatility estimator.\n\n        Citation: [A6]\n\n        Formula:\n             = 0.5  [ln(H/L)] - (2ln2 - 1)  [ln(C/O)]\n\n        Efficiency: 8x more efficient than close-to-close\n        \"\"\"\n        log_hl = np.log(high / low)\n        log_co = np.log(close / open_p)\n\n        variance = 0.5 * log_hl ** 2 - (2 * np.log(2) - 1) * log_co ** 2\n\n        vol = np.sqrt(variance.rolling(self.window).mean().clip(lower=0)) * self.annualization_factor\n        return vol",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "VolatilityEstimators"
  },
  {
    "name": "rogers_satchell",
    "category": "volatility",
    "formula": " = ln(H/C)  ln(H/O) + ln(L/C)  ln(L/O) | vol",
    "explanation": "Rogers-Satchell (1991) volatility estimator.\n\nCitation: [A9]\n\nFormula:\n     = ln(H/C)  ln(H/O) + ln(L/C)  ln(L/O)\n\nAdvantage: Handles drift (non-zero mean returns)",
    "python_code": "def rogers_satchell(self,\n                        open_p: pd.Series,\n                        high: pd.Series,\n                        low: pd.Series,\n                        close: pd.Series) -> pd.Series:\n        \"\"\"\n        Rogers-Satchell (1991) volatility estimator.\n\n        Citation: [A9]\n\n        Formula:\n             = ln(H/C)  ln(H/O) + ln(L/C)  ln(L/O)\n\n        Advantage: Handles drift (non-zero mean returns)\n        \"\"\"\n        log_ho = np.log(high / open_p)\n        log_lo = np.log(low / open_p)\n        log_hc = np.log(high / close)\n        log_lc = np.log(low / close)\n\n        variance = log_ho * log_hc + log_lo * log_lc\n\n        vol = np.sqrt(variance.rolling(self.window).mean().clip(lower=0)) * self.annualization_factor\n        return vol",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "VolatilityEstimators"
  },
  {
    "name": "yang_zhang",
    "category": "volatility",
    "formula": " = _o + k  _c + (1-k)  _rs | _o = Var(ln(O_t / C_{t-1}))  (overnight volatility) | _c = Var(ln(C_t / O_t))      (close-to-open volatility)",
    "explanation": "Yang-Zhang (2000) volatility estimator.\n\nCitation: [A7]\n\nFormula:\n     = _o + k  _c + (1-k)  _rs\n\n    _o = Var(ln(O_t / C_{t-1}))  (overnight volatility)\n    _c = Var(ln(C_t / O_t))      (close-to-open volatility)\n    _rs = Rogers-Satchell variance\n    k = 0.34 / (1.34 + (n+1)/(n-1))\n\nAdvantage: Accounts for overnight gaps (important for forex)\nEfficiency: Most efficient OHLC estimator",
    "python_code": "def yang_zhang(self,\n                   open_p: pd.Series,\n                   high: pd.Series,\n                   low: pd.Series,\n                   close: pd.Series) -> pd.Series:\n        \"\"\"\n        Yang-Zhang (2000) volatility estimator.\n\n        Citation: [A7]\n\n        Formula:\n             = _o + k  _c + (1-k)  _rs\n\n            _o = Var(ln(O_t / C_{t-1}))  (overnight volatility)\n            _c = Var(ln(C_t / O_t))      (close-to-open volatility)\n            _rs = Rogers-Satchell variance\n            k = 0.34 / (1.34 + (n+1)/(n-1))\n\n        Advantage: Accounts for overnight gaps (important for forex)\n        Efficiency: Most efficient OHLC estimator\n        \"\"\"\n        n = self.window\n        k = 0.34 / (1.34 + (n + 1) / (n - 1))\n\n        # Overnight return (open vs previous close)\n        log_oc = np.log(open_p / close.shift(1))\n\n        # Open-to-close return\n        log_co = np.log(close / open_p)\n\n        # Rogers-Satchell\n        rs_var = self.rogers_satchell(open_p, high, low, close) ** 2 / (self.annualization_factor ** 2)\n\n        # Component variances\n        var_o = log_oc.rolling(n).var()\n        var_c = log_co.rolling(n).var()\n\n        # Yang-Zhang variance\n        variance = var_o + k * var_c + (1 - k) * rs_var\n\n        vol = np.sqrt(variance.clip(lower=0)) * self.annualization_factor\n        return vol",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "VolatilityEstimators"
  },
  {
    "name": "calculate_all",
    "category": "volatility",
    "formula": "result.replace([np.inf, -np.inf], np.nan).fillna(0)",
    "explanation": "Calculate all volatility estimators.",
    "python_code": "def calculate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Calculate all volatility estimators.\n        \"\"\"\n        result = pd.DataFrame(index=df.index)\n\n        o = df['open'] if 'open' in df.columns else df['close']\n        h = df['high'] if 'high' in df.columns else df['close']\n        l = df['low'] if 'low' in df.columns else df['close']\n        c = df['close']\n\n        # All estimators\n        result['vol_parkinson'] = self.parkinson(h, l)\n        result['vol_garman_klass'] = self.garman_klass(o, h, l, c)\n        result['vol_rogers_satchell'] = self.rogers_satchell(o, h, l, c)\n        result['vol_yang_zhang'] = self.yang_zhang(o, h, l, c)\n\n        # Close-to-close for comparison\n        result['vol_close_to_close'] = c.pct_change().rolling(self.window).std() * self.annualization_factor\n\n        # Volatility ratios (regime detection)\n        result['vol_ratio_pk_cc'] = result['vol_parkinson'] / (result['vol_close_to_close'] + 1e-10)\n        result['vol_ratio_yz_cc'] = result['vol_yang_zhang'] / (result['vol_close_to_close'] + 1e-10)\n\n        return result.replace([np.inf, -np.inf], np.nan).fillna(0)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "VolatilityEstimators"
  },
  {
    "name": "kyle_lambda",
    "category": "microstructure",
    "formula": "r_t =   S_t + _t | direction) | kyle_lambda.fillna(0) * 1e6",
    "explanation": "Kyle's Lambda - Market impact coefficient.\n\nCitation: [A5] Kyle (1985), [A10] Hasbrouck (2009)\n\nModel:\n    r_t =   S_t + _t\n\n    r_t: Return in bps\n    S_t: Signed square-root dollar volume\n    : Price impact (Kyle's lambda)\n\nInterpretation:\n    Higher  = more illiquid market\n     = cost of demanding liquidity",
    "python_code": "def kyle_lambda(self, df: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Kyle's Lambda - Market impact coefficient.\n\n        Citation: [A5] Kyle (1985), [A10] Hasbrouck (2009)\n\n        Model:\n            r_t =   S_t + _t\n\n            r_t: Return in bps\n            S_t: Signed square-root dollar volume\n            : Price impact (Kyle's lambda)\n\n        Interpretation:\n            Higher  = more illiquid market\n             = cost of demanding liquidity\n        \"\"\"\n        returns = df['close'].pct_change() * 10000  # bps\n        volume = df['volume'] if 'volume' in df.columns else pd.Series(1, index=df.index)\n\n        # Signed volume (using return direction)\n        signed_vol = np.sign(returns) * np.sqrt(np.abs(volume))\n\n        # Rolling regression\n        cov = returns.rolling(self.window).cov(signed_vol)\n        var = signed_vol.rolling(self.window).var()\n\n        kyle_lambda = cov / (var + 1e-10)\n\n        return kyle_lambda.fillna(0) * 1e6",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "MarketImpactFactors"
  },
  {
    "name": "amihud_illiquidity",
    "category": "microstructure",
    "formula": "ILLIQ_i = (1/D)   |r_{i,d}| / DollarVolume_{i,d} | ILLIQ_i = (1/D)   |r_{i,d}| / DollarVolume_{i,d} | ILLIQ = less liquid",
    "explanation": "Amihud Illiquidity Ratio.\n\nCitation: [A1] Amihud (2002)\n\nFormula:\n    ILLIQ_i = (1/D)   |r_{i,d}| / DollarVolume_{i,d}\n\nInterpretation:\n    Higher ILLIQ = less liquid\n    Price impact per dollar traded",
    "python_code": "def amihud_illiquidity(self, df: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Amihud Illiquidity Ratio.\n\n        Citation: [A1] Amihud (2002)\n\n        Formula:\n            ILLIQ_i = (1/D)   |r_{i,d}| / DollarVolume_{i,d}\n\n        Interpretation:\n            Higher ILLIQ = less liquid\n            Price impact per dollar traded\n        \"\"\"\n        returns = df['close'].pct_change().abs()\n        volume = df['volume'] if 'volume' in df.columns else pd.Series(1, index=df.index)\n        dollar_volume = volume * df['close']\n\n        # Daily illiquidity\n        daily_illiq = returns / (dollar_volume + 1e-10)\n\n        # Rolling average\n        amihud = daily_illiq.rolling(self.window).mean()\n\n        return amihud.fillna(0) * 1e6",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "MarketImpactFactors"
  },
  {
    "name": "calculate_basic",
    "category": "reinforcement_learning",
    "formula": "(bid + ask) / 2 | mid + imbalance * spread / 2",
    "explanation": "Calculate basic microprice.",
    "python_code": "def calculate_basic(bid: float, ask: float,\n                       bid_size: float, ask_size: float) -> float:\n        \"\"\"\n        Calculate basic microprice.\n        \"\"\"\n        total_size = bid_size + ask_size\n        if total_size == 0:\n            return (bid + ask) / 2\n\n        imbalance = (bid_size - ask_size) / total_size\n        spread = ask - bid\n        mid = (bid + ask) / 2\n\n        return mid + imbalance * spread / 2",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "MicropriceCalculator"
  },
  {
    "name": "calculate_weighted",
    "category": "reinforcement_learning",
    "formula": "(bid + ask) / 2 | (ask_size * bid + bid_size * ask) / total_size",
    "explanation": "Calculate size-weighted microprice.\n\nIntuition: If more size at bid, fair price closer to ask",
    "python_code": "def calculate_weighted(bid: float, ask: float,\n                          bid_size: float, ask_size: float) -> float:\n        \"\"\"\n        Calculate size-weighted microprice.\n\n        Intuition: If more size at bid, fair price closer to ask\n        \"\"\"\n        total_size = bid_size + ask_size\n        if total_size == 0:\n            return (bid + ask) / 2\n\n        return (ask_size * bid + bid_size * ask) / total_size",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "MicropriceCalculator"
  },
  {
    "name": "calculate_from_df",
    "category": "reinforcement_learning",
    "formula": "result.replace([np.inf, -np.inf], np.nan).fillna(0)",
    "explanation": "Calculate microprice features from DataFrame.\n\nRequires columns: bid, ask (and optionally bid_size, ask_size)",
    "python_code": "def calculate_from_df(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Calculate microprice features from DataFrame.\n\n        Requires columns: bid, ask (and optionally bid_size, ask_size)\n        \"\"\"\n        result = pd.DataFrame(index=df.index)\n\n        if 'bid' not in df.columns or 'ask' not in df.columns:\n            # Estimate from close and spread assumption\n            spread = df['close'] * 0.0001  # 1 pip assumption\n            df = df.copy()\n            df['bid'] = df['close'] - spread / 2\n            df['ask'] = df['close'] + spread / 2\n\n        bid_size = df['bid_size'] if 'bid_size' in df.columns else pd.Series(1, index=df.index)\n        ask_size = df['ask_size'] if 'ask_size' in df.columns else pd.Series(1, index=df.index)\n\n        # Microprice calculations\n        result['microprice'] = [\n            self.calculate_basic(b, a, bs, as_)\n            for b, a, bs, as_ in zip(df['bid'], df['ask'], bid_size, ask_size)\n        ]\n\n        result['microprice_weighted'] = [\n            self.calculate_weighted(b, a, bs, as_)\n            for b, a, bs, as_ in zip(df['bid'], df['ask'], bid_size, ask_size)\n        ]\n\n        # Imbalance\n        total_size = bid_size + ask_size + 1e-10\n        result['book_imbalance'] = (bid_size - ask_size) / total_size\n\n        # Spread\n        result['spread'] = df['ask'] - df['bid']\n        result['spread_pct'] = result['spread'] / ((df['bid'] + df['ask']) / 2) * 10000  # bps\n\n        # Microprice vs mid deviation\n        mid = (df['bid'] + df['ask']) / 2\n        result['microprice_deviation'] = result['microprice'] - mid\n\n        return result.replace([np.inf, -np.inf], np.nan).fillna(0)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "MicropriceCalculator"
  },
  {
    "name": "generate_all",
    "category": "reinforcement_learning",
    "formula": "result",
    "explanation": "Generate all Gitee Chinese factors.\n\nArgs:\n    df: DataFrame with OHLCV columns\n        Required: close\n        Optional: open, high, low, volume, bid, ask, bid_size, ask_size\n\nReturns:\n    DataFrame with all factor columns (~60-80 features)",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all Gitee Chinese factors.\n\n        Args:\n            df: DataFrame with OHLCV columns\n                Required: close\n                Optional: open, high, low, volume, bid, ask, bid_size, ask_size\n\n        Returns:\n            DataFrame with all factor columns (~60-80 features)\n        \"\"\"\n        result = pd.DataFrame(index=df.index)\n\n        # 1. Alpha191 Factors (16 factors)\n        try:\n            alpha191 = Alpha191ForexFactors(df)\n            alpha191_features = alpha191.compute_all()\n            for col in alpha191_features.columns:\n                result[col] = alpha191_features[col]\n        except Exception as e:\n            print(f\"Alpha191 calculation failed: {e}\")\n\n        # 2. Smart Money Factor 2.0 (4 factors)\n        try:\n            smart_money_features = self.smart_money.calculate_enhanced_factor(df)\n            for col in smart_money_features.columns:\n                result[f'sm_{col}'] = smart_money_features[col]\n        except Exception as e:\n            print(f\"Smart Money calculation failed: {e}\")\n\n        # 3. VPIN Enhanced (6 factors)\n        try:\n            vpin_features = self.vpin.calculate_all_signals(df)\n            for col in vpin_features.columns:\n                result[f'vpin_{col}'] = vpin_features[col]\n        except Exception as e:\n            print(f\"VPIN calculation failed: {e}\")\n\n        # 4. Integrated OFI (6 factors)\n        try:\n            ofi_features = self.ofi.calculate_all_ofi_features(df)\n            for col in ofi_features.columns:\n                result[f'ofi_{col}'] = ofi_features[col]\n        except Exception as e:\n            print(f\"OFI calculation failed: {e}\")\n\n        # 5. Volatility Estimators (7 factors)\n        try:\n            vol_features = self.volatility.calculate_all(df)\n            for col in vol_features.columns:\n                result[col] = vol_features[col]\n        except Exception as e:\n            print(f\"Volatility calculation failed: {e}\")\n\n        # 6. Market Impact Factors (5 factors)\n        try:\n            impact_features = self.market_impact.calculate_all(df)\n            for col in impact_features.columns:\n                result[f'impact_{col}'] = impact_features[col]\n        except Exception as e:\n            print(f\"Market Impact calculation failed: {e}\")\n\n        # 7. Microprice (if L2 data available) (5 factors)\n        try:\n            if 'bid' in df.columns and 'ask' in df.columns:\n                microprice_features = self.microprice.calculate_from_df(df)\n                for col in microprice_features.columns:\n                    result[f'mp_{col}'] = microprice_features[col]\n        except Exception as e:\n            print(f\"Microprice calculation failed: {e}\")\n\n        # Clean up\n        result = result.replace([np.inf, -np.inf], np.nan)\n        result = result.fillna(0)\n\n        return result",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "GiteeChineseFactorGenerator"
  },
  {
    "name": "get_feature_groups",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "Get feature names grouped by source/category.",
    "python_code": "def get_feature_groups(self) -> Dict[str, List[str]]:\n        \"\"\"\n        Get feature names grouped by source/category.\n        \"\"\"\n        return {\n            'alpha191': [f'alpha191_{i:03d}' for i in [1, 2, 14, 18, 20, 31, 46, 53, 54, 60, 101, 126, 130, 160, 188, 191]],\n            'smart_money': ['sm_smart_money_q', 'sm_is_smart_trade', 'sm_s_indicator_zscore', 'sm_smart_money_momentum'],\n            'vpin': ['vpin_vpin', 'vpin_vpin_cdf', 'vpin_toxicity_high', 'vpin_toxicity_low', 'vpin_vpin_momentum', 'vpin_vpin_zscore'],\n            'ofi': ['ofi_ofi', 'ofi_ofi_lambda', 'ofi_ofi_momentum_5', 'ofi_ofi_momentum_20', 'ofi_ofi_zscore', 'ofi_ofi_signal', 'ofi_ofi_regime'],\n            'volatility': ['vol_parkinson', 'vol_garman_klass', 'vol_rogers_satchell', 'vol_yang_zhang', 'vol_close_to_close', 'vol_ratio_pk_cc', 'vol_ratio_yz_cc'],\n            'market_impact': ['impact_kyle_lambda', 'impact_amihud_illiq', 'impact_kyle_lambda_zscore', 'impact_amihud_illiq_zscore', 'impact_liquidity_regime'],\n            'microprice': ['mp_microprice', 'mp_microprice_weighted', 'mp_book_imbalance', 'mp_spread', 'mp_spread_pct', 'mp_microprice_deviation']\n        }",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "GiteeChineseFactorGenerator"
  },
  {
    "name": "generate_gitee_chinese_factors",
    "category": "reinforcement_learning",
    "formula": "df = pd.read_parquet('forex_data.parquet') | factors = generate_gitee_chinese_factors(df) | generator.generate_all(df)",
    "explanation": "Generate all Gitee Chinese factors in one call.\n\nArgs:\n    df: DataFrame with OHLCV columns\n\nReturns:\n    DataFrame with ~60-80 factor columns\n\nExample:\n    >>> import pandas as pd\n    >>> df = pd.read_parquet('forex_data.parquet')\n    >>> factors = generate_gitee_chinese_factors(df)\n    >>> print(factors.columns.tolist())",
    "python_code": "def generate_gitee_chinese_factors(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generate all Gitee Chinese factors in one call.\n\n    Args:\n        df: DataFrame with OHLCV columns\n\n    Returns:\n        DataFrame with ~60-80 factor columns\n\n    Example:\n        >>> import pandas as pd\n        >>> df = pd.read_parquet('forex_data.parquet')\n        >>> factors = generate_gitee_chinese_factors(df)\n        >>> print(factors.columns.tolist())\n    \"\"\"\n    generator = GiteeChineseFactorGenerator()\n    return generator.generate_all(df)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": null
  },
  {
    "name": "get_citation_info",
    "category": "reinforcement_learning",
    "formula": "__doc__",
    "explanation": "Get full citation information for this module.",
    "python_code": "def get_citation_info() -> str:\n    \"\"\"\n    Get full citation information for this module.\n    \"\"\"\n    return __doc__",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": null
  },
  {
    "name": "identify_window",
    "category": "reinforcement_learning",
    "formula": "result[-1]",
    "explanation": "",
    "python_code": "def identify_window(window_data):\n            s_vals = window_data['s'].values\n            vol_vals = window_data['volume'].values\n\n            # Sort by S descending\n            sorted_idx = np.argsort(s_vals)[::-1]\n\n            # Cumulative volume\n            sorted_vol = vol_vals[sorted_idx]\n            cum_vol = np.cumsum(sorted_vol)\n            total_vol = cum_vol[-1]\n\n            # Mark top 20% by cumulative volume as smart\n            is_smart = cum_vol <= total_vol * self.smart_pct\n\n            # Map back to original order\n            result = np.zeros(len(s_vals), dtype=bool)\n            for i, idx in enumerate(sorted_idx):\n                result[idx] = is_smart[i]\n\n            return result[-1]",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": null
  },
  {
    "name": "calc_q",
    "category": "reinforcement_learning",
    "formula": "1.0 | 1.0 | smart_vwap / (all_vwap + 1e-10)",
    "explanation": "",
    "python_code": "def calc_q(window_idx):\n            if len(window_idx) < 5:\n                return 1.0\n\n            window_df = df.loc[window_idx]\n            s_vals = s_indicator.loc[window_idx].values\n\n            # Threshold for smart money\n            threshold = np.percentile(s_vals[~np.isnan(s_vals)], (1 - self.smart_pct) * 100)\n            is_smart = s_vals >= threshold\n\n            if is_smart.sum() == 0:\n                return 1.0\n\n            prices = window_df['close'].values\n            vols = window_df['volume'].values if 'volume' in window_df.columns else np.ones(len(prices))\n\n            # VWAP calculations\n            smart_vwap = np.sum(prices[is_smart] * vols[is_smart]) / (np.sum(vols[is_smart]) + 1e-10)\n            all_vwap = np.sum(prices * vols) / (np.sum(vols) + 1e-10)\n\n            return smart_vwap / (all_vwap + 1e-10)",
    "source_file": "core\\features\\gitee_chinese_factors.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "microstructure",
    "formula": "",
    "explanation": "Initialize OFI calculator.\n\nArgs:\n    normalize: Whether to normalize OFI by rolling std\n    window: Window for normalization",
    "python_code": "def __init__(self, normalize: bool = True, window: int = 20):\n        \"\"\"\n        Initialize OFI calculator.\n\n        Args:\n            normalize: Whether to normalize OFI by rolling std\n            window: Window for normalization\n        \"\"\"\n        self.normalize = normalize\n        self.window = window",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "OrderFlowImbalance"
  },
  {
    "name": "calculate",
    "category": "microstructure",
    "formula": "OFIResult(",
    "explanation": "Calculate Order Flow Imbalance.\n\nArgs:\n    bid_prices: Best bid price series\n    bid_volumes: Best bid volume series\n    ask_prices: Best ask price series\n    ask_volumes: Best ask volume series\n\nReturns:\n    OFIResult with OFI values and components",
    "python_code": "def calculate(\n        self,\n        bid_prices: pd.Series,\n        bid_volumes: pd.Series,\n        ask_prices: pd.Series,\n        ask_volumes: pd.Series\n    ) -> OFIResult:\n        \"\"\"\n        Calculate Order Flow Imbalance.\n\n        Args:\n            bid_prices: Best bid price series\n            bid_volumes: Best bid volume series\n            ask_prices: Best ask price series\n            ask_volumes: Best ask volume series\n\n        Returns:\n            OFIResult with OFI values and components\n        \"\"\"\n        # Price changes\n        bid_price_change = bid_prices.diff()\n        ask_price_change = ask_prices.diff()\n\n        # Volume changes\n        bid_vol_change = bid_volumes.diff()\n        ask_vol_change = ask_volumes.diff()\n\n        # Bid side delta (W)\n        # Eq. (3) from Cont et al. (2014)\n        delta_W = np.where(\n            bid_price_change > 0,\n            bid_volumes,  # New higher bid level\n            np.where(\n                bid_price_change == 0,\n                bid_vol_change,  # Volume change at same level\n                -bid_volumes.shift(1)  # Level removed\n            )\n        )\n\n        # Ask side delta (V)\n        # Inverse logic for ask side\n        delta_V = np.where(\n            ask_price_change < 0,\n            ask_volumes,  # New lower ask level\n            np.where(\n                ask_price_change == 0,\n                ask_vol_change,  # Volume change at same level\n                -ask_volumes.shift(1)  # Level removed\n            )\n        )\n\n        # Convert to Series\n        delta_W = pd.Series(delta_W, index=bid_prices.index)\n        delta_V = pd.Series(delta_V, index=ask_prices.index)\n\n        # OFI = W - V (Eq. 4)\n        ofi = delta_W - delta_V\n\n        # Normalized OFI\n        if self.normalize:\n            ofi_std = ofi.rolling(self.window).std()\n            ofi_normalized = ofi / (ofi_std + 1e-10)\n        else:\n            ofi_normalized = ofi\n\n        return OFIResult(\n            ofi=ofi,\n            ofi_normalized=ofi_normalized,\n            buy_pressure=delta_W,\n            sell_pressure=delta_V\n        )",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "OrderFlowImbalance"
  },
  {
    "name": "calculate_from_ohlcv",
    "category": "microstructure",
    "formula": "ofi",
    "explanation": "Approximate OFI from OHLCV data (for forex without L2 data).\n\nUses tick rule to classify trades as buys/sells.\n\nArgs:\n    df: DataFrame with OHLCV columns\n    spread_col: Column name for spread (optional)\n\nReturns:\n    Approximated OFI series",
    "python_code": "def calculate_from_ohlcv(\n        self,\n        df: pd.DataFrame,\n        spread_col: str = 'spread'\n    ) -> pd.Series:\n        \"\"\"\n        Approximate OFI from OHLCV data (for forex without L2 data).\n\n        Uses tick rule to classify trades as buys/sells.\n\n        Args:\n            df: DataFrame with OHLCV columns\n            spread_col: Column name for spread (optional)\n\n        Returns:\n            Approximated OFI series\n        \"\"\"\n        close = df['close']\n        volume = df.get('volume', pd.Series(1, index=df.index))\n\n        # Tick rule: classify direction based on price change\n        price_change = close.diff()\n\n        # Volume-weighted direction\n        buy_volume = volume.where(price_change > 0, 0)\n        sell_volume = volume.where(price_change < 0, 0)\n\n        # OFI approximation\n        ofi = buy_volume - sell_volume\n\n        if self.normalize:\n            ofi = ofi / (ofi.rolling(self.window).std() + 1e-10)\n\n        return ofi",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "OrderFlowImbalance"
  },
  {
    "name": "bulk_volume_classification",
    "category": "volatility",
    "formula": "buy_volume, sell_volume",
    "explanation": "Bulk Volume Classification (BVC) for trade direction.\n\nFrom Easley et al. (2012), Section III.A.\n\nArgs:\n    prices: Price series\n    volumes: Volume series\n    sigma: Price volatility (if None, estimated from data)\n\nReturns:\n    Tuple of (buy_volume, sell_volume) series",
    "python_code": "def bulk_volume_classification(\n        self,\n        prices: pd.Series,\n        volumes: pd.Series,\n        sigma: float = None\n    ) -> Tuple[pd.Series, pd.Series]:\n        \"\"\"\n        Bulk Volume Classification (BVC) for trade direction.\n\n        From Easley et al. (2012), Section III.A.\n\n        Args:\n            prices: Price series\n            volumes: Volume series\n            sigma: Price volatility (if None, estimated from data)\n\n        Returns:\n            Tuple of (buy_volume, sell_volume) series\n        \"\"\"\n        from scipy.stats import norm\n\n        if sigma is None:\n            sigma = prices.pct_change().std()\n\n        # Standardized price change\n        price_change = prices.diff()\n        z = price_change / (sigma * prices.shift(1) + 1e-10)\n\n        # Probability of buy\n        prob_buy = pd.Series(norm.cdf(z), index=prices.index)\n\n        # Classified volumes\n        buy_volume = volumes * prob_buy\n        sell_volume = volumes * (1 - prob_buy)\n\n        return buy_volume, sell_volume",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "VPIN"
  },
  {
    "name": "calculate",
    "category": "microstructure",
    "formula": "VPINResult( | VPINResult(",
    "explanation": "Calculate VPIN from price and volume series.\n\nArgs:\n    prices: Price series\n    volumes: Volume series\n\nReturns:\n    VPINResult with VPIN values and diagnostics",
    "python_code": "def calculate(\n        self,\n        prices: pd.Series,\n        volumes: pd.Series\n    ) -> VPINResult:\n        \"\"\"\n        Calculate VPIN from price and volume series.\n\n        Args:\n            prices: Price series\n            volumes: Volume series\n\n        Returns:\n            VPINResult with VPIN values and diagnostics\n        \"\"\"\n        # Classify volumes\n        buy_vol, sell_vol = self.bulk_volume_classification(prices, volumes)\n\n        # Create volume buckets\n        buckets = []\n        current_bucket = {'buy': 0, 'sell': 0, 'volume': 0}\n\n        for i in range(len(prices)):\n            remaining_buy = buy_vol.iloc[i]\n            remaining_sell = sell_vol.iloc[i]\n            remaining_total = remaining_buy + remaining_sell\n\n            while remaining_total > 0:\n                space = self.bucket_size - current_bucket['volume']\n                fill = min(remaining_total, space)\n\n                # Proportional fill\n                if remaining_total > 0:\n                    buy_fill = fill * (remaining_buy / remaining_total)\n                    sell_fill = fill * (remaining_sell / remaining_total)\n                else:\n                    buy_fill = sell_fill = 0\n\n                current_bucket['buy'] += buy_fill\n                current_bucket['sell'] += sell_fill\n                current_bucket['volume'] += fill\n\n                remaining_buy -= buy_fill\n                remaining_sell -= sell_fill\n                remaining_total = remaining_buy + remaining_sell\n\n                # Bucket full\n                if current_bucket['volume'] >= self.bucket_size:\n                    buckets.append(current_bucket.copy())\n                    current_bucket = {'buy': 0, 'sell': 0, 'volume': 0}\n\n        # Calculate VPIN over rolling window\n        if len(buckets) < self.n_buckets:\n            # Not enough data\n            return VPINResult(\n                vpin=np.nan,\n                vpin_series=pd.Series(dtype=float),\n                bucket_imbalances=[],\n                toxicity_alert=False\n            )\n\n        # Rolling VPIN\n        vpin_values = []\n        imbalances = []\n\n        for i in range(self.n_buckets, len(buckets) + 1):\n            window_buckets = buckets[i - self.n_buckets:i]\n            window_imbalances = [\n                abs(b['buy'] - b['sell']) for b in window_buckets\n            ]\n            imbalances.extend(window_imbalances[-1:])\n\n            vpin = sum(window_imbalances) / (self.n_buckets * self.bucket_size)\n            vpin_values.append(vpin)\n\n        # Current VPIN\n        current_vpin = vpin_values[-1] if vpin_values else np.nan\n\n        return VPINResult(\n            vpin=current_vpin,\n            vpin_series=pd.Series(vpin_values),\n            bucket_imbalances=imbalances,\n            toxicity_alert=current_vpin > self.toxicity_threshold\n        )",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "VPIN"
  },
  {
    "name": "calculate_from_ohlcv",
    "category": "microstructure",
    "formula": "vpin",
    "explanation": "Calculate VPIN approximation from OHLCV data.\n\nFor forex without tick data, uses BVC on bar data.\n\nArgs:\n    df: DataFrame with OHLCV columns\n\nReturns:\n    Rolling VPIN series",
    "python_code": "def calculate_from_ohlcv(self, df: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Calculate VPIN approximation from OHLCV data.\n\n        For forex without tick data, uses BVC on bar data.\n\n        Args:\n            df: DataFrame with OHLCV columns\n\n        Returns:\n            Rolling VPIN series\n        \"\"\"\n        close = df['close']\n        volume = df.get('volume', df.get('tick_count', pd.Series(1, index=df.index)))\n\n        buy_vol, sell_vol = self.bulk_volume_classification(close, volume)\n\n        # Rolling order imbalance as VPIN proxy\n        imbalance = (buy_vol - sell_vol).abs()\n        total_vol = buy_vol + sell_vol\n\n        vpin = imbalance.rolling(self.n_buckets).sum() / (\n            total_vol.rolling(self.n_buckets).sum() + 1e-10\n        )\n\n        return vpin",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "VPIN"
  },
  {
    "name": "close_to_close",
    "category": "volatility",
    "formula": " = std(ln(C_t / C_{t-1}))  periods | vol",
    "explanation": "Standard close-to-close volatility.\n\nFormula:  = std(ln(C_t / C_{t-1}))  periods\n\nArgs:\n    close: Close price series\n\nReturns:\n    Annualized volatility series",
    "python_code": "def close_to_close(\n        self,\n        close: pd.Series\n    ) -> pd.Series:\n        \"\"\"\n        Standard close-to-close volatility.\n\n        Formula:  = std(ln(C_t / C_{t-1}))  periods\n\n        Args:\n            close: Close price series\n\n        Returns:\n            Annualized volatility series\n        \"\"\"\n        log_returns = np.log(close / close.shift(1))\n        vol = log_returns.rolling(self.window).std()\n\n        if self.annualize:\n            vol = vol * np.sqrt(self.periods)\n\n        return vol",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "VolatilityEstimators"
  },
  {
    "name": "parkinson",
    "category": "volatility",
    "formula": " = (1 / 4ln(2))  E[ln(H/L)] | vol",
    "explanation": "Parkinson Volatility Estimator.\n\nFormula:  = (1 / 4ln(2))  E[ln(H/L)]\n\n5.2x more efficient than close-to-close.\nAssumes no drift and continuous trading.\n\nCitation: Parkinson (1980) [4]\n\nArgs:\n    high: High price series\n    low: Low price series\n\nReturns:\n    Annualized Parkinson volatility",
    "python_code": "def parkinson(\n        self,\n        high: pd.Series,\n        low: pd.Series\n    ) -> pd.Series:\n        \"\"\"\n        Parkinson Volatility Estimator.\n\n        Formula:  = (1 / 4ln(2))  E[ln(H/L)]\n\n        5.2x more efficient than close-to-close.\n        Assumes no drift and continuous trading.\n\n        Citation: Parkinson (1980) [4]\n\n        Args:\n            high: High price series\n            low: Low price series\n\n        Returns:\n            Annualized Parkinson volatility\n        \"\"\"\n        # Eq. (4) from Parkinson (1980)\n        log_hl = np.log(high / low)\n        factor = 1 / (4 * np.log(2))\n\n        variance = factor * (log_hl ** 2).rolling(self.window).mean()\n        vol = np.sqrt(variance)\n\n        if self.annualize:\n            vol = vol * np.sqrt(self.periods)\n\n        return vol",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "VolatilityEstimators"
  },
  {
    "name": "garman_klass",
    "category": "volatility",
    "formula": " = 0.5  ln(H/L) - (2ln(2)-1)  ln(C/O) | vol",
    "explanation": "Garman-Klass Volatility Estimator.\n\nFormula:  = 0.5  ln(H/L) - (2ln(2)-1)  ln(C/O)\n\n7.4x more efficient than close-to-close.\nAssumes no drift and continuous trading.\n\nCitation: Garman & Klass (1980) [3]\n\nArgs:\n    open_: Open price series\n    high: High price series\n    low: Low price series\n    close: Close price series\n\nReturns:\n    Annualized Garman-Klass volatility",
    "python_code": "def garman_klass(\n        self,\n        open_: pd.Series,\n        high: pd.Series,\n        low: pd.Series,\n        close: pd.Series\n    ) -> pd.Series:\n        \"\"\"\n        Garman-Klass Volatility Estimator.\n\n        Formula:  = 0.5  ln(H/L) - (2ln(2)-1)  ln(C/O)\n\n        7.4x more efficient than close-to-close.\n        Assumes no drift and continuous trading.\n\n        Citation: Garman & Klass (1980) [3]\n\n        Args:\n            open_: Open price series\n            high: High price series\n            low: Low price series\n            close: Close price series\n\n        Returns:\n            Annualized Garman-Klass volatility\n        \"\"\"\n        # Eq. (14) from Garman & Klass (1980)\n        log_hl = np.log(high / low)\n        log_co = np.log(close / open_)\n\n        variance = (\n            0.5 * (log_hl ** 2) -\n            (2 * np.log(2) - 1) * (log_co ** 2)\n        ).rolling(self.window).mean()\n\n        vol = np.sqrt(variance.clip(lower=0))\n\n        if self.annualize:\n            vol = vol * np.sqrt(self.periods)\n\n        return vol",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "VolatilityEstimators"
  },
  {
    "name": "rogers_satchell",
    "category": "volatility",
    "formula": " = ln(H/C)ln(H/O) + ln(L/C)ln(L/O) | vol",
    "explanation": "Rogers-Satchell Volatility Estimator.\n\nFormula:  = ln(H/C)ln(H/O) + ln(L/C)ln(L/O)\n\nHandles non-zero drift (trending markets).\n\nCitation: Rogers & Satchell (1991) [5]\n\nArgs:\n    open_: Open price series\n    high: High price series\n    low: Low price series\n    close: Close price series\n\nReturns:\n    Annualized Rogers-Satchell volatility",
    "python_code": "def rogers_satchell(\n        self,\n        open_: pd.Series,\n        high: pd.Series,\n        low: pd.Series,\n        close: pd.Series\n    ) -> pd.Series:\n        \"\"\"\n        Rogers-Satchell Volatility Estimator.\n\n        Formula:  = ln(H/C)ln(H/O) + ln(L/C)ln(L/O)\n\n        Handles non-zero drift (trending markets).\n\n        Citation: Rogers & Satchell (1991) [5]\n\n        Args:\n            open_: Open price series\n            high: High price series\n            low: Low price series\n            close: Close price series\n\n        Returns:\n            Annualized Rogers-Satchell volatility\n        \"\"\"\n        # From Rogers & Satchell (1991)\n        log_hc = np.log(high / close)\n        log_ho = np.log(high / open_)\n        log_lc = np.log(low / close)\n        log_lo = np.log(low / open_)\n\n        variance = (\n            log_hc * log_ho + log_lc * log_lo\n        ).rolling(self.window).mean()\n\n        vol = np.sqrt(variance.clip(lower=0))\n\n        if self.annualize:\n            vol = vol * np.sqrt(self.periods)\n\n        return vol",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "VolatilityEstimators"
  },
  {
    "name": "yang_zhang",
    "category": "volatility",
    "formula": " = _overnight + k_open + (1-k)_RS | vol",
    "explanation": "Yang-Zhang Volatility Estimator.\n\nFormula:  = _overnight + k_open + (1-k)_RS\n\nHandles both drift AND opening jumps.\nMost comprehensive estimator.\n\nCitation: Yang & Zhang (2000) [6]\n\nArgs:\n    open_: Open price series\n    high: High price series\n    low: Low price series\n    close: Close price series\n    k: Weighting factor (0.34 is optimal for daily data)\n\nReturns:\n    Annualized Yang-Zhang volatility",
    "python_code": "def yang_zhang(\n        self,\n        open_: pd.Series,\n        high: pd.Series,\n        low: pd.Series,\n        close: pd.Series,\n        k: float = 0.34\n    ) -> pd.Series:\n        \"\"\"\n        Yang-Zhang Volatility Estimator.\n\n        Formula:  = _overnight + k_open + (1-k)_RS\n\n        Handles both drift AND opening jumps.\n        Most comprehensive estimator.\n\n        Citation: Yang & Zhang (2000) [6]\n\n        Args:\n            open_: Open price series\n            high: High price series\n            low: Low price series\n            close: Close price series\n            k: Weighting factor (0.34 is optimal for daily data)\n\n        Returns:\n            Annualized Yang-Zhang volatility\n        \"\"\"\n        # Overnight variance (close-to-open)\n        log_oc = np.log(open_ / close.shift(1))\n        var_overnight = log_oc.rolling(self.window).var()\n\n        # Open-to-close variance\n        log_co = np.log(close / open_)\n        var_open = log_co.rolling(self.window).var()\n\n        # Rogers-Satchell variance\n        var_rs = self.rogers_satchell(open_, high, low, close) ** 2\n        if self.annualize:\n            var_rs = var_rs / self.periods  # De-annualize for combination\n\n        # Yang-Zhang combination\n        variance = var_overnight + k * var_open + (1 - k) * var_rs\n\n        vol = np.sqrt(variance.clip(lower=0))\n\n        if self.annualize:\n            vol = vol * np.sqrt(self.periods)\n\n        return vol",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "VolatilityEstimators"
  },
  {
    "name": "generate_all",
    "category": "volatility",
    "formula": "result",
    "explanation": "Generate all volatility estimators.\n\nArgs:\n    df: DataFrame with OHLC columns\n\nReturns:\n    DataFrame with volatility columns",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all volatility estimators.\n\n        Args:\n            df: DataFrame with OHLC columns\n\n        Returns:\n            DataFrame with volatility columns\n        \"\"\"\n        result = pd.DataFrame(index=df.index)\n\n        open_ = df['open']\n        high = df['high']\n        low = df['low']\n        close = df['close']\n\n        result['VOL_CLOSE'] = self.close_to_close(close)\n        result['VOL_PARKINSON'] = self.parkinson(high, low)\n        result['VOL_GARMAN_KLASS'] = self.garman_klass(open_, high, low, close)\n        result['VOL_ROGERS_SATCHELL'] = self.rogers_satchell(open_, high, low, close)\n        result['VOL_YANG_ZHANG'] = self.yang_zhang(open_, high, low, close)\n\n        # Volatility ratios (regime indicators)\n        result['VOL_RATIO_PK_CC'] = result['VOL_PARKINSON'] / (result['VOL_CLOSE'] + 1e-10)\n        result['VOL_RATIO_GK_RS'] = result['VOL_GARMAN_KLASS'] / (result['VOL_ROGERS_SATCHELL'] + 1e-10)\n\n        return result",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "VolatilityEstimators"
  },
  {
    "name": "sharpe_ratio",
    "category": "risk",
    "formula": "SR = (R_p - R_f) / _p  periods | SR = (R_p - R_f) / _p  periods | 0",
    "explanation": "Sharpe Ratio.\n\nFormula: SR = (R_p - R_f) / _p  periods\n\nCitation: Sharpe (1966) [9]\n\nArgs:\n    returns: Return series\n\nReturns:\n    Annualized Sharpe ratio",
    "python_code": "def sharpe_ratio(self, returns: pd.Series) -> float:\n        \"\"\"\n        Sharpe Ratio.\n\n        Formula: SR = (R_p - R_f) / _p  periods\n\n        Citation: Sharpe (1966) [9]\n\n        Args:\n            returns: Return series\n\n        Returns:\n            Annualized Sharpe ratio\n        \"\"\"\n        excess_returns = returns - self.rf / self.periods\n        if excess_returns.std() == 0:\n            return 0\n        return (excess_returns.mean() / excess_returns.std()) * np.sqrt(self.periods)",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "RiskMetrics"
  },
  {
    "name": "sortino_ratio",
    "category": "risk",
    "formula": "SoR = (R_p - R_f) / _downside  periods | SoR = (R_p - R_f) / _downside  periods | _downside = ((R_t) for R_t < 0) / N",
    "explanation": "Sortino Ratio.\n\nFormula: SoR = (R_p - R_f) / _downside  periods\n\nwhere _downside = ((R_t) for R_t < 0) / N\n\nCitation: Sortino & van der Meer (1991) [10]\n\nArgs:\n    returns: Return series\n\nReturns:\n    Annualized Sortino ratio",
    "python_code": "def sortino_ratio(self, returns: pd.Series) -> float:\n        \"\"\"\n        Sortino Ratio.\n\n        Formula: SoR = (R_p - R_f) / _downside  periods\n\n        where _downside = ((R_t) for R_t < 0) / N\n\n        Citation: Sortino & van der Meer (1991) [10]\n\n        Args:\n            returns: Return series\n\n        Returns:\n            Annualized Sortino ratio\n        \"\"\"\n        excess_returns = returns - self.rf / self.periods\n        downside = np.sqrt((returns[returns < 0] ** 2).sum() / len(returns))\n\n        if downside == 0:\n            return 0\n        return (excess_returns.mean() / downside) * np.sqrt(self.periods)",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "RiskMetrics"
  },
  {
    "name": "calmar_ratio",
    "category": "risk",
    "formula": "CR = CAGR / |Max Drawdown| | CR = CAGR / |Max Drawdown| | 0",
    "explanation": "Calmar Ratio.\n\nFormula: CR = CAGR / |Max Drawdown|\n\nArgs:\n    returns: Return series\n\nReturns:\n    Calmar ratio",
    "python_code": "def calmar_ratio(self, returns: pd.Series) -> float:\n        \"\"\"\n        Calmar Ratio.\n\n        Formula: CR = CAGR / |Max Drawdown|\n\n        Args:\n            returns: Return series\n\n        Returns:\n            Calmar ratio\n        \"\"\"\n        cagr = self.cagr(returns)\n        max_dd = self.max_drawdown(returns)\n\n        if max_dd == 0:\n            return 0\n        return cagr / abs(max_dd)",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "RiskMetrics"
  },
  {
    "name": "max_drawdown",
    "category": "risk",
    "formula": "MDD = max(peak_t - trough_t) / peak_t | MDD = max(peak_t - trough_t) / peak_t | drawdown.min()",
    "explanation": "Maximum Drawdown.\n\nFormula: MDD = max(peak_t - trough_t) / peak_t\n\nArgs:\n    returns: Return series\n\nReturns:\n    Maximum drawdown (negative value)",
    "python_code": "def max_drawdown(self, returns: pd.Series) -> float:\n        \"\"\"\n        Maximum Drawdown.\n\n        Formula: MDD = max(peak_t - trough_t) / peak_t\n\n        Args:\n            returns: Return series\n\n        Returns:\n            Maximum drawdown (negative value)\n        \"\"\"\n        cumulative = (1 + returns).cumprod()\n        running_max = cumulative.expanding().max()\n        drawdown = (cumulative - running_max) / running_max\n        return drawdown.min()",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "RiskMetrics"
  },
  {
    "name": "cagr",
    "category": "feature_engineering",
    "formula": "CAGR = (V_final / V_initial)^(1/years) - 1 | CAGR = (V_final / V_initial)^(1/years) - 1 | = (1 + returns).prod() - 1",
    "explanation": "Compound Annual Growth Rate.\n\nFormula: CAGR = (V_final / V_initial)^(1/years) - 1\n\nArgs:\n    returns: Return series\n\nReturns:\n    CAGR",
    "python_code": "def cagr(self, returns: pd.Series) -> float:\n        \"\"\"\n        Compound Annual Growth Rate.\n\n        Formula: CAGR = (V_final / V_initial)^(1/years) - 1\n\n        Args:\n            returns: Return series\n\n        Returns:\n            CAGR\n        \"\"\"\n        total_return = (1 + returns).prod() - 1\n        n_years = len(returns) / self.periods\n        if n_years == 0:\n            return 0\n        return (1 + total_return) ** (1 / n_years) - 1",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "RiskMetrics"
  },
  {
    "name": "value_at_risk",
    "category": "reinforcement_learning",
    "formula": "np.percentile(returns, (1 - confidence) * 100)",
    "explanation": "Value at Risk (Historical).\n\nArgs:\n    returns: Return series\n    confidence: Confidence level (e.g., 0.95 for 95%)\n\nReturns:\n    VaR (negative value representing loss)",
    "python_code": "def value_at_risk(self, returns: pd.Series, confidence: float = 0.95) -> float:\n        \"\"\"\n        Value at Risk (Historical).\n\n        Args:\n            returns: Return series\n            confidence: Confidence level (e.g., 0.95 for 95%)\n\n        Returns:\n            VaR (negative value representing loss)\n        \"\"\"\n        return np.percentile(returns, (1 - confidence) * 100)",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "RiskMetrics"
  },
  {
    "name": "conditional_var",
    "category": "risk",
    "formula": "CVaR = E[R | R < VaR] | CVaR = E[R | R < VaR] | returns[returns <= var].mean()",
    "explanation": "Conditional Value at Risk (Expected Shortfall).\n\nFormula: CVaR = E[R | R < VaR]\n\nArgs:\n    returns: Return series\n    confidence: Confidence level\n\nReturns:\n    CVaR (negative value)",
    "python_code": "def conditional_var(self, returns: pd.Series, confidence: float = 0.95) -> float:\n        \"\"\"\n        Conditional Value at Risk (Expected Shortfall).\n\n        Formula: CVaR = E[R | R < VaR]\n\n        Args:\n            returns: Return series\n            confidence: Confidence level\n\n        Returns:\n            CVaR (negative value)\n        \"\"\"\n        var = self.value_at_risk(returns, confidence)\n        return returns[returns <= var].mean()",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "RiskMetrics"
  },
  {
    "name": "generate_all",
    "category": "feature_engineering",
    "formula": "{",
    "explanation": "Generate all risk metrics.\n\nArgs:\n    returns: Return series\n\nReturns:\n    Dict with all metrics",
    "python_code": "def generate_all(self, returns: pd.Series) -> Dict[str, float]:\n        \"\"\"\n        Generate all risk metrics.\n\n        Args:\n            returns: Return series\n\n        Returns:\n            Dict with all metrics\n        \"\"\"\n        return {\n            'sharpe_ratio': self.sharpe_ratio(returns),\n            'sortino_ratio': self.sortino_ratio(returns),\n            'calmar_ratio': self.calmar_ratio(returns),\n            'max_drawdown': self.max_drawdown(returns),\n            'cagr': self.cagr(returns),\n            'var_95': self.value_at_risk(returns, 0.95),\n            'cvar_95': self.conditional_var(returns, 0.95),\n            'volatility': returns.std() * np.sqrt(self.periods),\n            'win_rate': (returns > 0).mean(),\n            'avg_win': returns[returns > 0].mean() if (returns > 0).any() else 0,\n            'avg_loss': returns[returns < 0].mean() if (returns < 0).any() else 0,\n        }",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "RiskMetrics"
  },
  {
    "name": "kelly_fraction",
    "category": "risk",
    "formula": "0 | max(kelly, 0)",
    "explanation": "Calculate full Kelly fraction.\n\nArgs:\n    win_rate: Probability of winning (0 to 1)\n    win_loss_ratio: Average win / Average loss\n\nReturns:\n    Optimal bet fraction (0 to 1)",
    "python_code": "def kelly_fraction(win_rate: float, win_loss_ratio: float) -> float:\n        \"\"\"\n        Calculate full Kelly fraction.\n\n        Args:\n            win_rate: Probability of winning (0 to 1)\n            win_loss_ratio: Average win / Average loss\n\n        Returns:\n            Optimal bet fraction (0 to 1)\n        \"\"\"\n        if win_loss_ratio <= 0:\n            return 0\n\n        q = 1 - win_rate\n        kelly = win_rate - (q / win_loss_ratio)\n        return max(kelly, 0)",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "KellyCriterion"
  },
  {
    "name": "fractional_kelly",
    "category": "volatility",
    "formula": "fraction=0.5) provides: | fraction=0.25) provides: | full_kelly * fraction",
    "explanation": "Calculate fractional Kelly (more conservative).\n\nHalf-Kelly (fraction=0.5) provides:\n- 75% of optimal growth rate\n- 50% of the volatility\n\nQuarter-Kelly (fraction=0.25) provides:\n- 43.75% of optimal growth rate\n- 25% of the volatility\n\nArgs:\n    win_rate: Probability of winning\n    win_loss_ratio: Win/loss ratio\n    fraction: Kelly fraction (0.25-0.5 recommended)\n\nReturns:\n    Fractional Kelly bet size",
    "python_code": "def fractional_kelly(\n        win_rate: float,\n        win_loss_ratio: float,\n        fraction: float = 0.25\n    ) -> float:\n        \"\"\"\n        Calculate fractional Kelly (more conservative).\n\n        Half-Kelly (fraction=0.5) provides:\n        - 75% of optimal growth rate\n        - 50% of the volatility\n\n        Quarter-Kelly (fraction=0.25) provides:\n        - 43.75% of optimal growth rate\n        - 25% of the volatility\n\n        Args:\n            win_rate: Probability of winning\n            win_loss_ratio: Win/loss ratio\n            fraction: Kelly fraction (0.25-0.5 recommended)\n\n        Returns:\n            Fractional Kelly bet size\n        \"\"\"\n        full_kelly = KellyCriterion.kelly_fraction(win_rate, win_loss_ratio)\n        return full_kelly * fraction",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "KellyCriterion"
  },
  {
    "name": "kelly_from_returns",
    "category": "risk",
    "formula": "f* =  /  | series | 0",
    "explanation": "Calculate Kelly from historical returns.\n\nFormula: f* =  / \n\nArgs:\n    returns: Historical return series\n\nReturns:\n    Optimal Kelly leverage",
    "python_code": "def kelly_from_returns(returns: pd.Series) -> float:\n        \"\"\"\n        Calculate Kelly from historical returns.\n\n        Formula: f* =  / \n\n        Args:\n            returns: Historical return series\n\n        Returns:\n            Optimal Kelly leverage\n        \"\"\"\n        if returns.var() <= 0:\n            return 0\n        return returns.mean() / returns.var()",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "KellyCriterion"
  },
  {
    "name": "position_size",
    "category": "risk",
    "formula": "account_value * kelly_pct",
    "explanation": "Calculate position size in currency.\n\nArgs:\n    account_value: Total account value\n    win_rate: Win probability\n    win_loss_ratio: Win/loss ratio\n    fraction: Kelly fraction (0.25 default)\n    max_position_pct: Maximum position cap (0.10 = 10%)\n\nReturns:\n    Position size in currency",
    "python_code": "def position_size(\n        account_value: float,\n        win_rate: float,\n        win_loss_ratio: float,\n        fraction: float = 0.25,\n        max_position_pct: float = 0.10\n    ) -> float:\n        \"\"\"\n        Calculate position size in currency.\n\n        Args:\n            account_value: Total account value\n            win_rate: Win probability\n            win_loss_ratio: Win/loss ratio\n            fraction: Kelly fraction (0.25 default)\n            max_position_pct: Maximum position cap (0.10 = 10%)\n\n        Returns:\n            Position size in currency\n        \"\"\"\n        kelly_pct = KellyCriterion.fractional_kelly(win_rate, win_loss_ratio, fraction)\n        kelly_pct = min(kelly_pct, max_position_pct)\n        return account_value * kelly_pct",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "KellyCriterion"
  },
  {
    "name": "intensity",
    "category": "feature_engineering",
    "formula": "intensity",
    "explanation": "Calculate conditional intensity at time t.\n\nArgs:\n    t: Current time\n    event_times: Array of past event times (< t)\n\nReturns:\n    Intensity (t)",
    "python_code": "def intensity(self, t: float, event_times: np.ndarray) -> float:\n        \"\"\"\n        Calculate conditional intensity at time t.\n\n        Args:\n            t: Current time\n            event_times: Array of past event times (< t)\n\n        Returns:\n            Intensity (t)\n        \"\"\"\n        past_events = event_times[event_times < t]\n\n        # Base intensity\n        intensity = self.mu\n\n        # Add excitation from past events\n        for ti in past_events:\n            intensity += self.alpha * np.exp(-self.beta * (t - ti))\n\n        return intensity",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HawkesProcess"
  },
  {
    "name": "intensity_series",
    "category": "feature_engineering",
    "formula": "np.array([self.intensity(t, event_times) for t in times])",
    "explanation": "Calculate intensity at multiple time points.\n\nArgs:\n    times: Array of evaluation times\n    event_times: Array of event times\n\nReturns:\n    Array of intensities",
    "python_code": "def intensity_series(\n        self,\n        times: np.ndarray,\n        event_times: np.ndarray\n    ) -> np.ndarray:\n        \"\"\"\n        Calculate intensity at multiple time points.\n\n        Args:\n            times: Array of evaluation times\n            event_times: Array of event times\n\n        Returns:\n            Array of intensities\n        \"\"\"\n        return np.array([self.intensity(t, event_times) for t in times])",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HawkesProcess"
  },
  {
    "name": "simulate",
    "category": "feature_engineering",
    "formula": "np.array(events)",
    "explanation": "Simulate Hawkes process using Ogata's thinning algorithm.\n\nArgs:\n    T: End time\n    seed: Random seed\n\nReturns:\n    Array of event times",
    "python_code": "def simulate(\n        self,\n        T: float,\n        seed: int = None\n    ) -> np.ndarray:\n        \"\"\"\n        Simulate Hawkes process using Ogata's thinning algorithm.\n\n        Args:\n            T: End time\n            seed: Random seed\n\n        Returns:\n            Array of event times\n        \"\"\"\n        if seed is not None:\n            np.random.seed(seed)\n\n        events = []\n        t = 0\n\n        # Upper bound for intensity\n        lambda_bar = self.mu / (1 - self.alpha / self.beta)\n\n        while t < T:\n            # Generate next candidate time\n            u = np.random.random()\n            t = t - np.log(u) / lambda_bar\n\n            if t > T:\n                break\n\n            # Thinning\n            lambda_t = self.intensity(t, np.array(events))\n            if np.random.random() <= lambda_t / lambda_bar:\n                events.append(t)\n\n        return np.array(events)",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HawkesProcess"
  },
  {
    "name": "branching_ratio",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Calculate branching ratio (criticality measure).\n\nn* = /\n\n- n* < 1: Subcritical (stable)\n- n* = 1: Critical (marginally stable)\n- n* > 1: Supercritical (explosive)\n\nReturns:\n    Branching ratio",
    "python_code": "def branching_ratio(self) -> float:\n        \"\"\"\n        Calculate branching ratio (criticality measure).\n\n        n* = /\n\n        - n* < 1: Subcritical (stable)\n        - n* = 1: Critical (marginally stable)\n        - n* > 1: Supercritical (explosive)\n\n        Returns:\n            Branching ratio\n        \"\"\"\n        return self.alpha / self.beta",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HawkesProcess"
  },
  {
    "name": "estimate_from_trades",
    "category": "feature_engineering",
    "formula": "{",
    "explanation": "Estimate Hawkes parameters from trade data (MLE).\n\nSimplified estimation using method of moments.\n\nArgs:\n    trade_times: Array of trade arrival times\n    T: Total time period\n\nReturns:\n    Dict with estimated parameters",
    "python_code": "def estimate_from_trades(\n        self,\n        trade_times: np.ndarray,\n        T: float\n    ) -> Dict[str, float]:\n        \"\"\"\n        Estimate Hawkes parameters from trade data (MLE).\n\n        Simplified estimation using method of moments.\n\n        Args:\n            trade_times: Array of trade arrival times\n            T: Total time period\n\n        Returns:\n            Dict with estimated parameters\n        \"\"\"\n        n = len(trade_times)\n\n        # Average intensity\n        avg_intensity = n / T\n\n        # Inter-arrival times\n        inter_arrivals = np.diff(trade_times)\n        mean_ia = inter_arrivals.mean()\n        var_ia = inter_arrivals.var()\n\n        # Method of moments estimates\n        #  = (1 - n*) where  is average intensity\n        # Estimate branching ratio from variance\n        cv = np.sqrt(var_ia) / mean_ia  # Coefficient of variation\n\n        # For Hawkes: CV > 1 indicates clustering\n        if cv > 1:\n            n_star = min(0.9, (cv ** 2 - 1) / (cv ** 2))\n        else:\n            n_star = 0.1\n\n        mu_est = avg_intensity * (1 - n_star)\n        beta_est = 1 / mean_ia  # Approximate decay\n        alpha_est = n_star * beta_est\n\n        return {\n            'mu': mu_est,\n            'alpha': alpha_est,\n            'beta': beta_est,\n            'branching_ratio': n_star,\n            'avg_intensity': avg_intensity\n        }",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "HawkesProcess"
  },
  {
    "name": "generate_microstructure_features",
    "category": "feature_engineering",
    "formula": "features",
    "explanation": "Generate microstructure features.\n\nArgs:\n    df: DataFrame with OHLCV columns\n\nReturns:\n    DataFrame with microstructure features",
    "python_code": "def generate_microstructure_features(\n        self,\n        df: pd.DataFrame\n    ) -> pd.DataFrame:\n        \"\"\"\n        Generate microstructure features.\n\n        Args:\n            df: DataFrame with OHLCV columns\n\n        Returns:\n            DataFrame with microstructure features\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n\n        close = df['close']\n        returns = close.pct_change()\n\n        # OFI approximation (without L2 data)\n        features['OFI'] = self.ofi.calculate_from_ohlcv(df)\n\n        # VPIN approximation\n        features['VPIN'] = self.vpin.calculate_from_ohlcv(df)\n\n        # All volatility estimators\n        vol_features = self.vol.generate_all(df)\n        for col in vol_features.columns:\n            features[col] = vol_features[col]\n\n        # Rolling risk metrics\n        window = 60\n        features['SHARPE_60'] = returns.rolling(window).apply(\n            lambda x: self.risk.sharpe_ratio(x)\n        )\n        features['SORTINO_60'] = returns.rolling(window).apply(\n            lambda x: self.risk.sortino_ratio(x)\n        )\n        features['MAX_DD_60'] = returns.rolling(window).apply(\n            lambda x: self.risk.max_drawdown(x)\n        )\n\n        # Clean up\n        features = features.replace([np.inf, -np.inf], np.nan)\n        features = features.fillna(0)\n\n        return features",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "GoldStandardFeatures"
  },
  {
    "name": "get_feature_names",
    "category": "feature_engineering",
    "formula": "[",
    "explanation": "Get list of all feature names.",
    "python_code": "def get_feature_names(self) -> List[str]:\n        \"\"\"Get list of all feature names.\"\"\"\n        return [\n            # Microstructure\n            'OFI', 'VPIN',\n            # Volatility\n            'VOL_CLOSE', 'VOL_PARKINSON', 'VOL_GARMAN_KLASS',\n            'VOL_ROGERS_SATCHELL', 'VOL_YANG_ZHANG',\n            'VOL_RATIO_PK_CC', 'VOL_RATIO_GK_RS',\n            # Risk\n            'SHARPE_60', 'SORTINO_60', 'MAX_DD_60'\n        ]",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "GoldStandardFeatures"
  },
  {
    "name": "generate_gold_standard_features",
    "category": "feature_engineering",
    "formula": "generator.generate_microstructure_features(df)",
    "explanation": "Generate all gold standard features.\n\nArgs:\n    df: OHLCV DataFrame\n\nReturns:\n    DataFrame with ~12 gold standard features",
    "python_code": "def generate_gold_standard_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generate all gold standard features.\n\n    Args:\n        df: OHLCV DataFrame\n\n    Returns:\n        DataFrame with ~12 gold standard features\n    \"\"\"\n    generator = GoldStandardFeatures()\n    return generator.generate_microstructure_features(df)",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": null
  },
  {
    "name": "calculate_kelly_position",
    "category": "risk",
    "formula": "avg_loss: Average losing trade return (positive number) | KellyCriterion.position_size(",
    "explanation": "Calculate position size using fractional Kelly.\n\nArgs:\n    win_rate: Historical win rate (0 to 1)\n    avg_win: Average winning trade return\n    avg_loss: Average losing trade return (positive number)\n    account_value: Total account value\n    fraction: Kelly fraction (0.25 = quarter Kelly)\n\nReturns:\n    Recommended position size",
    "python_code": "def calculate_kelly_position(\n    win_rate: float,\n    avg_win: float,\n    avg_loss: float,\n    account_value: float,\n    fraction: float = 0.25\n) -> float:\n    \"\"\"\n    Calculate position size using fractional Kelly.\n\n    Args:\n        win_rate: Historical win rate (0 to 1)\n        avg_win: Average winning trade return\n        avg_loss: Average losing trade return (positive number)\n        account_value: Total account value\n        fraction: Kelly fraction (0.25 = quarter Kelly)\n\n    Returns:\n        Recommended position size\n    \"\"\"\n    win_loss_ratio = abs(avg_win / avg_loss) if avg_loss != 0 else 1\n    return KellyCriterion.position_size(\n        account_value=account_value,\n        win_rate=win_rate,\n        win_loss_ratio=win_loss_ratio,\n        fraction=fraction\n    )",
    "source_file": "core\\features\\github_gold_standard.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "deep_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, n_heads: int = 4):\n        self.n_heads = n_heads",
    "source_file": "core\\features\\gnn_temporal.py",
    "academic_reference": null,
    "class_name": "GraphAttentionLayer"
  },
  {
    "name": "compute_attention",
    "category": "technical",
    "formula": "np.dot(weights, values)",
    "explanation": "Compute scaled dot-product attention.\n\nArgs:\n    query: Query vector\n    keys: Key matrix\n    values: Value matrix\n    mask: Optional attention mask\n\nReturns:\n    Attention-weighted output",
    "python_code": "def compute_attention(\n        self,\n        query: np.ndarray,\n        keys: np.ndarray,\n        values: np.ndarray,\n        mask: Optional[np.ndarray] = None\n    ) -> np.ndarray:\n        \"\"\"\n        Compute scaled dot-product attention.\n\n        Args:\n            query: Query vector\n            keys: Key matrix\n            values: Value matrix\n            mask: Optional attention mask\n\n        Returns:\n            Attention-weighted output\n        \"\"\"\n        d_k = query.shape[-1] if query.ndim > 1 else 1\n\n        # Compute attention scores\n        scores = np.dot(keys, query) / np.sqrt(d_k + 1e-10)\n\n        if mask is not None:\n            scores = np.where(mask, scores, -1e9)\n\n        # Softmax\n        weights = softmax(scores)\n\n        # Weighted sum\n        return np.dot(weights, values)",
    "source_file": "core\\features\\gnn_temporal.py",
    "academic_reference": null,
    "class_name": "GraphAttentionLayer"
  },
  {
    "name": "_correlation_graph_features",
    "category": "statistical",
    "formula": "features",
    "explanation": "Correlation-based graph features.\n\nModels price series as nodes in a temporal graph where edges\nrepresent lagged correlations.\n\nReference: Wu et al. (2020) \"Connecting the Dots\" KDD",
    "python_code": "def _correlation_graph_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Correlation-based graph features.\n\n        Models price series as nodes in a temporal graph where edges\n        represent lagged correlations.\n\n        Reference: Wu et al. (2020) \"Connecting the Dots\" KDD\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Auto-correlation at different lags (self-edges in temporal graph)\n        for lag in [1, 5, 10]:\n            autocorr = returns.rolling(self.corr_window, min_periods=10).apply(\n                lambda x: np.corrcoef(x[:-lag], x[lag:])[0, 1] if len(x) > lag + 1 else 0,\n                raw=True\n            )\n            features[f'GNN_autocorr_{lag}'] = autocorr\n\n        # 2. Serial correlation decay (graph edge weight decay)\n        # Measures how quickly information propagates\n        ac1 = features.get('GNN_autocorr_1', returns.rolling(20).apply(\n            lambda x: np.corrcoef(x[:-1], x[1:])[0, 1] if len(x) > 2 else 0,\n            raw=True\n        ))\n        ac5 = features.get('GNN_autocorr_5', returns.rolling(20).apply(\n            lambda x: np.corrcoef(x[:-5], x[5:])[0, 1] if len(x) > 6 else 0,\n            raw=True\n        ))\n        features['GNN_corr_decay'] = ac1 - ac5\n\n        return features",
    "source_file": "core\\features\\gnn_temporal.py",
    "academic_reference": null,
    "class_name": "TemporalGNNFeatures"
  },
  {
    "name": "_temporal_attention_features",
    "category": "deep_learning",
    "formula": "0 | np.sum(weights * values) if len(values) == len(weights) else 0 | 0",
    "explanation": "Temporal attention-based features.\n\nComputes attention weights over historical observations.\n\nReference: Vaswani et al. (2017) \"Attention Is All You Need\"",
    "python_code": "def _temporal_attention_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Temporal attention-based features.\n\n        Computes attention weights over historical observations.\n\n        Reference: Vaswani et al. (2017) \"Attention Is All You Need\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n        volume = df.get('volume', pd.Series(1, index=df.index))\n\n        # 1. Price attention score\n        # Attention weight = importance of past prices for current prediction\n        def compute_price_attention(prices):\n            if len(prices) < 5:\n                return 0\n            # Use recent prices as query, past prices as keys\n            query = prices[-1]\n            keys = prices[:-1]\n            values = np.diff(prices)  # Price changes as values\n\n            # Compute attention\n            scores = (keys * query) / (np.std(keys) + 1e-10)\n            weights = softmax(scores)\n            return np.sum(weights * values) if len(values) == len(weights) else 0\n\n        features['GNN_price_attn'] = close.rolling(self.attn_window, min_periods=5).apply(\n            compute_price_attention, raw=True\n        )\n\n        # 2. Volume attention score\n        def compute_volume_attention(vol):\n            if len(vol) < 5:\n                return 0\n            weights = softmax(vol[:-1] / (np.sum(vol[:-1]) + 1e-10))\n            return np.sum(weights * np.diff(vol)) if len(weights) == len(vol) - 1 else 0\n\n        features['GNN_volume_attn'] = volume.rolling(self.attn_window, min_periods=5).apply(\n            compute_volume_attention, raw=True\n        )\n\n        # 3. Volatility attention (attention on volatility regimes)\n        vol = returns.abs().rolling(5, min_periods=2).mean()\n\n        def compute_vol_attention(v):\n            if len(v) < 5:\n                return 0\n            # High volatility periods get more attention\n            weights = softmax(v / (np.std(v) + 1e-10))\n            return np.sum(weights * v)\n\n        features['GNN_vol_attn'] = vol.rolling(self.attn_window, min_periods=5).apply(\n            compute_vol_attention, raw=True\n        )\n\n        # 4. Attention entropy (diversity of attention)\n        def attention_entropy(prices):\n            if len(prices) < 5:\n                return 0\n            scores = np.abs(prices - prices.mean()) / (np.std(prices) + 1e-10)\n            weights = softmax(scores)\n            # Entropy of attention distribution\n            return -np.sum(weights * np.log(weights + 1e-10))\n\n        features['GNN_attn_entropy'] = close.rolling(self.attn_window, min_periods=5).apply(\n            attention_entropy, raw=True\n        )\n\n        return features",
    "source_file": "core\\features\\gnn_temporal.py",
    "academic_reference": null,
    "class_name": "TemporalGNNFeatures"
  },
  {
    "name": "_message_passing_features",
    "category": "deep_learning",
    "formula": "prices[-1] if len(prices) > 0 else 0 | np.sum(weights * prices) | messages)",
    "explanation": "Message passing-based features.\n\nSimulates graph neural network message passing without full GNN.\nAggregates information from temporal neighbors.\n\nReference: Gilmer et al. (2017) \"Neural Message Passing for\nQuantum Chemistry\" ICML",
    "python_code": "def _message_passing_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Message passing-based features.\n\n        Simulates graph neural network message passing without full GNN.\n        Aggregates information from temporal neighbors.\n\n        Reference: Gilmer et al. (2017) \"Neural Message Passing for\n        Quantum Chemistry\" ICML\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n        high = df.get('high', close)\n        low = df.get('low', close)\n\n        # 1. Price propagation (message from past to present)\n        # Weighted sum of past prices where weights decay exponentially\n        def message_aggregate(prices, decay: float = 0.9):\n            if len(prices) < 2:\n                return prices[-1] if len(prices) > 0 else 0\n            weights = np.array([decay ** i for i in range(len(prices)-1, -1, -1)])\n            weights = weights / weights.sum()\n            return np.sum(weights * prices)\n\n        features['GNN_msg_price'] = close.rolling(self.attn_window, min_periods=2).apply(\n            message_aggregate, raw=True\n        )\n        features['GNN_msg_price'] = (close - features['GNN_msg_price']) / (close + 1e-10)\n\n        # 2. Volatility propagation\n        vol = (high - low) / (close + 1e-10)\n        features['GNN_msg_vol'] = vol.rolling(self.attn_window, min_periods=2).apply(\n            message_aggregate, raw=True\n        )\n\n        # 3. Trend propagation (cumulative return messages)\n        def trend_message(rets, decay: float = 0.95):\n            if len(rets) < 2:\n                return 0\n            weights = np.array([decay ** i for i in range(len(rets)-1, -1, -1)])\n            weights = weights / weights.sum()\n            return np.sum(weights * rets)\n\n        features['GNN_msg_trend'] = returns.rolling(self.attn_window, min_periods=2).apply(\n            trend_message, raw=True\n        )\n\n        # 4. Residual connection (skip connection in GNN)\n        # Combines raw signal with aggregated message\n        features['GNN_residual'] = returns + features['GNN_msg_trend'].fillna(0)\n\n        return features",
    "source_file": "core\\features\\gnn_temporal.py",
    "academic_reference": null,
    "class_name": "TemporalGNNFeatures"
  },
  {
    "name": "_graph_statistics_features",
    "category": "deep_learning",
    "formula": "magnitude relative to neighbors | 0 | 1 - np.mean(transitions) / 2  # 0 = chaotic, 1 = consistent",
    "explanation": "Graph topological statistics.\n\nComputes statistics that would be relevant in a full graph network.\n\nReference: Kipf & Welling (2017) \"Semi-Supervised Classification\nwith Graph Convolutional Networks\" ICLR",
    "python_code": "def _graph_statistics_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Graph topological statistics.\n\n        Computes statistics that would be relevant in a full graph network.\n\n        Reference: Kipf & Welling (2017) \"Semi-Supervised Classification\n        with Graph Convolutional Networks\" ICLR\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Node centrality proxy (how important is current time point)\n        # Based on return magnitude relative to neighbors\n        ret_mag = returns.abs()\n        neighborhood_mag = ret_mag.rolling(self.attn_window, min_periods=2).mean()\n        features['GNN_centrality'] = ret_mag / (neighborhood_mag + 1e-10)\n\n        # 2. Clustering coefficient proxy (local connectivity)\n        # Measures consistency of price movements in neighborhood\n        def local_consistency(rets):\n            if len(rets) < 5:\n                return 0\n            # Count consistent direction changes\n            signs = np.sign(rets)\n            transitions = np.abs(np.diff(signs))\n            return 1 - np.mean(transitions) / 2  # 0 = chaotic, 1 = consistent\n\n        features['GNN_clustering'] = returns.rolling(self.attn_window, min_periods=5).apply(\n            local_consistency, raw=True\n        )\n\n        # 3. Graph connectivity (information flow strength)\n        # Based on autocorrelation strength\n        def connectivity(rets):\n            if len(rets) < 5:\n                return 0\n            try:\n                ac = np.corrcoef(rets[:-1], rets[1:])[0, 1]\n                return np.abs(ac) if not np.isnan(ac) else 0\n            except:\n                return 0\n\n        features['GNN_connectivity'] = returns.rolling(self.attn_window, min_periods=5).apply(\n            connectivity, raw=True\n        )\n\n        return features",
    "source_file": "core\\features\\gnn_temporal.py",
    "academic_reference": null,
    "class_name": "TemporalGNNFeatures"
  },
  {
    "name": "generate_all",
    "category": "feature_engineering",
    "formula": "result",
    "explanation": "Generate all GNN-based features.\n\nArgs:\n    df: DataFrame with OHLCV data\n\nReturns:\n    DataFrame with 15 GNN features",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all GNN-based features.\n\n        Args:\n            df: DataFrame with OHLCV data\n\n        Returns:\n            DataFrame with 15 GNN features\n        \"\"\"\n        # Validate input\n        if 'close' not in df.columns:\n            raise ValueError(\"Missing required column: 'close'\")\n\n        df = df.copy()\n        if 'open' not in df.columns:\n            df['open'] = df['close'].shift(1).fillna(df['close'])\n        if 'high' not in df.columns:\n            df['high'] = df['close']\n        if 'low' not in df.columns:\n            df['low'] = df['close']\n        if 'volume' not in df.columns:\n            df['volume'] = 1\n\n        # Generate all feature groups\n        corr_features = self._correlation_graph_features(df)\n        attn_features = self._temporal_attention_features(df)\n        msg_features = self._message_passing_features(df)\n        graph_features = self._graph_statistics_features(df)\n\n        # Combine\n        result = pd.concat([\n            corr_features, attn_features, msg_features, graph_features\n        ], axis=1)\n\n        # Clean up\n        result = result.replace([np.inf, -np.inf], np.nan)\n        result = result.fillna(0)\n\n        return result",
    "source_file": "core\\features\\gnn_temporal.py",
    "academic_reference": null,
    "class_name": "TemporalGNNFeatures"
  },
  {
    "name": "get_feature_names",
    "category": "feature_engineering",
    "formula": "[",
    "explanation": "Get list of all feature names.",
    "python_code": "def get_feature_names(self) -> List[str]:\n        \"\"\"Get list of all feature names.\"\"\"\n        return [\n            # Correlation Graph (4)\n            'GNN_autocorr_1', 'GNN_autocorr_5', 'GNN_autocorr_10', 'GNN_corr_decay',\n            # Temporal Attention (4)\n            'GNN_price_attn', 'GNN_volume_attn', 'GNN_vol_attn', 'GNN_attn_entropy',\n            # Message Passing (4)\n            'GNN_msg_price', 'GNN_msg_vol', 'GNN_msg_trend', 'GNN_residual',\n            # Graph Statistics (3)\n            'GNN_centrality', 'GNN_clustering', 'GNN_connectivity'\n        ]",
    "source_file": "core\\features\\gnn_temporal.py",
    "academic_reference": null,
    "class_name": "TemporalGNNFeatures"
  },
  {
    "name": "get_citations",
    "category": "feature_engineering",
    "formula": "{",
    "explanation": "Get academic citations for GNN trading.",
    "python_code": "def get_citations() -> Dict[str, str]:\n        \"\"\"Get academic citations for GNN trading.\"\"\"\n        return {\n            'TGNS': \"\"\"Tsinghua THUNLP Lab (2023). \"Graph Attention Networks for\n                       Financial Time Series Prediction.\"\n                       Application of GAT to financial forecasting.\"\"\",\n            'GAT': \"\"\"Velikovi, P. et al. (2018). \"Graph Attention Networks\" ICLR.\n                      Foundation paper for graph attention mechanism.\"\"\",\n            'MTGNN': \"\"\"Wu, Z. et al. (2020). \"Connecting the Dots: Multivariate Time\n                        Series Forecasting with Graph Neural Networks\" KDD.\n                        GNN architecture for multivariate time series.\"\"\",\n            'GCN': \"\"\"Kipf, T.N. & Welling, M. (2017). \"Semi-Supervised Classification\n                      with Graph Convolutional Networks\" ICLR.\n                      Foundation for graph convolutional networks.\"\"\"\n        }",
    "source_file": "core\\features\\gnn_temporal.py",
    "academic_reference": null,
    "class_name": "TemporalGNNFeatures"
  },
  {
    "name": "generate_gnn_features",
    "category": "feature_engineering",
    "formula": "generator.generate_all(df)",
    "explanation": "Generate GNN temporal features.\n\nArgs:\n    df: OHLCV DataFrame\n\nReturns:\n    DataFrame with 15 GNN features",
    "python_code": "def generate_gnn_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generate GNN temporal features.\n\n    Args:\n        df: OHLCV DataFrame\n\n    Returns:\n        DataFrame with 15 GNN features\n    \"\"\"\n    generator = TemporalGNNFeatures()\n    return generator.generate_all(df)",
    "source_file": "core\\features\\gnn_temporal.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "compute_price_attention",
    "category": "deep_learning",
    "formula": "0 | np.sum(weights * values) if len(values) == len(weights) else 0",
    "explanation": "",
    "python_code": "def compute_price_attention(prices):\n            if len(prices) < 5:\n                return 0\n            # Use recent prices as query, past prices as keys\n            query = prices[-1]\n            keys = prices[:-1]\n            values = np.diff(prices)  # Price changes as values\n\n            # Compute attention\n            scores = (keys * query) / (np.std(keys) + 1e-10)\n            weights = softmax(scores)\n            return np.sum(weights * values) if len(values) == len(weights) else 0",
    "source_file": "core\\features\\gnn_temporal.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "compute_volume_attention",
    "category": "deep_learning",
    "formula": "0 | np.sum(weights * np.diff(vol)) if len(weights) == len(vol) - 1 else 0",
    "explanation": "",
    "python_code": "def compute_volume_attention(vol):\n            if len(vol) < 5:\n                return 0\n            weights = softmax(vol[:-1] / (np.sum(vol[:-1]) + 1e-10))\n            return np.sum(weights * np.diff(vol)) if len(weights) == len(vol) - 1 else 0",
    "source_file": "core\\features\\gnn_temporal.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "compute_vol_attention",
    "category": "volatility",
    "formula": "0 | np.sum(weights * v)",
    "explanation": "",
    "python_code": "def compute_vol_attention(v):\n            if len(v) < 5:\n                return 0\n            # High volatility periods get more attention\n            weights = softmax(v / (np.std(v) + 1e-10))\n            return np.sum(weights * v)",
    "source_file": "core\\features\\gnn_temporal.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "attention_entropy",
    "category": "deep_learning",
    "formula": "0 | -np.sum(weights * np.log(weights + 1e-10))",
    "explanation": "",
    "python_code": "def attention_entropy(prices):\n            if len(prices) < 5:\n                return 0\n            scores = np.abs(prices - prices.mean()) / (np.std(prices) + 1e-10)\n            weights = softmax(scores)\n            # Entropy of attention distribution\n            return -np.sum(weights * np.log(weights + 1e-10))",
    "source_file": "core\\features\\gnn_temporal.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "message_aggregate",
    "category": "feature_engineering",
    "formula": "prices[-1] if len(prices) > 0 else 0 | np.sum(weights * prices)",
    "explanation": "",
    "python_code": "def message_aggregate(prices, decay: float = 0.9):\n            if len(prices) < 2:\n                return prices[-1] if len(prices) > 0 else 0\n            weights = np.array([decay ** i for i in range(len(prices)-1, -1, -1)])\n            weights = weights / weights.sum()\n            return np.sum(weights * prices)",
    "source_file": "core\\features\\gnn_temporal.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "trend_message",
    "category": "feature_engineering",
    "formula": "0 | np.sum(weights * rets)",
    "explanation": "",
    "python_code": "def trend_message(rets, decay: float = 0.95):\n            if len(rets) < 2:\n                return 0\n            weights = np.array([decay ** i for i in range(len(rets)-1, -1, -1)])\n            weights = weights / weights.sum()\n            return np.sum(weights * rets)",
    "source_file": "core\\features\\gnn_temporal.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "local_consistency",
    "category": "feature_engineering",
    "formula": "0 | 1 - np.mean(transitions) / 2",
    "explanation": "",
    "python_code": "def local_consistency(rets):\n            if len(rets) < 5:\n                return 0\n            # Count consistent direction changes\n            signs = np.sign(rets)\n            transitions = np.abs(np.diff(signs))\n            return 1 - np.mean(transitions) / 2",
    "source_file": "core\\features\\gnn_temporal.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "connectivity",
    "category": "feature_engineering",
    "formula": "0 | np.abs(ac) if not np.isnan(ac) else 0 | 0",
    "explanation": "",
    "python_code": "def connectivity(rets):\n            if len(rets) < 5:\n                return 0\n            try:\n                ac = np.corrcoef(rets[:-1], rets[1:])[0, 1]\n                return np.abs(ac) if not np.isnan(ac) else 0\n            except:\n                return 0",
    "source_file": "core\\features\\gnn_temporal.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Initialize India Quant Features.\n\nArgs:\n    interest_rate_differential: Assumed INR-USD rate differential (default 5%)\n    monsoon_months: Monsoon season months (default: June-September)",
    "python_code": "def __init__(\n        self,\n        interest_rate_differential: float = 0.05,\n        monsoon_months: List[int] = None\n    ):\n        \"\"\"\n        Initialize India Quant Features.\n\n        Args:\n            interest_rate_differential: Assumed INR-USD rate differential (default 5%)\n            monsoon_months: Monsoon season months (default: June-September)\n        \"\"\"\n        self.ir_diff = interest_rate_differential\n        self.monsoon_months = monsoon_months or [6, 7, 8, 9]",
    "source_file": "core\\features\\india_quant.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "IndiaQuantFeatures"
  },
  {
    "name": "_carry_factors",
    "category": "microstructure",
    "formula": "adjusted for assumed IR differential | features",
    "explanation": "INR Carry Decomposition.\n\nIndia maintains relatively high interest rates compared to developed markets.\nThis creates carry trade opportunities where investors borrow in low-yield\ncurrencies and invest in INR-denominated assets.\n\nReferences:\n- IIQF carry trade curriculum\n- RBI interest rate policy impacts",
    "python_code": "def _carry_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        INR Carry Decomposition.\n\n        India maintains relatively high interest rates compared to developed markets.\n        This creates carry trade opportunities where investors borrow in low-yield\n        currencies and invest in INR-denominated assets.\n\n        References:\n        - IIQF carry trade curriculum\n        - RBI interest rate policy impacts\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Carry proxy: rolling return adjusted for assumed IR differential\n        # Higher values = stronger carry environment\n        for d in [5, 20, 60]:\n            features[f'IND_CARRY_{d}d'] = returns.rolling(d, min_periods=1).mean() + (self.ir_diff / 252)\n\n        # 2. Carry momentum: change in carry environment\n        carry_proxy = returns.rolling(20, min_periods=1).mean()\n        features['IND_CARRY_MOM'] = carry_proxy - carry_proxy.shift(10)\n\n        # 3. Carry risk-adjusted (Sharpe-like): reward per unit volatility\n        ret_mean = returns.rolling(60, min_periods=1).mean()\n        ret_std = returns.rolling(60, min_periods=2).std()\n        features['IND_CARRY_SR'] = (ret_mean + self.ir_diff / 252) / (ret_std + 1e-12)\n\n        return features",
    "source_file": "core\\features\\india_quant.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "IndiaQuantFeatures"
  },
  {
    "name": "_seasonality_factors",
    "category": "volatility",
    "formula": "deviation from seasonal average | seasonal_avg = returns.expanding(min_periods=20).mean() | features",
    "explanation": "Monsoon & Festival Seasonality Effects.\n\nThe Indian economy has strong agricultural ties, making INR sensitive to:\n- Monsoon season (June-September): Agricultural output expectations\n- Festival season (October-November): Consumer spending surge\n- Fiscal year end (March): Corporate forex hedging\n\nReferences:\n- IIM Sirmaur: \"Agricultural commodities and INR volatility\"\n- RBI: \"Seasonal patterns in forex market activity\"",
    "python_code": "def _seasonality_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Monsoon & Festival Seasonality Effects.\n\n        The Indian economy has strong agricultural ties, making INR sensitive to:\n        - Monsoon season (June-September): Agricultural output expectations\n        - Festival season (October-November): Consumer spending surge\n        - Fiscal year end (March): Corporate forex hedging\n\n        References:\n        - IIM Sirmaur: \"Agricultural commodities and INR volatility\"\n        - RBI: \"Seasonal patterns in forex market activity\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # Extract datetime components if available\n        if hasattr(df.index, 'month'):\n            month = df.index.month\n        else:\n            # Fallback: create cyclic approximation based on position\n            position = np.arange(len(df)) % 252  # Assume yearly cycle\n            month = ((position / 252) * 12).astype(int) + 1\n\n        # 1. Monsoon season indicator (binary)\n        is_monsoon = pd.Series(\n            [1 if m in self.monsoon_months else 0 for m in month],\n            index=df.index\n        )\n        features['IND_MONSOON_IND'] = is_monsoon\n\n        # 2. Monsoon season volatility adjustment\n        # Volatility typically increases during monsoon uncertainty\n        vol_20 = returns.rolling(20, min_periods=2).std()\n        features['IND_MONSOON_VOL'] = vol_20 * (1 + 0.3 * is_monsoon)  # 30% vol increase expected\n\n        # 3. Festival season indicator (Diwali: Oct-Nov)\n        is_festival = pd.Series(\n            [1 if m in [10, 11] else 0 for m in month],\n            index=df.index\n        )\n        features['IND_FESTIVAL_IND'] = is_festival\n\n        # 4. Fiscal year end effect (March)\n        is_fiscal_end = pd.Series(\n            [1 if m == 3 else 0 for m in month],\n            index=df.index\n        )\n        features['IND_FISCAL_END'] = is_fiscal_end\n\n        # 5. Seasonal momentum: return deviation from seasonal average\n        # Uses expanding mean to calculate expected seasonal return\n        seasonal_avg = returns.expanding(min_periods=20).mean()\n        features['IND_SEASON_ANOM'] = returns - seasonal_avg\n\n        return features",
    "source_file": "core\\features\\india_quant.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "IndiaQuantFeatures"
  },
  {
    "name": "_intervention_factors",
    "category": "volatility",
    "formula": "= returns.shift(-1).fillna(0) | features",
    "explanation": "RBI Intervention Detection Signals.\n\nThe Reserve Bank of India actively intervenes in forex markets to:\n- Smooth excessive volatility\n- Prevent disorderly market conditions\n- Manage INR appreciation/depreciation\n\nDetection methodology:\n- Unusual volume spikes\n- Price reversals after large moves\n- Volatility regime changes\n\nReferences:\n- RBI Working Papers on forex intervention\n- ScienceDirect: \"Central bank intervention detection in emerging markets\"",
    "python_code": "def _intervention_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        RBI Intervention Detection Signals.\n\n        The Reserve Bank of India actively intervenes in forex markets to:\n        - Smooth excessive volatility\n        - Prevent disorderly market conditions\n        - Manage INR appreciation/depreciation\n\n        Detection methodology:\n        - Unusual volume spikes\n        - Price reversals after large moves\n        - Volatility regime changes\n\n        References:\n        - RBI Working Papers on forex intervention\n        - ScienceDirect: \"Central bank intervention detection in emerging markets\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n        volume = df.get('volume', pd.Series(1, index=df.index))\n\n        # 1. Large move detection: z-score of returns\n        ret_mean = returns.rolling(20, min_periods=1).mean()\n        ret_std = returns.rolling(20, min_periods=2).std()\n        features['IND_LARGE_MOVE'] = (returns - ret_mean) / (ret_std + 1e-12)\n\n        # 2. Reversal after large move: potential intervention\n        large_move = np.abs(features['IND_LARGE_MOVE']) > 2\n        next_return = returns.shift(-1).fillna(0)\n        # Intervention reversal: large move followed by opposite direction\n        features['IND_INTERVENTION'] = np.where(\n            large_move & (np.sign(returns) != np.sign(next_return)),\n            1, 0\n        )\n\n        # 3. Volume anomaly: unusual volume might indicate intervention\n        vol_ma = volume.rolling(20, min_periods=1).mean()\n        vol_std = volume.rolling(20, min_periods=2).std()\n        features['IND_VOL_ANOM'] = (volume - vol_ma) / (vol_std + 1e-12)\n\n        # 4. Volatility regime break: sudden calming of volatility\n        vol_5 = returns.rolling(5, min_periods=2).std()\n        vol_20 = returns.rolling(20, min_periods=2).std()\n        features['IND_VOL_BREAK'] = vol_5 / (vol_20 + 1e-12)\n\n        # 5. Price support/resistance detection\n        # Potential intervention levels often create support/resistance\n        rolling_min = close.rolling(20, min_periods=1).min()\n        rolling_max = close.rolling(20, min_periods=1).max()\n        range_position = (close - rolling_min) / (rolling_max - rolling_min + 1e-12)\n        features['IND_RANGE_POS'] = range_position\n\n        return features",
    "source_file": "core\\features\\india_quant.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "IndiaQuantFeatures"
  },
  {
    "name": "_microstructure_factors",
    "category": "microstructure",
    "formula": "features",
    "explanation": "Indian Market Microstructure Features.\n\nPatterns specific to Indian markets:\n- Opening auction dynamics\n- Pre-close patterns\n- T+1 settlement effects\n- Retail vs institutional flow\n\nReferences:\n- NSE Academy: \"Market microstructure in emerging markets\"\n- ScienceDirect (2024): \"60.63% profitable trades methodology\"",
    "python_code": "def _microstructure_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Indian Market Microstructure Features.\n\n        Patterns specific to Indian markets:\n        - Opening auction dynamics\n        - Pre-close patterns\n        - T+1 settlement effects\n        - Retail vs institutional flow\n\n        References:\n        - NSE Academy: \"Market microstructure in emerging markets\"\n        - ScienceDirect (2024): \"60.63% profitable trades methodology\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        open_ = df.get('open', close.shift(1).fillna(close))\n        high = df.get('high', close)\n        low = df.get('low', close)\n        returns = close.pct_change()\n\n        # 1. Opening gap: overnight information arrival\n        features['IND_OPEN_GAP'] = (open_ - close.shift(1)) / (close.shift(1) + 1e-12)\n\n        # 2. Intraday momentum: open to close vs close to close\n        intraday_ret = (close - open_) / (open_ + 1e-12)\n        overnight_ret = features['IND_OPEN_GAP']\n        features['IND_INTRA_MOM'] = intraday_ret - overnight_ret\n\n        # 3. High-low range normalized: intraday volatility\n        hl_range = (high - low) / (close + 1e-12)\n        features['IND_HL_RANGE'] = hl_range\n\n        # 4. Close location value: where close is relative to day's range\n        features['IND_CLV'] = (close - low) / (high - low + 1e-12)\n\n        # 5. Accumulation/Distribution proxy\n        # Money flow multiplier\n        mf_multiplier = ((close - low) - (high - close)) / (high - low + 1e-12)\n        volume = df.get('volume', pd.Series(1, index=df.index))\n        mf_volume = mf_multiplier * volume\n        features['IND_AD_LINE'] = mf_volume.cumsum() / (volume.cumsum() + 1e-12)\n\n        return features",
    "source_file": "core\\features\\india_quant.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "IndiaQuantFeatures"
  },
  {
    "name": "_arbitrage_factors",
    "category": "volatility",
    "formula": "features",
    "explanation": "Triangular Arbitrage & Cross-Rate Signals.\n\nClassic forex arbitrage signals from IIQF curriculum:\n- Cross-rate deviation\n- Triangular arbitrage potential\n- Covered interest parity deviation\n\nFor single-pair analysis, we proxy using:\n- Price deviation from moving averages\n- Volatility-adjusted mean reversion\n- Statistical arbitrage signals\n\nReferences:\n- IIQF: \"Triangular arbitrage in forex markets\"\n- NSE Academy: \"Statistical arbitrage strategies\"",
    "python_code": "def _arbitrage_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Triangular Arbitrage & Cross-Rate Signals.\n\n        Classic forex arbitrage signals from IIQF curriculum:\n        - Cross-rate deviation\n        - Triangular arbitrage potential\n        - Covered interest parity deviation\n\n        For single-pair analysis, we proxy using:\n        - Price deviation from moving averages\n        - Volatility-adjusted mean reversion\n        - Statistical arbitrage signals\n\n        References:\n        - IIQF: \"Triangular arbitrage in forex markets\"\n        - NSE Academy: \"Statistical arbitrage strategies\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Cross-rate deviation proxy: deviation from MA\n        # In real triangular arb: USD/INR * INR/EUR should equal USD/EUR\n        # We proxy using deviation from equilibrium (moving average)\n        for d in [20, 60]:\n            ma = close.rolling(d, min_periods=1).mean()\n            std = close.rolling(d, min_periods=2).std()\n            features[f'IND_CROSS_DEV_{d}'] = (close - ma) / (std + 1e-12)\n\n        # 2. Covered Interest Parity (CIP) deviation proxy\n        # Real CIP: F/S = (1 + r_d)/(1 + r_f)\n        # Proxy: forward premium implied by momentum\n        fwd_premium = returns.rolling(20, min_periods=1).mean() * 252\n        cip_theoretical = self.ir_diff  # Assumed differential\n        features['IND_CIP_DEV'] = fwd_premium - cip_theoretical\n\n        # 3. Mean reversion z-score\n        # Statistical arbitrage: price will revert to mean\n        ma_120 = close.rolling(120, min_periods=1).mean()\n        std_120 = close.rolling(120, min_periods=2).std()\n        features['IND_MR_ZSCORE'] = (close - ma_120) / (std_120 + 1e-12)\n\n        # 4. Momentum-value combination\n        # Arbitrage between momentum and value signals\n        momentum = returns.rolling(20, min_periods=1).sum()\n        value = (ma_120 - close) / (close + 1e-12)\n        features['IND_MOM_VAL'] = momentum + value\n\n        return features",
    "source_file": "core\\features\\india_quant.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "IndiaQuantFeatures"
  },
  {
    "name": "generate_all",
    "category": "reinforcement_learning",
    "formula": "features",
    "explanation": "Generate all India Quant features.\n\nArgs:\n    df: DataFrame with columns: open, high, low, close, volume\n\nReturns:\n    DataFrame with 25 factor columns",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all India Quant features.\n\n        Args:\n            df: DataFrame with columns: open, high, low, close, volume\n\n        Returns:\n            DataFrame with 25 factor columns\n        \"\"\"\n        # Ensure required columns\n        if 'close' not in df.columns:\n            raise ValueError(\"Missing required column: 'close'\")\n\n        # Fill missing OHLC from close\n        if 'open' not in df.columns:\n            df = df.copy()\n            df['open'] = df['close'].shift(1).fillna(df['close'])\n        if 'high' not in df.columns:\n            df = df.copy()\n            df['high'] = df['close']\n        if 'low' not in df.columns:\n            df = df.copy()\n            df['low'] = df['close']\n\n        # Generate all factor groups\n        carry = self._carry_factors(df)\n        seasonality = self._seasonality_factors(df)\n        intervention = self._intervention_factors(df)\n        microstructure = self._microstructure_factors(df)\n        arbitrage = self._arbitrage_factors(df)\n\n        # Combine all features\n        features = pd.concat([\n            carry, seasonality, intervention, microstructure, arbitrage\n        ], axis=1)\n\n        # Clean up\n        features = features.replace([np.inf, -np.inf], np.nan)\n        features = features.fillna(0)\n\n        return features",
    "source_file": "core\\features\\india_quant.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "IndiaQuantFeatures"
  },
  {
    "name": "get_feature_names",
    "category": "feature_engineering",
    "formula": "names",
    "explanation": "Get list of all feature names.",
    "python_code": "def get_feature_names(self) -> List[str]:\n        \"\"\"Get list of all feature names.\"\"\"\n        names = []\n\n        # Carry (5)\n        names.extend(['IND_CARRY_5d', 'IND_CARRY_20d', 'IND_CARRY_60d',\n                      'IND_CARRY_MOM', 'IND_CARRY_SR'])\n\n        # Seasonality (5)\n        names.extend(['IND_MONSOON_IND', 'IND_MONSOON_VOL', 'IND_FESTIVAL_IND',\n                      'IND_FISCAL_END', 'IND_SEASON_ANOM'])\n\n        # Intervention (5)\n        names.extend(['IND_LARGE_MOVE', 'IND_INTERVENTION', 'IND_VOL_ANOM',\n                      'IND_VOL_BREAK', 'IND_RANGE_POS'])\n\n        # Microstructure (5)\n        names.extend(['IND_OPEN_GAP', 'IND_INTRA_MOM', 'IND_HL_RANGE',\n                      'IND_CLV', 'IND_AD_LINE'])\n\n        # Arbitrage (5)\n        names.extend(['IND_CROSS_DEV_20', 'IND_CROSS_DEV_60', 'IND_CIP_DEV',\n                      'IND_MR_ZSCORE', 'IND_MOM_VAL'])\n\n        return names",
    "source_file": "core\\features\\india_quant.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "IndiaQuantFeatures"
  },
  {
    "name": "get_factor_category",
    "category": "reinforcement_learning",
    "formula": "'Carry (Interest Rate)' | 'Seasonality' | 'Intervention Detection'",
    "explanation": "Get the category of a factor by name.",
    "python_code": "def get_factor_category(self, factor_name: str) -> str:\n        \"\"\"Get the category of a factor by name.\"\"\"\n        if 'CARRY' in factor_name:\n            return 'Carry (Interest Rate)'\n        elif any(x in factor_name for x in ['MONSOON', 'FESTIVAL', 'FISCAL', 'SEASON']):\n            return 'Seasonality'\n        elif any(x in factor_name for x in ['INTERVENTION', 'LARGE_MOVE', 'VOL_ANOM', 'VOL_BREAK', 'RANGE_POS']):\n            return 'Intervention Detection'\n        elif any(x in factor_name for x in ['OPEN_GAP', 'INTRA', 'HL_RANGE', 'CLV', 'AD_LINE']):\n            return 'Microstructure'\n        elif any(x in factor_name for x in ['CROSS', 'CIP', 'MR_', 'MOM_VAL']):\n            return 'Arbitrage'\n        return 'Unknown'",
    "source_file": "core\\features\\india_quant.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "IndiaQuantFeatures"
  },
  {
    "name": "generate_india_features",
    "category": "reinforcement_learning",
    "formula": "features.generate_all(df)",
    "explanation": "Generate India Quant features.\n\nArgs:\n    df: OHLCV DataFrame\n\nReturns:\n    DataFrame with 25 Indian market factors",
    "python_code": "def generate_india_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generate India Quant features.\n\n    Args:\n        df: OHLCV DataFrame\n\n    Returns:\n        DataFrame with 25 Indian market factors\n    \"\"\"\n    features = IndiaQuantFeatures()\n    return features.generate_all(df)",
    "source_file": "core\\features\\india_quant.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Initialize Japan Quant Features.\n\nArgs:\n    japan_session_start_utc: Japan session start hour in UTC\n    japan_session_end_utc: Japan session end hour in UTC",
    "python_code": "def __init__(\n        self,\n        japan_session_start_utc: int = 0,  # Midnight UTC = 9 AM Tokyo\n        japan_session_end_utc: int = 6,    # 6 AM UTC = 3 PM Tokyo\n    ):\n        \"\"\"\n        Initialize Japan Quant Features.\n\n        Args:\n            japan_session_start_utc: Japan session start hour in UTC\n            japan_session_end_utc: Japan session end hour in UTC\n        \"\"\"\n        self.session_start = japan_session_start_utc\n        self.session_end = japan_session_end_utc",
    "source_file": "core\\features\\japan_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JapanQuantFeatures"
  },
  {
    "name": "_carry_factors",
    "category": "reinforcement_learning",
    "formula": "rates = borrow in JPY, invest elsewhere | off = JPY appreciation | (negative = JPY appreciation)",
    "explanation": "Yen Carry Trade Institutional Models.\n\nJapan's decades-long low interest rate environment makes JPY\nthe primary funding currency for carry trades globally.\n\nInstitutional carry dynamics:\n- Low JPY rates = borrow in JPY, invest elsewhere\n- Carry unwind during risk-off = JPY appreciation\n- BOJ policy expectations\n\nReferences:\n- BOJ: \"The Yen Carry Trade and Financial Stability\"\n- BIS: \"Yen carry trade unwinding dynamics\"",
    "python_code": "def _carry_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Yen Carry Trade Institutional Models.\n\n        Japan's decades-long low interest rate environment makes JPY\n        the primary funding currency for carry trades globally.\n\n        Institutional carry dynamics:\n        - Low JPY rates = borrow in JPY, invest elsewhere\n        - Carry unwind during risk-off = JPY appreciation\n        - BOJ policy expectations\n\n        References:\n        - BOJ: \"The Yen Carry Trade and Financial Stability\"\n        - BIS: \"Yen carry trade unwinding dynamics\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Carry proxy: rolling return (negative = JPY appreciation)\n        # For USDJPY: positive return = JPY weakening (carry favorable)\n        for d in [10, 30]:\n            features[f'JPN_CARRY_{d}d'] = returns.rolling(d, min_periods=1).mean()\n\n        # 2. Carry unwind detector: sudden JPY strength\n        # Sharp negative returns suggest carry unwind\n        vol = returns.rolling(20, min_periods=2).std()\n        features['JPN_CARRY_UNWIND'] = np.where(\n            returns < -2 * vol,\n            1, 0\n        )\n\n        # 3. Carry risk-reward: return per unit vol (low for funding currency)\n        ret_60 = returns.rolling(60, min_periods=1).mean()\n        vol_60 = returns.rolling(60, min_periods=2).std()\n        features['JPN_CARRY_SR'] = ret_60 / (vol_60 + 1e-12)\n\n        return features",
    "source_file": "core\\features\\japan_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JapanQuantFeatures"
  },
  {
    "name": "_hft_factors",
    "category": "execution",
    "formula": "features",
    "explanation": "Japanese HFT/Arrowhead Microstructure.\n\nTSE Arrowhead system characteristics:\n- 2ms order execution\n- 25.9% of volume from HFT\n- Market-making focus (vs arbitrage in US)\n\nReferences:\n- JPX Working Paper No.4: \"Analysis of HFT at TSE\"\n- Springer: \"High-Frequency Trading in Japan\"",
    "python_code": "def _hft_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Japanese HFT/Arrowhead Microstructure.\n\n        TSE Arrowhead system characteristics:\n        - 2ms order execution\n        - 25.9% of volume from HFT\n        - Market-making focus (vs arbitrage in US)\n\n        References:\n        - JPX Working Paper No.4: \"Analysis of HFT at TSE\"\n        - Springer: \"High-Frequency Trading in Japan\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        high = df.get('high', close)\n        low = df.get('low', close)\n        volume = df.get('volume', pd.Series(1, index=df.index))\n        returns = close.pct_change()\n\n        # 1. Tick volatility: high-frequency price variability\n        # Arrowhead enables rapid price discovery\n        tick_vol = returns.abs().rolling(10, min_periods=1).mean()\n        features['JPN_TICK_VOL'] = tick_vol\n\n        # 2. Quote intensity proxy: range relative to close\n        # Tight ranges = high quote activity\n        range_ratio = (high - low) / (close + 1e-12)\n        features['JPN_QUOTE_INT'] = range_ratio\n\n        # 3. Volume acceleration: HFT creates volume bursts\n        vol_ma = volume.rolling(20, min_periods=1).mean()\n        vol_accel = volume / (vol_ma + 1e-12)\n        features['JPN_VOL_ACCEL'] = vol_accel\n\n        # 4. Mean reversion speed: HFT market-making creates faster reversion\n        # Japanese market-makers focus on inventory management\n        dev_from_ma = close - close.rolling(5, min_periods=1).mean()\n        reversion = -dev_from_ma / (close.rolling(5, min_periods=2).std() + 1e-12)\n        features['JPN_MR_SPEED'] = reversion\n\n        return features",
    "source_file": "core\\features\\japan_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JapanQuantFeatures"
  },
  {
    "name": "_session_factors",
    "category": "microstructure",
    "formula": "accumulator | features",
    "explanation": "Japanese Session-Specific Patterns.\n\nTokyo session (9 AM - 3 PM JST / 0:00 - 6:00 UTC):\n- First major session of the day\n- Sets tone for Asian currencies\n- BOJ announcements impact\n\nReferences:\n- J-Quants: \"Session-based trading strategies\"\n- University of Tokyo: \"Asian session price discovery\"",
    "python_code": "def _session_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Japanese Session-Specific Patterns.\n\n        Tokyo session (9 AM - 3 PM JST / 0:00 - 6:00 UTC):\n        - First major session of the day\n        - Sets tone for Asian currencies\n        - BOJ announcements impact\n\n        References:\n        - J-Quants: \"Session-based trading strategies\"\n        - University of Tokyo: \"Asian session price discovery\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        open_ = df.get('open', close.shift(1).fillna(close))\n        returns = close.pct_change()\n\n        # Session detection (approximate using position in data)\n        if hasattr(df.index, 'hour'):\n            hour = df.index.hour\n            is_japan_session = (hour >= self.session_start) & (hour < self.session_end)\n        else:\n            # Fallback: cyclical approximation\n            is_japan_session = pd.Series(False, index=df.index)\n\n        # 1. Japan session return accumulator\n        # Track returns during Japan session\n        features['JPN_SESSION_RET'] = returns.rolling(6, min_periods=1).sum()\n\n        # 2. Opening momentum: first-hour price action\n        # Japan session sets direction for Asian trading\n        features['JPN_OPEN_MOM'] = (close - open_) / (open_ + 1e-12)\n\n        # 3. Session volatility ratio: Japan vs other sessions\n        session_vol = returns.rolling(6, min_periods=2).std()\n        daily_vol = returns.rolling(24, min_periods=2).std()\n        features['JPN_VOL_RATIO'] = session_vol / (daily_vol + 1e-12)\n\n        # 4. Session trend strength\n        session_ret = returns.rolling(6, min_periods=1).sum()\n        session_abs_ret = returns.abs().rolling(6, min_periods=1).sum()\n        features['JPN_TREND_STR'] = session_ret / (session_abs_ret + 1e-12)\n\n        return features",
    "source_file": "core\\features\\japan_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JapanQuantFeatures"
  },
  {
    "name": "_market_making_factors",
    "category": "microstructure",
    "formula": "features",
    "explanation": "Japanese Market-Making Features.\n\nUnlike US markets where HFT focuses on arbitrage, Japanese markets\nshow more market-making activity:\n- Inventory management signals\n- Quote-driven price discovery\n- Spread dynamics\n\nReferences:\n- Springer: \"Market-making in Japanese equity markets\"\n- JPX: \"Evolution of trading strategies in Japan\"",
    "python_code": "def _market_making_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Japanese Market-Making Features.\n\n        Unlike US markets where HFT focuses on arbitrage, Japanese markets\n        show more market-making activity:\n        - Inventory management signals\n        - Quote-driven price discovery\n        - Spread dynamics\n\n        References:\n        - Springer: \"Market-making in Japanese equity markets\"\n        - JPX: \"Evolution of trading strategies in Japan\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        high = df.get('high', close)\n        low = df.get('low', close)\n        returns = close.pct_change()\n\n        # 1. Inventory imbalance proxy: cumulative order flow direction\n        signed_returns = np.sign(returns) * returns.abs()\n        features['JPN_INV_IMB'] = signed_returns.rolling(20, min_periods=1).sum()\n\n        # 2. Spread proxy: high-low range normalized\n        # Tighter spreads = competitive market-making\n        spread_proxy = (high - low) / (close + 1e-12)\n        features['JPN_SPREAD'] = spread_proxy.rolling(10, min_periods=1).mean()\n\n        # 3. Quote pressure: price position within range\n        # Market-makers adjust quotes based on inventory\n        quote_pos = (close - low) / (high - low + 1e-12)\n        features['JPN_QUOTE_PRES'] = 2 * quote_pos - 1  # -1 to 1\n\n        # 4. Mean-reversion intensity: MM activity creates reversion\n        dev = close - close.rolling(10, min_periods=1).mean()\n        features['JPN_MR_INT'] = -dev / (close.rolling(10, min_periods=2).std() + 1e-12)\n\n        return features",
    "source_file": "core\\features\\japan_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JapanQuantFeatures"
  },
  {
    "name": "_safe_haven_factors",
    "category": "volatility",
    "formula": "= JPY strength | features",
    "explanation": "JPY Safe Haven Dynamics.\n\nYen traditionally appreciates during:\n- Global risk-off events\n- Market stress/volatility spikes\n- Geopolitical uncertainty\n\nSafe haven characteristics:\n- Negative correlation with risk assets\n- Appreciation during VIX spikes (proxied)\n- Flight-to-quality flows\n\nReferences:\n- BOJ: \"JPY as a safe haven currency\"\n- IMF: \"Safe haven currencies and financial stability\"",
    "python_code": "def _safe_haven_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        JPY Safe Haven Dynamics.\n\n        Yen traditionally appreciates during:\n        - Global risk-off events\n        - Market stress/volatility spikes\n        - Geopolitical uncertainty\n\n        Safe haven characteristics:\n        - Negative correlation with risk assets\n        - Appreciation during VIX spikes (proxied)\n        - Flight-to-quality flows\n\n        References:\n        - BOJ: \"JPY as a safe haven currency\"\n        - IMF: \"Safe haven currencies and financial stability\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Risk-off proxy: JPY strength during high volatility\n        vol = returns.rolling(20, min_periods=2).std()\n        vol_zscore = (vol - vol.rolling(60, min_periods=10).mean()) / (vol.rolling(60, min_periods=10).std() + 1e-12)\n\n        # For USDJPY: negative return = JPY strength\n        # During high vol (risk-off), expect JPY strength (negative returns for USDJPY)\n        features['JPN_SAFE_HAVEN'] = -returns * vol_zscore  # Positive when JPY strengthens in high vol\n\n        # 2. Volatility regime indicator\n        features['JPN_VOL_REGIME'] = np.where(vol_zscore > 1, 1,\n                                              np.where(vol_zscore < -1, -1, 0))\n\n        # 3. Flight-to-quality momentum\n        # Sustained JPY strength during volatility\n        safe_haven_ret = returns.where(vol_zscore > 1, 0)\n        features['JPN_FTQ_MOM'] = safe_haven_ret.rolling(10, min_periods=1).sum()\n\n        # 4. Risk appetite indicator (inverse of safe haven flow)\n        # Weak JPY = risk-on environment\n        features['JPN_RISK_APP'] = returns.rolling(20, min_periods=1).mean()\n\n        return features",
    "source_file": "core\\features\\japan_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JapanQuantFeatures"
  },
  {
    "name": "_har_rv_factors",
    "category": "volatility",
    "formula": "features",
    "explanation": "HAR-RV (Heterogeneous Autoregressive Realized Volatility).\n\nThe HAR model decomposes volatility into:\n- Daily component: Short-term traders\n- Weekly component: Medium-term traders\n- Monthly component: Long-term investors\n\nReference:\nCorsi, F. (2009). \"A Simple Approximate Long-Memory Model of\nRealized Volatility\" Journal of Financial Econometrics, 7(2), 174-196.",
    "python_code": "def _har_rv_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        HAR-RV (Heterogeneous Autoregressive Realized Volatility).\n\n        The HAR model decomposes volatility into:\n        - Daily component: Short-term traders\n        - Weekly component: Medium-term traders\n        - Monthly component: Long-term investors\n\n        Reference:\n        Corsi, F. (2009). \"A Simple Approximate Long-Memory Model of\n        Realized Volatility\" Journal of Financial Econometrics, 7(2), 174-196.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        high = df.get('high', close)\n        low = df.get('low', close)\n        returns = close.pct_change()\n\n        # Realized volatility components\n        # Daily RV (1-day)\n        rv_daily = returns.abs().rolling(1, min_periods=1).mean() * np.sqrt(252)\n\n        # Weekly RV (5-day average)\n        rv_weekly = returns.abs().rolling(5, min_periods=1).mean() * np.sqrt(252)\n\n        # Monthly RV (22-day average)\n        rv_monthly = returns.abs().rolling(22, min_periods=1).mean() * np.sqrt(252)\n\n        # 1. HAR-RV daily component\n        features['JPN_HAR_daily'] = rv_daily\n\n        # 2. HAR-RV weekly component\n        features['JPN_HAR_weekly'] = rv_weekly\n\n        # 3. HAR-RV monthly component\n        features['JPN_HAR_monthly'] = rv_monthly\n\n        # 4. HAR-RV forecast (weighted combination as in original paper)\n        # RV(t+1)  c + _d * RV_d + _w * RV_w + _m * RV_m\n        # Using typical weights from empirical studies\n        features['JPN_HAR_forecast'] = (\n            0.4 * rv_daily +\n            0.3 * rv_weekly +\n            0.3 * rv_monthly\n        )\n\n        # 5. HAR-RV term structure (short vs long vol)\n        features['JPN_HAR_term'] = rv_daily / (rv_monthly + 1e-10)\n\n        return features",
    "source_file": "core\\features\\japan_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JapanQuantFeatures"
  },
  {
    "name": "_power_law_factors",
    "category": "reinforcement_learning",
    "formula": "distribution | 2.0  # Default | 2.0",
    "explanation": "Power-Law Order Flow Features.\n\nResearch shows order sizes follow power-law distribution:\nP(size > x) ~ x^(-)\n\nTypical   1.5-2.5 for major markets.\n\nReference:\nJPX Working Paper No.4 (2016). \"Analysis of HFT at TSE\"\nGabaix et al. (2003). \"A Theory of Power-Law Distributions\"",
    "python_code": "def _power_law_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Power-Law Order Flow Features.\n\n        Research shows order sizes follow power-law distribution:\n        P(size > x) ~ x^(-)\n\n        Typical   1.5-2.5 for major markets.\n\n        Reference:\n        JPX Working Paper No.4 (2016). \"Analysis of HFT at TSE\"\n        Gabaix et al. (2003). \"A Theory of Power-Law Distributions\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        volume = df.get('volume', pd.Series(1, index=df.index))\n        returns = close.pct_change()\n\n        # 1. Power-law tail exponent proxy\n        # Estimated from return distribution\n        def estimate_tail_exponent(x):\n            if len(x) < 20:\n                return 2.0  # Default\n            # Use Hill estimator for tail exponent\n            abs_x = np.abs(x[x != 0])\n            if len(abs_x) < 10:\n                return 2.0\n            sorted_x = np.sort(abs_x)[::-1]\n            k = max(5, len(sorted_x) // 10)  # Top 10%\n            if k < 2:\n                return 2.0\n            log_ratios = np.log(sorted_x[:k] / sorted_x[k])\n            alpha = k / np.sum(log_ratios) if np.sum(log_ratios) > 0 else 2.0\n            return np.clip(alpha, 0.5, 5.0)\n\n        features['JPN_POWER_alpha'] = returns.rolling(60, min_periods=20).apply(\n            estimate_tail_exponent, raw=True\n        )\n\n        # 2. Large order probability (tail weight)\n        # Frequency of returns beyond 2 std devs\n        vol = returns.rolling(20, min_periods=2).std()\n        large_moves = (np.abs(returns) > 2 * vol).rolling(60, min_periods=10).mean()\n        features['JPN_POWER_tail'] = large_moves\n\n        # 3. Volume clustering (power-law in volume)\n        # Large volume tends to cluster\n        vol_normalized = volume / (volume.rolling(20, min_periods=5).mean() + 1e-10)\n        vol_extremes = (vol_normalized > 2).rolling(20, min_periods=5).mean()\n        features['JPN_POWER_vol_cluster'] = vol_extremes\n\n        return features",
    "source_file": "core\\features\\japan_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JapanQuantFeatures"
  },
  {
    "name": "_boj_intervention_factors",
    "category": "reinforcement_learning",
    "formula": "features",
    "explanation": "BOJ Intervention Detection Features.\n\nBank of Japan occasionally intervenes in FX markets to:\n- Prevent excessive JPY strength (buy USD/sell JPY)\n- Prevent excessive JPY weakness (sell USD/buy JPY)\n\nIntervention signals:\n- Sudden large moves against prevailing trend\n- High volume with price stabilization\n- Statements from MOF/BOJ officials\n\nReference:\nFratzscher (2008). \"Oral Interventions Versus Actual Interventions\nin FX Markets\" Economic Journal.",
    "python_code": "def _boj_intervention_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        BOJ Intervention Detection Features.\n\n        Bank of Japan occasionally intervenes in FX markets to:\n        - Prevent excessive JPY strength (buy USD/sell JPY)\n        - Prevent excessive JPY weakness (sell USD/buy JPY)\n\n        Intervention signals:\n        - Sudden large moves against prevailing trend\n        - High volume with price stabilization\n        - Statements from MOF/BOJ officials\n\n        Reference:\n        Fratzscher (2008). \"Oral Interventions Versus Actual Interventions\n        in FX Markets\" Economic Journal.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n        volume = df.get('volume', pd.Series(1, index=df.index))\n\n        # 1. Intervention probability signal\n        # Large move with subsequent stabilization\n        vol = returns.rolling(20, min_periods=2).std()\n        standardized = returns / (vol + 1e-10)\n\n        # Intervention = large move followed by smaller moves\n        large_move = np.abs(standardized) > 2.5\n        subsequent_calm = (np.abs(standardized).shift(-1).rolling(3, min_periods=1).mean() < 1)\n\n        intervention_proxy = (large_move & subsequent_calm.shift(1)).astype(float)\n        features['JPN_BOJ_intv_prob'] = intervention_proxy.rolling(10, min_periods=1).sum() / 10\n\n        # 2. Intervention timing signal\n        # BOJ tends to intervene after sustained moves\n        cumulative_move = returns.rolling(10, min_periods=1).sum()\n        move_reversal = -np.sign(cumulative_move) * np.abs(cumulative_move)\n        features['JPN_BOJ_timing'] = move_reversal.clip(-0.05, 0.05) / 0.05\n\n        return features",
    "source_file": "core\\features\\japan_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JapanQuantFeatures"
  },
  {
    "name": "generate_all",
    "category": "reinforcement_learning",
    "formula": "features",
    "explanation": "Generate all Japan Quant features.\n\nArgs:\n    df: DataFrame with columns: open, high, low, close, volume\n\nReturns:\n    DataFrame with 20 factor columns",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all Japan Quant features.\n\n        Args:\n            df: DataFrame with columns: open, high, low, close, volume\n\n        Returns:\n            DataFrame with 20 factor columns\n        \"\"\"\n        # Ensure required columns\n        if 'close' not in df.columns:\n            raise ValueError(\"Missing required column: 'close'\")\n\n        # Fill missing OHLC from close\n        df = df.copy()\n        if 'open' not in df.columns:\n            df['open'] = df['close'].shift(1).fillna(df['close'])\n        if 'high' not in df.columns:\n            df['high'] = df['close']\n        if 'low' not in df.columns:\n            df['low'] = df['close']\n\n        # Generate all factor groups\n        carry = self._carry_factors(df)\n        hft = self._hft_factors(df)\n        session = self._session_factors(df)\n        mm = self._market_making_factors(df)\n        safe_haven = self._safe_haven_factors(df)\n        har_rv = self._har_rv_factors(df)\n        power_law = self._power_law_factors(df)\n        boj = self._boj_intervention_factors(df)\n\n        # Combine all features\n        features = pd.concat([\n            carry, hft, session, mm, safe_haven, har_rv, power_law, boj\n        ], axis=1)\n\n        # Clean up\n        features = features.replace([np.inf, -np.inf], np.nan)\n        features = features.fillna(0)\n\n        return features",
    "source_file": "core\\features\\japan_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JapanQuantFeatures"
  },
  {
    "name": "get_feature_names",
    "category": "feature_engineering",
    "formula": "names",
    "explanation": "Get list of all feature names.",
    "python_code": "def get_feature_names(self) -> List[str]:\n        \"\"\"Get list of all feature names.\"\"\"\n        names = []\n\n        # Carry (4)\n        names.extend(['JPN_CARRY_10d', 'JPN_CARRY_30d', 'JPN_CARRY_UNWIND',\n                      'JPN_CARRY_SR'])\n\n        # HFT (4)\n        names.extend(['JPN_TICK_VOL', 'JPN_QUOTE_INT', 'JPN_VOL_ACCEL',\n                      'JPN_MR_SPEED'])\n\n        # Session (4)\n        names.extend(['JPN_SESSION_RET', 'JPN_OPEN_MOM', 'JPN_VOL_RATIO',\n                      'JPN_TREND_STR'])\n\n        # Market Making (4)\n        names.extend(['JPN_INV_IMB', 'JPN_SPREAD', 'JPN_QUOTE_PRES',\n                      'JPN_MR_INT'])\n\n        # Safe Haven (4)\n        names.extend(['JPN_SAFE_HAVEN', 'JPN_VOL_REGIME', 'JPN_FTQ_MOM',\n                      'JPN_RISK_APP'])\n\n        # HAR-RV Volatility (5) - NEW\n        names.extend(['JPN_HAR_daily', 'JPN_HAR_weekly', 'JPN_HAR_monthly',\n                      'JPN_HAR_forecast', 'JPN_HAR_term'])\n\n        # Power-Law Microstructure (3) - NEW\n        names.extend(['JPN_POWER_alpha', 'JPN_POWER_tail', 'JPN_POWER_vol_cluster'])\n\n        # BOJ Intervention (2) - NEW\n        names.extend(['JPN_BOJ_intv_prob', 'JPN_BOJ_timing'])\n\n        return names",
    "source_file": "core\\features\\japan_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JapanQuantFeatures"
  },
  {
    "name": "get_factor_category",
    "category": "reinforcement_learning",
    "formula": "'Carry (Yen Funding)' | 'HFT Microstructure' | 'Session Patterns'",
    "explanation": "Get the category of a factor by name.",
    "python_code": "def get_factor_category(self, factor_name: str) -> str:\n        \"\"\"Get the category of a factor by name.\"\"\"\n        if 'CARRY' in factor_name:\n            return 'Carry (Yen Funding)'\n        elif any(x in factor_name for x in ['TICK', 'QUOTE_INT', 'VOL_ACCEL', 'MR_SPEED']):\n            return 'HFT Microstructure'\n        elif any(x in factor_name for x in ['SESSION', 'OPEN_MOM', 'VOL_RATIO', 'TREND_STR']):\n            return 'Session Patterns'\n        elif any(x in factor_name for x in ['INV_IMB', 'SPREAD', 'QUOTE_PRES', 'MR_INT']):\n            return 'Market Making'\n        elif any(x in factor_name for x in ['SAFE_HAVEN', 'VOL_REGIME', 'FTQ', 'RISK_APP']):\n            return 'Safe Haven'\n        elif 'HAR' in factor_name:\n            return 'HAR-RV Volatility'\n        elif 'POWER' in factor_name:\n            return 'Power-Law Microstructure'\n        elif 'BOJ' in factor_name:\n            return 'BOJ Intervention'\n        return 'Unknown'",
    "source_file": "core\\features\\japan_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JapanQuantFeatures"
  },
  {
    "name": "get_citations",
    "category": "feature_engineering",
    "formula": "{",
    "explanation": "Get academic citations for Japan Quant methods.",
    "python_code": "def get_citations() -> Dict[str, str]:\n        \"\"\"Get academic citations for Japan Quant methods.\"\"\"\n        return {\n            'HAR_RV': \"\"\"Corsi, F. (2009). \"A Simple Approximate Long-Memory Model\n                         of Realized Volatility\" Journal of Financial Econometrics,\n                         7(2), 174-196.\n                         Foundation for HAR-RV volatility forecasting.\"\"\",\n            'Power_Law': \"\"\"Gabaix, X. et al. (2003). \"A Theory of Power-Law\n                           Distributions in Financial Market Fluctuations\"\n                           Nature, 423, 267-270.\n                           Power-law distribution in financial markets.\"\"\",\n            'JPX_HFT': \"\"\"Japan Exchange Group (2016). \"Analysis of High-Frequency\n                         Trading at Tokyo Stock Exchange\" JPX Working Paper No.4.\n                         Japanese HFT microstructure analysis.\"\"\",\n            'BOJ_Intervention': \"\"\"Fratzscher, M. (2008). \"Oral Interventions Versus\n                                  Actual Interventions in FX Markets\" Economic Journal.\n                                  Central bank intervention effectiveness.\"\"\"\n        }",
    "source_file": "core\\features\\japan_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JapanQuantFeatures"
  },
  {
    "name": "generate_japan_features",
    "category": "reinforcement_learning",
    "formula": "features.generate_all(df)",
    "explanation": "Generate Japan Quant features.\n\nArgs:\n    df: OHLCV DataFrame\n\nReturns:\n    DataFrame with 30 Japanese market factors (including HAR-RV, Power-Law, BOJ)",
    "python_code": "def generate_japan_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generate Japan Quant features.\n\n    Args:\n        df: OHLCV DataFrame\n\n    Returns:\n        DataFrame with 30 Japanese market factors (including HAR-RV, Power-Law, BOJ)\n    \"\"\"\n    features = JapanQuantFeatures()\n    return features.generate_all(df)",
    "source_file": "core\\features\\japan_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "estimate_tail_exponent",
    "category": "feature_engineering",
    "formula": "2.0  # Default | 2.0 | 2.0",
    "explanation": "",
    "python_code": "def estimate_tail_exponent(x):\n            if len(x) < 20:\n                return 2.0  # Default\n            # Use Hill estimator for tail exponent\n            abs_x = np.abs(x[x != 0])\n            if len(abs_x) < 10:\n                return 2.0\n            sorted_x = np.sort(abs_x)[::-1]\n            k = max(5, len(sorted_x) // 10)  # Top 10%\n            if k < 2:\n                return 2.0\n            log_ratios = np.log(sorted_x[:k] / sorted_x[k])\n            alpha = k / np.sum(log_ratios) if np.sum(log_ratios) > 0 else 2.0\n            return np.clip(alpha, 0.5, 5.0)",
    "source_file": "core\\features\\japan_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "volatility",
    "formula": "",
    "explanation": "Initialize Korea Quant Features.\n\nArgs:\n    regime_lookback: Lookback for regime detection\n    vol_window: Window for volatility estimation\n    jump_threshold: Threshold for jump detection (in std devs)",
    "python_code": "def __init__(\n        self,\n        regime_lookback: int = 60,\n        vol_window: int = 20,\n        jump_threshold: float = 3.0\n    ):\n        \"\"\"\n        Initialize Korea Quant Features.\n\n        Args:\n            regime_lookback: Lookback for regime detection\n            vol_window: Window for volatility estimation\n            jump_threshold: Threshold for jump detection (in std devs)\n        \"\"\"\n        self.regime_lookback = regime_lookback\n        self.vol_window = vol_window\n        self.jump_threshold = jump_threshold",
    "source_file": "core\\features\\korea_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "KoreaQuantFeatures"
  },
  {
    "name": "_ms_garch_features",
    "category": "volatility",
    "formula": "features",
    "explanation": "Markov-Switching GARCH Regime Features.\n\nImplements simplified MS-GARCH for regime detection without full\nestimation. Uses proxy methods for regime identification.\n\nReferences:\n[1] Hamilton (1989) - Regime switching models\n[2] Gray (1996) - MS-GARCH specification\n[3] Haas, Mittnik, Paolella (2004) - MS-GARCH for financial data",
    "python_code": "def _ms_garch_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Markov-Switching GARCH Regime Features.\n\n        Implements simplified MS-GARCH for regime detection without full\n        estimation. Uses proxy methods for regime identification.\n\n        References:\n        [1] Hamilton (1989) - Regime switching models\n        [2] Gray (1996) - MS-GARCH specification\n        [3] Haas, Mittnik, Paolella (2004) - MS-GARCH for financial data\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Volatility regime proxy (based on rolling volatility percentile)\n        vol = returns.rolling(self.vol_window, min_periods=2).std()\n        vol_pctl = vol.rolling(self.regime_lookback, min_periods=10).apply(\n            lambda x: stats.percentileofscore(x, x.iloc[-1]) / 100 if len(x) > 1 else 0.5,\n            raw=False\n        )\n        features['KR_MSGARCH_vol_regime'] = np.where(\n            vol_pctl > 0.8, 2,  # High vol regime\n            np.where(vol_pctl < 0.2, 0, 1)  # Low vol regime / Normal\n        )\n\n        # 2. Regime transition probability proxy\n        # Based on regime persistence\n        regime = features['KR_MSGARCH_vol_regime']\n        regime_change = (regime != regime.shift(1)).astype(int)\n        features['KR_MSGARCH_trans_prob'] = regime_change.rolling(\n            self.regime_lookback, min_periods=5\n        ).mean()\n\n        # 3. Regime duration\n        regime_groups = (regime != regime.shift(1)).cumsum()\n        features['KR_MSGARCH_duration'] = regime_groups.groupby(regime_groups).cumcount() + 1\n\n        # 4. Conditional volatility (regime-dependent)\n        # Different vol estimation based on regime\n        high_vol_mask = features['KR_MSGARCH_vol_regime'] == 2\n        low_vol_mask = features['KR_MSGARCH_vol_regime'] == 0\n\n        # Exponential weighting for recent observations\n        vol_ema = returns.abs().ewm(span=self.vol_window, min_periods=2).mean()\n        features['KR_MSGARCH_cond_vol'] = vol_ema\n\n        # 5. Regime certainty (distance from regime boundaries)\n        features['KR_MSGARCH_certainty'] = np.abs(vol_pctl - 0.5) * 2\n\n        return features",
    "source_file": "core\\features\\korea_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "KoreaQuantFeatures"
  },
  {
    "name": "_cgmy_features",
    "category": "technical",
    "formula": "features",
    "explanation": "CGMY Lvy Process Features.\n\nThe CGMY model captures infinite activity jump processes in returns.\nParameters: C (measure of activity), G (rate of exponential decay\nfor negative jumps), M (rate for positive jumps), Y (fine structure).\n\nReference:\nCarr, Geman, Madan, Yor (2002). \"The Fine Structure of Asset Returns:\nAn Empirical Investigation\" Journal of Business.",
    "python_code": "def _cgmy_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        CGMY Lvy Process Features.\n\n        The CGMY model captures infinite activity jump processes in returns.\n        Parameters: C (measure of activity), G (rate of exponential decay\n        for negative jumps), M (rate for positive jumps), Y (fine structure).\n\n        Reference:\n        Carr, Geman, Madan, Yor (2002). \"The Fine Structure of Asset Returns:\n        An Empirical Investigation\" Journal of Business.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Jump intensity proxy (C parameter)\n        # Based on frequency of large moves\n        vol = returns.rolling(self.vol_window, min_periods=2).std()\n        standardized = returns / (vol + 1e-10)\n        jump_count = (np.abs(standardized) > self.jump_threshold).rolling(\n            self.regime_lookback, min_periods=5\n        ).sum()\n        features['KR_CGMY_intensity'] = jump_count / self.regime_lookback\n\n        # 2. Negative jump rate (G parameter proxy)\n        neg_jumps = ((standardized < -self.jump_threshold) &\n                     (returns < 0)).rolling(self.regime_lookback, min_periods=5).sum()\n        features['KR_CGMY_neg_rate'] = neg_jumps / (jump_count + 1)\n\n        # 3. Positive jump rate (M parameter proxy)\n        pos_jumps = ((standardized > self.jump_threshold) &\n                     (returns > 0)).rolling(self.regime_lookback, min_periods=5).sum()\n        features['KR_CGMY_pos_rate'] = pos_jumps / (jump_count + 1)\n\n        # 4. Fine structure (Y parameter proxy)\n        # Based on kurtosis - higher kurtosis = more jump activity\n        kurtosis = returns.rolling(self.regime_lookback, min_periods=10).apply(\n            lambda x: stats.kurtosis(x) if len(x) > 3 else 0,\n            raw=True\n        )\n        # Y is typically between 0 and 2, normalized\n        features['KR_CGMY_fine_struct'] = (kurtosis / 10).clip(-1, 1)\n\n        return features",
    "source_file": "core\\features\\korea_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "KoreaQuantFeatures"
  },
  {
    "name": "_vkospi_features",
    "category": "volatility",
    "formula": "features",
    "explanation": "VKOSPI-Inspired Features.\n\nVKOSPI is the Korean volatility index, analogous to VIX.\nWe create proxy features based on implied volatility concepts.\n\nReference:\nKRX Research Papers on VKOSPI methodology.",
    "python_code": "def _vkospi_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        VKOSPI-Inspired Features.\n\n        VKOSPI is the Korean volatility index, analogous to VIX.\n        We create proxy features based on implied volatility concepts.\n\n        Reference:\n        KRX Research Papers on VKOSPI methodology.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        high = df.get('high', close)\n        low = df.get('low', close)\n        returns = close.pct_change()\n\n        # 1. Implied volatility proxy (using range-based estimator)\n        # Parkinson volatility as IV proxy\n        log_hl = np.log(high / (low + 1e-10) + 1e-10)\n        parkinson = np.sqrt(log_hl ** 2 / (4 * np.log(2)))\n        features['KR_VKOSPI_iv_proxy'] = parkinson.rolling(\n            self.vol_window, min_periods=2\n        ).mean() * np.sqrt(252)\n\n        # 2. Realized-Implied spread proxy\n        realized_vol = returns.rolling(self.vol_window, min_periods=2).std() * np.sqrt(252)\n        features['KR_VKOSPI_rv_spread'] = features['KR_VKOSPI_iv_proxy'] - realized_vol\n\n        # 3. Term structure proxy (short vs long vol)\n        vol_short = returns.rolling(5, min_periods=2).std() * np.sqrt(252)\n        vol_long = returns.rolling(60, min_periods=5).std() * np.sqrt(252)\n        features['KR_VKOSPI_term'] = vol_short / (vol_long + 1e-10)\n\n        # 4. Volatility skew proxy\n        # Difference between downside and upside volatility\n        neg_returns = returns.where(returns < 0, 0)\n        pos_returns = returns.where(returns > 0, 0)\n        neg_vol = neg_returns.rolling(self.vol_window, min_periods=2).std()\n        pos_vol = pos_returns.rolling(self.vol_window, min_periods=2).std()\n        features['KR_VKOSPI_skew'] = (neg_vol - pos_vol) / (pos_vol + 1e-10)\n\n        return features",
    "source_file": "core\\features\\korea_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "KoreaQuantFeatures"
  },
  {
    "name": "_gew_lstm_features",
    "category": "technical",
    "formula": "0 | np.max(np.abs(eigvals)) / total | 0",
    "explanation": "GEW-LSTM Hybrid Features.\n\nBased on Korean research combining Gram Eigenvalue Windowing with LSTM.\nGram matrices capture pairwise relationships in time series.\n\nReference:\nKim et al. (2019). \"GEW-LSTM: A Hybrid Deep Learning Model for\nFinancial Time Series Prediction\" Korean Journal of Financial Studies.",
    "python_code": "def _gew_lstm_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        GEW-LSTM Hybrid Features.\n\n        Based on Korean research combining Gram Eigenvalue Windowing with LSTM.\n        Gram matrices capture pairwise relationships in time series.\n\n        Reference:\n        Kim et al. (2019). \"GEW-LSTM: A Hybrid Deep Learning Model for\n        Financial Time Series Prediction\" Korean Journal of Financial Studies.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n        volume = df.get('volume', pd.Series(1, index=df.index))\n\n        # 1. Gram eigenvalue ratio\n        # Largest eigenvalue indicates dominant pattern strength\n        def gram_eigenvalue_ratio(x):\n            if len(x) < 5:\n                return 0\n            # Simple Gram matrix: X @ X.T\n            x_centered = x - np.mean(x)\n            gram = np.outer(x_centered, x_centered)\n            try:\n                eigvals = np.linalg.eigvalsh(gram)\n                total = np.sum(np.abs(eigvals))\n                if total > 0:\n                    return np.max(np.abs(eigvals)) / total\n                return 0\n            except:\n                return 0\n\n        features['KR_GEW_eig_ratio'] = returns.rolling(\n            self.vol_window, min_periods=5\n        ).apply(gram_eigenvalue_ratio, raw=True)\n\n        # 2. Gram matrix trace (total variance proxy)\n        def gram_trace(x):\n            if len(x) < 3:\n                return 0\n            return np.sum(x ** 2)\n\n        features['KR_GEW_trace'] = returns.rolling(\n            self.vol_window, min_periods=3\n        ).apply(gram_trace, raw=True)\n\n        # 3. Cross-gram feature (price-volume relationship)\n        # Rolling correlation between returns and volume changes\n        vol_change = volume.pct_change().fillna(0)\n        features['KR_GEW_cross'] = returns.rolling(\n            self.vol_window, min_periods=3\n        ).corr(vol_change).fillna(0)\n\n        # 4. Memory cell proxy (LSTM gate analog)\n        # Exponential smoothing as memory mechanism\n        alpha = 0.1\n        features['KR_GEW_memory'] = returns.ewm(alpha=alpha, min_periods=2).mean()\n\n        return features",
    "source_file": "core\\features\\korea_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "KoreaQuantFeatures"
  },
  {
    "name": "_regime_aware_features",
    "category": "technical",
    "formula": "features",
    "explanation": "Regime-Aware Prediction Features.\n\nAdapts predictions based on detected market regime.\n\nReference:\nSeoul National University research on regime-dependent trading.",
    "python_code": "def _regime_aware_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Regime-Aware Prediction Features.\n\n        Adapts predictions based on detected market regime.\n\n        Reference:\n        Seoul National University research on regime-dependent trading.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # Get regime from MS-GARCH features\n        vol = returns.rolling(self.vol_window, min_periods=2).std()\n        vol_pctl = vol.rolling(self.regime_lookback, min_periods=10).apply(\n            lambda x: stats.percentileofscore(x, x.iloc[-1]) / 100 if len(x) > 1 else 0.5,\n            raw=False\n        )\n\n        # 1. Regime-adjusted momentum\n        # Momentum works better in low vol, mean-rev in high vol\n        mom_20 = returns.rolling(20, min_periods=1).sum()\n        mr_signal = -returns.rolling(5, min_periods=1).sum()\n\n        # Blend based on regime\n        regime_weight = vol_pctl.fillna(0.5)\n        features['KR_REGIME_signal'] = (1 - regime_weight) * mom_20 + regime_weight * mr_signal\n\n        # 2. Regime-specific volatility forecast\n        # High vol regime: use recent vol\n        # Low vol regime: use longer average\n        vol_short = vol\n        vol_long = returns.rolling(60, min_periods=5).std()\n        features['KR_REGIME_vol_forecast'] = (\n            regime_weight * vol_short + (1 - regime_weight) * vol_long\n        )\n\n        # 3. Regime transition signal\n        # Predicts regime changes\n        vol_diff = vol.diff(5)\n        vol_accel = vol_diff.diff(5)\n        features['KR_REGIME_transition'] = np.sign(vol_accel)\n\n        return features",
    "source_file": "core\\features\\korea_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "KoreaQuantFeatures"
  },
  {
    "name": "generate_all",
    "category": "reinforcement_learning",
    "formula": "features",
    "explanation": "Generate all Korea Quant features.\n\nArgs:\n    df: DataFrame with columns: open, high, low, close, volume\n\nReturns:\n    DataFrame with 20 factor columns",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all Korea Quant features.\n\n        Args:\n            df: DataFrame with columns: open, high, low, close, volume\n\n        Returns:\n            DataFrame with 20 factor columns\n        \"\"\"\n        # Ensure required columns\n        if 'close' not in df.columns:\n            raise ValueError(\"Missing required column: 'close'\")\n\n        # Fill missing OHLC from close\n        df = df.copy()\n        if 'open' not in df.columns:\n            df['open'] = df['close'].shift(1).fillna(df['close'])\n        if 'high' not in df.columns:\n            df['high'] = df['close']\n        if 'low' not in df.columns:\n            df['low'] = df['close']\n        if 'volume' not in df.columns:\n            df['volume'] = 1\n\n        # Generate all factor groups\n        msgarch = self._ms_garch_features(df)\n        cgmy = self._cgmy_features(df)\n        vkospi = self._vkospi_features(df)\n        gew = self._gew_lstm_features(df)\n        regime = self._regime_aware_features(df)\n\n        # Combine all features\n        features = pd.concat([\n            msgarch, cgmy, vkospi, gew, regime\n        ], axis=1)\n\n        # Clean up\n        features = features.replace([np.inf, -np.inf], np.nan)\n        features = features.fillna(0)\n\n        return features",
    "source_file": "core\\features\\korea_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "KoreaQuantFeatures"
  },
  {
    "name": "get_feature_names",
    "category": "feature_engineering",
    "formula": "names",
    "explanation": "Get list of all feature names.",
    "python_code": "def get_feature_names(self) -> List[str]:\n        \"\"\"Get list of all feature names.\"\"\"\n        names = []\n\n        # MS-GARCH (5)\n        names.extend(['KR_MSGARCH_vol_regime', 'KR_MSGARCH_trans_prob',\n                      'KR_MSGARCH_duration', 'KR_MSGARCH_cond_vol',\n                      'KR_MSGARCH_certainty'])\n\n        # CGMY Lvy (4)\n        names.extend(['KR_CGMY_intensity', 'KR_CGMY_neg_rate',\n                      'KR_CGMY_pos_rate', 'KR_CGMY_fine_struct'])\n\n        # VKOSPI (4)\n        names.extend(['KR_VKOSPI_iv_proxy', 'KR_VKOSPI_rv_spread',\n                      'KR_VKOSPI_term', 'KR_VKOSPI_skew'])\n\n        # GEW-LSTM (4)\n        names.extend(['KR_GEW_eig_ratio', 'KR_GEW_trace',\n                      'KR_GEW_cross', 'KR_GEW_memory'])\n\n        # Regime-Aware (3)\n        names.extend(['KR_REGIME_signal', 'KR_REGIME_vol_forecast',\n                      'KR_REGIME_transition'])\n\n        return names",
    "source_file": "core\\features\\korea_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "KoreaQuantFeatures"
  },
  {
    "name": "get_citations",
    "category": "feature_engineering",
    "formula": "{",
    "explanation": "Get academic citations for Korean quant methods.",
    "python_code": "def get_citations() -> Dict[str, str]:\n        \"\"\"Get academic citations for Korean quant methods.\"\"\"\n        return {\n            'MS_GARCH': \"\"\"Hamilton, J.D. (1989). \"A New Approach to the Economic\n                           Analysis of Nonstationary Time Series and the Business\n                           Cycle\" Econometrica, 57(2), 357-384.\n                           Foundation for Markov-Switching models.\"\"\",\n            'CGMY': \"\"\"Carr, P., Geman, H., Madan, D.B., Yor, M. (2002). \"The Fine\n                       Structure of Asset Returns: An Empirical Investigation\"\n                       Journal of Business, 75(2), 305-332.\n                       CGMY Lvy process for jump modeling.\"\"\",\n            'GEW_LSTM': \"\"\"Kim, S.H. et al. (2019). \"GEW-LSTM: A Hybrid Deep Learning\n                          Model for Financial Time Series Prediction\"\n                          Korean Journal of Financial Studies.\n                          Gram matrix + LSTM hybrid architecture.\"\"\",\n            'VKOSPI': \"\"\"Korea Exchange (KRX). \"VKOSPI Index Methodology\"\n                        KRX Research Papers.\n                        Korean volatility index methodology.\"\"\"\n        }",
    "source_file": "core\\features\\korea_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "KoreaQuantFeatures"
  },
  {
    "name": "generate_korea_features",
    "category": "reinforcement_learning",
    "formula": "features.generate_all(df)",
    "explanation": "Generate Korea Quant features.\n\nArgs:\n    df: OHLCV DataFrame\n\nReturns:\n    DataFrame with 20 Korean market factors",
    "python_code": "def generate_korea_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generate Korea Quant features.\n\n    Args:\n        df: OHLCV DataFrame\n\n    Returns:\n        DataFrame with 20 Korean market factors\n    \"\"\"\n    features = KoreaQuantFeatures()\n    return features.generate_all(df)",
    "source_file": "core\\features\\korea_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "gram_eigenvalue_ratio",
    "category": "feature_engineering",
    "formula": "0 | np.max(np.abs(eigvals)) / total | 0",
    "explanation": "",
    "python_code": "def gram_eigenvalue_ratio(x):\n            if len(x) < 5:\n                return 0\n            # Simple Gram matrix: X @ X.T\n            x_centered = x - np.mean(x)\n            gram = np.outer(x_centered, x_centered)\n            try:\n                eigvals = np.linalg.eigvalsh(gram)\n                total = np.sum(np.abs(eigvals))\n                if total > 0:\n                    return np.max(np.abs(eigvals)) / total\n                return 0\n            except:\n                return 0",
    "source_file": "core\\features\\korea_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "gram_trace",
    "category": "feature_engineering",
    "formula": "0 | np.sum(x ** 2)",
    "explanation": "",
    "python_code": "def gram_trace(x):\n            if len(x) < 3:\n                return 0\n            return np.sum(x ** 2)",
    "source_file": "core\\features\\korea_quant.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "detect_momentum_signature",
    "category": "technical",
    "formula": "autocorr.clip(-1, 1)",
    "explanation": "Detect momentum agent activity (positive autocorrelation).",
    "python_code": "def detect_momentum_signature(self, returns: pd.Series, window: int = 20) -> pd.Series:\n        \"\"\"Detect momentum agent activity (positive autocorrelation).\"\"\"\n        autocorr = returns.rolling(window, min_periods=5).apply(\n            lambda x: np.corrcoef(x[:-1], x[1:])[0, 1] if len(x) > 2 else 0,\n            raw=True\n        )\n        return autocorr.clip(-1, 1)",
    "source_file": "core\\features\\marl_trading.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "AgentTypeDetector"
  },
  {
    "name": "detect_value_signature",
    "category": "statistical",
    "formula": "-autocorr",
    "explanation": "Detect value agent activity (negative autocorrelation).",
    "python_code": "def detect_value_signature(self, returns: pd.Series, window: int = 20) -> pd.Series:\n        \"\"\"Detect value agent activity (negative autocorrelation).\"\"\"\n        autocorr = self.detect_momentum_signature(returns, window)\n        return -autocorr",
    "source_file": "core\\features\\marl_trading.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "AgentTypeDetector"
  },
  {
    "name": "detect_noise_signature",
    "category": "statistical",
    "formula": "noise_sig",
    "explanation": "Detect noise trader activity (high variance, low autocorrelation).",
    "python_code": "def detect_noise_signature(self, returns: pd.Series, window: int = 20) -> pd.Series:\n        \"\"\"Detect noise trader activity (high variance, low autocorrelation).\"\"\"\n        vol = returns.rolling(window, min_periods=2).std()\n        autocorr = self.detect_momentum_signature(returns, window)\n        # High vol + low autocorr = noise\n        noise_sig = vol / (vol.rolling(60, min_periods=10).mean() + 1e-10) * (1 - np.abs(autocorr))\n        return noise_sig",
    "source_file": "core\\features\\marl_trading.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "AgentTypeDetector"
  },
  {
    "name": "detect_informed_signature",
    "category": "feature_engineering",
    "formula": "informed_sig.fillna(0)",
    "explanation": "Detect informed trader activity.\n\nInformed traders: large volume precedes price moves",
    "python_code": "def detect_informed_signature(\n        self,\n        returns: pd.Series,\n        volume: pd.Series,\n        window: int = 20\n    ) -> pd.Series:\n        \"\"\"\n        Detect informed trader activity.\n\n        Informed traders: large volume precedes price moves\n        \"\"\"\n        # Volume leads price (informed trading signature)\n        vol_lead = volume.shift(1).rolling(5, min_periods=1).mean()\n        ret_follow = returns.abs().rolling(5, min_periods=1).mean()\n\n        # Correlation between lagged volume and current returns\n        informed_sig = vol_lead.rolling(window, min_periods=5).corr(ret_follow)\n        return informed_sig.fillna(0)",
    "source_file": "core\\features\\marl_trading.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "AgentTypeDetector"
  },
  {
    "name": "__init__",
    "category": "microstructure",
    "formula": "",
    "explanation": "Initialize MARL feature generator.\n\nArgs:\n    detection_window: Window for agent type detection\n    consensus_window: Window for consensus signals\n    impact_window: Window for market impact estimation",
    "python_code": "def __init__(\n        self,\n        detection_window: int = 20,\n        consensus_window: int = 10,\n        impact_window: int = 30\n    ):\n        \"\"\"\n        Initialize MARL feature generator.\n\n        Args:\n            detection_window: Window for agent type detection\n            consensus_window: Window for consensus signals\n            impact_window: Window for market impact estimation\n        \"\"\"\n        self.detection_window = detection_window\n        self.consensus_window = consensus_window\n        self.impact_window = impact_window\n        self.agent_detector = AgentTypeDetector()",
    "source_file": "core\\features\\marl_trading.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "MultiAgentRLFeatures"
  },
  {
    "name": "_agent_detection_features",
    "category": "feature_engineering",
    "formula": "features",
    "explanation": "Detect signatures of different agent types.\n\nReference: Sun et al. (2023) MARL Survey",
    "python_code": "def _agent_detection_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Detect signatures of different agent types.\n\n        Reference: Sun et al. (2023) MARL Survey\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n        volume = df.get('volume', pd.Series(1, index=df.index))\n\n        # 1. Momentum agent signature\n        features['MARL_agent_momentum'] = self.agent_detector.detect_momentum_signature(\n            returns, self.detection_window\n        )\n\n        # 2. Value agent signature\n        features['MARL_agent_value'] = self.agent_detector.detect_value_signature(\n            returns, self.detection_window\n        )\n\n        # 3. Noise trader signature\n        features['MARL_agent_noise'] = self.agent_detector.detect_noise_signature(\n            returns, self.detection_window\n        )\n\n        # 4. Informed trader signature\n        features['MARL_agent_informed'] = self.agent_detector.detect_informed_signature(\n            returns, volume, self.detection_window\n        )\n\n        return features",
    "source_file": "core\\features\\marl_trading.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "MultiAgentRLFeatures"
  },
  {
    "name": "_agent_consensus_features",
    "category": "volatility",
    "formula": "features",
    "explanation": "Agent consensus and disagreement features.\n\nWhen agents agree, trends are stronger.\nWhen agents disagree, volatility increases.\n\nReference: Littman (1994) Markov Games",
    "python_code": "def _agent_consensus_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Agent consensus and disagreement features.\n\n        When agents agree, trends are stronger.\n        When agents disagree, volatility increases.\n\n        Reference: Littman (1994) Markov Games\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n        volume = df.get('volume', pd.Series(1, index=df.index))\n\n        # Get agent signatures\n        mom_sig = self.agent_detector.detect_momentum_signature(returns, self.detection_window)\n        val_sig = self.agent_detector.detect_value_signature(returns, self.detection_window)\n\n        # 1. Agent agreement (both momentum and value aligned)\n        # When both positive or both negative = agreement\n        agreement = mom_sig * val_sig\n        features['MARL_consensus'] = agreement.rolling(self.consensus_window, min_periods=2).mean()\n\n        # 2. Agent divergence (momentum vs value disagreement)\n        divergence = np.abs(mom_sig - val_sig)\n        features['MARL_divergence'] = divergence.rolling(self.consensus_window, min_periods=2).mean()\n\n        # 3. Dominant agent type\n        # Which agent type is winning\n        mom_strength = np.abs(mom_sig)\n        val_strength = np.abs(val_sig)\n        features['MARL_dominant'] = np.where(\n            mom_strength > val_strength, 1,  # Momentum dominant\n            np.where(val_strength > mom_strength, -1, 0)  # Value dominant / balanced\n        )\n\n        # 4. Market balance (how evenly matched agents are)\n        total_strength = mom_strength + val_strength + 1e-10\n        features['MARL_balance'] = 1 - np.abs(mom_strength - val_strength) / total_strength\n\n        return features",
    "source_file": "core\\features\\marl_trading.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "MultiAgentRLFeatures"
  },
  {
    "name": "_market_impact_features",
    "category": "microstructure",
    "formula": "per unit volume) | features",
    "explanation": "Market impact from agent actions.\n\nModels how agent trading affects prices.\n\nReference: Almgren & Chriss (2001) Optimal Execution",
    "python_code": "def _market_impact_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Market impact from agent actions.\n\n        Models how agent trading affects prices.\n\n        Reference: Almgren & Chriss (2001) Optimal Execution\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n        volume = df.get('volume', pd.Series(1, index=df.index))\n\n        # 1. Price impact (return per unit volume)\n        # Kyle's lambda proxy\n        vol_normalized = volume / (volume.rolling(self.impact_window, min_periods=5).mean() + 1e-10)\n        impact = returns.abs() / (vol_normalized + 0.1)\n        features['MARL_price_impact'] = impact.rolling(self.impact_window, min_periods=2).mean()\n\n        # 2. Volume impact (volume effect on future returns)\n        vol_signal = (volume - volume.rolling(self.impact_window, min_periods=5).mean()) / \\\n                     (volume.rolling(self.impact_window, min_periods=5).std() + 1e-10)\n        future_ret = returns.shift(-1).fillna(0)\n        vol_impact = vol_signal.rolling(self.impact_window, min_periods=5).corr(future_ret)\n        features['MARL_vol_impact'] = vol_impact.fillna(0)\n\n        # 3. Impact persistence (how long price impact lasts)\n        ret_autocorr_1 = returns.rolling(self.impact_window, min_periods=5).apply(\n            lambda x: np.corrcoef(x[:-1], x[1:])[0, 1] if len(x) > 2 else 0,\n            raw=True\n        )\n        ret_autocorr_5 = returns.rolling(self.impact_window, min_periods=10).apply(\n            lambda x: np.corrcoef(x[:-5], x[5:])[0, 1] if len(x) > 6 else 0,\n            raw=True\n        )\n        features['MARL_impact_persist'] = ret_autocorr_1 - ret_autocorr_5\n\n        # 4. Impact decay rate\n        # How quickly impact diminishes\n        decay = -np.log(np.abs(ret_autocorr_5) + 0.01) / 5\n        features['MARL_impact_decay'] = decay.clip(-1, 1)\n\n        return features",
    "source_file": "core\\features\\marl_trading.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "MultiAgentRLFeatures"
  },
  {
    "name": "_equilibrium_features",
    "category": "deep_learning",
    "formula": "features",
    "explanation": "Nash equilibrium estimation features.\n\nIn multi-agent setting, prices should converge to Nash equilibrium.\n\nReference: Hu & Wellman (2003) Nash Q-Learning",
    "python_code": "def _equilibrium_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Nash equilibrium estimation features.\n\n        In multi-agent setting, prices should converge to Nash equilibrium.\n\n        Reference: Hu & Wellman (2003) Nash Q-Learning\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Nash equilibrium price estimate\n        # Rolling fair value based on multiple signals\n        ma_short = close.rolling(10, min_periods=1).mean()\n        ma_long = close.rolling(50, min_periods=1).mean()\n\n        # Equilibrium is weighted average of different timeframe views\n        mom_fair = close + (close - ma_short)  # Momentum: extrapolate trend\n        val_fair = ma_long  # Value: revert to mean\n\n        # Get agent strengths\n        mom_sig = np.abs(self.agent_detector.detect_momentum_signature(\n            returns, self.detection_window))\n        val_sig = np.abs(self.agent_detector.detect_value_signature(\n            returns, self.detection_window))\n\n        total = mom_sig + val_sig + 1e-10\n        nash_price = (mom_sig * mom_fair + val_sig * val_fair) / total\n        features['MARL_nash_price'] = (close - nash_price) / (close + 1e-10)\n\n        # 2. Deviation from equilibrium\n        deviation = np.abs(features['MARL_nash_price'])\n        features['MARL_nash_deviation'] = deviation\n\n        # 3. Convergence indicator\n        # Price approaching equilibrium = low and decreasing deviation\n        dev_change = deviation.diff(5)\n        features['MARL_convergence'] = -dev_change.clip(-0.1, 0.1) / 0.1\n\n        return features",
    "source_file": "core\\features\\marl_trading.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "MultiAgentRLFeatures"
  },
  {
    "name": "generate_all",
    "category": "feature_engineering",
    "formula": "result",
    "explanation": "Generate all MARL features.\n\nArgs:\n    df: DataFrame with OHLCV data\n\nReturns:\n    DataFrame with 15 MARL features",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all MARL features.\n\n        Args:\n            df: DataFrame with OHLCV data\n\n        Returns:\n            DataFrame with 15 MARL features\n        \"\"\"\n        # Validate input\n        if 'close' not in df.columns:\n            raise ValueError(\"Missing required column: 'close'\")\n\n        df = df.copy()\n        if 'open' not in df.columns:\n            df['open'] = df['close'].shift(1).fillna(df['close'])\n        if 'high' not in df.columns:\n            df['high'] = df['close']\n        if 'low' not in df.columns:\n            df['low'] = df['close']\n        if 'volume' not in df.columns:\n            df['volume'] = 1\n\n        # Generate all feature groups\n        agent_features = self._agent_detection_features(df)\n        consensus_features = self._agent_consensus_features(df)\n        impact_features = self._market_impact_features(df)\n        equilibrium_features = self._equilibrium_features(df)\n\n        # Combine\n        result = pd.concat([\n            agent_features, consensus_features, impact_features, equilibrium_features\n        ], axis=1)\n\n        # Clean up\n        result = result.replace([np.inf, -np.inf], np.nan)\n        result = result.fillna(0)\n\n        return result",
    "source_file": "core\\features\\marl_trading.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "MultiAgentRLFeatures"
  },
  {
    "name": "get_feature_names",
    "category": "feature_engineering",
    "formula": "[",
    "explanation": "Get list of all feature names.",
    "python_code": "def get_feature_names(self) -> List[str]:\n        \"\"\"Get list of all feature names.\"\"\"\n        return [\n            # Agent Detection (4)\n            'MARL_agent_momentum', 'MARL_agent_value',\n            'MARL_agent_noise', 'MARL_agent_informed',\n            # Agent Consensus (4)\n            'MARL_consensus', 'MARL_divergence',\n            'MARL_dominant', 'MARL_balance',\n            # Market Impact (4)\n            'MARL_price_impact', 'MARL_vol_impact',\n            'MARL_impact_persist', 'MARL_impact_decay',\n            # Equilibrium (3)\n            'MARL_nash_price', 'MARL_nash_deviation', 'MARL_convergence'\n        ]",
    "source_file": "core\\features\\marl_trading.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "MultiAgentRLFeatures"
  },
  {
    "name": "get_citations",
    "category": "feature_engineering",
    "formula": "{",
    "explanation": "Get academic citations for MARL trading.",
    "python_code": "def get_citations() -> Dict[str, str]:\n        \"\"\"Get academic citations for MARL trading.\"\"\"\n        return {\n            'MARL_Survey': \"\"\"Sun, S. et al. (2023). \"Multi-Agent Reinforcement Learning\n                              for Quantitative Trading: A Survey\" arXiv:2302.13753.\n                              Comprehensive survey of MARL approaches in trading.\"\"\",\n            'Markov_Games': \"\"\"Littman, M.L. (1994). \"Markov Games as a Framework for\n                               Multi-Agent Reinforcement Learning\" ICML.\n                               Foundation for multi-agent RL in games.\"\"\",\n            'Nash_Q': \"\"\"Hu, J. & Wellman, M.P. (2003). \"Nash Q-Learning for General-Sum\n                         Stochastic Games\" JMLR, 4, 1039-1069.\n                         Nash equilibrium learning in multi-agent settings.\"\"\",\n            'Kyle': \"\"\"Kyle, A.S. (1985). \"Continuous Auctions and Insider Trading\"\n                       Econometrica, 53(6), 1315-1335.\n                       Foundation for informed vs noise trader distinction.\"\"\"\n        }",
    "source_file": "core\\features\\marl_trading.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "MultiAgentRLFeatures"
  },
  {
    "name": "generate_marl_features",
    "category": "feature_engineering",
    "formula": "generator.generate_all(df)",
    "explanation": "Generate MARL trading features.\n\nArgs:\n    df: OHLCV DataFrame\n\nReturns:\n    DataFrame with 15 MARL features",
    "python_code": "def generate_marl_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generate MARL trading features.\n\n    Args:\n        df: OHLCV DataFrame\n\n    Returns:\n        DataFrame with 15 MARL features\n    \"\"\"\n    generator = MultiAgentRLFeatures()\n    return generator.generate_all(df)",
    "source_file": "core\\features\\marl_trading.py",
    "academic_reference": "arXiv:2302.13753",
    "class_name": null
  },
  {
    "name": "rsrs",
    "category": "regime",
    "formula": "RSRS = bullish regime, Low RSRS = bearish regime. | zscore",
    "explanation": "RSRS () - Relative Strength Rating System.\nChinese quant standard for regime detection.\n\nCalculates slope of high vs low regression, then z-scores it.\nHigh RSRS = bullish regime, Low RSRS = bearish regime.",
    "python_code": "def rsrs(high: pd.Series, low: pd.Series, window: int = 18) -> pd.Series:\n        \"\"\"\n        RSRS () - Relative Strength Rating System.\n        Chinese quant standard for regime detection.\n\n        Calculates slope of high vs low regression, then z-scores it.\n        High RSRS = bullish regime, Low RSRS = bearish regime.\n        \"\"\"\n        slopes = []\n        for i in range(len(high)):\n            if i < window:\n                slopes.append(np.nan)\n            else:\n                y = high.iloc[i-window:i].values\n                x = low.iloc[i-window:i].values\n                try:\n                    slope, _, _, _, _ = stats.linregress(x, y)\n                    slopes.append(slope)\n                except:\n                    slopes.append(np.nan)\n\n        slope_series = pd.Series(slopes, index=high.index)\n        # Standardize over 250 periods (or available data)\n        lookback = min(250, len(slope_series) - 1)\n        zscore = (slope_series - slope_series.rolling(lookback, min_periods=20).mean()) / \\\n                 (slope_series.rolling(lookback, min_periods=20).std() + 1e-10)\n        return zscore",
    "source_file": "core\\features\\mega_generator.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "ChineseHFTAdditions"
  },
  {
    "name": "yang_zhang_volatility",
    "category": "volatility",
    "formula": "np.sqrt(yz_var.clip(lower=0) * 252)",
    "explanation": "Yang-Zhang volatility estimator - more efficient than close-to-close.\nCombines overnight, open-close, and Rogers-Satchell components.",
    "python_code": "def yang_zhang_volatility(open_: pd.Series, high: pd.Series,\n                               low: pd.Series, close: pd.Series,\n                               window: int = 20) -> pd.Series:\n        \"\"\"\n        Yang-Zhang volatility estimator - more efficient than close-to-close.\n        Combines overnight, open-close, and Rogers-Satchell components.\n        \"\"\"\n        # Overnight volatility\n        overnight = np.log(open_ / close.shift(1))\n        overnight_var = overnight.rolling(window, min_periods=5).var()\n\n        # Open-to-close volatility\n        open_close = np.log(close / open_)\n        oc_var = open_close.rolling(window, min_periods=5).var()\n\n        # Rogers-Satchell volatility\n        rs = np.log(high / close) * np.log(high / open_) + \\\n             np.log(low / close) * np.log(low / open_)\n        rs_var = rs.rolling(window, min_periods=5).mean()\n\n        # Combine (Yang-Zhang weights)\n        k = 0.34 / (1.34 + (window + 1) / (window - 1))\n        yz_var = overnight_var + k * oc_var + (1 - k) * rs_var.abs()\n\n        return np.sqrt(yz_var.clip(lower=0) * 252)",
    "source_file": "core\\features\\mega_generator.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "ChineseHFTAdditions"
  },
  {
    "name": "volume_clock_acceleration",
    "category": "regime",
    "formula": "vol_acceleration / (vol_ma + 1e-10)",
    "explanation": "Volume clock acceleration ().\nSecond derivative of volume - detects volume regime changes.",
    "python_code": "def volume_clock_acceleration(volume: pd.Series, window: int = 20) -> pd.Series:\n        \"\"\"\n        Volume clock acceleration ().\n        Second derivative of volume - detects volume regime changes.\n        \"\"\"\n        vol_ma = volume.rolling(window, min_periods=5).mean()\n        vol_velocity = vol_ma.diff()\n        vol_acceleration = vol_velocity.diff()\n        return vol_acceleration / (vol_ma + 1e-10)",
    "source_file": "core\\features\\mega_generator.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "ChineseHFTAdditions"
  },
  {
    "name": "hawkes_ofi",
    "category": "microstructure",
    "formula": "np.nan | (buy_weighted - sell_weighted) / total | ofi",
    "explanation": "Hawkes-inspired Order Flow Imbalance.\nDecaying impact of past order flow on current intensity.",
    "python_code": "def hawkes_ofi(returns: pd.Series, volume: pd.Series,\n                   alpha: float = 0.5, beta: float = 1.0,\n                   window: int = 50) -> pd.Series:\n        \"\"\"\n        Hawkes-inspired Order Flow Imbalance.\n        Decaying impact of past order flow on current intensity.\n        \"\"\"\n        # Classify trades as buy/sell based on returns\n        buy_volume = volume.where(returns > 0, 0)\n        sell_volume = volume.where(returns < 0, 0)\n\n        # Exponential decay weights\n        weights = np.array([alpha * np.exp(-beta * i) for i in range(window)])\n        weights = weights / weights.sum()\n\n        # Weighted imbalance\n        def weighted_imbalance(buy, sell):\n            if len(buy) < window:\n                return np.nan\n            buy_arr = buy[-window:].values\n            sell_arr = sell[-window:].values\n            buy_weighted = np.sum(buy_arr * weights[::-1])\n            sell_weighted = np.sum(sell_arr * weights[::-1])\n            total = buy_weighted + sell_weighted + 1e-10\n            return (buy_weighted - sell_weighted) / total\n\n        ofi = pd.Series(index=returns.index, dtype=float)\n        for i in range(window, len(returns)):\n            ofi.iloc[i] = weighted_imbalance(\n                buy_volume.iloc[i-window:i+1],\n                sell_volume.iloc[i-window:i+1]\n            )\n        return ofi",
    "source_file": "core\\features\\mega_generator.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "ChineseHFTAdditions"
  },
  {
    "name": "session_reversal",
    "category": "feature_engineering",
    "formula": "= returns.rolling(window, min_periods=1).sum() | -session_return",
    "explanation": "Session reversal effect (T+1 adapted for forex).\nContrarian signal based on recent session returns.",
    "python_code": "def session_reversal(returns: pd.Series, window: int = 5) -> pd.Series:\n        \"\"\"\n        Session reversal effect (T+1 adapted for forex).\n        Contrarian signal based on recent session returns.\n        \"\"\"\n        session_return = returns.rolling(window, min_periods=1).sum()\n        return -session_return",
    "source_file": "core\\features\\mega_generator.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "ChineseHFTAdditions"
  },
  {
    "name": "trade_intensity",
    "category": "feature_engineering",
    "formula": "(volume - vol_ma) / (vol_std + 1e-10)",
    "explanation": "Trade intensity - volume relative to recent average.\nHigh intensity suggests institutional activity.",
    "python_code": "def trade_intensity(volume: pd.Series, window: int = 20) -> pd.Series:\n        \"\"\"\n        Trade intensity - volume relative to recent average.\n        High intensity suggests institutional activity.\n        \"\"\"\n        vol_ma = volume.rolling(window, min_periods=5).mean()\n        vol_std = volume.rolling(window, min_periods=5).std()\n        return (volume - vol_ma) / (vol_std + 1e-10)",
    "source_file": "core\\features\\mega_generator.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "ChineseHFTAdditions"
  },
  {
    "name": "price_pressure",
    "category": "feature_engineering",
    "formula": "lower_pressure - upper_pressure",
    "explanation": "Price pressure indicator.\nMeasures where close is relative to range.",
    "python_code": "def price_pressure(high: pd.Series, low: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"\n        Price pressure indicator.\n        Measures where close is relative to range.\n        \"\"\"\n        range_ = high - low + 1e-10\n        upper_pressure = (high - close) / range_\n        lower_pressure = (close - low) / range_\n        return lower_pressure - upper_pressure",
    "source_file": "core\\features\\mega_generator.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "ChineseHFTAdditions"
  },
  {
    "name": "generate_all",
    "category": "feature_engineering",
    "formula": "result",
    "explanation": "Generate all Chinese HFT addition features.",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Generate all Chinese HFT addition features.\"\"\"\n        result = pd.DataFrame(index=df.index)\n\n        open_ = df.get('open', df['close'].shift(1).fillna(df['close']))\n        high = df.get('high', df['close'])\n        low = df.get('low', df['close'])\n        close = df['close']\n        volume = df.get('volume', pd.Series(1, index=df.index))\n        returns = close.pct_change().fillna(0)\n\n        # RSRS - Multiple windows\n        result['rsrs_18'] = self.rsrs(high, low, 18)\n        result['rsrs_10'] = self.rsrs(high, low, 10)\n        result['rsrs_25'] = self.rsrs(high, low, 25)\n\n        # Yang-Zhang Volatility - Multiple windows\n        result['yz_vol_10'] = self.yang_zhang_volatility(open_, high, low, close, 10)\n        result['yz_vol_20'] = self.yang_zhang_volatility(open_, high, low, close, 20)\n        result['yz_vol_30'] = self.yang_zhang_volatility(open_, high, low, close, 30)\n\n        # Volume Clock Acceleration\n        result['vol_accel_10'] = self.volume_clock_acceleration(volume, 10)\n        result['vol_accel_20'] = self.volume_clock_acceleration(volume, 20)\n\n        # Hawkes OFI - Multiple decay rates\n        result['hawkes_ofi_fast'] = self.hawkes_ofi(returns, volume, alpha=0.7, beta=1.5, window=30)\n        result['hawkes_ofi_slow'] = self.hawkes_ofi(returns, volume, alpha=0.3, beta=0.5, window=50)\n\n        # Session Reversal\n        result['session_rev_5'] = self.session_reversal(returns, 5)\n        result['session_rev_10'] = self.session_reversal(returns, 10)\n        result['session_rev_20'] = self.session_reversal(returns, 20)\n\n        # Trade Intensity\n        result['trade_intensity_10'] = self.trade_intensity(volume, 10)\n        result['trade_intensity_20'] = self.trade_intensity(volume, 20)\n\n        # Price Pressure\n        result['price_pressure'] = self.price_pressure(high, low, close)\n\n        # Derived signals\n        result['rsrs_momentum'] = result['rsrs_18'].diff(5)  # RSRS momentum\n        result['yz_vol_ratio'] = result['yz_vol_10'] / (result['yz_vol_30'] + 1e-10)  # Vol term structure\n\n        # RSRS regime signal (discrete)\n        result['rsrs_regime'] = np.where(result['rsrs_18'] > 1, 1,\n                                          np.where(result['rsrs_18'] < -1, -1, 0))\n\n        # Combined signals\n        result['chinese_composite'] = (\n            result['rsrs_18'].fillna(0) * 0.3 +\n            result['hawkes_ofi_fast'].fillna(0) * 0.3 +\n            result['session_rev_5'].fillna(0) * 0.2 +\n            result['price_pressure'].fillna(0) * 0.2\n        )\n\n        return result",
    "source_file": "core\\features\\mega_generator.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "ChineseHFTAdditions"
  },
  {
    "name": "__init__",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Initialize MegaFeatureGenerator.\n\nArgs:\n    enable_*: Enable/disable specific feature sets\n    alpha360_lookback: Lookback for Alpha360 (20=compact, 60=full)\n    verbose: Print progress",
    "python_code": "def __init__(\n        self,\n        enable_alpha101: bool = True,\n        enable_alpha158: bool = True,\n        enable_alpha360: bool = True,\n        enable_barra: bool = True,\n        enable_alpha191: bool = True,\n        enable_chinese_hft: bool = True,\n        enable_chinese_additions: bool = True,\n        enable_chinese_gold: bool = True,\n        enable_us_academic: bool = True,\n        enable_mlfinlab: bool = True,\n        enable_timeseries: bool = True,\n        enable_renaissance: bool = True,\n        # Global expansion (2026-01-17)\n        enable_india: bool = True,\n        enable_japan: bool = True,\n        enable_europe: bool = True,\n        enable_emerging: bool = True,\n        enable_universal_math: bool = True,\n        # Reinforcement Learning (2026-01-17)\n        enable_rl: bool = True,\n        # Eastern Asia Gold Standard (2026-01-17)\n        enable_moe: bool = True,           # Mixture of Experts (MIGA - 24% excess return)\n        enable_gnn: bool = True,           # Graph Neural Network features\n        enable_korea: bool = True,         # Korea Quant (MS-GARCH, CGMY)\n        enable_asia_fx: bool = True,       # Asia FX Spread (CNH-CNY, HKD peg)\n        enable_marl: bool = True,          # Multi-Agent RL\n        alpha360_lookback: int = 20,\n        verbose: bool = False\n    ):\n        \"\"\"\n        Initialize MegaFeatureGenerator.\n\n        Args:\n            enable_*: Enable/disable specific feature sets\n            alpha360_lookback: Lookback for Alpha360 (20=compact, 60=full)\n            verbose: Print progress\n        \"\"\"\n        self.verbose = verbose\n        self.enable_flags = {\n            'alpha101': enable_alpha101,\n            'alpha158': enable_alpha158,\n            'alpha360': enable_alpha360,\n            'barra': enable_barra,\n            'alpha191': enable_alpha191,\n            'chinese_hft': enable_chinese_hft,\n            'chinese_additions': enable_chinese_additions,\n            'chinese_gold': enable_chinese_gold,\n            'us_academic': enable_us_academic,\n            'mlfinlab': enable_mlfinlab,\n            'timeseries': enable_timeseries,\n            'renaissance': enable_renaissance,\n            # Global expansion (2026-01-17)\n            'india': enable_india,\n            'japan': enable_japan,\n            'europe': enable_europe,\n            'emerging': enable_emerging,\n            'universal_math': enable_universal_math,\n            # Reinforcement Learning (2026-01-17)\n            'rl': enable_rl,\n            # Eastern Asia Gold Standard (2026-01-17)\n            'moe': enable_moe,\n            'gnn': enable_gnn,\n            'korea': enable_korea,\n            'asia_fx': enable_asia_fx,\n            'marl': enable_marl,\n        }\n        self.alpha360_lookback = alpha360_lookback\n\n        # Lazy load generators (avoids import errors if not enabled)\n        self._generators = {}",
    "source_file": "core\\features\\mega_generator.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "MegaFeatureGenerator"
  },
  {
    "name": "_get_generator",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Lazy load generator to avoid import issues.",
    "python_code": "def _get_generator(self, name: str):\n        \"\"\"Lazy load generator to avoid import issues.\"\"\"\n        if name not in self._generators:\n            if name == 'alpha101':\n                from .alpha101 import Alpha101Complete\n                self._generators[name] = Alpha101Complete()\n            elif name == 'alpha158':\n                from .alpha158 import Alpha158\n                self._generators[name] = Alpha158()\n            elif name == 'alpha360':\n                from .alpha360 import Alpha360Compact\n                self._generators[name] = Alpha360Compact(lookback=self.alpha360_lookback)\n            elif name == 'barra':\n                from .barra_cne6 import BarraCNE6Forex\n                self._generators[name] = BarraCNE6Forex()\n            elif name == 'alpha191':\n                from core._experimental.alpha191_guotaijunan import Alpha191GuotaiJunan\n                self._generators[name] = Alpha191GuotaiJunan()\n            elif name == 'chinese_hft':\n                from core._experimental.chinese_hft_factors import ChineseHFTFactorEngine\n                self._generators[name] = ChineseHFTFactorEngine()\n            elif name == 'chinese_additions':\n                self._generators[name] = ChineseHFTAdditions()\n            elif name == 'chinese_gold':\n                from .chinese_gold_standard import ChineseGoldStandardFeatures\n                self._generators[name] = ChineseGoldStandardFeatures()\n            elif name == 'us_academic':\n                from .us_academic_factors import USAcademicFactors\n                self._generators[name] = USAcademicFactors()\n            elif name == 'mlfinlab':\n                from .mlfinlab_features import MLFinLabFeatures\n                self._generators[name] = MLFinLabFeatures()\n            elif name == 'timeseries':\n                from .timeseries_features import TimeSeriesLibraryFeatures\n                self._generators[name] = TimeSeriesLibraryFeatures()\n            elif name == 'renaissance':\n                from .renaissance import RenaissanceSignalGenerator\n                self._generators[name] = RenaissanceSignalGenerator()\n            # Global expansion (2026-01-17)\n            elif name == 'india':\n                from .india_quant import IndiaQuantFeatures\n                self._generators[name] = IndiaQuantFeatures()\n            elif name == 'japan':\n                from .japan_quant import JapanQuantFeatures\n                self._generators[name] = JapanQuantFeatures()\n            elif name == 'europe':\n                from .europe_quant import EuropeQuantFeatures\n                self._generators[name] = EuropeQuantFeatures()\n            elif name == 'emerging':\n                from .emerging_quant import EmergingMarketsFeatures\n                self._generators[name] = EmergingMarketsFeatures()\n            elif name == 'universal_math':\n                from .universal_math import UniversalMathFeatures\n                self._generators[name] = UniversalMathFeatures()\n            # ",
    "source_file": "core\\features\\mega_generator.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "MegaFeatureGenerator"
  },
  {
    "name": "_log",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Log message if verbose.",
    "python_code": "def _log(self, msg: str):\n        \"\"\"Log message if verbose.\"\"\"\n        if self.verbose:\n            print(f\"[MegaFeatureGenerator] {msg}\")",
    "source_file": "core\\features\\mega_generator.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "MegaFeatureGenerator"
  },
  {
    "name": "generate_all",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Generate ALL 806+ features.\n\nArgs:\n    df: DataFrame with OHLCV data\n        Required columns: open, high, low, close, volume\n        Optional: vwap, returns, bid, ask, price\n    prefix_features: Add source prefix to feature names (e.g., ALPHA101_*)\n\nReturns:\n    DataFrame with all features (NaN/inf cleaned)",
    "python_code": "def generate_all(\n        self,\n        df: pd.DataFrame,\n        prefix_features: bool = True\n    ) -> pd.DataFrame:\n        \"\"\"\n        Generate ALL 806+ features.\n\n        Args:\n            df: DataFrame with OHLCV data\n                Required columns: open, high, low, close, volume\n                Optional: vwap, returns, bid, ask, price\n            prefix_features: Add source prefix to feature names (e.g., ALPHA101_*)\n\n        Returns:\n            DataFrame with all features (NaN/inf cleaned)\n        \"\"\"\n        # Validate input\n        df = self._prepare_dataframe(df)\n\n        feature_sets = []\n        feature_counts = {}\n\n        # 1. Chinese Quant - Alpha101 (62 features)\n        if self.enable_flags['alpha101']:\n            self._log(\"Generating Alpha101 (62 features)...\")\n            try:\n                gen = self._get_generator('alpha101')\n                alpha101_features = gen.generate_all_alphas(df)\n                if prefix_features:\n                    alpha101_features.columns = [f'ALPHA101_{c}' for c in alpha101_features.columns]\n                feature_sets.append(alpha101_features)\n                feature_counts['alpha101'] = len(alpha101_features.columns)\n            except Exception as e:\n                logger.warning(f\"Alpha101 failed: {e}\")\n                feature_counts['alpha101'] = 0\n\n        # 2. Chinese Quant - Alpha158 (179 features)\n        if self.enable_flags['alpha158']:\n            self._log(\"Generating Alpha158 (179 features)...\")\n            try:\n                gen = self._get_generator('alpha158')\n                alpha158_features = gen.generate_all(df)\n                if prefix_features:\n                    alpha158_features.columns = [f'ALPHA158_{c}' for c in alpha158_features.columns]\n                feature_sets.append(alpha158_features)\n                feature_counts['alpha158'] = len(alpha158_features.columns)\n            except Exception as e:\n                logger.warning(f\"Alpha158 failed: {e}\")\n                feature_counts['alpha158'] = 0\n\n        # 3. Chinese Quant - Alpha360 Compact (200 features with lookback=20, 360 with lookback=60)\n        if self.enable_flags['alpha360']:\n            expected = self.alpha360_lookback * 6  # 6 price fields\n            self._log(f\"Generating Alpha360 ({expected} features)...\")\n            try:\n                gen = self._get_generator('alpha360')\n                alpha360_features = gen.generate_all(df)\n                if prefix_features:\n                    alpha360_features.columns = [f'ALPHA360_{c}' for c in alpha360_features.columns]\n                feature_sets.append(alpha360_features)\n                feature_counts['alpha360'] = len(alpha360_features.columns)\n            except Exception as e:\n                logger.warning(f\"Alpha360 failed: {e}\")\n                feature_counts['alpha360'] = 0\n\n        # 4. Chinese Quant - Barra CNE6 (46 features)\n        if self.enable_flags['barra']:\n            self._log(\"Generating Barra CNE6 (46 features)...\"",
    "source_file": "core\\features\\mega_generator.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "MegaFeatureGenerator"
  },
  {
    "name": "_prepare_dataframe",
    "category": "execution",
    "formula": "ohlcv",
    "explanation": "Prepare DataFrame with required columns.\n\nEnsures: open, high, low, close, volume, returns, vwap, price\nReturns ONLY the required OHLCV columns - excludes any target/feature columns.",
    "python_code": "def _prepare_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Prepare DataFrame with required columns.\n\n        Ensures: open, high, low, close, volume, returns, vwap, price\n        Returns ONLY the required OHLCV columns - excludes any target/feature columns.\n        \"\"\"\n        # Required column\n        if 'close' not in df.columns:\n            raise ValueError(\"Missing required column: 'close'\")\n\n        # Create minimal OHLCV dataframe (exclude all other columns to avoid leakage)\n        ohlcv = pd.DataFrame(index=df.index)\n        ohlcv['close'] = df['close'].values\n\n        # Create open/high/low from close if not in original\n        if 'open' in df.columns:\n            ohlcv['open'] = df['open'].values\n        else:\n            ohlcv['open'] = df['close'].shift(1).fillna(df['close']).values\n\n        if 'high' in df.columns:\n            ohlcv['high'] = df['high'].values\n        else:\n            ohlcv['high'] = df['close'].values\n\n        if 'low' in df.columns:\n            ohlcv['low'] = df['low'].values\n        else:\n            ohlcv['low'] = df['close'].values\n\n        # Volume\n        if 'volume' in df.columns:\n            ohlcv['volume'] = df['volume'].values\n        else:\n            ohlcv['volume'] = 1  # Tick count proxy\n\n        # Returns\n        ohlcv['returns'] = ohlcv['close'].pct_change()\n\n        # VWAP\n        if 'vwap' in df.columns:\n            ohlcv['vwap'] = df['vwap'].values\n        else:\n            ohlcv['vwap'] = (ohlcv['high'] + ohlcv['low'] + ohlcv['close']) / 3\n\n        # Price (for Renaissance signals)\n        if 'bid' in df.columns and 'ask' in df.columns:\n            ohlcv['price'] = (df['bid'].values + df['ask'].values) / 2\n            ohlcv['bid'] = df['bid'].values\n            ohlcv['ask'] = df['ask'].values\n        else:\n            ohlcv['price'] = ohlcv['close']\n\n        return ohlcv",
    "source_file": "core\\features\\mega_generator.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "MegaFeatureGenerator"
  },
  {
    "name": "get_feature_counts",
    "category": "feature_engineering",
    "formula": "enabled_counts",
    "explanation": "Get expected feature counts for each library.",
    "python_code": "def get_feature_counts(self) -> Dict[str, int]:\n        \"\"\"Get expected feature counts for each library.\"\"\"\n        counts = {\n            'alpha101': 62,\n            'alpha158': 179,\n            'alpha360': self.alpha360_lookback * 6,  # 6 price fields\n            'barra': 46,\n            'alpha191': 191,\n            'chinese_hft': 50,\n            'chinese_additions': 25,\n            'chinese_gold': 35,  # VPIN, OFI, HMM, Kalman, etc.\n            'us_academic': 50,\n            'mlfinlab': 17,\n            'timeseries': 41,\n            'renaissance': 51,\n            # Global expansion (2026-01-17)\n            'india': 25,\n            'japan': 30,  # Updated: +10 (HAR-RV, Power-Law, BOJ)\n            'europe': 15,\n            'emerging': 20,\n            'universal_math': 30,\n            # Reinforcement Learning (2026-01-17)\n            'rl': 35,\n            # Eastern Asia Gold Standard (2026-01-17)\n            'moe': 20,     # Mixture of Experts (MIGA - 24% excess return)\n            'gnn': 15,     # Graph Neural Network temporal\n            'korea': 20,   # MS-GARCH, CGMY Lvy, VKOSPI\n            'asia_fx': 15, # CNH-CNY spread, HKD peg\n            'marl': 15,    # Multi-Agent RL\n        }\n\n        # Filter by enabled\n        enabled_counts = {k: v for k, v in counts.items() if self.enable_flags.get(k, False)}\n        enabled_counts['total'] = sum(enabled_counts.values())\n\n        return enabled_counts",
    "source_file": "core\\features\\mega_generator.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "MegaFeatureGenerator"
  },
  {
    "name": "get_feature_names",
    "category": "feature_engineering",
    "formula": "names",
    "explanation": "Get all feature names (without generating them).",
    "python_code": "def get_feature_names(self, prefix: bool = True) -> List[str]:\n        \"\"\"Get all feature names (without generating them).\"\"\"\n        names = []\n\n        if self.enable_flags['alpha101']:\n            alpha_names = [f'alpha_{i:03d}' for i in range(1, 102)]\n            if prefix:\n                alpha_names = [f'ALPHA101_{n}' for n in alpha_names]\n            names.extend(alpha_names)\n\n        # Note: Exact names depend on generator implementation\n        # This is an approximation\n\n        return names",
    "source_file": "core\\features\\mega_generator.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "MegaFeatureGenerator"
  },
  {
    "name": "generate_mega_features",
    "category": "alpha_factor",
    "formula": "generator.generate_all(df)",
    "explanation": "Generate all 1100+ features in one call.\n\nArgs:\n    df: OHLCV DataFrame\n    alpha360_lookback: 20 for compact (120 features), 60 for full (360 features)\n    verbose: Print progress\n\nReturns:\n    DataFrame with 1100+ features",
    "python_code": "def generate_mega_features(\n    df: pd.DataFrame,\n    alpha360_lookback: int = 20,\n    verbose: bool = False\n) -> pd.DataFrame:\n    \"\"\"\n    Generate all 1100+ features in one call.\n\n    Args:\n        df: OHLCV DataFrame\n        alpha360_lookback: 20 for compact (120 features), 60 for full (360 features)\n        verbose: Print progress\n\n    Returns:\n        DataFrame with 1100+ features\n    \"\"\"\n    generator = MegaFeatureGenerator(\n        alpha360_lookback=alpha360_lookback,\n        verbose=verbose\n    )\n    return generator.generate_all(df)",
    "source_file": "core\\features\\mega_generator.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": null
  },
  {
    "name": "generate_chinese_quant_features",
    "category": "feature_engineering",
    "formula": "generator.generate_all(df)",
    "explanation": "Generate only Chinese Quant features (800+).",
    "python_code": "def generate_chinese_quant_features(df: pd.DataFrame, verbose: bool = False) -> pd.DataFrame:\n    \"\"\"Generate only Chinese Quant features (800+).\"\"\"\n    generator = MegaFeatureGenerator(\n        enable_alpha101=True,\n        enable_alpha158=True,\n        enable_alpha360=True,\n        enable_barra=True,\n        enable_alpha191=True,\n        enable_chinese_hft=True,\n        enable_chinese_additions=True,\n        enable_chinese_gold=True,\n        enable_us_academic=False,\n        enable_mlfinlab=False,\n        enable_timeseries=False,\n        enable_renaissance=False,\n        verbose=verbose\n    )\n    return generator.generate_all(df)",
    "source_file": "core\\features\\mega_generator.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": null
  },
  {
    "name": "generate_academic_features",
    "category": "feature_engineering",
    "formula": "generator.generate_all(df)",
    "explanation": "Generate USA Academic + MLFinLab features (67).",
    "python_code": "def generate_academic_features(df: pd.DataFrame, verbose: bool = False) -> pd.DataFrame:\n    \"\"\"Generate USA Academic + MLFinLab features (67).\"\"\"\n    generator = MegaFeatureGenerator(\n        enable_alpha101=False,\n        enable_alpha158=False,\n        enable_alpha360=False,\n        enable_barra=False,\n        enable_alpha191=False,\n        enable_chinese_hft=False,\n        enable_chinese_additions=False,\n        enable_chinese_gold=False,\n        enable_us_academic=True,\n        enable_mlfinlab=True,\n        enable_timeseries=False,\n        enable_renaissance=False,\n        verbose=verbose\n    )\n    return generator.generate_all(df)",
    "source_file": "core\\features\\mega_generator.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": null
  },
  {
    "name": "generate_renaissance_features",
    "category": "feature_engineering",
    "formula": "generator.generate_all(df)",
    "explanation": "Generate Renaissance weak signals (51).",
    "python_code": "def generate_renaissance_features(df: pd.DataFrame, verbose: bool = False) -> pd.DataFrame:\n    \"\"\"Generate Renaissance weak signals (51).\"\"\"\n    generator = MegaFeatureGenerator(\n        enable_alpha101=False,\n        enable_alpha158=False,\n        enable_alpha360=False,\n        enable_barra=False,\n        enable_alpha191=False,\n        enable_chinese_hft=False,\n        enable_chinese_additions=False,\n        enable_chinese_gold=False,\n        enable_us_academic=False,\n        enable_mlfinlab=False,\n        enable_timeseries=False,\n        enable_renaissance=True,\n        verbose=verbose\n    )\n    return generator.generate_all(df)",
    "source_file": "core\\features\\mega_generator.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": null
  },
  {
    "name": "weighted_imbalance",
    "category": "feature_engineering",
    "formula": "np.nan | (buy_weighted - sell_weighted) / total",
    "explanation": "",
    "python_code": "def weighted_imbalance(buy, sell):\n            if len(buy) < window:\n                return np.nan\n            buy_arr = buy[-window:].values\n            sell_arr = sell[-window:].values\n            buy_weighted = np.sum(buy_arr * weights[::-1])\n            sell_weighted = np.sum(sell_arr * weights[::-1])\n            total = buy_weighted + sell_weighted + 1e-10\n            return (buy_weighted - sell_weighted) / total",
    "source_file": "core\\features\\mega_generator.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "deep_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, config: MLAConfig):\n        super().__init__()\n        self.config = config\n\n        # Query projection (standard)\n        self.W_q = nn.Linear(config.d_model, config.d_model, bias=config.bias)\n\n        # Compression: X -> c\n        # This is the KEY INNOVATION of MLA\n        self.W_compress = nn.Linear(config.d_model, config.d_compress, bias=config.bias)\n\n        # Decompression: c -> K, V\n        # Instead of projecting from X directly, project from compressed c\n        self.W_k_decompress = nn.Linear(config.d_compress, config.d_model, bias=config.bias)\n        self.W_v_decompress = nn.Linear(config.d_compress, config.d_model, bias=config.bias)\n\n        # Output projection\n        self.W_o = nn.Linear(config.d_model, config.d_model, bias=config.bias)\n\n        # Dropout\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.out_dropout = nn.Dropout(config.dropout)\n\n        # Scaling factor\n        self.scale = 1.0 / math.sqrt(config.d_head)\n\n        logger.info(\n            f\"Initialized MLA: {config.n_heads} heads, \"\n            f\"compression {config.d_model*2} -> {config.d_compress} \"\n            f\"({config.compression_ratio:.1%} of original)\"\n        )",
    "source_file": "core\\features\\mla_attention.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MLAAttention"
  },
  {
    "name": "forward",
    "category": "deep_learning",
    "formula": "compressed latent | output, compressed | output",
    "explanation": "Forward pass with MLA.\n\nArgs:\n    x: (batch_size, seq_len, d_model)\n    mask: (batch_size, seq_len, seq_len) optional attention mask\n    return_compressed: If True, return compressed latent\n\nReturns:\n    output: (batch_size, seq_len, d_model)\n    compressed: (batch_size, seq_len, d_compress) if return_compressed",
    "python_code": "def forward(\n        self,\n        x: torch.Tensor,\n        mask: Optional[torch.Tensor] = None,\n        return_compressed: bool = False\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n        \"\"\"\n        Forward pass with MLA.\n\n        Args:\n            x: (batch_size, seq_len, d_model)\n            mask: (batch_size, seq_len, seq_len) optional attention mask\n            return_compressed: If True, return compressed latent\n\n        Returns:\n            output: (batch_size, seq_len, d_model)\n            compressed: (batch_size, seq_len, d_compress) if return_compressed\n        \"\"\"\n        batch_size, seq_len, d_model = x.shape\n        n_heads = self.config.n_heads\n        d_head = self.config.d_head\n\n        # 1. Compress input to latent representation\n        #    This is the core MLA innovation: compress BEFORE creating K, V\n        compressed = self.W_compress(x)  # (B, L, d_compress)\n\n        # 2. Query (standard, no compression)\n        Q = self.W_q(x)  # (B, L, d_model)\n        Q = Q.view(batch_size, seq_len, n_heads, d_head).transpose(1, 2)  # (B, H, L, d_head)\n\n        # 3. Decompress latent to K, V\n        #    Instead of projecting from x (size d_model), project from compressed (size d_compress)\n        K = self.W_k_decompress(compressed)  # (B, L, d_model)\n        V = self.W_v_decompress(compressed)  # (B, L, d_model)\n\n        K = K.view(batch_size, seq_len, n_heads, d_head).transpose(1, 2)  # (B, H, L, d_head)\n        V = V.view(batch_size, seq_len, n_heads, d_head).transpose(1, 2)  # (B, H, L, d_head)\n\n        # 4. Standard scaled dot-product attention\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale  # (B, H, L, L)\n\n        # Apply mask if provided\n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n\n        # Softmax + dropout\n        attn_weights = F.softmax(attn_scores, dim=-1)  # (B, H, L, L)\n        attn_weights = self.attn_dropout(attn_weights)\n\n        # 5. Apply attention to values\n        attn_output = torch.matmul(attn_weights, V)  # (B, H, L, d_head)\n\n        # 6. Concatenate heads and project\n        attn_output = attn_output.transpose(1, 2).contiguous()  # (B, L, H, d_head)\n        attn_output = attn_output.view(batch_size, seq_len, d_model)  # (B, L, d_model)\n\n        # Output projection\n        output = self.W_o(attn_output)\n        output = self.out_dropout(output)\n\n        if return_compressed:\n            return output, compressed\n        else:\n            return output",
    "source_file": "core\\features\\mla_attention.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MLAAttention"
  },
  {
    "name": "get_kv_cache_size",
    "category": "deep_learning",
    "formula": "compressed_size",
    "explanation": "Calculate KV cache size in bytes.\n\nArgs:\n    seq_len: Sequence length\n\nReturns:\n    Cache size in bytes",
    "python_code": "def get_kv_cache_size(self, seq_len: int) -> int:\n        \"\"\"\n        Calculate KV cache size in bytes.\n\n        Args:\n            seq_len: Sequence length\n\n        Returns:\n            Cache size in bytes\n        \"\"\"\n        # MLA only stores compressed latent\n        # Standard attention stores K + V (both d_model)\n        compressed_size = seq_len * self.config.d_compress * 4  # float32 = 4 bytes\n        return compressed_size",
    "source_file": "core\\features\\mla_attention.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MLAAttention"
  },
  {
    "name": "get_standard_kv_cache_size",
    "category": "deep_learning",
    "formula": "standard_size",
    "explanation": "Calculate standard attention KV cache size for comparison.\n\nArgs:\n    seq_len: Sequence length\n\nReturns:\n    Cache size in bytes",
    "python_code": "def get_standard_kv_cache_size(self, seq_len: int) -> int:\n        \"\"\"\n        Calculate standard attention KV cache size for comparison.\n\n        Args:\n            seq_len: Sequence length\n\n        Returns:\n            Cache size in bytes\n        \"\"\"\n        standard_size = seq_len * self.config.d_model * 2 * 4  # K + V, float32\n        return standard_size",
    "source_file": "core\\features\\mla_attention.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MLAAttention"
  },
  {
    "name": "forward",
    "category": "technical",
    "formula": "x",
    "explanation": "Forward pass.\n\nArgs:\n    x: (batch_size, seq_len, d_model)\n    mask: Optional attention mask\n\nReturns:\n    output: (batch_size, seq_len, d_model)",
    "python_code": "def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        \"\"\"\n        Forward pass.\n\n        Args:\n            x: (batch_size, seq_len, d_model)\n            mask: Optional attention mask\n\n        Returns:\n            output: (batch_size, seq_len, d_model)\n        \"\"\"\n        # MLA with residual + LayerNorm\n        x = x + self.mla(self.norm1(x), mask=mask)\n\n        # FFN with residual + LayerNorm\n        x = x + self.ffn(self.norm2(x))\n\n        return x",
    "source_file": "core\\features\\mla_attention.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MLATransformerBlock"
  },
  {
    "name": "forward",
    "category": "machine_learning",
    "formula": "output",
    "explanation": "Forward pass.\n\nArgs:\n    x: (batch_size, seq_len, input_dim)\n\nReturns:\n    predictions: (batch_size, seq_len, output_dim)",
    "python_code": "def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Forward pass.\n\n        Args:\n            x: (batch_size, seq_len, input_dim)\n\n        Returns:\n            predictions: (batch_size, seq_len, output_dim)\n        \"\"\"\n        batch_size, seq_len, _ = x.shape\n\n        # Project input\n        x = self.input_proj(x)  # (B, L, d_model)\n\n        # Add positional encoding\n        x = x + self.pos_embedding[:, :seq_len, :]\n\n        # Pass through MLA blocks\n        for block in self.blocks:\n            x = block(x)\n\n        # Output projection\n        output = self.output_head(x)\n\n        return output",
    "source_file": "core\\features\\mla_attention.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MLATimeSeries"
  },
  {
    "name": "create_mla_model",
    "category": "machine_learning",
    "formula": "MLATimeSeries(",
    "explanation": "Create MLA model with DeepSeek-V3 inspired hyperparameters.\n\nArgs:\n    input_dim: Number of input features\n    output_dim: Number of output predictions\n    context_length: Maximum sequence length to process\n\nReturns:\n    MLATimeSeries model",
    "python_code": "def create_mla_model(\n    input_dim: int,\n    output_dim: int = 1,\n    context_length: int = 10000\n) -> MLATimeSeries:\n    \"\"\"\n    Create MLA model with DeepSeek-V3 inspired hyperparameters.\n\n    Args:\n        input_dim: Number of input features\n        output_dim: Number of output predictions\n        context_length: Maximum sequence length to process\n\n    Returns:\n        MLATimeSeries model\n    \"\"\"\n    return MLATimeSeries(\n        input_dim=input_dim,\n        d_model=128,\n        n_heads=8,\n        n_layers=4,\n        d_compress=32,  # 4:1 compression ratio\n        output_dim=output_dim,\n        max_seq_len=context_length,\n        dropout=0.1\n    )",
    "source_file": "core\\features\\mla_attention.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "volatility",
    "formula": "",
    "explanation": "Initialize Triple Barrier labeling.\n\nArgs:\n    profit_take: Multiplier for upper barrier (x volatility)\n    stop_loss: Multiplier for lower barrier (x volatility)\n    max_holding: Maximum holding period (vertical barrier)\n    volatility_window: Window for volatility estimation",
    "python_code": "def __init__(\n        self,\n        profit_take: float = 2.0,\n        stop_loss: float = 2.0,\n        max_holding: int = 10,\n        volatility_window: int = 20\n    ):\n        \"\"\"\n        Initialize Triple Barrier labeling.\n\n        Args:\n            profit_take: Multiplier for upper barrier (x volatility)\n            stop_loss: Multiplier for lower barrier (x volatility)\n            max_holding: Maximum holding period (vertical barrier)\n            volatility_window: Window for volatility estimation\n        \"\"\"\n        self.profit_take = profit_take\n        self.stop_loss = stop_loss\n        self.max_holding = max_holding\n        self.volatility_window = volatility_window",
    "source_file": "core\\features\\mlfinlab_features.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "TripleBarrierLabeling"
  },
  {
    "name": "get_daily_volatility",
    "category": "volatility",
    "formula": "returns.ewm(span=self.volatility_window).std()",
    "explanation": "Estimate daily volatility using exponential weighted std.",
    "python_code": "def get_daily_volatility(self, close: pd.Series) -> pd.Series:\n        \"\"\"Estimate daily volatility using exponential weighted std.\"\"\"\n        returns = close.pct_change()\n        return returns.ewm(span=self.volatility_window).std()",
    "source_file": "core\\features\\mlfinlab_features.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "TripleBarrierLabeling"
  },
  {
    "name": "apply_triple_barrier",
    "category": "feature_engineering",
    "formula": "ret = (exit_price / close.iloc[loc]) - 1 | pd.DataFrame(results)",
    "explanation": "Apply triple barrier method to generate labels.\n\nArgs:\n    close: Price series\n    events: Event timestamps (if None, use all)\n\nReturns:\n    DataFrame with columns: t1 (barrier time), ret, label",
    "python_code": "def apply_triple_barrier(\n        self,\n        close: pd.Series,\n        events: pd.DatetimeIndex = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Apply triple barrier method to generate labels.\n\n        Args:\n            close: Price series\n            events: Event timestamps (if None, use all)\n\n        Returns:\n            DataFrame with columns: t1 (barrier time), ret, label\n        \"\"\"\n        if events is None:\n            events = close.index\n\n        # Get volatility\n        volatility = self.get_daily_volatility(close)\n\n        results = []\n        for idx in events:\n            if idx not in close.index:\n                continue\n\n            loc = close.index.get_loc(idx)\n            if loc >= len(close) - 1:\n                continue\n\n            # Get volatility at event time\n            vol = volatility.iloc[loc] if loc < len(volatility) else volatility.iloc[-1]\n            if pd.isna(vol) or vol == 0:\n                vol = close.pct_change().std()\n\n            # Set barriers\n            upper_barrier = close.iloc[loc] * (1 + self.profit_take * vol)\n            lower_barrier = close.iloc[loc] * (1 - self.stop_loss * vol)\n            vertical_barrier = min(loc + self.max_holding, len(close) - 1)\n\n            # Find first barrier touch\n            label = 0\n            exit_time = vertical_barrier\n            exit_price = close.iloc[vertical_barrier]\n\n            for t in range(loc + 1, vertical_barrier + 1):\n                price = close.iloc[t]\n\n                if price >= upper_barrier:\n                    label = 1\n                    exit_time = t\n                    exit_price = price\n                    break\n                elif price <= lower_barrier:\n                    label = -1\n                    exit_time = t\n                    exit_price = price\n                    break\n\n            # Calculate return\n            ret = (exit_price / close.iloc[loc]) - 1\n\n            results.append({\n                'event_time': idx,\n                't1': close.index[exit_time],\n                'ret': ret,\n                'label': label,\n                'upper': upper_barrier,\n                'lower': lower_barrier,\n                'vol': vol\n            })\n\n        return pd.DataFrame(results)",
    "source_file": "core\\features\\mlfinlab_features.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "TripleBarrierLabeling"
  },
  {
    "name": "generate_meta_labels",
    "category": "machine_learning",
    "formula": "direction | meta_labels",
    "explanation": "Generate meta-labels based on primary signal correctness.\n\nArgs:\n    primary_signal: Primary model predictions (-1, 0, 1)\n    actual_returns: Actual forward returns\n\nReturns:\n    Series of meta-labels (1 = correct, 0 = incorrect)",
    "python_code": "def generate_meta_labels(\n        self,\n        primary_signal: pd.Series,\n        actual_returns: pd.Series\n    ) -> pd.Series:\n        \"\"\"\n        Generate meta-labels based on primary signal correctness.\n\n        Args:\n            primary_signal: Primary model predictions (-1, 0, 1)\n            actual_returns: Actual forward returns\n\n        Returns:\n            Series of meta-labels (1 = correct, 0 = incorrect)\n        \"\"\"\n        # Meta-label is 1 if primary signal direction matches return direction\n        signal_direction = np.sign(primary_signal)\n        return_direction = np.sign(actual_returns)\n\n        meta_labels = (signal_direction == return_direction).astype(int)\n\n        # Where primary signal is 0, meta-label is also 0\n        meta_labels[primary_signal == 0] = 0\n\n        return meta_labels",
    "source_file": "core\\features\\mlfinlab_features.py",
    "academic_reference": null,
    "class_name": "MetaLabeling"
  },
  {
    "name": "get_weights",
    "category": "feature_engineering",
    "formula": "np.array(weights[::-1])",
    "explanation": "Calculate fractional differentiation weights.",
    "python_code": "def get_weights(self, d: float, size: int) -> np.ndarray:\n        \"\"\"Calculate fractional differentiation weights.\"\"\"\n        weights = [1.0]\n        for k in range(1, size):\n            w = -weights[-1] * (d - k + 1) / k\n            if abs(w) < self.threshold:\n                break\n            weights.append(w)\n        return np.array(weights[::-1])",
    "source_file": "core\\features\\mlfinlab_features.py",
    "academic_reference": null,
    "class_name": "FractionalDifferentiation"
  },
  {
    "name": "frac_diff",
    "category": "feature_engineering",
    "formula": "result",
    "explanation": "Apply fractional differentiation.\n\nArgs:\n    series: Input time series\n\nReturns:\n    Fractionally differentiated series",
    "python_code": "def frac_diff(self, series: pd.Series) -> pd.Series:\n        \"\"\"\n        Apply fractional differentiation.\n\n        Args:\n            series: Input time series\n\n        Returns:\n            Fractionally differentiated series\n        \"\"\"\n        weights = self.get_weights(self.d, len(series))\n        width = len(weights)\n\n        result = pd.Series(index=series.index, dtype=float)\n\n        for i in range(width - 1, len(series)):\n            result.iloc[i] = np.dot(weights, series.iloc[i - width + 1:i + 1].values)\n\n        return result",
    "source_file": "core\\features\\mlfinlab_features.py",
    "academic_reference": null,
    "class_name": "FractionalDifferentiation"
  },
  {
    "name": "get_events",
    "category": "filtering",
    "formula": "pd.DatetimeIndex(events)",
    "explanation": "Get event timestamps using CUSUM filter.\n\nArgs:\n    close: Price series\n\nReturns:\n    DatetimeIndex of event times",
    "python_code": "def get_events(self, close: pd.Series) -> pd.DatetimeIndex:\n        \"\"\"\n        Get event timestamps using CUSUM filter.\n\n        Args:\n            close: Price series\n\n        Returns:\n            DatetimeIndex of event times\n        \"\"\"\n        returns = close.pct_change().dropna()\n\n        if self.threshold is None:\n            h = returns.std()\n        else:\n            h = self.threshold\n\n        events = []\n        s_pos = 0\n        s_neg = 0\n\n        for i, r in enumerate(returns):\n            s_pos = max(0, s_pos + r)\n            s_neg = min(0, s_neg + r)\n\n            if s_neg < -h:\n                s_neg = 0\n                events.append(returns.index[i])\n            elif s_pos > h:\n                s_pos = 0\n                events.append(returns.index[i])\n\n        return pd.DatetimeIndex(events)",
    "source_file": "core\\features\\mlfinlab_features.py",
    "academic_reference": null,
    "class_name": "CUSUMFilter"
  },
  {
    "name": "shannon_entropy",
    "category": "statistical",
    "formula": "-np.sum(probs * np.log2(probs)) | series.rolling(self.window).apply(entropy, raw=True)",
    "explanation": "Calculate rolling Shannon entropy.",
    "python_code": "def shannon_entropy(self, series: pd.Series) -> pd.Series:\n        \"\"\"Calculate rolling Shannon entropy.\"\"\"\n        def entropy(x):\n            # Discretize into bins\n            counts = np.histogram(x, bins=10)[0]\n            probs = counts / counts.sum()\n            probs = probs[probs > 0]  # Remove zeros\n            return -np.sum(probs * np.log2(probs))\n\n        return series.rolling(self.window).apply(entropy, raw=True)",
    "source_file": "core\\features\\mlfinlab_features.py",
    "academic_reference": null,
    "class_name": "EntropyFeatures"
  },
  {
    "name": "approximate_entropy",
    "category": "statistical",
    "formula": "Lower = more regular, Higher = more random. | np.nan | np.log(count / (n - m_val + 1))",
    "explanation": "Calculate rolling Approximate Entropy.\n\nMeasures regularity/predictability of time series.\nLower = more regular, Higher = more random.",
    "python_code": "def approximate_entropy(self, series: pd.Series, m: int = 2, r: float = None) -> pd.Series:\n        \"\"\"\n        Calculate rolling Approximate Entropy.\n\n        Measures regularity/predictability of time series.\n        Lower = more regular, Higher = more random.\n        \"\"\"\n        if r is None:\n            r = 0.2 * series.std()\n\n        def apen(x):\n            n = len(x)\n            if n < m + 1:\n                return np.nan\n\n            def _phi(m_val):\n                patterns = np.array([x[i:i + m_val] for i in range(n - m_val + 1)])\n                count = 0\n                for i in range(len(patterns)):\n                    for j in range(len(patterns)):\n                        if np.max(np.abs(patterns[i] - patterns[j])) <= r:\n                            count += 1\n                return np.log(count / (n - m_val + 1))\n\n            return _phi(m) - _phi(m + 1)\n\n        return series.rolling(self.window).apply(apen, raw=True)",
    "source_file": "core\\features\\mlfinlab_features.py",
    "academic_reference": null,
    "class_name": "EntropyFeatures"
  },
  {
    "name": "generate_all",
    "category": "feature_engineering",
    "formula": "features",
    "explanation": "Generate all MLFinLab features.\n\nArgs:\n    df: DataFrame with close price\n\nReturns:\n    DataFrame with MLFinLab features",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all MLFinLab features.\n\n        Args:\n            df: DataFrame with close price\n\n        Returns:\n            DataFrame with MLFinLab features\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Fractionally Differentiated Features\n        for d in [0.3, 0.5, 0.7]:\n            fd = FractionalDifferentiation(d=d)\n            features[f'FRAC_DIFF_{int(d*10)}'] = fd.frac_diff(close)\n\n        # 2. Volatility Features (Lopez de Prado style)\n        # Exponential weighted volatility\n        features['VOL_EWM20'] = returns.ewm(span=20).std()\n        features['VOL_EWM60'] = returns.ewm(span=60).std()\n\n        # Parkinson volatility\n        if 'high' in df.columns and 'low' in df.columns:\n            log_hl = np.log(df['high'] / df['low'] + 1e-12)\n            features['VOL_PARKINSON'] = np.sqrt(\n                (log_hl ** 2).rolling(20).mean() / (4 * np.log(2))\n            )\n\n        # 3. CUSUM-based features\n        cusum = CUSUMFilter()\n        s_pos = returns.cumsum().clip(lower=0)\n        s_neg = returns.cumsum().clip(upper=0)\n        features['CUSUM_POS'] = s_pos\n        features['CUSUM_NEG'] = s_neg\n        features['CUSUM_DIFF'] = s_pos + s_neg\n\n        # 4. Entropy Features\n        features['ENTROPY_SHANNON'] = self.entropy.shannon_entropy(returns)\n\n        # 5. Structural Break Indicators\n        # Rolling mean shift\n        ma_20 = close.rolling(20).mean()\n        ma_60 = close.rolling(60).mean()\n        features['STRUCT_MA_CROSS'] = (ma_20 - ma_60) / (ma_60 + 1e-12)\n\n        # Variance ratio\n        var_5 = returns.rolling(5).var()\n        var_20 = returns.rolling(20).var()\n        features['STRUCT_VAR_RATIO'] = var_5 / (var_20 + 1e-12)\n\n        # 6. Information-driven features\n        # Volume-synchronized probability of informed trading (VPIN proxy)\n        if 'volume' in df.columns:\n            volume = df['volume']\n            buy_vol = volume.where(returns > 0, 0)\n            sell_vol = volume.where(returns < 0, 0)\n            features['INFO_VPIN'] = np.abs(buy_vol - sell_vol).rolling(20).sum() / (\n                volume.rolling(20).sum() + 1e-12\n            )\n\n        # 7. Bet sizing features (for meta-labeling)\n        # Signal confidence proxy\n        ret_20 = returns.rolling(20).sum()\n        vol_20 = returns.rolling(20).std()\n        features['BET_CONFIDENCE'] = np.abs(ret_20) / (vol_20 * np.sqrt(20) + 1e-12)\n\n        # 8. Triple barrier proxy features\n        # Distance to rolling high/low as barrier proxy\n        roll_high = close.rolling(20).max()\n        roll_low = close.rolling(20).min()\n        features['TB_UPPER_DIST'] = (roll_high - close) / (close + 1e-12)\n        features['TB_LOWER_DIST'] = (close - roll_low) / (close + 1e-12)\n        features['TB_RANGE_POS'] = (close - roll_low) / (roll_high - roll_low + 1e-12)\n\n        # Clean up\n        features = fe",
    "source_file": "core\\features\\mlfinlab_features.py",
    "academic_reference": null,
    "class_name": "MLFinLabFeatures"
  },
  {
    "name": "get_feature_names",
    "category": "feature_engineering",
    "formula": "[",
    "explanation": "Get list of feature names.",
    "python_code": "def get_feature_names(self) -> List[str]:\n        \"\"\"Get list of feature names.\"\"\"\n        return [\n            'FRAC_DIFF_3', 'FRAC_DIFF_5', 'FRAC_DIFF_7',\n            'VOL_EWM20', 'VOL_EWM60', 'VOL_PARKINSON',\n            'CUSUM_POS', 'CUSUM_NEG', 'CUSUM_DIFF',\n            'ENTROPY_SHANNON',\n            'STRUCT_MA_CROSS', 'STRUCT_VAR_RATIO',\n            'INFO_VPIN',\n            'BET_CONFIDENCE',\n            'TB_UPPER_DIST', 'TB_LOWER_DIST', 'TB_RANGE_POS'\n        ]",
    "source_file": "core\\features\\mlfinlab_features.py",
    "academic_reference": null,
    "class_name": "MLFinLabFeatures"
  },
  {
    "name": "generate_mlfinlab_features",
    "category": "feature_engineering",
    "formula": "generator.generate_all(df)",
    "explanation": "Generate MLFinLab features.\n\nArgs:\n    df: OHLCV DataFrame\n\nReturns:\n    DataFrame with ~17 MLFinLab features",
    "python_code": "def generate_mlfinlab_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generate MLFinLab features.\n\n    Args:\n        df: OHLCV DataFrame\n\n    Returns:\n        DataFrame with ~17 MLFinLab features\n    \"\"\"\n    generator = MLFinLabFeatures()\n    return generator.generate_all(df)",
    "source_file": "core\\features\\mlfinlab_features.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "entropy",
    "category": "feature_engineering",
    "formula": "-np.sum(probs * np.log2(probs))",
    "explanation": "",
    "python_code": "def entropy(x):\n            # Discretize into bins\n            counts = np.histogram(x, bins=10)[0]\n            probs = counts / counts.sum()\n            probs = probs[probs > 0]  # Remove zeros\n            return -np.sum(probs * np.log2(probs))",
    "source_file": "core\\features\\mlfinlab_features.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "apen",
    "category": "feature_engineering",
    "formula": "np.nan | np.log(count / (n - m_val + 1)) | _phi(m) - _phi(m + 1)",
    "explanation": "",
    "python_code": "def apen(x):\n            n = len(x)\n            if n < m + 1:\n                return np.nan\n\n            def _phi(m_val):\n                patterns = np.array([x[i:i + m_val] for i in range(n - m_val + 1)])\n                count = 0\n                for i in range(len(patterns)):\n                    for j in range(len(patterns)):\n                        if np.max(np.abs(patterns[i] - patterns[j])) <= r:\n                            count += 1\n                return np.log(count / (n - m_val + 1))\n\n            return _phi(m) - _phi(m + 1)",
    "source_file": "core\\features\\mlfinlab_features.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "_phi",
    "category": "feature_engineering",
    "formula": "np.log(count / (n - m_val + 1))",
    "explanation": "",
    "python_code": "def _phi(m_val):\n                patterns = np.array([x[i:i + m_val] for i in range(n - m_val + 1)])\n                count = 0\n                for i in range(len(patterns)):\n                    for j in range(len(patterns)):\n                        if np.max(np.abs(patterns[i] - patterns[j])) <= r:\n                            count += 1\n                return np.log(count / (n - m_val + 1))",
    "source_file": "core\\features\\mlfinlab_features.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "technical",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, horizons: List[int] = [5, 10, 20, 50]):\n        self.horizons = horizons",
    "source_file": "core\\features\\moe_trading.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "MomentumExpert"
  },
  {
    "name": "generate_signals",
    "category": "technical",
    "formula": "features",
    "explanation": "Generate momentum expert signals.",
    "python_code": "def generate_signals(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Generate momentum expert signals.\"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # Price momentum at multiple horizons\n        for h in self.horizons:\n            # Simple momentum\n            features[f'MOM_EXPERT_ret_{h}'] = returns.rolling(h, min_periods=1).sum()\n\n            # Momentum strength (normalized)\n            ret_h = returns.rolling(h, min_periods=1).sum()\n            vol_h = returns.rolling(h, min_periods=2).std() + 1e-10\n            features[f'MOM_EXPERT_strength_{h}'] = ret_h / vol_h\n\n        # Momentum acceleration (second derivative)\n        mom_20 = returns.rolling(20, min_periods=1).sum()\n        features['MOM_EXPERT_accel'] = mom_20.diff(5)\n\n        # Cross-sectional momentum rank (within series)\n        for h in [10, 20]:\n            ret_h = returns.rolling(h, min_periods=1).sum()\n            features[f'MOM_EXPERT_rank_{h}'] = ret_h.rolling(60, min_periods=10).apply(\n                lambda x: stats.percentileofscore(x, x.iloc[-1]) / 100 if len(x) > 1 else 0.5,\n                raw=False\n            )\n\n        # Trend following signal\n        ma_short = close.rolling(10, min_periods=1).mean()\n        ma_long = close.rolling(50, min_periods=1).mean()\n        features['MOM_EXPERT_trend'] = (ma_short - ma_long) / (ma_long + 1e-10)\n\n        # Expert confidence (based on trend consistency)\n        pos_returns = (returns > 0).rolling(20, min_periods=1).mean()\n        features['MOM_EXPERT_confidence'] = 2 * pos_returns - 1  # -1 to 1\n\n        return features",
    "source_file": "core\\features\\moe_trading.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "MomentumExpert"
  },
  {
    "name": "_estimate_halflife",
    "category": "technical",
    "formula": "halflife.fillna(halflife.median())",
    "explanation": "Estimate mean-reversion half-life using rolling regression.",
    "python_code": "def _estimate_halflife(self, prices: pd.Series, window: int) -> pd.Series:\n        \"\"\"Estimate mean-reversion half-life using rolling regression.\"\"\"\n        halflife = pd.Series(index=prices.index, dtype=float)\n\n        for i in range(window, len(prices)):\n            y = prices.iloc[i-window:i].diff().dropna().values\n            x = prices.iloc[i-window:i-1].values\n            if len(x) > 2 and np.std(x) > 0:\n                try:\n                    slope, _, _, _, _ = stats.linregress(x, y)\n                    if slope < 0:\n                        halflife.iloc[i] = -np.log(2) / slope\n                    else:\n                        halflife.iloc[i] = np.nan\n                except:\n                    halflife.iloc[i] = np.nan\n            else:\n                halflife.iloc[i] = np.nan\n\n        return halflife.fillna(halflife.median())",
    "source_file": "core\\features\\moe_trading.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "MeanReversionExpert"
  },
  {
    "name": "generate_signals",
    "category": "feature_engineering",
    "formula": "0 | stats.linregress(range(len(x)), x)[0] | features",
    "explanation": "Generate trend expert signals.",
    "python_code": "def generate_signals(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Generate trend expert signals.\"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        high = df.get('high', close)\n        low = df.get('low', close)\n        returns = close.pct_change()\n\n        # ADX (Average Directional Index)\n        tr = np.maximum(high - low,\n                        np.maximum(np.abs(high - close.shift(1)),\n                                   np.abs(low - close.shift(1))))\n\n        dm_plus = np.where((high - high.shift(1)) > (low.shift(1) - low),\n                           np.maximum(high - high.shift(1), 0), 0)\n        dm_minus = np.where((low.shift(1) - low) > (high - high.shift(1)),\n                            np.maximum(low.shift(1) - low, 0), 0)\n\n        dm_plus = pd.Series(dm_plus, index=df.index)\n        dm_minus = pd.Series(dm_minus, index=df.index)\n        tr = pd.Series(tr, index=df.index)\n\n        atr_14 = tr.rolling(14, min_periods=1).mean()\n        di_plus = 100 * dm_plus.rolling(14, min_periods=1).mean() / (atr_14 + 1e-10)\n        di_minus = 100 * dm_minus.rolling(14, min_periods=1).mean() / (atr_14 + 1e-10)\n\n        dx = 100 * np.abs(di_plus - di_minus) / (di_plus + di_minus + 1e-10)\n        adx = pd.Series(dx, index=df.index).rolling(14, min_periods=1).mean()\n\n        features['TREND_EXPERT_adx'] = adx / 100  # Normalize to 0-1\n        features['TREND_EXPERT_direction'] = np.sign(di_plus - di_minus)\n\n        # Moving average alignment\n        ma_10 = close.rolling(10, min_periods=1).mean()\n        ma_20 = close.rolling(20, min_periods=1).mean()\n        ma_50 = close.rolling(50, min_periods=1).mean()\n\n        # Alignment score: +1 if all MAs aligned bullish, -1 if bearish\n        bull_align = (ma_10 > ma_20) & (ma_20 > ma_50)\n        bear_align = (ma_10 < ma_20) & (ma_20 < ma_50)\n        features['TREND_EXPERT_align'] = np.where(bull_align, 1, np.where(bear_align, -1, 0))\n\n        # Trend persistence (runs test)\n        signs = np.sign(returns)\n        run_length = signs.groupby((signs != signs.shift()).cumsum()).cumcount() + 1\n        features['TREND_EXPERT_persist'] = run_length.rolling(20, min_periods=1).mean() / 10\n\n        # Trend strength (slope of regression)\n        def calc_slope(x):\n            if len(x) < 2:\n                return 0\n            return stats.linregress(range(len(x)), x)[0]\n\n        features['TREND_EXPERT_slope'] = close.rolling(20, min_periods=2).apply(calc_slope, raw=True)\n        features['TREND_EXPERT_slope'] = features['TREND_EXPERT_slope'] / (close + 1e-10)\n\n        # Expert confidence (strong ADX = confident)\n        features['TREND_EXPERT_confidence'] = (adx / 50).clip(0, 1)\n\n        return features",
    "source_file": "core\\features\\moe_trading.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "TrendExpert"
  },
  {
    "name": "__init__",
    "category": "feature_engineering",
    "formula": "lower = more selective)",
    "explanation": "Initialize gating network.\n\nArgs:\n    temperature: Softmax temperature (lower = more selective)",
    "python_code": "def __init__(self, temperature: float = 1.0):\n        \"\"\"\n        Initialize gating network.\n\n        Args:\n            temperature: Softmax temperature (lower = more selective)\n        \"\"\"\n        self.temperature = temperature",
    "source_file": "core\\features\\moe_trading.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "GatingNetwork"
  },
  {
    "name": "compute_gates",
    "category": "statistical",
    "formula": "pd.DataFrame(",
    "explanation": "Compute gating weights from expert confidences.\n\nArgs:\n    expert_confidences: DataFrame with confidence columns from each expert\n\nReturns:\n    DataFrame with normalized gate weights",
    "python_code": "def compute_gates(self, expert_confidences: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Compute gating weights from expert confidences.\n\n        Args:\n            expert_confidences: DataFrame with confidence columns from each expert\n\n        Returns:\n            DataFrame with normalized gate weights\n        \"\"\"\n        # Softmax over expert confidences\n        confidences = expert_confidences.fillna(0.5).values\n        gates = softmax(confidences / self.temperature, axis=1)\n\n        return pd.DataFrame(\n            gates,\n            index=expert_confidences.index,\n            columns=[c.replace('confidence', 'gate') for c in expert_confidences.columns]\n        )",
    "source_file": "core\\features\\moe_trading.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "GatingNetwork"
  },
  {
    "name": "generate_all",
    "category": "feature_engineering",
    "formula": "result",
    "explanation": "Generate all MOE features.\n\nArgs:\n    df: DataFrame with OHLCV data\n\nReturns:\n    DataFrame with 20 MOE features",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all MOE features.\n\n        Args:\n            df: DataFrame with OHLCV data\n\n        Returns:\n            DataFrame with 20 MOE features\n        \"\"\"\n        # Validate input\n        if 'close' not in df.columns:\n            raise ValueError(\"Missing required column: 'close'\")\n\n        df = df.copy()\n        if 'open' not in df.columns:\n            df['open'] = df['close'].shift(1).fillna(df['close'])\n        if 'high' not in df.columns:\n            df['high'] = df['close']\n        if 'low' not in df.columns:\n            df['low'] = df['close']\n\n        # Generate expert signals\n        mom_features = self.momentum_expert.generate_signals(df)\n        mr_features = self.mr_expert.generate_signals(df)\n        vol_features = self.vol_expert.generate_signals(df)\n        trend_features = self.trend_expert.generate_signals(df)\n\n        # Extract key signals from each expert (for final output)\n        result = pd.DataFrame(index=df.index)\n\n        # Momentum Expert summary (4 features)\n        result['MOE_MOM_signal'] = mom_features['MOM_EXPERT_trend']\n        result['MOE_MOM_strength'] = mom_features.get('MOM_EXPERT_strength_20', mom_features['MOM_EXPERT_trend'])\n        result['MOE_MOM_accel'] = mom_features['MOM_EXPERT_accel']\n        result['MOE_MOM_conf'] = mom_features['MOM_EXPERT_confidence']\n\n        # Mean Reversion Expert summary (3 features)\n        result['MOE_MR_signal'] = mr_features['MR_EXPERT_zscore_20']\n        result['MOE_MR_bb'] = mr_features['MR_EXPERT_bb_pos']\n        result['MOE_MR_conf'] = mr_features['MR_EXPERT_confidence']\n\n        # Volatility Expert summary (3 features)\n        result['MOE_VOL_term'] = vol_features['VOL_EXPERT_term_short']\n        result['MOE_VOL_zscore'] = vol_features['VOL_EXPERT_zscore']\n        result['MOE_VOL_conf'] = vol_features['VOL_EXPERT_confidence']\n\n        # Trend Expert summary (3 features)\n        result['MOE_TREND_adx'] = trend_features['TREND_EXPERT_adx']\n        result['MOE_TREND_align'] = trend_features['TREND_EXPERT_align']\n        result['MOE_TREND_conf'] = trend_features['TREND_EXPERT_confidence']\n\n        # Gating weights (4 features)\n        confidences = pd.DataFrame({\n            'MOM_confidence': result['MOE_MOM_conf'],\n            'MR_confidence': result['MOE_MR_conf'],\n            'VOL_confidence': result['MOE_VOL_conf'],\n            'TREND_confidence': result['MOE_TREND_conf']\n        })\n        gates = self.gating.compute_gates(confidences)\n        result['MOE_GATE_momentum'] = gates.iloc[:, 0]\n        result['MOE_GATE_meanrev'] = gates.iloc[:, 1]\n        result['MOE_GATE_volatility'] = gates.iloc[:, 2]\n        result['MOE_GATE_trend'] = gates.iloc[:, 3]\n\n        # Combined MOE output (3 features)\n        # Weighted sum of expert signals\n        result['MOE_COMBINED_signal'] = (\n            result['MOE_GATE_momentum'] * result['MOE_MOM_signal'] +\n            result['MOE_GATE_meanrev'] * result['M",
    "source_file": "core\\features\\moe_trading.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "MixtureOfExpertsFeatures"
  },
  {
    "name": "get_feature_names",
    "category": "feature_engineering",
    "formula": "[",
    "explanation": "Get list of all feature names.",
    "python_code": "def get_feature_names(self) -> List[str]:\n        \"\"\"Get list of all feature names.\"\"\"\n        return [\n            # Momentum (4)\n            'MOE_MOM_signal', 'MOE_MOM_strength', 'MOE_MOM_accel', 'MOE_MOM_conf',\n            # Mean Reversion (3)\n            'MOE_MR_signal', 'MOE_MR_bb', 'MOE_MR_conf',\n            # Volatility (3)\n            'MOE_VOL_term', 'MOE_VOL_zscore', 'MOE_VOL_conf',\n            # Trend (3)\n            'MOE_TREND_adx', 'MOE_TREND_align', 'MOE_TREND_conf',\n            # Gating (4)\n            'MOE_GATE_momentum', 'MOE_GATE_meanrev', 'MOE_GATE_volatility', 'MOE_GATE_trend',\n            # Combined (3)\n            'MOE_COMBINED_signal', 'MOE_AGREEMENT', 'MOE_DOMINANT'\n        ]",
    "source_file": "core\\features\\moe_trading.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "MixtureOfExpertsFeatures"
  },
  {
    "name": "get_citations",
    "category": "feature_engineering",
    "formula": "{ | with MOE architecture.\"\"\",",
    "explanation": "Get academic citations for MOE trading.",
    "python_code": "def get_citations() -> Dict[str, str]:\n        \"\"\"Get academic citations for MOE trading.\"\"\"\n        return {\n            'MIGA': \"\"\"Zou, W. et al. (2024). \"Trading like Gurus: Mixture of Instructions\n                       for Profitable Intelligent Trading.\" arXiv.\n                       Key finding: 24% excess annual return with MOE architecture.\"\"\",\n            'MOE_Original': \"\"\"Jacobs, R.A. et al. (1991). \"Adaptive Mixtures of Local Experts\"\n                              Neural Computation, 3(1), 79-87.\n                              Foundation paper for mixture of experts.\"\"\",\n            'Sparse_MOE': \"\"\"Shazeer, N. et al. (2017). \"Outrageously Large Neural Networks:\n                            The Sparsely-Gated Mixture-of-Experts Layer\" ICLR.\n                            Modern sparse gating mechanisms.\"\"\"\n        }",
    "source_file": "core\\features\\moe_trading.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "MixtureOfExpertsFeatures"
  },
  {
    "name": "generate_moe_features",
    "category": "feature_engineering",
    "formula": "generator.generate_all(df)",
    "explanation": "Generate MOE trading features.\n\nArgs:\n    df: OHLCV DataFrame\n\nReturns:\n    DataFrame with 20 MOE features",
    "python_code": "def generate_moe_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generate MOE trading features.\n\n    Args:\n        df: OHLCV DataFrame\n\n    Returns:\n        DataFrame with 20 MOE features\n    \"\"\"\n    generator = MixtureOfExpertsFeatures()\n    return generator.generate_all(df)",
    "source_file": "core\\features\\moe_trading.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": null
  },
  {
    "name": "calc_slope",
    "category": "feature_engineering",
    "formula": "0 | stats.linregress(range(len(x)), x)[0]",
    "explanation": "",
    "python_code": "def calc_slope(x):\n            if len(x) < 2:\n                return 0\n            return stats.linregress(range(len(x)), x)[0]",
    "source_file": "core\\features\\moe_trading.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self):\n        self.signals = {}",
    "source_file": "core\\features\\renaissance.py",
    "academic_reference": null,
    "class_name": "RenaissanceSignalGenerator"
  },
  {
    "name": "generate_all_signals",
    "category": "feature_engineering",
    "formula": "df",
    "explanation": "Generate all Renaissance-style signals",
    "python_code": "def generate_all_signals(self, data: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Generate all Renaissance-style signals\"\"\"\n        df = data.copy()\n\n        # Ensure we have required columns\n        if 'price' not in df.columns and 'bid' in df.columns and 'ask' in df.columns:\n            df['price'] = (df['bid'] + df['ask']) / 2\n\n        # ========================================\n        # TREND SIGNALS (10 signals)\n        # ========================================\n        df = self._trend_signals(df)\n\n        # ========================================\n        # MEAN REVERSION SIGNALS (10 signals)\n        # ========================================\n        df = self._mean_reversion_signals(df)\n\n        # ========================================\n        # MOMENTUM SIGNALS (10 signals)\n        # ========================================\n        df = self._momentum_signals(df)\n\n        # ========================================\n        # VOLATILITY SIGNALS (10 signals)\n        # ========================================\n        df = self._volatility_signals(df)\n\n        # ========================================\n        # MICROSTRUCTURE SIGNALS (10 signals)\n        # ========================================\n        df = self._microstructure_signals(df)\n\n        # Store all signal columns\n        signal_cols = [col for col in df.columns if col.startswith('signal_')]\n        self.signals = signal_cols\n\n        return df",
    "source_file": "core\\features\\renaissance.py",
    "academic_reference": null,
    "class_name": "RenaissanceSignalGenerator"
  },
  {
    "name": "_trend_signals",
    "category": "feature_engineering",
    "formula": "pd.Series(slopes, index=series.index) | df",
    "explanation": "Trend-following signals (10)",
    "python_code": "def _trend_signals(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Trend-following signals (10)\"\"\"\n\n        # Signal 1: MA crossover (multiple periods)\n        df['signal_ma_cross_20_50'] = np.where(\n            df['price'].rolling(20).mean() > df['price'].rolling(50).mean(), 1, -1\n        )\n\n        df['signal_ma_cross_10_30'] = np.where(\n            df['price'].rolling(10).mean() > df['price'].rolling(30).mean(), 1, -1\n        )\n\n        df['signal_ma_cross_5_20'] = np.where(\n            df['price'].rolling(5).mean() > df['price'].rolling(20).mean(), 1, -1\n        )\n\n        # Signal 2: Price vs MA\n        ma_50 = df['price'].rolling(50).mean()\n        df['signal_price_vs_ma50'] = np.where(df['price'] > ma_50, 1, -1)\n\n        ma_100 = df['price'].rolling(100).mean()\n        df['signal_price_vs_ma100'] = np.where(df['price'] > ma_100, 1, -1)\n\n        # Signal 3: MA slope\n        ma_20 = df['price'].rolling(20).mean()\n        df['signal_ma_slope_20'] = np.where(ma_20.diff() > 0, 1, -1)\n\n        df['signal_ma_slope_50'] = np.where(\n            df['price'].rolling(50).mean().diff() > 0, 1, -1\n        )\n\n        # Signal 4: Multiple timeframe alignment\n        ma_5 = df['price'].rolling(5).mean()\n        ma_10 = df['price'].rolling(10).mean()\n        ma_20 = df['price'].rolling(20).mean()\n\n        bullish_align = (df['price'] > ma_5) & (ma_5 > ma_10) & (ma_10 > ma_20)\n        bearish_align = (df['price'] < ma_5) & (ma_5 < ma_10) & (ma_10 < ma_20)\n\n        df['signal_timeframe_align'] = np.where(bullish_align, 1,\n                                                np.where(bearish_align, -1, 0))\n\n        # Signal 5: Linear regression slope\n        def rolling_linreg_slope(series, window=20):\n            slopes = []\n            for i in range(len(series)):\n                if i < window:\n                    slopes.append(0)\n                else:\n                    y = series.iloc[i-window:i].values\n                    x = np.arange(window)\n                    slope, _ = np.polyfit(x, y, 1)\n                    slopes.append(slope)\n            return pd.Series(slopes, index=series.index)\n\n        df['signal_linreg_slope'] = np.where(\n            rolling_linreg_slope(df['price']) > 0, 1, -1\n        )\n\n        # Signal 6: Exponential MA crossover\n        ema_12 = df['price'].ewm(span=12).mean()\n        ema_26 = df['price'].ewm(span=26).mean()\n        df['signal_ema_cross_12_26'] = np.where(ema_12 > ema_26, 1, -1)\n\n        return df",
    "source_file": "core\\features\\renaissance.py",
    "academic_reference": null,
    "class_name": "RenaissanceSignalGenerator"
  },
  {
    "name": "_mean_reversion_signals",
    "category": "technical",
    "formula": "series.rolling(window).apply( | df",
    "explanation": "Mean reversion signals (10)",
    "python_code": "def _mean_reversion_signals(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Mean reversion signals (10)\"\"\"\n\n        # Signal 1: Z-score reversion\n        ma_20 = df['price'].rolling(20).mean()\n        std_20 = df['price'].rolling(20).std()\n        z_score = (df['price'] - ma_20) / std_20\n\n        df['signal_zscore_revert'] = np.where(z_score < -2, 1,  # Oversold\n                                              np.where(z_score > 2, -1, 0))  # Overbought\n\n        df['signal_zscore_mild'] = np.where(z_score < -1, 1,\n                                           np.where(z_score > 1, -1, 0))\n\n        # Signal 2: Bollinger Bands\n        bb_upper = ma_20 + 2 * std_20\n        bb_lower = ma_20 - 2 * std_20\n\n        df['signal_bb_revert'] = np.where(df['price'] < bb_lower, 1,\n                                          np.where(df['price'] > bb_upper, -1, 0))\n\n        # Signal 3: RSI divergence\n        delta = df['price'].diff()\n        gain = (delta.where(delta > 0, 0)).rolling(14).mean()\n        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n        rs = gain / loss\n        rsi = 100 - (100 / (1 + rs))\n\n        df['signal_rsi'] = np.where(rsi < 30, 1,  # Oversold\n                                    np.where(rsi > 70, -1, 0))  # Overbought\n\n        df['signal_rsi_extreme'] = np.where(rsi < 20, 1,\n                                           np.where(rsi > 80, -1, 0))\n\n        # Signal 4: Distance from MA (percent)\n        distance_pct = (df['price'] - ma_20) / ma_20\n\n        df['signal_distance_revert'] = np.where(distance_pct < -0.01, 1,\n                                               np.where(distance_pct > 0.01, -1, 0))\n\n        # Signal 5: Return reversion\n        returns_5 = df['price'].pct_change(5)\n        df['signal_return_revert'] = np.where(returns_5 < -0.01, 1,\n                                              np.where(returns_5 > 0.01, -1, 0))\n\n        # Signal 6: Stochastic oscillator\n        low_14 = df['price'].rolling(14).min()\n        high_14 = df['price'].rolling(14).max()\n        stoch = 100 * (df['price'] - low_14) / (high_14 - low_14)\n\n        df['signal_stoch'] = np.where(stoch < 20, 1,\n                                      np.where(stoch > 80, -1, 0))\n\n        # Signal 7: Williams %R\n        williams_r = -100 * (high_14 - df['price']) / (high_14 - low_14)\n        df['signal_williams'] = np.where(williams_r < -80, 1,\n                                        np.where(williams_r > -20, -1, 0))\n\n        # Signal 8: Percent rank reversion\n        def percent_rank(series, window=50):\n            return series.rolling(window).apply(\n                lambda x: (x[-1] > x[:-1]).sum() / (len(x) - 1) if len(x) > 1 else 0.5,\n                raw=True\n            )\n\n        pct_rank = percent_rank(df['price'])\n        df['signal_pctrank_revert'] = np.where(pct_rank < 0.2, 1,\n                                              np.where(pct_rank > 0.8, -1, 0))\n\n        return df",
    "source_file": "core\\features\\renaissance.py",
    "academic_reference": "Poterba & Summers (1988) 'Mean Reversion' JFE",
    "class_name": "RenaissanceSignalGenerator"
  },
  {
    "name": "_momentum_signals",
    "category": "technical",
    "formula": "df",
    "explanation": "Momentum signals (10)",
    "python_code": "def _momentum_signals(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Momentum signals (10)\"\"\"\n\n        # Signal 1: Rate of change\n        roc_10 = df['price'].pct_change(10)\n        df['signal_roc_10'] = np.where(roc_10 > 0, 1, -1)\n\n        roc_20 = df['price'].pct_change(20)\n        df['signal_roc_20'] = np.where(roc_20 > 0, 1, -1)\n\n        # Signal 2: Acceleration\n        velocity = df['price'].diff()\n        acceleration = velocity.diff()\n        df['signal_acceleration'] = np.where(acceleration > 0, 1, -1)\n\n        # Signal 3: MACD\n        ema_12 = df['price'].ewm(span=12).mean()\n        ema_26 = df['price'].ewm(span=26).mean()\n        macd = ema_12 - ema_26\n        signal_line = macd.ewm(span=9).mean()\n\n        df['signal_macd'] = np.where(macd > signal_line, 1, -1)\n\n        # Signal 4: Momentum strength\n        momentum_20 = df['price'] - df['price'].shift(20)\n        df['signal_momentum_strength'] = np.where(momentum_20 > 0, 1, -1)\n\n        # Signal 5: Trix (triple EMA)\n        ema1 = df['price'].ewm(span=15).mean()\n        ema2 = ema1.ewm(span=15).mean()\n        ema3 = ema2.ewm(span=15).mean()\n        trix = ema3.pct_change()\n\n        df['signal_trix'] = np.where(trix > 0, 1, -1)\n\n        # Signal 6: Consecutive returns\n        returns = df['price'].pct_change()\n        consecutive_up = (returns > 0).rolling(3).sum()\n        df['signal_consecutive'] = np.where(consecutive_up >= 2, 1,\n                                           np.where(consecutive_up <= 1, -1, 0))\n\n        # Signal 7: Volume-weighted momentum (if volume available)\n        if 'volume' in df.columns:\n            vwm = (df['price'].diff() * df['volume']).rolling(20).sum()\n            df['signal_volume_momentum'] = np.where(vwm > 0, 1, -1)\n        else:\n            df['signal_volume_momentum'] = 0\n\n        # Signal 8: Relative momentum\n        returns_10 = df['price'].pct_change(10)\n        returns_20 = df['price'].pct_change(20)\n        df['signal_relative_momentum'] = np.where(returns_10 > returns_20, 1, -1)\n\n        # Signal 9: Elder Ray Index\n        ema_13 = df['price'].ewm(span=13).mean()\n        bull_power = df['price'] - ema_13  # Using price as proxy for high\n        df['signal_elder_ray'] = np.where(bull_power > 0, 1, -1)\n\n        # Signal 10: Commodity Channel Index (CCI)\n        typical_price = df['price']  # Simplified (usually (H+L+C)/3)\n        sma_20 = typical_price.rolling(20).mean()\n        mad = (typical_price - sma_20).abs().rolling(20).mean()\n        cci = (typical_price - sma_20) / (0.015 * mad)\n\n        df['signal_cci'] = np.where(cci > 100, 1,\n                                    np.where(cci < -100, -1, 0))\n\n        return df",
    "source_file": "core\\features\\renaissance.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "RenaissanceSignalGenerator"
  },
  {
    "name": "_volatility_signals",
    "category": "volatility",
    "formula": "df",
    "explanation": "Volatility-based signals (10)",
    "python_code": "def _volatility_signals(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Volatility-based signals (10)\"\"\"\n\n        returns = df['price'].pct_change()\n\n        # Signal 1: Volatility regime\n        vol_20 = returns.rolling(20).std()\n        vol_50 = returns.rolling(50).std()\n\n        df['signal_vol_regime'] = np.where(vol_20 < vol_50, 1, -1)  # Low vol = bullish\n\n        # Signal 2: Volatility breakout\n        vol_threshold = vol_20.rolling(50).quantile(0.75)\n        df['signal_vol_breakout'] = np.where(vol_20 > vol_threshold, 1, 0)\n\n        # Signal 3: ATR (Average True Range)\n        tr = df['price'].diff().abs()\n        atr = tr.rolling(14).mean()\n        atr_signal = atr / df['price']  # Normalized\n\n        df['signal_atr'] = np.where(atr_signal < atr_signal.rolling(50).mean(), 1, -1)\n\n        # Signal 4: Bollinger Band width\n        ma_20 = df['price'].rolling(20).mean()\n        std_20 = df['price'].rolling(20).std()\n        bb_width = (4 * std_20) / ma_20\n\n        df['signal_bb_width'] = np.where(bb_width < bb_width.rolling(50).quantile(0.25), 1, 0)\n\n        # Signal 5: Historical volatility percentile\n        vol_percentile = vol_20.rolling(100).apply(\n            lambda x: (x[-1] > x).sum() / len(x) if len(x) > 0 else 0.5,\n            raw=True\n        )\n        df['signal_vol_percentile'] = np.where(vol_percentile < 0.3, 1,\n                                              np.where(vol_percentile > 0.7, -1, 0))\n\n        # Signal 6: Volatility mean reversion\n        vol_zscore = (vol_20 - vol_20.rolling(50).mean()) / vol_20.rolling(50).std()\n        df['signal_vol_revert'] = np.where(vol_zscore < -1, 1,\n                                          np.where(vol_zscore > 1, -1, 0))\n\n        # Signal 7: Keltner Channels\n        ema_20 = df['price'].ewm(span=20).mean()\n        kelt_upper = ema_20 + 2 * atr\n        kelt_lower = ema_20 - 2 * atr\n\n        df['signal_keltner'] = np.where(df['price'] < kelt_lower, 1,\n                                       np.where(df['price'] > kelt_upper, -1, 0))\n\n        # Signal 8: Donchian Channels\n        high_20 = df['price'].rolling(20).max()\n        low_20 = df['price'].rolling(20).min()\n\n        df['signal_donchian'] = np.where(df['price'] == high_20, 1,\n                                        np.where(df['price'] == low_20, -1, 0))\n\n        # Signal 9: Chaikin Volatility\n        high_low_range = (df['price'].rolling(10).max() - df['price'].rolling(10).min())\n        chaikin_vol = high_low_range.pct_change(10)\n\n        df['signal_chaikin_vol'] = np.where(chaikin_vol < 0, 1, -1)\n\n        # Signal 10: Volatility skew\n        recent_vol = returns.rolling(10).std()\n        longer_vol = returns.rolling(30).std()\n\n        df['signal_vol_skew'] = np.where(recent_vol < longer_vol, 1, -1)\n\n        return df",
    "source_file": "core\\features\\renaissance.py",
    "academic_reference": null,
    "class_name": "RenaissanceSignalGenerator"
  },
  {
    "name": "_microstructure_signals",
    "category": "feature_engineering",
    "formula": "(overnight effect) | df",
    "explanation": "Market microstructure signals (10)",
    "python_code": "def _microstructure_signals(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Market microstructure signals (10)\"\"\"\n\n        # Signal 1: Bid-ask spread\n        if 'bid' in df.columns and 'ask' in df.columns:\n            spread = df['ask'] - df['bid']\n            spread_ma = spread.rolling(20).mean()\n\n            df['signal_spread'] = np.where(spread < spread_ma, 1, -1)  # Tight spread = bullish\n\n            # Signal 2: Spread volatility\n            spread_vol = spread.rolling(20).std()\n            df['signal_spread_vol'] = np.where(spread_vol < spread_vol.rolling(50).mean(), 1, -1)\n\n            # Signal 3: Mid-price momentum\n            mid = (df['bid'] + df['ask']) / 2\n            mid_momentum = mid.diff()\n            df['signal_mid_momentum'] = np.where(mid_momentum > 0, 1, -1)\n\n            # Signal 4: Quote intensity (if available)\n            # Placeholder: use price change frequency\n            price_changes = (df['price'].diff() != 0).rolling(20).sum()\n            df['signal_quote_intensity'] = np.where(\n                price_changes > price_changes.rolling(50).mean(), 1, -1\n            )\n\n        else:\n            # Fallback if bid/ask not available\n            df['signal_spread'] = 0\n            df['signal_spread_vol'] = 0\n            df['signal_mid_momentum'] = 0\n            df['signal_quote_intensity'] = 0\n\n        # Signal 5: Price reversal detection\n        # Find local peaks and troughs\n        prices = df['price'].values\n        peaks, _ = find_peaks(prices, distance=10)\n        troughs, _ = find_peaks(-prices, distance=10)\n\n        df['signal_reversal'] = 0\n        if len(peaks) > 0:\n            df.loc[df.index[peaks], 'signal_reversal'] = -1  # Sell at peaks\n        if len(troughs) > 0:\n            df.loc[df.index[troughs], 'signal_reversal'] = 1  # Buy at troughs\n\n        # Signal 6: Time since last trade (tick frequency)\n        try:\n            if 'timestamp' in df.columns:\n                time_series = pd.to_datetime(df['timestamp'])\n                df['time_diff'] = time_series.diff().dt.total_seconds()\n            elif pd.api.types.is_datetime64_any_dtype(df.index):\n                df['time_diff'] = df.index.to_series().diff().dt.total_seconds()\n            else:\n                df['time_diff'] = 1  # Default to constant interval\n            time_diff_ma = df['time_diff'].rolling(20).mean()\n            df['signal_tick_frequency'] = np.where(df['time_diff'] < time_diff_ma, 1, -1)\n        except:\n            df['signal_tick_frequency'] = 0\n\n        # Signal 7: Price clustering\n        # Check if price is at round numbers\n        price_decimal = (df['price'] * 10000) % 10\n        df['signal_price_cluster'] = np.where(price_decimal < 2, -1, 0)  # Resistance at round numbers\n\n        # Signal 8: Order flow imbalance (simplified)\n        # Use price momentum as proxy for buy/sell pressure\n        flow = df['price'].diff().rolling(20).sum()\n        df['signal_order_flow'] = np.where(flow > 0, 1, -1)\n\n        # Signal 9: Roll retu",
    "source_file": "core\\features\\renaissance.py",
    "academic_reference": null,
    "class_name": "RenaissanceSignalGenerator"
  },
  {
    "name": "ensemble_signals",
    "category": "machine_learning",
    "formula": "df | df",
    "explanation": "Combine weak signals into ensemble prediction\n\nArgs:\n    method: 'average', 'weighted', 'vote'",
    "python_code": "def ensemble_signals(self, df: pd.DataFrame, method='average') -> pd.DataFrame:\n        \"\"\"\n        Combine weak signals into ensemble prediction\n\n        Args:\n            method: 'average', 'weighted', 'vote'\n        \"\"\"\n        signal_cols = [col for col in df.columns if col.startswith('signal_')]\n\n        if not signal_cols:\n            df['ensemble_signal'] = 0\n            df['ensemble_confidence'] = 0\n            return df\n\n        # Get all signals as matrix\n        signals_matrix = df[signal_cols].fillna(0).values\n\n        if method == 'average':\n            # Simple average\n            ensemble = signals_matrix.mean(axis=1)\n\n        elif method == 'weighted':\n            # Weight by signal strength (simplified - equal weights for now)\n            weights = np.ones(len(signal_cols)) / len(signal_cols)\n            ensemble = (signals_matrix * weights).sum(axis=1)\n\n        elif method == 'vote':\n            # Majority vote\n            ensemble = np.where(\n                (signals_matrix == 1).sum(axis=1) > (signals_matrix == -1).sum(axis=1),\n                1, -1\n            )\n\n        # Convert to discrete signals\n        df['ensemble_signal'] = np.where(ensemble > 0.2, 1,\n                                        np.where(ensemble < -0.2, -1, 0))\n\n        # Calculate confidence (agreement among signals)\n        agreement = np.abs(signals_matrix.sum(axis=1)) / len(signal_cols)\n        df['ensemble_confidence'] = agreement\n\n        return df",
    "source_file": "core\\features\\renaissance.py",
    "academic_reference": null,
    "class_name": "RenaissanceSignalGenerator"
  },
  {
    "name": "test_renaissance_signals",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Test Renaissance signals generation",
    "python_code": "def test_renaissance_signals():\n    \"\"\"Test Renaissance signals generation\"\"\"\n    print(\"=\"*70)\n    print(\"RENAISSANCE SIGNALS TEST\")\n    print(\"=\"*70)\n\n    # Create sample data\n    dates = pd.date_range('2026-01-01', periods=1000, freq='1min')\n    price = 1.16 + np.cumsum(np.random.randn(1000) * 0.0001)\n\n    data = pd.DataFrame({\n        'timestamp': dates,\n        'price': price,\n        'bid': price - 0.0001,\n        'ask': price + 0.0001\n    })\n    data.set_index('timestamp', inplace=True)\n\n    # Generate signals\n    generator = RenaissanceSignalGenerator()\n    data_with_signals = generator.generate_all_signals(data)\n\n    # Ensemble\n    data_with_signals = generator.ensemble_signals(data_with_signals, method='average')\n\n    # Print summary\n    signal_cols = [col for col in data_with_signals.columns if col.startswith('signal_')]\n    print(f\"\\nGenerated {len(signal_cols)} weak signals\")\n\n    ensemble_signals = data_with_signals['ensemble_signal']\n    print(f\"\\nEnsemble signals:\")\n    print(f\"  Buy:  {(ensemble_signals == 1).sum()} ({(ensemble_signals == 1).sum() / len(ensemble_signals) * 100:.1f}%)\")\n    print(f\"  Sell: {(ensemble_signals == -1).sum()} ({(ensemble_signals == -1).sum() / len(ensemble_signals) * 100:.1f}%)\")\n    print(f\"  Hold: {(ensemble_signals == 0).sum()} ({(ensemble_signals == 0).sum() / len(ensemble_signals) * 100:.1f}%)\")\n\n    print(f\"\\nAvg confidence: {data_with_signals['ensemble_confidence'].mean():.2f}\")\n    print()",
    "source_file": "core\\features\\renaissance.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "rolling_linreg_slope",
    "category": "statistical",
    "formula": "pd.Series(slopes, index=series.index)",
    "explanation": "",
    "python_code": "def rolling_linreg_slope(series, window=20):\n            slopes = []\n            for i in range(len(series)):\n                if i < window:\n                    slopes.append(0)\n                else:\n                    y = series.iloc[i-window:i].values\n                    x = np.arange(window)\n                    slope, _ = np.polyfit(x, y, 1)\n                    slopes.append(slope)\n            return pd.Series(slopes, index=series.index)",
    "source_file": "core\\features\\renaissance.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "percent_rank",
    "category": "statistical",
    "formula": "series.rolling(window).apply(",
    "explanation": "",
    "python_code": "def percent_rank(series, window=50):\n            return series.rolling(window).apply(\n                lambda x: (x[-1] > x[:-1]).sum() / (len(x) - 1) if len(x) > 1 else 0.5,\n                raw=True\n            )",
    "source_file": "core\\features\\renaissance.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "to_tuple",
    "category": "feature_engineering",
    "formula": "(self.trend, self.volatility, self.momentum)",
    "explanation": "",
    "python_code": "def to_tuple(self) -> Tuple[int, int, int]:\n        return (self.trend, self.volatility, self.momentum)",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "RLState"
  },
  {
    "name": "__init__",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(\n        self,\n        alpha: float = 0.1,      # Learning rate\n        gamma: float = 0.95,     # Discount factor\n        epsilon: float = 0.1,    # Exploration rate\n        n_actions: int = 3       # Buy, Hold, Sell\n    ):\n        self.alpha = alpha\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.n_actions = n_actions\n        self.q_table = defaultdict(lambda: np.zeros(n_actions))\n        self.action_map = {0: -1, 1: 0, 2: 1}",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "QLearningTrader"
  },
  {
    "name": "discretize_state",
    "category": "deep_learning",
    "formula": "= returns[-5:].mean() if len(returns) >= 5 else 0 | > 0.001: | < -0.001:",
    "explanation": "Convert continuous market data to discrete state.",
    "python_code": "def discretize_state(self, returns: np.ndarray, volatility: float) -> RLState:\n        \"\"\"Convert continuous market data to discrete state.\"\"\"\n        # Trend: based on recent returns\n        recent_return = returns[-5:].mean() if len(returns) >= 5 else 0\n        if recent_return > 0.001:\n            trend = 1\n        elif recent_return < -0.001:\n            trend = -1\n        else:\n            trend = 0\n\n        # Volatility regime\n        if volatility < 0.01:\n            vol_state = 0\n        elif volatility < 0.02:\n            vol_state = 1\n        else:\n            vol_state = 2\n\n        # Momentum\n        if len(returns) >= 20:\n            mom = returns[-5:].sum() - returns[-20:-5].sum()\n            momentum = 1 if mom > 0.002 else (-1 if mom < -0.002 else 0)\n        else:\n            momentum = 0\n\n        return RLState(trend=trend, volatility=vol_state, momentum=momentum)",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "QLearningTrader"
  },
  {
    "name": "get_q_value",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Get Q-value for state-action pair.",
    "python_code": "def get_q_value(self, state: RLState, action: int) -> float:\n        \"\"\"Get Q-value for state-action pair.\"\"\"\n        return self.q_table[state.to_tuple()][action]",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "QLearningTrader"
  },
  {
    "name": "get_best_action",
    "category": "feature_engineering",
    "formula": "np.argmax(self.q_table[state.to_tuple()])",
    "explanation": "Select action with highest Q-value (exploitation).",
    "python_code": "def get_best_action(self, state: RLState) -> int:\n        \"\"\"Select action with highest Q-value (exploitation).\"\"\"\n        return np.argmax(self.q_table[state.to_tuple()])",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "QLearningTrader"
  },
  {
    "name": "get_action_signal",
    "category": "deep_learning",
    "formula": "probs[2] - probs[0]",
    "explanation": "Convert best action to trading signal [-1, 1].",
    "python_code": "def get_action_signal(self, state: RLState) -> float:\n        \"\"\"Convert best action to trading signal [-1, 1].\"\"\"\n        q_values = self.q_table[state.to_tuple()]\n        # Softmax to get action probabilities\n        exp_q = np.exp(q_values - np.max(q_values))\n        probs = exp_q / exp_q.sum()\n        # Weighted signal: -1 * P(sell) + 0 * P(hold) + 1 * P(buy)\n        return probs[2] - probs[0]",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "QLearningTrader"
  },
  {
    "name": "update",
    "category": "feature_engineering",
    "formula": "td_error",
    "explanation": "Q-Learning update.\n\nReturns TD error for analysis.",
    "python_code": "def update(self, state: RLState, action: int, reward: float,\n               next_state: RLState) -> float:\n        \"\"\"\n        Q-Learning update.\n\n        Returns TD error for analysis.\n        \"\"\"\n        current_q = self.q_table[state.to_tuple()][action]\n        max_next_q = np.max(self.q_table[next_state.to_tuple()])\n\n        # TD error\n        td_error = reward + self.gamma * max_next_q - current_q\n\n        # Update Q-value\n        self.q_table[state.to_tuple()][action] += self.alpha * td_error\n\n        return td_error",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "QLearningTrader"
  },
  {
    "name": "epsilon_greedy_action",
    "category": "technical",
    "formula": "np.random.randint(self.n_actions) | np.argmax(self.q_table[state.to_tuple()])",
    "explanation": "Select action using epsilon-greedy policy.",
    "python_code": "def epsilon_greedy_action(self, state: RLState) -> int:\n        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n        if np.random.random() < self.epsilon:\n            return np.random.randint(self.n_actions)\n        return np.argmax(self.q_table[state.to_tuple()])",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "SARSATrader"
  },
  {
    "name": "update",
    "category": "technical",
    "formula": "td_error, self.eligibility.get(state.to_tuple(), 0)",
    "explanation": "TD() update with eligibility traces.\n\nReturns (TD error, eligibility of current state).",
    "python_code": "def update(self, state: RLState, reward: float,\n               next_state: RLState) -> Tuple[float, float]:\n        \"\"\"\n        TD() update with eligibility traces.\n\n        Returns (TD error, eligibility of current state).\n        \"\"\"\n        # TD error\n        current_v = self.v_table[state.to_tuple()]\n        next_v = self.v_table[next_state.to_tuple()]\n        td_error = reward + self.gamma * next_v - current_v\n\n        # Update eligibility trace for current state\n        self.eligibility[state.to_tuple()] += 1\n\n        # Update all states proportional to eligibility\n        for s, e in list(self.eligibility.items()):\n            self.v_table[s] += self.alpha * td_error * e\n            # Decay eligibility\n            self.eligibility[s] = self.gamma * self.lambda_ * e\n            if self.eligibility[s] < 1e-6:\n                del self.eligibility[s]\n\n        return td_error, self.eligibility.get(state.to_tuple(), 0)",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "TDLambdaTrader"
  },
  {
    "name": "get_value_signal",
    "category": "technical",
    "formula": "",
    "explanation": "Get normalized value estimate.",
    "python_code": "def get_value_signal(self, state: RLState) -> float:\n        \"\"\"Get normalized value estimate.\"\"\"\n        return self.v_table[state.to_tuple()]",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "TDLambdaTrader"
  },
  {
    "name": "update_real",
    "category": "feature_engineering",
    "formula": "td_error",
    "explanation": "Update from real experience.",
    "python_code": "def update_real(self, state: RLState, action: int, reward: float,\n                    next_state: RLState) -> float:\n        \"\"\"Update from real experience.\"\"\"\n        # Q-learning update\n        current_q = self.q_table[state.to_tuple()][action]\n        max_next_q = np.max(self.q_table[next_state.to_tuple()])\n        td_error = reward + self.gamma * max_next_q - current_q\n        self.q_table[state.to_tuple()][action] += self.alpha * td_error\n\n        # Update model\n        sa = (state.to_tuple(), action)\n        self.model[sa] = (next_state.to_tuple(), reward)\n        if sa not in self.visited_sa:\n            self.visited_sa.append(sa)\n\n        return td_error",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "DynaQTrader"
  },
  {
    "name": "planning_step",
    "category": "feature_engineering",
    "formula": "updates",
    "explanation": "Perform planning steps using model.\n\nReturns number of updates made.",
    "python_code": "def planning_step(self) -> int:\n        \"\"\"\n        Perform planning steps using model.\n\n        Returns number of updates made.\n        \"\"\"\n        updates = 0\n        for _ in range(self.n_planning):\n            if not self.visited_sa:\n                break\n\n            # Sample random previously visited state-action\n            sa = self.visited_sa[np.random.randint(len(self.visited_sa))]\n            state, action = sa\n\n            # Get simulated experience from model\n            if sa in self.model:\n                next_state, reward = self.model[sa]\n\n                # Q-learning update on simulated experience\n                current_q = self.q_table[state][action]\n                max_next_q = np.max(self.q_table[next_state])\n                td_error = reward + self.gamma * max_next_q - current_q\n                self.q_table[state][action] += self.alpha * td_error\n                updates += 1\n\n        return updates",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "DynaQTrader"
  },
  {
    "name": "__init__",
    "category": "feature_engineering",
    "formula": "higher = more conservative)",
    "explanation": "Initialize with uniform priors.\n\nArgs:\n    n_actions: Number of trading actions (sell, hold, buy)\n    prior_strength: Strength of prior (higher = more conservative)",
    "python_code": "def __init__(self, n_actions: int = 3, prior_strength: float = 1.0):\n        \"\"\"\n        Initialize with uniform priors.\n\n        Args:\n            n_actions: Number of trading actions (sell, hold, buy)\n            prior_strength: Strength of prior (higher = more conservative)\n        \"\"\"\n        self.n_actions = n_actions\n        # Beta distribution parameters (alpha, beta) for each action\n        # Start with uniform prior Beta(1, 1)\n        self.alpha = np.ones(n_actions) * prior_strength\n        self.beta = np.ones(n_actions) * prior_strength\n        self.action_counts = np.zeros(n_actions)",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "ThompsonSamplingBandit"
  },
  {
    "name": "sample_action",
    "category": "feature_engineering",
    "formula": "np.argmax(samples), samples",
    "explanation": "Thompson Sampling: sample from posterior and select best.\n\nReturns (selected action, sampled probabilities).",
    "python_code": "def sample_action(self) -> Tuple[int, np.ndarray]:\n        \"\"\"\n        Thompson Sampling: sample from posterior and select best.\n\n        Returns (selected action, sampled probabilities).\n        \"\"\"\n        # Sample from Beta posterior for each action\n        samples = np.array([\n            np.random.beta(self.alpha[i], self.beta[i])\n            for i in range(self.n_actions)\n        ])\n        return np.argmax(samples), samples",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "ThompsonSamplingBandit"
  },
  {
    "name": "update",
    "category": "statistical",
    "formula": "",
    "explanation": "Update posterior based on observed reward.\n\nArgs:\n    action: Action taken\n    reward: Binary reward (1 = success, 0 = failure)\n            or continuous reward (normalized to [0, 1])",
    "python_code": "def update(self, action: int, reward: float):\n        \"\"\"\n        Update posterior based on observed reward.\n\n        Args:\n            action: Action taken\n            reward: Binary reward (1 = success, 0 = failure)\n                    or continuous reward (normalized to [0, 1])\n        \"\"\"\n        # Clip reward to [0, 1]\n        reward = np.clip(reward, 0, 1)\n\n        # Update Beta parameters\n        self.alpha[action] += reward\n        self.beta[action] += (1 - reward)\n        self.action_counts[action] += 1",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "ThompsonSamplingBandit"
  },
  {
    "name": "get_action_signal",
    "category": "feature_engineering",
    "formula": "expected[2] - expected[0]",
    "explanation": "Get trading signal based on expected values.\n\nReturns weighted signal in [-1, 1].",
    "python_code": "def get_action_signal(self) -> float:\n        \"\"\"\n        Get trading signal based on expected values.\n\n        Returns weighted signal in [-1, 1].\n        \"\"\"\n        # Expected value of Beta distribution =  / ( + )\n        expected = self.alpha / (self.alpha + self.beta)\n        # Signal: P(buy) - P(sell), normalized\n        return expected[2] - expected[0]",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "ThompsonSamplingBandit"
  },
  {
    "name": "get_uncertainty",
    "category": "feature_engineering",
    "formula": "(self.alpha * self.beta) / (total ** 2 * (total + 1))",
    "explanation": "Get uncertainty (variance) for each action.",
    "python_code": "def get_uncertainty(self) -> np.ndarray:\n        \"\"\"Get uncertainty (variance) for each action.\"\"\"\n        # Variance of Beta =  / ((+)(++1))\n        total = self.alpha + self.beta\n        return (self.alpha * self.beta) / (total ** 2 * (total + 1))",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "ThompsonSamplingBandit"
  },
  {
    "name": "optimal_trajectory",
    "category": "volatility",
    "formula": "trajectory",
    "explanation": "Calculate optimal execution trajectory.\n\nArgs:\n    total_shares: Total shares to execute\n    n_periods: Number of trading periods\n    volatility: Price volatility ()\n\nReturns:\n    Array of shares to trade in each period",
    "python_code": "def optimal_trajectory(\n        self,\n        total_shares: float,\n        n_periods: int,\n        volatility: float\n    ) -> np.ndarray:\n        \"\"\"\n        Calculate optimal execution trajectory.\n\n        Args:\n            total_shares: Total shares to execute\n            n_periods: Number of trading periods\n            volatility: Price volatility ()\n\n        Returns:\n            Array of shares to trade in each period\n        \"\"\"\n        # Calculate \n        kappa = np.sqrt(self.lambda_ * volatility ** 2 / self.eta)\n\n        # Generate trajectory\n        T = n_periods\n        trajectory = np.zeros(n_periods)\n        remaining = total_shares\n\n        for k in range(n_periods):\n            t_k = k\n            if kappa * T < 1e-6:\n                # Small : uniform (TWAP)\n                trajectory[k] = total_shares / n_periods\n            else:\n                # Almgren-Chriss formula\n                n_k = remaining * np.sinh(kappa * (T - t_k)) / np.sinh(kappa * (T - t_k + 1))\n                trajectory[k] = n_k\n                remaining -= n_k\n\n        return trajectory",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AlmgrenChrisExecutor"
  },
  {
    "name": "expected_cost",
    "category": "execution",
    "formula": "expected, variance",
    "explanation": "Calculate expected execution cost and variance.\n\nReturns (expected_cost, variance).",
    "python_code": "def expected_cost(\n        self,\n        trajectory: np.ndarray,\n        initial_price: float,\n        volatility: float\n    ) -> Tuple[float, float]:\n        \"\"\"\n        Calculate expected execution cost and variance.\n\n        Returns (expected_cost, variance).\n        \"\"\"\n        n = len(trajectory)\n\n        # Temporary impact cost:  v_k\n        temp_cost = self.eta * np.sum(trajectory ** 2)\n\n        # Permanent impact cost:  v_kX_k\n        remaining = np.cumsum(trajectory[::-1])[::-1]\n        perm_cost = self.gamma * np.sum(trajectory * remaining)\n\n        # Variance\n        variance = volatility ** 2 * np.sum(remaining ** 2)\n\n        expected = temp_cost + perm_cost\n        return expected, variance",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "AlmgrenChrisExecutor"
  },
  {
    "name": "execution_urgency",
    "category": "execution",
    "formula": "urgency = execute faster (market moving against us) | urgency = execute slower (favorable conditions) | base_urgency * spread_factor",
    "explanation": "Calculate execution urgency signal.\n\nHigh urgency = execute faster (market moving against us)\nLow urgency = execute slower (favorable conditions)",
    "python_code": "def execution_urgency(self, volatility: float, spread: float) -> float:\n        \"\"\"\n        Calculate execution urgency signal.\n\n        High urgency = execute faster (market moving against us)\n        Low urgency = execute slower (favorable conditions)\n        \"\"\"\n        kappa = np.sqrt(self.lambda_ * volatility ** 2 / self.eta)\n\n        # Urgency based on vol-adjusted impact\n        base_urgency = kappa * volatility\n\n        # Adjust for spread (wider spread = less urgency)\n        spread_factor = 1 / (1 + spread / volatility)\n\n        return base_urgency * spread_factor",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "AlmgrenChrisExecutor"
  },
  {
    "name": "_discretize_states",
    "category": "deep_learning",
    "formula": "states",
    "explanation": "Convert price series to discrete RL states.",
    "python_code": "def _discretize_states(self, df: pd.DataFrame) -> List[RLState]:\n        \"\"\"Convert price series to discrete RL states.\"\"\"\n        returns = df['close'].pct_change().fillna(0)\n        volatility = returns.rolling(20, min_periods=2).std().fillna(0.01)\n\n        states = []\n        for i in range(len(df)):\n            if i < 20:\n                ret_arr = returns.iloc[:i+1].values if i > 0 else np.array([0])\n            else:\n                ret_arr = returns.iloc[i-20:i+1].values\n\n            state = self.q_learner.discretize_state(ret_arr, volatility.iloc[i])\n            states.append(state)\n\n        return states",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "RLFeatureGenerator"
  },
  {
    "name": "_calculate_rewards",
    "category": "reinforcement_learning",
    "formula": "returns.values",
    "explanation": "Calculate trading rewards (forward returns).",
    "python_code": "def _calculate_rewards(self, df: pd.DataFrame) -> np.ndarray:\n        \"\"\"Calculate trading rewards (forward returns).\"\"\"\n        returns = df['close'].pct_change().shift(-1).fillna(0)\n        return returns.values",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "RLFeatureGenerator"
  },
  {
    "name": "_q_learning_features",
    "category": "feature_engineering",
    "formula": "sign) | features",
    "explanation": "Generate Q-Learning based features.",
    "python_code": "def _q_learning_features(self, df: pd.DataFrame, states: List[RLState],\n                             rewards: np.ndarray) -> pd.DataFrame:\n        \"\"\"Generate Q-Learning based features.\"\"\"\n        features = pd.DataFrame(index=df.index)\n\n        signals = []\n        td_errors = []\n        q_values_buy = []\n        q_values_sell = []\n\n        for i in range(len(states) - 1):\n            state = states[i]\n            next_state = states[i + 1]\n            reward = rewards[i]\n\n            # Get signal before update\n            signal = self.q_learner.get_action_signal(state)\n            signals.append(signal)\n\n            # Get Q-values\n            q_values_buy.append(self.q_learner.get_q_value(state, 2))\n            q_values_sell.append(self.q_learner.get_q_value(state, 0))\n\n            # Determine action (based on return sign)\n            if reward > 0:\n                action = 2  # Buy was correct\n            elif reward < 0:\n                action = 0  # Sell was correct\n            else:\n                action = 1  # Hold\n\n            # Update Q-table\n            td_error = self.q_learner.update(state, action, reward * 100, next_state)\n            td_errors.append(td_error)\n\n        # Pad last values\n        signals.append(signals[-1] if signals else 0)\n        td_errors.append(0)\n        q_values_buy.append(q_values_buy[-1] if q_values_buy else 0)\n        q_values_sell.append(q_values_sell[-1] if q_values_sell else 0)\n\n        features['RL_QLEARN_SIGNAL'] = signals\n        features['RL_QLEARN_TD_ERROR'] = td_errors\n        features['RL_QLEARN_Q_BUY'] = q_values_buy\n        features['RL_QLEARN_Q_SELL'] = q_values_sell\n        features['RL_QLEARN_Q_SPREAD'] = np.array(q_values_buy) - np.array(q_values_sell)\n        features['RL_QLEARN_CONFIDENCE'] = np.abs(np.array(q_values_buy) - np.array(q_values_sell))\n\n        return features",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "RLFeatureGenerator"
  },
  {
    "name": "_sarsa_features",
    "category": "feature_engineering",
    "formula": "features",
    "explanation": "Generate SARSA-based features (safer than Q-learning).",
    "python_code": "def _sarsa_features(self, df: pd.DataFrame, states: List[RLState],\n                        rewards: np.ndarray) -> pd.DataFrame:\n        \"\"\"Generate SARSA-based features (safer than Q-learning).\"\"\"\n        features = pd.DataFrame(index=df.index)\n\n        signals = []\n        td_errors = []\n\n        prev_action = 1  # Start with hold\n\n        for i in range(len(states) - 1):\n            state = states[i]\n            next_state = states[i + 1]\n            reward = rewards[i]\n\n            # Get signal\n            signal = self.sarsa.get_action_signal(state)\n            signals.append(signal)\n\n            # Get action using policy\n            action = self.sarsa.epsilon_greedy_action(state)\n            next_action = self.sarsa.epsilon_greedy_action(next_state)\n\n            # Update (SARSA uses actual next action, not max)\n            td_error = self.sarsa.update(state, action, reward * 100,\n                                         next_state, next_action)\n            td_errors.append(td_error)\n\n        signals.append(signals[-1] if signals else 0)\n        td_errors.append(0)\n\n        features['RL_SARSA_SIGNAL'] = signals\n        features['RL_SARSA_TD_ERROR'] = td_errors\n        # SARSA vs Q-learning divergence (indicates market danger)\n        features['RL_SARSA_Q_DIV'] = (\n            np.array(features['RL_SARSA_SIGNAL']) -\n            np.array(self._q_learning_features(df, states, rewards)['RL_QLEARN_SIGNAL'])\n        )\n        features['RL_SARSA_SAFE'] = np.where(np.abs(features['RL_SARSA_Q_DIV']) > 0.2, 1, 0)\n        features['RL_SARSA_CAUTION'] = features['RL_SARSA_Q_DIV'].rolling(10, min_periods=1).std()\n\n        return features",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "RLFeatureGenerator"
  },
  {
    "name": "_td_lambda_features",
    "category": "feature_engineering",
    "formula": "features",
    "explanation": "Generate TD() eligibility trace features.",
    "python_code": "def _td_lambda_features(self, df: pd.DataFrame, states: List[RLState],\n                            rewards: np.ndarray) -> pd.DataFrame:\n        \"\"\"Generate TD() eligibility trace features.\"\"\"\n        features = pd.DataFrame(index=df.index)\n\n        values = []\n        td_errors = []\n        eligibilities = []\n\n        for i in range(len(states) - 1):\n            state = states[i]\n            next_state = states[i + 1]\n            reward = rewards[i]\n\n            # Get value estimate\n            value = self.td_lambda.get_value_signal(state)\n            values.append(value)\n\n            # Update with eligibility traces\n            td_error, eligibility = self.td_lambda.update(state, reward * 100, next_state)\n            td_errors.append(td_error)\n            eligibilities.append(eligibility)\n\n        values.append(values[-1] if values else 0)\n        td_errors.append(0)\n        eligibilities.append(0)\n\n        features['RL_TD_VALUE'] = values\n        features['RL_TD_ERROR'] = td_errors\n        features['RL_TD_ELIGIBILITY'] = eligibilities\n        features['RL_TD_CREDIT'] = np.array(td_errors) * np.array(eligibilities)\n        # Long-term vs short-term value ( effect)\n        features['RL_TD_HORIZON'] = pd.Series(values).diff(10).fillna(0).values\n\n        return features",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "RLFeatureGenerator"
  },
  {
    "name": "_dyna_features",
    "category": "feature_engineering",
    "formula": "features",
    "explanation": "Generate Dyna-Q model-based features.",
    "python_code": "def _dyna_features(self, df: pd.DataFrame, states: List[RLState],\n                       rewards: np.ndarray) -> pd.DataFrame:\n        \"\"\"Generate Dyna-Q model-based features.\"\"\"\n        features = pd.DataFrame(index=df.index)\n\n        signals = []\n        planning_counts = []\n        model_sizes = []\n\n        for i in range(len(states) - 1):\n            state = states[i]\n            next_state = states[i + 1]\n            reward = rewards[i]\n\n            # Get signal\n            signal = self.dyna.get_action_signal(state)\n            signals.append(signal)\n\n            # Determine action\n            action = 2 if reward > 0 else (0 if reward < 0 else 1)\n\n            # Real experience update\n            self.dyna.update_real(state, action, reward * 100, next_state)\n\n            # Planning (simulated experience)\n            n_updates = self.dyna.planning_step()\n            planning_counts.append(n_updates)\n            model_sizes.append(len(self.dyna.model))\n\n        signals.append(signals[-1] if signals else 0)\n        planning_counts.append(0)\n        model_sizes.append(model_sizes[-1] if model_sizes else 0)\n\n        features['RL_DYNA_SIGNAL'] = signals\n        features['RL_DYNA_PLANNING'] = planning_counts\n        features['RL_DYNA_MODEL_SIZE'] = model_sizes\n        # Model confidence: more experience = higher confidence\n        features['RL_DYNA_CONFIDENCE'] = np.log1p(model_sizes)\n        features['RL_DYNA_NOVELTY'] = 1 / (1 + np.array(model_sizes))\n\n        return features",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "RLFeatureGenerator"
  },
  {
    "name": "_thompson_features",
    "category": "feature_engineering",
    "formula": "if reward > 0: | features",
    "explanation": "Generate Thompson Sampling bandit features.",
    "python_code": "def _thompson_features(self, df: pd.DataFrame, rewards: np.ndarray) -> pd.DataFrame:\n        \"\"\"Generate Thompson Sampling bandit features.\"\"\"\n        features = pd.DataFrame(index=df.index)\n\n        signals = []\n        uncertainties = []\n        sampled_probs = []\n\n        for i in range(len(rewards)):\n            reward = rewards[i]\n\n            # Get current signal\n            signal = self.thompson.get_action_signal()\n            signals.append(signal)\n\n            # Get uncertainty\n            unc = self.thompson.get_uncertainty()\n            uncertainties.append(unc.mean())\n\n            # Sample action\n            _, probs = self.thompson.sample_action()\n            sampled_probs.append(probs[2] - probs[0])  # Buy - Sell probability\n\n            # Update based on return\n            if reward > 0:\n                self.thompson.update(2, 1)  # Buy success\n            elif reward < 0:\n                self.thompson.update(0, 1)  # Sell success\n            else:\n                self.thompson.update(1, 0.5)  # Hold neutral\n\n        features['RL_TS_SIGNAL'] = signals\n        features['RL_TS_UNCERTAINTY'] = uncertainties\n        features['RL_TS_SAMPLED'] = sampled_probs\n        features['RL_TS_EXPLORE'] = np.array(uncertainties) > np.median(uncertainties)\n        features['RL_TS_EXPLOIT'] = 1 - features['RL_TS_EXPLORE']\n\n        return features",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "RLFeatureGenerator"
  },
  {
    "name": "_execution_features",
    "category": "execution",
    "formula": "features",
    "explanation": "Generate Almgren-Chriss execution features.",
    "python_code": "def _execution_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Generate Almgren-Chriss execution features.\"\"\"\n        features = pd.DataFrame(index=df.index)\n\n        returns = df['close'].pct_change().fillna(0)\n        volatility = returns.rolling(20, min_periods=2).std().fillna(0.01)\n        spread = (df.get('high', df['close']) - df.get('low', df['close'])) / df['close']\n\n        urgency = []\n        trajectory_aggression = []\n\n        for i in range(len(df)):\n            vol = volatility.iloc[i]\n            sprd = spread.iloc[i] if i < len(spread) else 0.001\n\n            # Execution urgency\n            urg = self.executor.execution_urgency(vol, sprd)\n            urgency.append(urg)\n\n            # Optimal trajectory aggression (first period as %)\n            if vol > 0:\n                traj = self.executor.optimal_trajectory(1.0, 10, vol)\n                trajectory_aggression.append(traj[0])\n            else:\n                trajectory_aggression.append(0.1)\n\n        features['RL_EXEC_URGENCY'] = urgency\n        features['RL_EXEC_AGGRESSION'] = trajectory_aggression\n        features['RL_EXEC_VOL_ADJ'] = np.array(urgency) * volatility.values\n        features['RL_EXEC_SPREAD_ADJ'] = np.array(urgency) / (spread.values + 1e-6)\n        # Execution regime\n        features['RL_EXEC_REGIME'] = np.where(\n            np.array(urgency) > np.median(urgency), 1, 0\n        )\n\n        return features",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "RLFeatureGenerator"
  },
  {
    "name": "_kelly_rl_features",
    "category": "risk",
    "formula": "features",
    "explanation": "Generate Kelly Criterion integrated with RL signals.",
    "python_code": "def _kelly_rl_features(self, df: pd.DataFrame, rewards: np.ndarray) -> pd.DataFrame:\n        \"\"\"Generate Kelly Criterion integrated with RL signals.\"\"\"\n        features = pd.DataFrame(index=df.index)\n\n        returns = df['close'].pct_change().fillna(0)\n\n        # Rolling win probability\n        win_prob = (returns > 0).rolling(60, min_periods=10).mean().fillna(0.5)\n\n        # Rolling win/loss ratio\n        wins = returns.where(returns > 0, np.nan)\n        losses = returns.where(returns < 0, np.nan).abs()\n        avg_win = wins.rolling(60, min_periods=10).mean().fillna(0.001)\n        avg_loss = losses.rolling(60, min_periods=10).mean().fillna(0.001)\n        wl_ratio = avg_win / (avg_loss + 1e-8)\n\n        # Kelly fraction\n        q = 1 - win_prob\n        kelly = (win_prob * wl_ratio - q) / (wl_ratio + 1e-8)\n        kelly = kelly.clip(-1, 1)\n\n        features['RL_KELLY_FRAC'] = kelly\n        features['RL_KELLY_EDGE'] = win_prob * avg_win - q * avg_loss\n        features['RL_KELLY_CONFIDENCE'] = win_prob.rolling(20, min_periods=5).std()\n        # Combine Kelly with RL signal\n        features['RL_KELLY_SIGNAL'] = kelly * self.thompson.get_action_signal()\n\n        return features",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "RLFeatureGenerator"
  },
  {
    "name": "generate_all",
    "category": "feature_engineering",
    "formula": "features",
    "explanation": "Generate all RL-based features.\n\nArgs:\n    df: DataFrame with OHLCV data\n\nReturns:\n    DataFrame with 35 RL features",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all RL-based features.\n\n        Args:\n            df: DataFrame with OHLCV data\n\n        Returns:\n            DataFrame with 35 RL features\n        \"\"\"\n        if 'close' not in df.columns:\n            raise ValueError(\"Missing required column: 'close'\")\n\n        df = df.copy()\n        if 'open' not in df.columns:\n            df['open'] = df['close'].shift(1).fillna(df['close'])\n        if 'high' not in df.columns:\n            df['high'] = df['close']\n        if 'low' not in df.columns:\n            df['low'] = df['close']\n\n        # Discretize states\n        states = self._discretize_states(df)\n        rewards = self._calculate_rewards(df)\n\n        # Generate all feature groups\n        q_features = self._q_learning_features(df, states, rewards)\n        sarsa_features = self._sarsa_features(df, states, rewards)\n        td_features = self._td_lambda_features(df, states, rewards)\n        dyna_features = self._dyna_features(df, states, rewards)\n        thompson_features = self._thompson_features(df, rewards)\n        exec_features = self._execution_features(df)\n        kelly_features = self._kelly_rl_features(df, rewards)\n\n        # Combine\n        features = pd.concat([\n            q_features, sarsa_features, td_features, dyna_features,\n            thompson_features, exec_features, kelly_features\n        ], axis=1)\n\n        # Clean up\n        features = features.replace([np.inf, -np.inf], np.nan)\n        features = features.fillna(0)\n\n        return features",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "RLFeatureGenerator"
  },
  {
    "name": "get_feature_names",
    "category": "feature_engineering",
    "formula": "[",
    "explanation": "Get list of all feature names.",
    "python_code": "def get_feature_names(self) -> List[str]:\n        \"\"\"Get list of all feature names.\"\"\"\n        return [\n            # Q-Learning (6)\n            'RL_QLEARN_SIGNAL', 'RL_QLEARN_TD_ERROR', 'RL_QLEARN_Q_BUY',\n            'RL_QLEARN_Q_SELL', 'RL_QLEARN_Q_SPREAD', 'RL_QLEARN_CONFIDENCE',\n            # SARSA (5)\n            'RL_SARSA_SIGNAL', 'RL_SARSA_TD_ERROR', 'RL_SARSA_Q_DIV',\n            'RL_SARSA_SAFE', 'RL_SARSA_CAUTION',\n            # TD() (5)\n            'RL_TD_VALUE', 'RL_TD_ERROR', 'RL_TD_ELIGIBILITY',\n            'RL_TD_CREDIT', 'RL_TD_HORIZON',\n            # Dyna-Q (5)\n            'RL_DYNA_SIGNAL', 'RL_DYNA_PLANNING', 'RL_DYNA_MODEL_SIZE',\n            'RL_DYNA_CONFIDENCE', 'RL_DYNA_NOVELTY',\n            # Thompson Sampling (5)\n            'RL_TS_SIGNAL', 'RL_TS_UNCERTAINTY', 'RL_TS_SAMPLED',\n            'RL_TS_EXPLORE', 'RL_TS_EXPLOIT',\n            # Execution (5)\n            'RL_EXEC_URGENCY', 'RL_EXEC_AGGRESSION', 'RL_EXEC_VOL_ADJ',\n            'RL_EXEC_SPREAD_ADJ', 'RL_EXEC_REGIME',\n            # Kelly-RL (4)\n            'RL_KELLY_FRAC', 'RL_KELLY_EDGE', 'RL_KELLY_CONFIDENCE',\n            'RL_KELLY_SIGNAL',\n        ]",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "RLFeatureGenerator"
  },
  {
    "name": "generate_rl_features",
    "category": "feature_engineering",
    "formula": "generator.generate_all(df)",
    "explanation": "Generate RL algorithm-based features.\n\nArgs:\n    df: OHLCV DataFrame\n\nReturns:\n    DataFrame with 35 pure algorithmic RL features",
    "python_code": "def generate_rl_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generate RL algorithm-based features.\n\n    Args:\n        df: OHLCV DataFrame\n\n    Returns:\n        DataFrame with 35 pure algorithmic RL features\n    \"\"\"\n    generator = RLFeatureGenerator()\n    return generator.generate_all(df)",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": null
  },
  {
    "name": "get_rl_citations",
    "category": "feature_engineering",
    "formula": "{",
    "explanation": "Get academic citations for all RL algorithms implemented.\n\nReturns:\n    Dictionary mapping algorithm names to list of citation dictionaries.\n    Each citation has: authors, year, title, journal/venue, doi (if available)",
    "python_code": "def get_rl_citations() -> Dict[str, List[Dict[str, str]]]:\n    \"\"\"\n    Get academic citations for all RL algorithms implemented.\n\n    Returns:\n        Dictionary mapping algorithm names to list of citation dictionaries.\n        Each citation has: authors, year, title, journal/venue, doi (if available)\n    \"\"\"\n    return {\n        'q_learning': [\n            {\n                'authors': 'Watkins, C.J.C.H.',\n                'year': '1989',\n                'title': 'Learning from Delayed Rewards',\n                'venue': 'PhD Thesis, King\\'s College, Cambridge University',\n                'doi': None,\n                'note': 'Original Q-learning algorithm'\n            },\n            {\n                'authors': 'Watkins, C.J.C.H. & Dayan, P.',\n                'year': '1992',\n                'title': 'Q-learning',\n                'venue': 'Machine Learning, 8(3-4), 279-292',\n                'doi': '10.1007/BF00992698',\n                'note': 'Convergence proof'\n            }\n        ],\n        'sarsa': [\n            {\n                'authors': 'Rummery, G.A. & Niranjan, M.',\n                'year': '1994',\n                'title': 'On-Line Q-Learning Using Connectionist Systems',\n                'venue': 'Technical Report CUED/F-INFENG/TR 166, Cambridge University',\n                'doi': None,\n                'note': 'Original SARSA algorithm'\n            }\n        ],\n        'td_lambda': [\n            {\n                'authors': 'Sutton, R.S.',\n                'year': '1988',\n                'title': 'Learning to Predict by the Methods of Temporal Differences',\n                'venue': 'Machine Learning, 3(1), 9-44',\n                'doi': '10.1007/BF00115009',\n                'note': 'Foundation of TD learning'\n            },\n            {\n                'authors': 'Tesauro, G.',\n                'year': '1994',\n                'title': 'TD-Gammon, a Self-Teaching Backgammon Program',\n                'venue': 'Neural Computation, 6(2), 215-219',\n                'doi': None,\n                'note': 'Famous TD() application'\n            }\n        ],\n        'dyna_q': [\n            {\n                'authors': 'Sutton, R.S.',\n                'year': '1991',\n                'title': 'Dyna, an Integrated Architecture for Learning, Planning, and Reacting',\n                'venue': 'ACM SIGART Bulletin, 2(4), 160-163',\n                'doi': None,\n                'note': 'Original Dyna architecture'\n            }\n        ],\n        'thompson_sampling': [\n            {\n                'authors': 'Thompson, W.R.',\n                'year': '1933',\n                'title': 'On the Likelihood that One Unknown Probability Exceeds Another',\n                'venue': 'Biometrika, 25(3-4), 285-294',\n                'doi': '10.2307/2332286',\n                'note': 'Original Thompson Sampling (90+ years old, still SOTA)'\n            },\n            {\n                'authors': 'Agrawal, S. & Goyal, N.',\n                'year': '2012',\n                'title': 'A",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": null
  },
  {
    "name": "print_citations",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Print all citations in academic format.",
    "python_code": "def print_citations():\n    \"\"\"Print all citations in academic format.\"\"\"\n    citations = get_rl_citations()\n\n    print(\"=\" * 80)\n    print(\"REINFORCEMENT LEARNING ALGORITHMS - ACADEMIC CITATIONS\")\n    print(\"=\" * 80)\n\n    for algo, refs in citations.items():\n        print(f\"\\n{algo.upper().replace('_', ' ')}:\")\n        print(\"-\" * 40)\n        for ref in refs:\n            authors = ref['authors']\n            year = ref['year']\n            title = ref['title']\n            venue = ref['venue']\n            doi = ref.get('doi')\n            note = ref.get('note', '')\n\n            print(f\"  {authors} ({year}).\")\n            print(f\"  \\\"{title}\\\"\")\n            print(f\"  {venue}\")\n            if doi:\n                print(f\"  DOI: {doi}\")\n            if note:\n                print(f\"  [{note}]\")\n            print()",
    "source_file": "core\\features\\rl_algorithms.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "risk",
    "formula": "Lower = more stable, slower adaptation | Higher = faster adaptation, more noise",
    "explanation": "Initialize Differential Sharpe Ratio calculator.\n\nArgs:\n    eta: Adaptation rate for exponential moving averages (0.01-0.1)\n         Lower = more stable, slower adaptation\n         Higher = faster adaptation, more noise",
    "python_code": "def __init__(self, eta: float = 0.01):\n        \"\"\"\n        Initialize Differential Sharpe Ratio calculator.\n\n        Args:\n            eta: Adaptation rate for exponential moving averages (0.01-0.1)\n                 Lower = more stable, slower adaptation\n                 Higher = faster adaptation, more noise\n        \"\"\"\n        self.eta = eta\n        self.A = 0.0  # EMA of returns\n        self.B = 0.0  # EMA of squared returns\n        self._initialized = False",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DifferentialSharpeRatio"
  },
  {
    "name": "reset",
    "category": "risk",
    "formula": "",
    "explanation": "Reset internal state for new episode.",
    "python_code": "def reset(self) -> None:\n        \"\"\"Reset internal state for new episode.\"\"\"\n        self.A = 0.0\n        self.B = 0.0\n        self._initialized = False",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DifferentialSharpeRatio"
  },
  {
    "name": "update",
    "category": "risk",
    "formula": "or simple) | 0.0 | dsr",
    "explanation": "Compute differential Sharpe ratio for new return.\n\nArgs:\n    return_t: Return at current timestep (can be log return or simple)\n\nReturns:\n    Differential Sharpe ratio (reward signal)",
    "python_code": "def update(self, return_t: float) -> float:\n        \"\"\"\n        Compute differential Sharpe ratio for new return.\n\n        Args:\n            return_t: Return at current timestep (can be log return or simple)\n\n        Returns:\n            Differential Sharpe ratio (reward signal)\n        \"\"\"\n        if not self._initialized:\n            self.A = return_t\n            self.B = return_t ** 2\n            self._initialized = True\n            return 0.0\n\n        # Compute deltas\n        delta_A = return_t - self.A\n        delta_B = return_t ** 2 - self.B\n\n        # Compute variance term (avoid division by zero)\n        var_term = self.B - self.A ** 2\n\n        if var_term > 1e-10:\n            # Differential Sharpe formula (Moody & Saffell 2001, Eq. 7)\n            dsr = (self.B * delta_A - 0.5 * self.A * delta_B) / (var_term ** 1.5)\n        else:\n            dsr = 0.0\n\n        # Update EMAs\n        self.A += self.eta * delta_A\n        self.B += self.eta * delta_B\n\n        return dsr",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DifferentialSharpeRatio"
  },
  {
    "name": "compute_batch",
    "category": "risk",
    "formula": "dsr_values",
    "explanation": "Compute DSR for a batch of returns.\n\nArgs:\n    returns: Array of returns\n\nReturns:\n    Array of differential Sharpe ratios",
    "python_code": "def compute_batch(self, returns: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute DSR for a batch of returns.\n\n        Args:\n            returns: Array of returns\n\n        Returns:\n            Array of differential Sharpe ratios\n        \"\"\"\n        self.reset()\n        dsr_values = np.zeros(len(returns))\n        for i, r in enumerate(returns):\n            dsr_values[i] = self.update(r)\n        return dsr_values",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DifferentialSharpeRatio"
  },
  {
    "name": "compute_static",
    "category": "risk",
    "formula": "total_dsr",
    "explanation": "Static method to compute final DSR value.\n\nArgs:\n    returns: Array of returns\n    eta: Adaptation rate\n\nReturns:\n    Final cumulative differential Sharpe ratio",
    "python_code": "def compute_static(returns: np.ndarray, eta: float = 0.01) -> float:\n        \"\"\"\n        Static method to compute final DSR value.\n\n        Args:\n            returns: Array of returns\n            eta: Adaptation rate\n\n        Returns:\n            Final cumulative differential Sharpe ratio\n        \"\"\"\n        dsr = DifferentialSharpeRatio(eta=eta)\n        total_dsr = 0.0\n        for r in returns:\n            total_dsr += dsr.update(r)\n        return total_dsr",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DifferentialSharpeRatio"
  },
  {
    "name": "__init__",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Args:\n    eta: Adaptation rate\n    mar: Minimum Acceptable Return (threshold for downside)",
    "python_code": "def __init__(self, eta: float = 0.01, mar: float = 0.0):\n        \"\"\"\n        Args:\n            eta: Adaptation rate\n            mar: Minimum Acceptable Return (threshold for downside)\n        \"\"\"\n        self.eta = eta\n        self.mar = mar\n        self.A = 0.0  # EMA of returns\n        self.D = 0.0  # EMA of downside squared returns\n        self._initialized = False",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DifferentialDownsideDeviation"
  },
  {
    "name": "update",
    "category": "reinforcement_learning",
    "formula": "0.0 | ddd",
    "explanation": "Compute differential downside deviation reward.",
    "python_code": "def update(self, return_t: float) -> float:\n        \"\"\"Compute differential downside deviation reward.\"\"\"\n        if not self._initialized:\n            self.A = return_t\n            downside = min(0, return_t - self.mar) ** 2\n            self.D = downside\n            self._initialized = True\n            return 0.0\n\n        delta_A = return_t - self.A\n        downside = min(0, return_t - self.mar) ** 2\n        delta_D = downside - self.D\n\n        if self.D > 1e-10:\n            # Sortino-like differential reward\n            ddd = (np.sqrt(self.D) * delta_A - 0.5 * self.A * delta_D / np.sqrt(self.D)) / self.D\n        else:\n            ddd = delta_A  # No downside yet, just use return\n\n        self.A += self.eta * delta_A\n        self.D += self.eta * delta_D\n\n        return ddd",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DifferentialDownsideDeviation"
  },
  {
    "name": "__init__",
    "category": "risk",
    "formula": "higher = more risk-averse)",
    "explanation": "Args:\n    penalty_scale: Multiplier for drawdown penalty (higher = more risk-averse)",
    "python_code": "def __init__(self, penalty_scale: float = 1.0):\n        \"\"\"\n        Args:\n            penalty_scale: Multiplier for drawdown penalty (higher = more risk-averse)\n        \"\"\"\n        self.penalty_scale = penalty_scale\n        self.peak_value = 1.0\n        self.current_value = 1.0",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MaxDrawdownPenalty"
  },
  {
    "name": "update",
    "category": "risk",
    "formula": "return_t - penalty",
    "explanation": "Compute drawdown-penalized reward.\n\nArgs:\n    return_t: Return at current timestep\n\nReturns:\n    Return minus drawdown penalty",
    "python_code": "def update(self, return_t: float) -> float:\n        \"\"\"\n        Compute drawdown-penalized reward.\n\n        Args:\n            return_t: Return at current timestep\n\n        Returns:\n            Return minus drawdown penalty\n        \"\"\"\n        # Update portfolio value\n        self.current_value *= (1 + return_t)\n\n        # Update peak\n        if self.current_value > self.peak_value:\n            self.peak_value = self.current_value\n\n        # Compute drawdown\n        drawdown = (self.peak_value - self.current_value) / self.peak_value\n\n        # Penalized reward\n        penalty = self.penalty_scale * drawdown\n        return return_t - penalty",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MaxDrawdownPenalty"
  },
  {
    "name": "current_drawdown",
    "category": "risk",
    "formula": "(self.peak_value - self.current_value) / self.peak_value",
    "explanation": "Current drawdown from peak.",
    "python_code": "def current_drawdown(self) -> float:\n        \"\"\"Current drawdown from peak.\"\"\"\n        return (self.peak_value - self.current_value) / self.peak_value",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MaxDrawdownPenalty"
  },
  {
    "name": "compute_reward",
    "category": "reinforcement_learning",
    "formula": "reward",
    "explanation": "Compute combined reward from multiple signals.\n\nArgs:\n    return_t: Return at current timestep\n\nReturns:\n    Combined reward signal",
    "python_code": "def compute_reward(self, return_t: float) -> float:\n        \"\"\"\n        Compute combined reward from multiple signals.\n\n        Args:\n            return_t: Return at current timestep\n\n        Returns:\n            Combined reward signal\n        \"\"\"\n        reward = 0.0\n\n        if self.config.use_dsr:\n            reward += self.config.dsr_weight * self.dsr.update(return_t)\n\n        if self.config.use_sortino:\n            reward += self.config.sortino_weight * self.ddd.update(return_t)\n\n        if self.config.use_drawdown_penalty:\n            reward += self.config.drawdown_weight * self.mdd.update(return_t)\n\n        return reward",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CombinedRewardFunction"
  },
  {
    "name": "historical",
    "category": "alpha_factor",
    "formula": "np.percentile(returns, alpha * 100)",
    "explanation": "Historical VaR using empirical quantile.\n\nArgs:\n    returns: Array of returns (can be negative for losses)\n    alpha: Significance level (0.05 = 95% confidence)\n\nReturns:\n    VaR value (will be negative for losses)",
    "python_code": "def historical(returns: np.ndarray, alpha: float = 0.05) -> float:\n        \"\"\"\n        Historical VaR using empirical quantile.\n\n        Args:\n            returns: Array of returns (can be negative for losses)\n            alpha: Significance level (0.05 = 95% confidence)\n\n        Returns:\n            VaR value (will be negative for losses)\n        \"\"\"\n        return np.percentile(returns, alpha * 100)",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ValueAtRisk"
  },
  {
    "name": "parametric",
    "category": "alpha_factor",
    "formula": "std: Standard deviation of returns | mean + z * std",
    "explanation": "Parametric VaR assuming normal distribution.\n\nArgs:\n    mean: Expected return\n    std: Standard deviation of returns\n    alpha: Significance level\n\nReturns:\n    VaR value",
    "python_code": "def parametric(mean: float, std: float, alpha: float = 0.05) -> float:\n        \"\"\"\n        Parametric VaR assuming normal distribution.\n\n        Args:\n            mean: Expected return\n            std: Standard deviation of returns\n            alpha: Significance level\n\n        Returns:\n            VaR value\n        \"\"\"\n        if not SCIPY_AVAILABLE:\n            # Approximate z-scores for common alpha values\n            z_scores = {0.01: -2.326, 0.05: -1.645, 0.10: -1.282}\n            z = z_scores.get(alpha, -1.645)\n        else:\n            z = norm.ppf(alpha)\n        return mean + z * std",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ValueAtRisk"
  },
  {
    "name": "cornish_fisher",
    "category": "alpha_factor",
    "formula": "distributions (fat tails). | ValueAtRisk.historical(returns, alpha) | mean + z_cf * std",
    "explanation": "Cornish-Fisher VaR with skewness and kurtosis adjustment.\n\nMore accurate for non-normal return distributions (fat tails).\n\nArgs:\n    returns: Array of returns\n    alpha: Significance level\n\nReturns:\n    Adjusted VaR value",
    "python_code": "def cornish_fisher(returns: np.ndarray, alpha: float = 0.05) -> float:\n        \"\"\"\n        Cornish-Fisher VaR with skewness and kurtosis adjustment.\n\n        More accurate for non-normal return distributions (fat tails).\n\n        Args:\n            returns: Array of returns\n            alpha: Significance level\n\n        Returns:\n            Adjusted VaR value\n        \"\"\"\n        if not SCIPY_AVAILABLE:\n            return ValueAtRisk.historical(returns, alpha)\n\n        mean = np.mean(returns)\n        std = np.std(returns)\n        skew = ((returns - mean) ** 3).mean() / std ** 3\n        kurt = ((returns - mean) ** 4).mean() / std ** 4 - 3  # Excess kurtosis\n\n        z = norm.ppf(alpha)\n\n        # Cornish-Fisher expansion\n        z_cf = (z + (z**2 - 1) * skew / 6 +\n                (z**3 - 3*z) * kurt / 24 -\n                (2*z**3 - 5*z) * skew**2 / 36)\n\n        return mean + z_cf * std",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ValueAtRisk"
  },
  {
    "name": "historical",
    "category": "alpha_factor",
    "formula": "var | tail_returns.mean()",
    "explanation": "Historical CVaR using empirical distribution.\n\nArgs:\n    returns: Array of returns\n    alpha: Significance level (0.05 = worst 5%)\n\nReturns:\n    CVaR value (expected loss in tail)",
    "python_code": "def historical(returns: np.ndarray, alpha: float = 0.05) -> float:\n        \"\"\"\n        Historical CVaR using empirical distribution.\n\n        Args:\n            returns: Array of returns\n            alpha: Significance level (0.05 = worst 5%)\n\n        Returns:\n            CVaR value (expected loss in tail)\n        \"\"\"\n        var = np.percentile(returns, alpha * 100)\n        tail_returns = returns[returns <= var]\n\n        if len(tail_returns) == 0:\n            return var\n        return tail_returns.mean()",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ConditionalValueAtRisk"
  },
  {
    "name": "parametric",
    "category": "alpha_factor",
    "formula": "std: Standard deviation | mean - std * phi_z / alpha | mean - std * phi_z / alpha",
    "explanation": "Parametric CVaR assuming normal distribution.\n\nArgs:\n    mean: Expected return\n    std: Standard deviation\n    alpha: Significance level\n\nReturns:\n    CVaR value",
    "python_code": "def parametric(mean: float, std: float, alpha: float = 0.05) -> float:\n        \"\"\"\n        Parametric CVaR assuming normal distribution.\n\n        Args:\n            mean: Expected return\n            std: Standard deviation\n            alpha: Significance level\n\n        Returns:\n            CVaR value\n        \"\"\"\n        if not SCIPY_AVAILABLE:\n            # Approximate for normal distribution\n            # CVaR  mean - std * phi(z) / alpha where z = Phi^{-1}(alpha)\n            z_scores = {0.01: -2.326, 0.05: -1.645, 0.10: -1.282}\n            phi_values = {0.01: 0.0267, 0.05: 0.1031, 0.10: 0.1755}\n            z = z_scores.get(alpha, -1.645)\n            phi_z = phi_values.get(alpha, 0.1031)\n            return mean - std * phi_z / alpha\n\n        z = norm.ppf(alpha)\n        phi_z = norm.pdf(z)\n        return mean - std * phi_z / alpha",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ConditionalValueAtRisk"
  },
  {
    "name": "from_quantiles",
    "category": "alpha_factor",
    "formula": "cvar / alpha if alpha > 0 else 0.0",
    "explanation": "Compute CVaR from quantile distribution (for Distributional RL).\n\nArgs:\n    quantiles: Quantile values from distributional RL\n    probs: Probability weights for each quantile\n    alpha: Significance level\n\nReturns:\n    CVaR value",
    "python_code": "def from_quantiles(quantiles: np.ndarray, probs: np.ndarray, alpha: float = 0.05) -> float:\n        \"\"\"\n        Compute CVaR from quantile distribution (for Distributional RL).\n\n        Args:\n            quantiles: Quantile values from distributional RL\n            probs: Probability weights for each quantile\n            alpha: Significance level\n\n        Returns:\n            CVaR value\n        \"\"\"\n        # Sort by quantile value\n        sorted_idx = np.argsort(quantiles)\n        sorted_q = quantiles[sorted_idx]\n        sorted_p = probs[sorted_idx]\n\n        # Accumulate until alpha\n        cum_prob = 0.0\n        cvar = 0.0\n\n        for q, p in zip(sorted_q, sorted_p):\n            if cum_prob + p <= alpha:\n                cvar += q * p\n                cum_prob += p\n            else:\n                # Partial weight for last quantile\n                remaining = alpha - cum_prob\n                cvar += q * remaining\n                break\n\n        return cvar / alpha if alpha > 0 else 0.0",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ConditionalValueAtRisk"
  },
  {
    "name": "compute",
    "category": "alpha_factor",
    "formula": "ConditionalValueAtRisk.historical(returns, alpha) | np.inf | np.inf",
    "explanation": "Compute EVaR via optimization.\n\nArgs:\n    returns: Array of returns (losses should be negative)\n    alpha: Significance level\n    t_range: Range for optimization parameter t\n\nReturns:\n    EVaR value",
    "python_code": "def compute(returns: np.ndarray, alpha: float = 0.05,\n                t_range: Tuple[float, float] = (1e-6, 100)) -> float:\n        \"\"\"\n        Compute EVaR via optimization.\n\n        Args:\n            returns: Array of returns (losses should be negative)\n            alpha: Significance level\n            t_range: Range for optimization parameter t\n\n        Returns:\n            EVaR value\n        \"\"\"\n        if not SCIPY_AVAILABLE:\n            # Fall back to CVaR\n            return ConditionalValueAtRisk.historical(returns, alpha)\n\n        def objective(t):\n            if t <= 0:\n                return np.inf\n            try:\n                moment = np.mean(np.exp(t * returns))\n                if moment <= 0:\n                    return np.inf\n                return (1/t) * np.log(moment) + (1/t) * np.log(1/alpha)\n            except (OverflowError, RuntimeWarning):\n                return np.inf\n\n        result = minimize_scalar(objective, bounds=t_range, method='bounded')\n        return result.fun if result.success else ConditionalValueAtRisk.historical(returns, alpha)",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "EntropicValueAtRisk"
  },
  {
    "name": "optimal_position",
    "category": "risk",
    "formula": "min(kelly, cvar_constrained)",
    "explanation": "Compute CVaR-constrained Kelly position.\n\nArgs:\n    win_prob: Probability of winning trade\n    win_loss_ratio: Average win / Average loss\n    return_std: Standard deviation of returns\n\nReturns:\n    Optimal position size (fraction of capital)",
    "python_code": "def optimal_position(self, win_prob: float, win_loss_ratio: float,\n                         return_std: float) -> float:\n        \"\"\"\n        Compute CVaR-constrained Kelly position.\n\n        Args:\n            win_prob: Probability of winning trade\n            win_loss_ratio: Average win / Average loss\n            return_std: Standard deviation of returns\n\n        Returns:\n            Optimal position size (fraction of capital)\n        \"\"\"\n        # Standard Kelly\n        kelly = (win_prob * win_loss_ratio - (1 - win_prob)) / win_loss_ratio\n        kelly = max(0, kelly) * self.kelly_fraction\n\n        # CVaR constraint (assuming normal for simplicity)\n        cvar_position = ConditionalValueAtRisk.parametric(0, return_std, self.alpha)\n\n        if cvar_position != 0:\n            # Scale position so CVaR meets limit\n            cvar_constrained = abs(self.cvar_limit / cvar_position)\n        else:\n            cvar_constrained = kelly\n\n        # Return minimum of Kelly and CVaR-constrained\n        return min(kelly, cvar_constrained)",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CVaRConstrainedKelly"
  },
  {
    "name": "compute_risk_metrics",
    "category": "risk",
    "formula": "= np.mean(returns) * 252 | / max_dd if max_dd > 0 else 0.0 | / downside_std if downside_std > 0 else 0.0",
    "explanation": "Compute comprehensive risk metrics.\n\nArgs:\n    returns: Array of returns\n    risk_free: Risk-free rate for Sortino\n\nReturns:\n    RiskMetricsResult with all metrics",
    "python_code": "def compute_risk_metrics(returns: np.ndarray, risk_free: float = 0.0) -> RiskMetricsResult:\n    \"\"\"\n    Compute comprehensive risk metrics.\n\n    Args:\n        returns: Array of returns\n        risk_free: Risk-free rate for Sortino\n\n    Returns:\n        RiskMetricsResult with all metrics\n    \"\"\"\n    # VaR\n    var_95 = ValueAtRisk.historical(returns, 0.05)\n    var_99 = ValueAtRisk.historical(returns, 0.01)\n\n    # CVaR\n    cvar_95 = ConditionalValueAtRisk.historical(returns, 0.05)\n    cvar_99 = ConditionalValueAtRisk.historical(returns, 0.01)\n\n    # EVaR (if scipy available)\n    try:\n        evar_95 = EntropicValueAtRisk.compute(returns, 0.05)\n    except:\n        evar_95 = None\n\n    # Max Drawdown\n    cum_returns = np.cumprod(1 + returns)\n    running_max = np.maximum.accumulate(cum_returns)\n    drawdowns = (running_max - cum_returns) / running_max\n    max_dd = drawdowns.max()\n\n    # Calmar Ratio\n    annualized_return = np.mean(returns) * 252\n    calmar = annualized_return / max_dd if max_dd > 0 else 0.0\n\n    # Sortino Ratio\n    excess_returns = returns - risk_free / 252\n    downside_returns = excess_returns[excess_returns < 0]\n    downside_std = np.std(downside_returns) * np.sqrt(252) if len(downside_returns) > 0 else 0.0\n    sortino = annualized_return / downside_std if downside_std > 0 else 0.0\n\n    return RiskMetricsResult(\n        var_95=var_95,\n        var_99=var_99,\n        cvar_95=cvar_95,\n        cvar_99=cvar_99,\n        evar_95=evar_95,\n        max_drawdown=max_dd,\n        calmar_ratio=calmar,\n        sortino_ratio=sortino\n    )",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "optimal_trajectory",
    "category": "volatility",
    "formula": "trajectory",
    "explanation": "Compute optimal execution trajectory.\n\nArgs:\n    shares: Total shares to execute\n    time_horizon: Time to complete execution (in trading days)\n    volatility: Asset volatility (daily)\n    n_steps: Number of time steps\n\nReturns:\n    Array of remaining shares at each time step",
    "python_code": "def optimal_trajectory(self, shares: float, time_horizon: float,\n                          volatility: float, n_steps: int = 100) -> np.ndarray:\n        \"\"\"\n        Compute optimal execution trajectory.\n\n        Args:\n            shares: Total shares to execute\n            time_horizon: Time to complete execution (in trading days)\n            volatility: Asset volatility (daily)\n            n_steps: Number of time steps\n\n        Returns:\n            Array of remaining shares at each time step\n        \"\"\"\n        gamma = self.config.permanent_impact\n        eta = self.config.temporary_impact\n        lam = self.config.risk_aversion\n        sigma = volatility\n\n        # Time discretization\n        tau = time_horizon / n_steps\n        t = np.linspace(0, time_horizon, n_steps + 1)\n\n        # Almgren-Chriss parameter\n        kappa_sq = lam * sigma**2 / eta\n        kappa = np.sqrt(kappa_sq) if kappa_sq > 0 else 0.0\n\n        if kappa > 1e-10:\n            # Optimal trajectory (Almgren-Chriss Eq. 20)\n            trajectory = shares * np.sinh(kappa * (time_horizon - t)) / np.sinh(kappa * time_horizon)\n        else:\n            # Linear trajectory (risk-neutral)\n            trajectory = shares * (1 - t / time_horizon)\n\n        return trajectory",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AlmgrenChriss"
  },
  {
    "name": "expected_cost",
    "category": "volatility",
    "formula": "total_cost, permanent_cost, temporary_cost",
    "explanation": "Compute expected execution cost.\n\nArgs:\n    shares: Total shares to execute\n    time_horizon: Time to complete execution\n    volatility: Asset volatility\n\nReturns:\n    Tuple of (total_cost, permanent_cost, temporary_cost)",
    "python_code": "def expected_cost(self, shares: float, time_horizon: float,\n                     volatility: float) -> Tuple[float, float, float]:\n        \"\"\"\n        Compute expected execution cost.\n\n        Args:\n            shares: Total shares to execute\n            time_horizon: Time to complete execution\n            volatility: Asset volatility\n\n        Returns:\n            Tuple of (total_cost, permanent_cost, temporary_cost)\n        \"\"\"\n        gamma = self.config.permanent_impact\n        eta = self.config.temporary_impact\n        sigma = volatility\n\n        # Permanent impact cost\n        permanent_cost = 0.5 * gamma * sigma**2 * shares**2 * time_horizon\n\n        # Temporary impact cost (for linear trajectory)\n        temporary_cost = eta * shares**2 / time_horizon\n\n        total_cost = permanent_cost + temporary_cost\n\n        return total_cost, permanent_cost, temporary_cost",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AlmgrenChriss"
  },
  {
    "name": "optimal_time_horizon",
    "category": "volatility",
    "formula": "higher = faster execution) | np.sqrt(eta / (lam * sigma**2)) | 1.0",
    "explanation": "Compute optimal execution time given urgency parameter.\n\nArgs:\n    shares: Total shares to execute\n    volatility: Asset volatility\n    urgency: Urgency multiplier (higher = faster execution)\n\nReturns:\n    Optimal time horizon",
    "python_code": "def optimal_time_horizon(self, shares: float, volatility: float,\n                            urgency: float = 1.0) -> float:\n        \"\"\"\n        Compute optimal execution time given urgency parameter.\n\n        Args:\n            shares: Total shares to execute\n            volatility: Asset volatility\n            urgency: Urgency multiplier (higher = faster execution)\n\n        Returns:\n            Optimal time horizon\n        \"\"\"\n        gamma = self.config.permanent_impact\n        eta = self.config.temporary_impact\n        lam = self.config.risk_aversion * urgency\n        sigma = volatility\n\n        # Optimal T* minimizes cost + risk\n        # T* = sqrt(eta / (lambda * sigma^2))\n        if lam * sigma**2 > 1e-10:\n            return np.sqrt(eta / (lam * sigma**2))\n        else:\n            return 1.0",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AlmgrenChriss"
  },
  {
    "name": "compute_state",
    "category": "volatility",
    "formula": "state",
    "explanation": "Compute state vector for RL agent.\n\nArgs:\n    remaining_shares: Shares left to execute\n    time_remaining: Time left (fraction of total)\n    volatility: Current volatility estimate\n    spread: Current bid-ask spread\n    order_imbalance: Current order book imbalance\n\nReturns:\n    State vector for RL",
    "python_code": "def compute_state(self, remaining_shares: float, time_remaining: float,\n                     volatility: float, spread: float,\n                     order_imbalance: float) -> np.ndarray:\n        \"\"\"\n        Compute state vector for RL agent.\n\n        Args:\n            remaining_shares: Shares left to execute\n            time_remaining: Time left (fraction of total)\n            volatility: Current volatility estimate\n            spread: Current bid-ask spread\n            order_imbalance: Current order book imbalance\n\n        Returns:\n            State vector for RL\n        \"\"\"\n        # Normalize features\n        state = np.array([\n            remaining_shares,  # Will be normalized by initial shares\n            time_remaining,\n            volatility * np.sqrt(252),  # Annualized\n            spread * 10000,  # In bps\n            order_imbalance  # Already in [-1, 1]\n        ])\n        return state",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RLOptimalExecution"
  },
  {
    "name": "compute_reward",
    "category": "execution",
    "formula": "-(slippage + time_penalty)",
    "explanation": "Compute execution reward (negative cost).\n\nArgs:\n    execution_price: Price at which trade executed\n    mid_price: Mid price at execution time\n    shares_executed: Number of shares executed this step\n    time_elapsed: Time elapsed this step\n\nReturns:\n    Reward (negative of implementation shortfall)",
    "python_code": "def compute_reward(self, execution_price: float, mid_price: float,\n                      shares_executed: float, time_elapsed: float) -> float:\n        \"\"\"\n        Compute execution reward (negative cost).\n\n        Args:\n            execution_price: Price at which trade executed\n            mid_price: Mid price at execution time\n            shares_executed: Number of shares executed this step\n            time_elapsed: Time elapsed this step\n\n        Returns:\n            Reward (negative of implementation shortfall)\n        \"\"\"\n        # Implementation shortfall\n        slippage = (execution_price - mid_price) * shares_executed\n\n        # Add time penalty (opportunity cost)\n        time_penalty = 0.0001 * time_elapsed * shares_executed\n\n        return -(slippage + time_penalty)",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RLOptimalExecution"
  },
  {
    "name": "reservation_price",
    "category": "technical",
    "formula": "mid_price - q * gamma * sigma**2 * time_remaining",
    "explanation": "Compute reservation price (Avellaneda-Stoikov Eq. 8).\n\nThe reservation price is where the market maker is indifferent\nto holding inventory. It adjusts based on current inventory.\n\nArgs:\n    mid_price: Current mid price\n    time_remaining: Time until end of session (fraction)\n\nReturns:\n    Reservation price",
    "python_code": "def reservation_price(self, mid_price: float, time_remaining: float) -> float:\n        \"\"\"\n        Compute reservation price (Avellaneda-Stoikov Eq. 8).\n\n        The reservation price is where the market maker is indifferent\n        to holding inventory. It adjusts based on current inventory.\n\n        Args:\n            mid_price: Current mid price\n            time_remaining: Time until end of session (fraction)\n\n        Returns:\n            Reservation price\n        \"\"\"\n        gamma = self.config.gamma\n        sigma = self.config.sigma\n        q = self.inventory\n\n        # r = s - q * gamma * sigma^2 * (T - t)\n        return mid_price - q * gamma * sigma**2 * time_remaining",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AvellanedaStoikovRL"
  },
  {
    "name": "optimal_spread",
    "category": "microstructure",
    "formula": "time_component + intensity_component",
    "explanation": "Compute optimal bid-ask spread (Avellaneda-Stoikov Eq. 10).\n\nArgs:\n    time_remaining: Time until end of session\n\nReturns:\n    Optimal spread (full, not half)",
    "python_code": "def optimal_spread(self, time_remaining: float) -> float:\n        \"\"\"\n        Compute optimal bid-ask spread (Avellaneda-Stoikov Eq. 10).\n\n        Args:\n            time_remaining: Time until end of session\n\n        Returns:\n            Optimal spread (full, not half)\n        \"\"\"\n        gamma = self.config.gamma\n        sigma = self.config.sigma\n        kappa = self.config.kappa\n\n        # delta = gamma * sigma^2 * (T - t) + (2/gamma) * log(1 + gamma/kappa)\n        time_component = gamma * sigma**2 * time_remaining\n        intensity_component = (2 / gamma) * np.log(1 + gamma / kappa)\n\n        return time_component + intensity_component",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AvellanedaStoikovRL"
  },
  {
    "name": "optimal_quotes",
    "category": "technical",
    "formula": "bid, ask",
    "explanation": "Compute optimal bid and ask prices.\n\nArgs:\n    mid_price: Current mid price\n    time_remaining: Time until end of session\n\nReturns:\n    Tuple of (bid_price, ask_price)",
    "python_code": "def optimal_quotes(self, mid_price: float, time_remaining: float) -> Tuple[float, float]:\n        \"\"\"\n        Compute optimal bid and ask prices.\n\n        Args:\n            mid_price: Current mid price\n            time_remaining: Time until end of session\n\n        Returns:\n            Tuple of (bid_price, ask_price)\n        \"\"\"\n        r = self.reservation_price(mid_price, time_remaining)\n        delta = self.optimal_spread(time_remaining)\n\n        bid = r - delta / 2\n        ask = r + delta / 2\n\n        # Round to tick size\n        tick = self.config.tick_size\n        bid = np.floor(bid / tick) * tick\n        ask = np.ceil(ask / tick) * tick\n\n        return bid, ask",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AvellanedaStoikovRL"
  },
  {
    "name": "compute_state",
    "category": "volatility",
    "formula": "np.array([",
    "explanation": "Compute state for RL agent.\n\nArgs:\n    mid_price: Current mid price\n    time_remaining: Time until session end\n    volatility: Current volatility estimate\n    order_flow: Recent order flow imbalance\n\nReturns:\n    State vector",
    "python_code": "def compute_state(self, mid_price: float, time_remaining: float,\n                     volatility: float, order_flow: float) -> np.ndarray:\n        \"\"\"\n        Compute state for RL agent.\n\n        Args:\n            mid_price: Current mid price\n            time_remaining: Time until session end\n            volatility: Current volatility estimate\n            order_flow: Recent order flow imbalance\n\n        Returns:\n            State vector\n        \"\"\"\n        return np.array([\n            self.inventory / self.config.max_inventory,  # Normalized inventory\n            time_remaining,\n            volatility / self.config.sigma,  # Relative volatility\n            order_flow,  # Order flow imbalance\n            mid_price  # For price-aware policies\n        ])",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AvellanedaStoikovRL"
  },
  {
    "name": "step",
    "category": "technical",
    "formula": "reward",
    "explanation": "Process one time step.\n\nArgs:\n    mid_price: Current mid price\n    time_remaining: Time remaining\n    bid_filled: Whether bid was hit\n    ask_filled: Whether ask was lifted\n\nReturns:\n    Reward for this step",
    "python_code": "def step(self, mid_price: float, time_remaining: float,\n            bid_filled: bool, ask_filled: bool) -> float:\n        \"\"\"\n        Process one time step.\n\n        Args:\n            mid_price: Current mid price\n            time_remaining: Time remaining\n            bid_filled: Whether bid was hit\n            ask_filled: Whether ask was lifted\n\n        Returns:\n            Reward for this step\n        \"\"\"\n        bid, ask = self.optimal_quotes(mid_price, time_remaining)\n\n        reward = 0.0\n\n        if bid_filled:\n            # Bought at bid\n            self.inventory += 1\n            self.pnl -= bid\n            reward += mid_price - bid  # Positive if bought below mid\n\n        if ask_filled:\n            # Sold at ask\n            self.inventory -= 1\n            self.pnl += ask\n            reward += ask - mid_price  # Positive if sold above mid\n\n        # Inventory penalty (quadratic)\n        inventory_penalty = 0.001 * self.inventory**2\n        reward -= inventory_penalty\n\n        self.mid_price_history.append(mid_price)\n\n        return reward",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AvellanedaStoikovRL"
  },
  {
    "name": "terminal_value",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Compute terminal value including inventory liquidation.\n\nArgs:\n    final_price: Price at session end\n\nReturns:\n    Total PnL including mark-to-market",
    "python_code": "def terminal_value(self, final_price: float) -> float:\n        \"\"\"\n        Compute terminal value including inventory liquidation.\n\n        Args:\n            final_price: Price at session end\n\n        Returns:\n            Total PnL including mark-to-market\n        \"\"\"\n        return self.pnl + self.inventory * final_price",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AvellanedaStoikovRL"
  },
  {
    "name": "set_quantiles",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Set quantile values from model output.",
    "python_code": "def set_quantiles(self, quantiles: np.ndarray) -> None:\n        \"\"\"Set quantile values from model output.\"\"\"\n        assert len(quantiles) == self.n_quantiles\n        self.quantiles = np.sort(quantiles)",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "QuantileDistribution"
  },
  {
    "name": "mean",
    "category": "feature_engineering",
    "formula": "0.0",
    "explanation": "Expected value (mean of distribution).",
    "python_code": "def mean(self) -> float:\n        \"\"\"Expected value (mean of distribution).\"\"\"\n        if self.quantiles is None:\n            return 0.0\n        return self.quantiles.mean()",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "QuantileDistribution"
  },
  {
    "name": "var",
    "category": "alpha_factor",
    "formula": "0.0",
    "explanation": "Value at Risk at level alpha.",
    "python_code": "def var(self, alpha: float = 0.05) -> float:\n        \"\"\"Value at Risk at level alpha.\"\"\"\n        if self.quantiles is None:\n            return 0.0\n        idx = int(alpha * self.n_quantiles)\n        return self.quantiles[idx]",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "QuantileDistribution"
  },
  {
    "name": "cvar",
    "category": "risk",
    "formula": "0.0",
    "explanation": "Conditional Value at Risk.",
    "python_code": "def cvar(self, alpha: float = 0.05) -> float:\n        \"\"\"Conditional Value at Risk.\"\"\"\n        if self.quantiles is None:\n            return 0.0\n        idx = int(alpha * self.n_quantiles)\n        return self.quantiles[:idx+1].mean()",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "QuantileDistribution"
  },
  {
    "name": "std",
    "category": "feature_engineering",
    "formula": "0.0",
    "explanation": "Standard deviation.",
    "python_code": "def std(self) -> float:\n        \"\"\"Standard deviation.\"\"\"\n        if self.quantiles is None:\n            return 0.0\n        return self.quantiles.std()",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "QuantileDistribution"
  },
  {
    "name": "iqr",
    "category": "feature_engineering",
    "formula": "0.0 | q75 - q25",
    "explanation": "Interquartile range.",
    "python_code": "def iqr(self) -> float:\n        \"\"\"Interquartile range.\"\"\"\n        if self.quantiles is None:\n            return 0.0\n        q25 = self.quantiles[int(0.25 * self.n_quantiles)]\n        q75 = self.quantiles[int(0.75 * self.n_quantiles)]\n        return q75 - q25",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "QuantileDistribution"
  },
  {
    "name": "risk_adjusted_value",
    "category": "technical",
    "formula": "",
    "explanation": "Risk-adjusted value: mean - risk_aversion * std.\n\nArgs:\n    risk_aversion: Risk aversion coefficient\n\nReturns:\n    Risk-adjusted expected value",
    "python_code": "def risk_adjusted_value(self, risk_aversion: float = 1.0) -> float:\n        \"\"\"\n        Risk-adjusted value: mean - risk_aversion * std.\n\n        Args:\n            risk_aversion: Risk aversion coefficient\n\n        Returns:\n            Risk-adjusted expected value\n        \"\"\"\n        return self.mean() - risk_aversion * self.std()",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "QuantileDistribution"
  },
  {
    "name": "adapt",
    "category": "reinforcement_learning",
    "formula": "adapted_params",
    "explanation": "Adapt to new regime using support set.\n\nArgs:\n    support_data: Support set features [n_support, feature_dim]\n    support_labels: Support set labels [n_support]\n\nReturns:\n    Adapted parameters (task-specific)",
    "python_code": "def adapt(self, support_data: np.ndarray, support_labels: np.ndarray) -> Dict:\n        \"\"\"\n        Adapt to new regime using support set.\n\n        Args:\n            support_data: Support set features [n_support, feature_dim]\n            support_labels: Support set labels [n_support]\n\n        Returns:\n            Adapted parameters (task-specific)\n        \"\"\"\n        # In practice, this would:\n        # 1. Clone meta_parameters\n        # 2. Take inner_steps gradient steps on support data\n        # 3. Return adapted parameters\n\n        # Placeholder for actual implementation\n        adapted_params = {\n            'inner_lr': self.inner_lr,\n            'steps': self.inner_steps,\n            'support_size': len(support_data)\n        }\n        return adapted_params",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MAMLTrader"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "np.zeros(len(query_data))",
    "explanation": "Make predictions using adapted parameters.\n\nArgs:\n    query_data: Query set features\n    adapted_params: Task-adapted parameters\n\nReturns:\n    Predictions",
    "python_code": "def predict(self, query_data: np.ndarray, adapted_params: Dict) -> np.ndarray:\n        \"\"\"\n        Make predictions using adapted parameters.\n\n        Args:\n            query_data: Query set features\n            adapted_params: Task-adapted parameters\n\n        Returns:\n            Predictions\n        \"\"\"\n        # In practice, forward pass with adapted_params\n        return np.zeros(len(query_data))",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MAMLTrader"
  },
  {
    "name": "meta_update",
    "category": "reinforcement_learning",
    "formula": "0.0",
    "explanation": "Meta-update across multiple tasks (regimes).\n\nArgs:\n    tasks: List of (support_data, query_data) tuples\n\nReturns:\n    Meta-loss",
    "python_code": "def meta_update(self, tasks: List[Tuple[np.ndarray, np.ndarray]]) -> float:\n        \"\"\"\n        Meta-update across multiple tasks (regimes).\n\n        Args:\n            tasks: List of (support_data, query_data) tuples\n\n        Returns:\n            Meta-loss\n        \"\"\"\n        # In practice:\n        # 1. For each task, compute adapted params\n        # 2. Evaluate on query set\n        # 3. Compute gradients through adaptation\n        # 4. Update meta_parameters\n\n        return 0.0",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MAMLTrader"
  },
  {
    "name": "update_regime",
    "category": "regime",
    "formula": "",
    "explanation": "Update regime belief.\n\nArgs:\n    regime_probs: Probability distribution over regimes from HMM\n\nReturns:\n    Most likely regime",
    "python_code": "def update_regime(self, regime_probs: np.ndarray) -> int:\n        \"\"\"\n        Update regime belief.\n\n        Args:\n            regime_probs: Probability distribution over regimes from HMM\n\n        Returns:\n            Most likely regime\n        \"\"\"\n        self.regime_probs = regime_probs\n        self.current_regime = np.argmax(regime_probs)\n        return self.current_regime",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RegimeAwareRL"
  },
  {
    "name": "get_action",
    "category": "reinforcement_learning",
    "formula": "np.zeros(1)",
    "explanation": "Get action from regime-appropriate policy.\n\nArgs:\n    state: Current state\n\nReturns:\n    Action",
    "python_code": "def get_action(self, state: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Get action from regime-appropriate policy.\n\n        Args:\n            state: Current state\n\n        Returns:\n            Action\n        \"\"\"\n        # Ensemble across regimes weighted by probability\n        # action = _i p(regime=i) * _i(state)\n\n        # Placeholder - would use actual policies\n        return np.zeros(1)",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RegimeAwareRL"
  },
  {
    "name": "compute_context_vector",
    "category": "volatility",
    "formula": "history | np.array([",
    "explanation": "Compute latent context for regime identification.\n\nArgs:\n    recent_returns: Recent return history\n    recent_volatility: Recent volatility estimates\n\nReturns:\n    Context vector",
    "python_code": "def compute_context_vector(self, recent_returns: np.ndarray,\n                               recent_volatility: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute latent context for regime identification.\n\n        Args:\n            recent_returns: Recent return history\n            recent_volatility: Recent volatility estimates\n\n        Returns:\n            Context vector\n        \"\"\"\n        return np.array([\n            np.mean(recent_returns),\n            np.std(recent_returns),\n            np.mean(recent_volatility),\n            np.std(recent_volatility),\n            np.corrcoef(recent_returns[:-1], recent_returns[1:])[0, 1]  # Autocorrelation\n        ])",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RegimeAwareRL"
  },
  {
    "name": "compute_state",
    "category": "statistical",
    "formula": "np.array(state)",
    "explanation": "Compute state for position sizing RL.\n\nArgs:\n    prediction_confidence: ML model confidence [0, 1]\n    regime_probs: Probability of each regime\n    current_position: Current position (fraction)\n    recent_pnl: Recent PnL (normalized)\n\nReturns:\n    State vector",
    "python_code": "def compute_state(self, prediction_confidence: float, regime_probs: np.ndarray,\n                     current_position: float, recent_pnl: float) -> np.ndarray:\n        \"\"\"\n        Compute state for position sizing RL.\n\n        Args:\n            prediction_confidence: ML model confidence [0, 1]\n            regime_probs: Probability of each regime\n            current_position: Current position (fraction)\n            recent_pnl: Recent PnL (normalized)\n\n        Returns:\n            State vector\n        \"\"\"\n        state = [\n            prediction_confidence,\n            current_position / self.config.max_position,\n            recent_pnl\n        ]\n\n        if self.config.use_regime:\n            state.extend(regime_probs.tolist())\n\n        return np.array(state)",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RLPositionSizer"
  },
  {
    "name": "kelly_baseline",
    "category": "volatility",
    "formula": "",
    "explanation": "Compute CVaR-constrained Kelly as baseline.\n\nArgs:\n    win_prob: Probability of winning\n    win_loss_ratio: Win/loss ratio\n    return_std: Return volatility\n\nReturns:\n    Baseline position size",
    "python_code": "def kelly_baseline(self, win_prob: float, win_loss_ratio: float,\n                      return_std: float) -> float:\n        \"\"\"\n        Compute CVaR-constrained Kelly as baseline.\n\n        Args:\n            win_prob: Probability of winning\n            win_loss_ratio: Win/loss ratio\n            return_std: Return volatility\n\n        Returns:\n            Baseline position size\n        \"\"\"\n        return self.kelly.optimal_position(win_prob, win_loss_ratio, return_std)",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RLPositionSizer"
  },
  {
    "name": "compute_reward",
    "category": "reinforcement_learning",
    "formula": "transaction_cost: Cost per unit traded | # Transaction cost | pnl - cost - penalty",
    "explanation": "Compute reward for position sizing decision.\n\nArgs:\n    position: Position taken\n    realized_return: Actual return\n    transaction_cost: Cost per unit traded\n\nReturns:\n    Reward (risk-adjusted return)",
    "python_code": "def compute_reward(self, position: float, realized_return: float,\n                      transaction_cost: float = 0.0001) -> float:\n        \"\"\"\n        Compute reward for position sizing decision.\n\n        Args:\n            position: Position taken\n            realized_return: Actual return\n            transaction_cost: Cost per unit traded\n\n        Returns:\n            Reward (risk-adjusted return)\n        \"\"\"\n        # Realized PnL\n        pnl = position * realized_return\n\n        # Transaction cost\n        cost = abs(position) * transaction_cost\n\n        # Track for risk metrics\n        self.recent_returns.append(pnl - cost)\n        if len(self.recent_returns) > self.max_recent:\n            self.recent_returns.pop(0)\n\n        # Risk-adjusted reward (use DSR if enough history)\n        if len(self.recent_returns) >= 10:\n            returns_arr = np.array(self.recent_returns)\n            cvar = ConditionalValueAtRisk.historical(returns_arr, 0.05)\n\n            # Penalize if CVaR exceeds limit\n            if cvar < self.config.cvar_limit:\n                penalty = 10 * (self.config.cvar_limit - cvar)\n            else:\n                penalty = 0\n\n            return pnl - cost - penalty\n        else:\n            return pnl - cost",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RLPositionSizer"
  },
  {
    "name": "generate",
    "category": "feature_engineering",
    "formula": "features | features",
    "explanation": "Generate RL-specific features.\n\nArgs:\n    returns: Recent returns\n    prices: Recent prices\n    volumes: Recent volumes (optional)\n\nReturns:\n    Dictionary of feature name -> value",
    "python_code": "def generate(self, returns: np.ndarray, prices: np.ndarray,\n                volumes: Optional[np.ndarray] = None) -> Dict[str, float]:\n        \"\"\"\n        Generate RL-specific features.\n\n        Args:\n            returns: Recent returns\n            prices: Recent prices\n            volumes: Recent volumes (optional)\n\n        Returns:\n            Dictionary of feature name -> value\n        \"\"\"\n        features = {}\n\n        if len(returns) < 10:\n            return features\n\n        # Risk metrics\n        risk = compute_risk_metrics(returns[-self.lookback:])\n        features['var_95'] = risk.var_95\n        features['cvar_95'] = risk.cvar_95\n        features['max_drawdown'] = risk.max_drawdown\n        features['sortino'] = risk.sortino_ratio\n        features['calmar'] = risk.calmar_ratio\n\n        # Reward statistics\n        dsr_values = self.dsr.compute_batch(returns[-self.lookback:])\n        features['dsr_current'] = dsr_values[-1] if len(dsr_values) > 0 else 0\n        features['dsr_mean'] = np.mean(dsr_values)\n        features['dsr_std'] = np.std(dsr_values)\n\n        # Rolling Sharpe\n        if len(returns) >= 20:\n            rolling_mean = np.mean(returns[-20:])\n            rolling_std = np.std(returns[-20:])\n            features['sharpe_20'] = rolling_mean / rolling_std if rolling_std > 0 else 0\n\n        # Volatility features\n        features['volatility'] = np.std(returns[-20:]) * np.sqrt(252)\n        features['volatility_ratio'] = (\n            np.std(returns[-10:]) / np.std(returns[-50:])\n            if len(returns) >= 50 and np.std(returns[-50:]) > 0\n            else 1.0\n        )\n\n        # Price momentum\n        if len(prices) >= 20:\n            features['momentum_20'] = (prices[-1] / prices[-20]) - 1\n\n        # Volume features (if available)\n        if volumes is not None and len(volumes) >= 20:\n            features['volume_ratio'] = np.mean(volumes[-5:]) / np.mean(volumes[-20:])\n\n        return features",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RLFeatureGenerator"
  },
  {
    "name": "create_reward_function",
    "category": "risk",
    "formula": "DifferentialSharpeRatio(**kwargs) | DifferentialDownsideDeviation(**kwargs) | MaxDrawdownPenalty(**kwargs)",
    "explanation": "Factory function to create reward functions.\n\nArgs:\n    reward_type: One of 'dsr', 'sortino', 'drawdown', 'combined'\n    **kwargs: Arguments passed to reward function\n\nReturns:\n    Reward function instance",
    "python_code": "def create_reward_function(reward_type: str = 'combined',\n                          **kwargs) -> Union[DifferentialSharpeRatio,\n                                            DifferentialDownsideDeviation,\n                                            MaxDrawdownPenalty,\n                                            CombinedRewardFunction]:\n    \"\"\"\n    Factory function to create reward functions.\n\n    Args:\n        reward_type: One of 'dsr', 'sortino', 'drawdown', 'combined'\n        **kwargs: Arguments passed to reward function\n\n    Returns:\n        Reward function instance\n    \"\"\"\n    if reward_type == 'dsr':\n        return DifferentialSharpeRatio(**kwargs)\n    elif reward_type == 'sortino':\n        return DifferentialDownsideDeviation(**kwargs)\n    elif reward_type == 'drawdown':\n        return MaxDrawdownPenalty(**kwargs)\n    elif reward_type == 'combined':\n        config = RewardConfig(**kwargs) if kwargs else None\n        return CombinedRewardFunction(config)\n    else:\n        raise ValueError(f\"Unknown reward type: {reward_type}\")",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "create_execution_model",
    "category": "execution",
    "formula": "AlmgrenChriss(config) | RLOptimalExecution(config)",
    "explanation": "Factory function to create execution models.\n\nArgs:\n    model_type: One of 'almgren_chriss', 'rl_execution'\n    **kwargs: Arguments passed to model\n\nReturns:\n    Execution model instance",
    "python_code": "def create_execution_model(model_type: str = 'almgren_chriss',\n                          **kwargs) -> Union[AlmgrenChriss, RLOptimalExecution]:\n    \"\"\"\n    Factory function to create execution models.\n\n    Args:\n        model_type: One of 'almgren_chriss', 'rl_execution'\n        **kwargs: Arguments passed to model\n\n    Returns:\n        Execution model instance\n    \"\"\"\n    config = ExecutionConfig(**kwargs) if kwargs else None\n\n    if model_type == 'almgren_chriss':\n        return AlmgrenChriss(config)\n    elif model_type == 'rl_execution':\n        return RLOptimalExecution(config)\n    else:\n        raise ValueError(f\"Unknown execution model: {model_type}\")",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "create_market_maker",
    "category": "reinforcement_learning",
    "formula": "AvellanedaStoikovRL(config)",
    "explanation": "Factory function to create market maker.\n\nArgs:\n    mm_type: Currently only 'avellaneda_stoikov'\n    **kwargs: Arguments passed to model\n\nReturns:\n    Market maker instance",
    "python_code": "def create_market_maker(mm_type: str = 'avellaneda_stoikov',\n                       **kwargs) -> AvellanedaStoikovRL:\n    \"\"\"\n    Factory function to create market maker.\n\n    Args:\n        mm_type: Currently only 'avellaneda_stoikov'\n        **kwargs: Arguments passed to model\n\n    Returns:\n        Market maker instance\n    \"\"\"\n    config = MarketMakingConfig(**kwargs) if kwargs else None\n    return AvellanedaStoikovRL(config)",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "generate_rl_features",
    "category": "deep_learning",
    "formula": "generator.generate(returns, prices, volumes)",
    "explanation": "Convenience function to generate RL features.\n\nArgs:\n    returns: Return series\n    prices: Price series\n    volumes: Volume series (optional)\n    lookback: Lookback period\n\nReturns:\n    Dictionary of RL features",
    "python_code": "def generate_rl_features(returns: np.ndarray, prices: np.ndarray,\n                        volumes: Optional[np.ndarray] = None,\n                        lookback: int = 100) -> Dict[str, float]:\n    \"\"\"\n    Convenience function to generate RL features.\n\n    Args:\n        returns: Return series\n        prices: Price series\n        volumes: Volume series (optional)\n        lookback: Lookback period\n\n    Returns:\n        Dictionary of RL features\n    \"\"\"\n    generator = RLFeatureGenerator(lookback=lookback)\n    return generator.generate(returns, prices, volumes)",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "objective",
    "category": "feature_engineering",
    "formula": "np.inf | np.inf | (1/t) * np.log(moment) + (1/t) * np.log(1/alpha)",
    "explanation": "",
    "python_code": "def objective(t):\n            if t <= 0:\n                return np.inf\n            try:\n                moment = np.mean(np.exp(t * returns))\n                if moment <= 0:\n                    return np.inf\n                return (1/t) * np.log(moment) + (1/t) * np.log(1/alpha)\n            except (OverflowError, RuntimeWarning):\n                return np.inf",
    "source_file": "core\\features\\rl_research.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Initialize multi-scale decomposition.\n\nArgs:\n    scales: List of scales (window sizes)",
    "python_code": "def __init__(self, scales: List[int] = None):\n        \"\"\"\n        Initialize multi-scale decomposition.\n\n        Args:\n            scales: List of scales (window sizes)\n        \"\"\"\n        self.scales = scales or [5, 10, 20, 40, 80]",
    "source_file": "core\\features\\timeseries_features.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": "MultiScaleDecomposition"
  },
  {
    "name": "decompose",
    "category": "feature_engineering",
    "formula": "features",
    "explanation": "Decompose series into trend and seasonal at multiple scales.\n\nReturns DataFrame with trend_N and seasonal_N for each scale.",
    "python_code": "def decompose(self, series: pd.Series) -> pd.DataFrame:\n        \"\"\"\n        Decompose series into trend and seasonal at multiple scales.\n\n        Returns DataFrame with trend_N and seasonal_N for each scale.\n        \"\"\"\n        features = pd.DataFrame(index=series.index)\n\n        for scale in self.scales:\n            # Trend component (moving average)\n            trend = series.rolling(scale, min_periods=1).mean()\n            features[f'TREND_{scale}'] = trend / (series + 1e-12)\n\n            # Seasonal/residual component\n            seasonal = series - trend\n            features[f'SEASONAL_{scale}'] = seasonal / (series.rolling(20).std() + 1e-12)\n\n        return features",
    "source_file": "core\\features\\timeseries_features.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": "MultiScaleDecomposition"
  },
  {
    "name": "extract_patch_features",
    "category": "feature_engineering",
    "formula": "features",
    "explanation": "Extract statistics from patches.\n\nReturns features for each patch: mean, std, trend.",
    "python_code": "def extract_patch_features(self, series: pd.Series) -> pd.DataFrame:\n        \"\"\"\n        Extract statistics from patches.\n\n        Returns features for each patch: mean, std, trend.\n        \"\"\"\n        features = pd.DataFrame(index=series.index)\n        total_window = self.patch_size * self.num_patches\n\n        for i in range(self.num_patches):\n            start_offset = i * self.patch_size\n            end_offset = (i + 1) * self.patch_size\n\n            # Patch mean (relative to current value)\n            patch_mean = series.rolling(total_window).apply(\n                lambda x: x[-end_offset:-start_offset].mean() if start_offset > 0\n                else x[-end_offset:].mean(),\n                raw=True\n            )\n            features[f'PATCH{i}_MEAN'] = patch_mean / (series + 1e-12)\n\n            # Patch std\n            patch_std = series.rolling(total_window).apply(\n                lambda x: x[-end_offset:-start_offset].std() if start_offset > 0\n                else x[-end_offset:].std(),\n                raw=True\n            )\n            features[f'PATCH{i}_STD'] = patch_std / (series.rolling(20).std() + 1e-12)\n\n        return features",
    "source_file": "core\\features\\timeseries_features.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": "PatchFeatures"
  },
  {
    "name": "detect_periods",
    "category": "feature_engineering",
    "formula": "[5, 10, 20] | [5, 10, 20] | sorted(periods)",
    "explanation": "Detect dominant periods using FFT.",
    "python_code": "def detect_periods(self, series: np.ndarray) -> List[int]:\n        \"\"\"Detect dominant periods using FFT.\"\"\"\n        n = len(series)\n        if n < 10:\n            return [5, 10, 20]\n\n        # Compute FFT\n        fft_vals = np.abs(fft(series - series.mean()))\n        freqs = np.fft.fftfreq(n)\n\n        # Get positive frequencies only\n        pos_mask = freqs > 0\n        fft_pos = fft_vals[pos_mask]\n        freqs_pos = freqs[pos_mask]\n\n        # Find top-k peaks\n        if len(fft_pos) < self.top_k_periods:\n            return [5, 10, 20]\n\n        top_indices = np.argsort(fft_pos)[-self.top_k_periods:]\n        periods = [int(1 / freqs_pos[i]) if freqs_pos[i] > 0 else 10\n                   for i in top_indices]\n\n        # Filter valid periods\n        periods = [p for p in periods if 2 < p < 100]\n        if not periods:\n            periods = [5, 10, 20]\n\n        return sorted(periods)",
    "source_file": "core\\features\\timeseries_features.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": "TemporalVariation2D"
  },
  {
    "name": "extract_period_features",
    "category": "feature_engineering",
    "formula": "features",
    "explanation": "Extract features based on detected periods.",
    "python_code": "def extract_period_features(self, series: pd.Series) -> pd.DataFrame:\n        \"\"\"\n        Extract features based on detected periods.\n        \"\"\"\n        features = pd.DataFrame(index=series.index)\n\n        # Use fixed periods (more stable for trading)\n        periods = [5, 10, 20, 40]\n\n        for period in periods:\n            # Autocorrelation at period lag\n            autocorr = series.rolling(period * 2).apply(\n                lambda x: pd.Series(x).autocorr(lag=period) if len(x) > period else 0,\n                raw=False\n            )\n            features[f'PERIOD{period}_AUTOCORR'] = autocorr\n\n            # Period-aligned returns\n            features[f'PERIOD{period}_RET'] = series / series.shift(period) - 1\n\n        return features",
    "source_file": "core\\features\\timeseries_features.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": "TemporalVariation2D"
  },
  {
    "name": "extract_variate_interactions",
    "category": "feature_engineering",
    "formula": "features",
    "explanation": "Extract cross-variate interaction features.\n\nFor OHLCV data, creates interactions between price components.",
    "python_code": "def extract_variate_interactions(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Extract cross-variate interaction features.\n\n        For OHLCV data, creates interactions between price components.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n\n        if 'open' in df.columns and 'close' in df.columns:\n            # Open-Close relationship\n            features['VAR_OC_RATIO'] = df['open'] / (df['close'] + 1e-12)\n            features['VAR_OC_DIFF'] = (df['close'] - df['open']) / (df['open'] + 1e-12)\n\n        if 'high' in df.columns and 'low' in df.columns:\n            # High-Low relationship\n            features['VAR_HL_RATIO'] = df['high'] / (df['low'] + 1e-12)\n            features['VAR_HL_RANGE'] = (df['high'] - df['low']) / (df['close'] + 1e-12)\n\n            # Body vs wick ratios\n            if 'open' in df.columns and 'close' in df.columns:\n                body = np.abs(df['close'] - df['open'])\n                upper_wick = df['high'] - np.maximum(df['open'], df['close'])\n                lower_wick = np.minimum(df['open'], df['close']) - df['low']\n                total_range = df['high'] - df['low'] + 1e-12\n\n                features['VAR_BODY_PCT'] = body / total_range\n                features['VAR_UPPER_WICK'] = upper_wick / total_range\n                features['VAR_LOWER_WICK'] = lower_wick / total_range\n\n        if 'volume' in df.columns and 'close' in df.columns:\n            # Volume-price relationship\n            returns = df['close'].pct_change()\n            features['VAR_VOL_RET_CORR'] = returns.rolling(20).corr(\n                df['volume'].pct_change()\n            )\n\n        return features",
    "source_file": "core\\features\\timeseries_features.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": "InvertedVariateFeatures"
  },
  {
    "name": "generate_all",
    "category": "feature_engineering",
    "formula": "features",
    "explanation": "Generate all Time-Series-Library inspired features.\n\nArgs:\n    df: DataFrame with OHLCV data\n\nReturns:\n    DataFrame with ~45 features",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all Time-Series-Library inspired features.\n\n        Args:\n            df: DataFrame with OHLCV data\n\n        Returns:\n            DataFrame with ~45 features\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n\n        # 1. Multi-scale decomposition (TimeMixer style)\n        ms_features = self.multi_scale.decompose(close)\n        features = pd.concat([features, ms_features], axis=1)\n\n        # 2. Patch features (PatchTST style)\n        patch_features = self.patch.extract_patch_features(close)\n        features = pd.concat([features, patch_features], axis=1)\n\n        # 3. Period features (TimesNet style)\n        period_features = self.temporal.extract_period_features(close)\n        features = pd.concat([features, period_features], axis=1)\n\n        # 4. Variate interactions (iTransformer style)\n        variate_features = self.inverted.extract_variate_interactions(df)\n        features = pd.concat([features, variate_features], axis=1)\n\n        # 5. Additional deep learning inspired features\n\n        # Attention-like features: softmax of returns\n        returns = close.pct_change()\n        for window in [5, 10, 20]:\n            # Softmax attention weights proxy\n            ret_std = returns.rolling(window).std()\n            normalized_ret = returns / (ret_std + 1e-12)\n            exp_ret = np.exp(normalized_ret.clip(-5, 5))  # Clip to prevent overflow\n            features[f'ATTN_WEIGHT_{window}'] = exp_ret / (exp_ret.rolling(window).sum() + 1e-12)\n\n        # Positional encoding proxy: sin/cos of position in trend\n        trend_pos = (close - close.rolling(60).min()) / (\n            close.rolling(60).max() - close.rolling(60).min() + 1e-12\n        )\n        features['POS_SIN'] = np.sin(2 * np.pi * trend_pos)\n        features['POS_COS'] = np.cos(2 * np.pi * trend_pos)\n\n        # Layer normalization proxy: z-score\n        features['LAYER_NORM'] = (close - close.rolling(20).mean()) / (\n            close.rolling(20).std() + 1e-12\n        )\n\n        # Residual connection proxy: current vs smoothed\n        features['RESIDUAL'] = close / close.rolling(10).mean() - 1\n\n        # Clean up\n        features = features.replace([np.inf, -np.inf], np.nan)\n        features = features.fillna(0)\n\n        return features",
    "source_file": "core\\features\\timeseries_features.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": "TimeSeriesLibraryFeatures"
  },
  {
    "name": "get_feature_names",
    "category": "feature_engineering",
    "formula": "names",
    "explanation": "Get list of feature names.",
    "python_code": "def get_feature_names(self) -> List[str]:\n        \"\"\"Get list of feature names.\"\"\"\n        names = []\n\n        # Multi-scale\n        for scale in self.multi_scale.scales:\n            names.extend([f'TREND_{scale}', f'SEASONAL_{scale}'])\n\n        # Patch\n        for i in range(self.patch.num_patches):\n            names.extend([f'PATCH{i}_MEAN', f'PATCH{i}_STD'])\n\n        # Period\n        for period in [5, 10, 20, 40]:\n            names.extend([f'PERIOD{period}_AUTOCORR', f'PERIOD{period}_RET'])\n\n        # Variate\n        names.extend([\n            'VAR_OC_RATIO', 'VAR_OC_DIFF', 'VAR_HL_RATIO', 'VAR_HL_RANGE',\n            'VAR_BODY_PCT', 'VAR_UPPER_WICK', 'VAR_LOWER_WICK', 'VAR_VOL_RET_CORR'\n        ])\n\n        # Attention/position\n        names.extend([\n            'ATTN_WEIGHT_5', 'ATTN_WEIGHT_10', 'ATTN_WEIGHT_20',\n            'POS_SIN', 'POS_COS', 'LAYER_NORM', 'RESIDUAL'\n        ])\n\n        return names",
    "source_file": "core\\features\\timeseries_features.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": "TimeSeriesLibraryFeatures"
  },
  {
    "name": "generate_timeseries_features",
    "category": "feature_engineering",
    "formula": "generator.generate_all(df)",
    "explanation": "Generate Time-Series-Library inspired features.\n\nArgs:\n    df: OHLCV DataFrame\n\nReturns:\n    DataFrame with ~45 features",
    "python_code": "def generate_timeseries_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generate Time-Series-Library inspired features.\n\n    Args:\n        df: OHLCV DataFrame\n\n    Returns:\n        DataFrame with ~45 features\n    \"\"\"\n    generator = TimeSeriesLibraryFeatures()\n    return generator.generate_all(df)",
    "source_file": "core\\features\\timeseries_features.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "risk",
    "formula": "",
    "explanation": "Initialize Universal Math Features.\n\nArgs:\n    ou_half_life: Half-life for OU process (default 20)\n    kelly_fraction: Fractional Kelly for sizing (default 0.25)\n    regime_lookback: Lookback for regime detection (default 60)",
    "python_code": "def __init__(\n        self,\n        ou_half_life: int = 20,        # OU process half-life in periods\n        kelly_fraction: float = 0.25,   # Kelly fraction for position sizing\n        regime_lookback: int = 60,      # Lookback for regime detection\n    ):\n        \"\"\"\n        Initialize Universal Math Features.\n\n        Args:\n            ou_half_life: Half-life for OU process (default 20)\n            kelly_fraction: Fractional Kelly for sizing (default 0.25)\n            regime_lookback: Lookback for regime detection (default 60)\n        \"\"\"\n        self.ou_half_life = ou_half_life\n        self.kelly_frac = kelly_fraction\n        self.regime_lookback = regime_lookback",
    "source_file": "core\\features\\universal_math.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "UniversalMathFeatures"
  },
  {
    "name": "_ou_factors",
    "category": "volatility",
    "formula": "dX_t = theta * (mu - X_t) * dt + sigma * dW_t | pd.Series(result, index=series.index) | features",
    "explanation": "Ornstein-Uhlenbeck Process Features.\n\nThe OU process models mean-reverting spreads:\ndX_t = theta * (mu - X_t) * dt + sigma * dW_t\n\nParameters:\n- theta: Speed of mean reversion\n- mu: Long-run mean\n- sigma: Volatility\n\nTrading signals:\n- Deviation from mean (z-score)\n- Expected time to reversion\n- Optimal entry/exit thresholds\n\nReferences:\n- arXiv:1811.09312: \"Statistical Arbitrage with Pairs Trading\"\n- Hudson & Thames: \"Mean Reversion Module\"",
    "python_code": "def _ou_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Ornstein-Uhlenbeck Process Features.\n\n        The OU process models mean-reverting spreads:\n        dX_t = theta * (mu - X_t) * dt + sigma * dW_t\n\n        Parameters:\n        - theta: Speed of mean reversion\n        - mu: Long-run mean\n        - sigma: Volatility\n\n        Trading signals:\n        - Deviation from mean (z-score)\n        - Expected time to reversion\n        - Optimal entry/exit thresholds\n\n        References:\n        - arXiv:1811.09312: \"Statistical Arbitrage with Pairs Trading\"\n        - Hudson & Thames: \"Mean Reversion Module\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. OU spread: deviation from rolling mean\n        # This is the \"spread\" in pairs trading terminology\n        mu = close.rolling(self.ou_half_life * 2, min_periods=10).mean()\n        spread = close - mu\n        std = close.rolling(self.ou_half_life * 2, min_periods=10).std()\n        features['UNI_OU_ZSCORE'] = spread / (std + 1e-12)\n\n        # 2. Mean reversion speed (theta) estimate\n        # theta = -log(autocorr) / dt\n        # Higher theta = faster mean reversion\n        def estimate_theta(series, window):\n            result = np.zeros(len(series))\n            for i in range(window, len(series)):\n                subseries = series.iloc[i-window:i]\n                if len(subseries) > 10:\n                    autocorr = subseries.autocorr(lag=1)\n                    if autocorr > 0 and autocorr < 1:\n                        result[i] = -np.log(autocorr)\n                    else:\n                        result[i] = 0\n            return pd.Series(result, index=series.index)\n\n        theta = estimate_theta(spread, 30)\n        features['UNI_OU_THETA'] = theta\n\n        # 3. Half-life of mean reversion\n        # t_half = ln(2) / theta\n        features['UNI_OU_HALFLIFE'] = np.log(2) / (theta + 1e-12)\n        features['UNI_OU_HALFLIFE'] = features['UNI_OU_HALFLIFE'].clip(-100, 100)\n\n        # 4. OU trading signal: entry when z-score is extreme\n        # Standard entry: |z| > 2, exit when |z| < 0.5\n        z = features['UNI_OU_ZSCORE']\n        features['UNI_OU_ENTRY'] = np.where(z > 2, -1,\n                                            np.where(z < -2, 1, 0))\n\n        # 5. Expected profit from mean reversion\n        # E[profit] = spread * (1 - exp(-theta * t))\n        expected_reversion = spread * (1 - np.exp(-theta.clip(0, 1) * 5))\n        features['UNI_OU_EXPECTED'] = expected_reversion / (std + 1e-12)\n\n        return features",
    "source_file": "core\\features\\universal_math.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "UniversalMathFeatures"
  },
  {
    "name": "_cointegration_factors",
    "category": "reinforcement_learning",
    "formula": "pd.Series(result, index=y.index) | pd.Series(result, index=y.index) | features",
    "explanation": "Cointegration-Based Features.\n\nEngle-Granger (1987) cointegration for single series:\n- Error correction model\n- Long-run equilibrium deviation\n- Speed of adjustment\n\nFor single series, we use self-cointegration with lagged values.\n\nReferences:\n- Engle & Granger (1987): \"Co-integration and Error Correction\"\n- Nobel Prize in Economics 2003",
    "python_code": "def _cointegration_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Cointegration-Based Features.\n\n        Engle-Granger (1987) cointegration for single series:\n        - Error correction model\n        - Long-run equilibrium deviation\n        - Speed of adjustment\n\n        For single series, we use self-cointegration with lagged values.\n\n        References:\n        - Engle & Granger (1987): \"Co-integration and Error Correction\"\n        - Nobel Prize in Economics 2003\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Log price for cointegration analysis\n        log_price = np.log(close + 1)\n\n        # 2. Error correction term: deviation from trend\n        # Using Hodrick-Prescott-like decomposition\n        trend = log_price.rolling(120, min_periods=20).mean()\n        error = log_price - trend\n        features['UNI_COINT_ERROR'] = error\n\n        # 3. Speed of adjustment: how fast errors correct\n        # Delta(price) = alpha * error(-1) + ...\n        error_lag = error.shift(1)\n        delta_price = log_price.diff()\n\n        def rolling_regression_coef(y, x, window):\n            result = np.zeros(len(y))\n            for i in range(window, len(y)):\n                y_sub = y.iloc[i-window:i].dropna()\n                x_sub = x.iloc[i-window:i].dropna()\n                if len(y_sub) > 10 and len(x_sub) > 10:\n                    try:\n                        slope, _, _, _, _ = stats.linregress(x_sub, y_sub)\n                        result[i] = slope\n                    except:\n                        result[i] = 0\n            return pd.Series(result, index=y.index)\n\n        alpha = rolling_regression_coef(delta_price, error_lag, 60)\n        features['UNI_COINT_ALPHA'] = alpha\n\n        # 4. Cointegration strength: R-squared of error correction\n        def rolling_r2(y, x, window):\n            result = np.zeros(len(y))\n            for i in range(window, len(y)):\n                y_sub = y.iloc[i-window:i].dropna()\n                x_sub = x.iloc[i-window:i].dropna()\n                if len(y_sub) > 10 and len(x_sub) > 10:\n                    try:\n                        _, _, r, _, _ = stats.linregress(x_sub, y_sub)\n                        result[i] = r ** 2\n                    except:\n                        result[i] = 0\n            return pd.Series(result, index=y.index)\n\n        r2 = rolling_r2(delta_price, error_lag, 60)\n        features['UNI_COINT_R2'] = r2\n\n        # 5. Disequilibrium signal: large errors predict correction\n        error_zscore = error / (error.rolling(60, min_periods=10).std() + 1e-12)\n        features['UNI_COINT_SIGNAL'] = -error_zscore  # Contrarian\n\n        return features",
    "source_file": "core\\features\\universal_math.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "UniversalMathFeatures"
  },
  {
    "name": "_market_making_factors",
    "category": "volatility",
    "formula": "bid = S - spread/2, ask = S + spread/2 | spread = gamma * sigma^2 * T + (2/gamma) * ln(1 + gamma/k) | features",
    "explanation": "Avellaneda-Stoikov Market Making Features.\n\nOptimal market making quotes:\nbid = S - spread/2, ask = S + spread/2\nspread = gamma * sigma^2 * T + (2/gamma) * ln(1 + gamma/k)\n\nParameters:\n- gamma: Risk aversion\n- sigma: Volatility\n- k: Order arrival rate\n- T: Time remaining\n\nReferences:\n- Avellaneda & Stoikov (2008): \"HFT in a limit order book\"\n- Cartea et al (2015): \"Algorithmic and HF Trading\"",
    "python_code": "def _market_making_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Avellaneda-Stoikov Market Making Features.\n\n        Optimal market making quotes:\n        bid = S - spread/2, ask = S + spread/2\n        spread = gamma * sigma^2 * T + (2/gamma) * ln(1 + gamma/k)\n\n        Parameters:\n        - gamma: Risk aversion\n        - sigma: Volatility\n        - k: Order arrival rate\n        - T: Time remaining\n\n        References:\n        - Avellaneda & Stoikov (2008): \"HFT in a limit order book\"\n        - Cartea et al (2015): \"Algorithmic and HF Trading\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        high = df.get('high', close)\n        low = df.get('low', close)\n        returns = close.pct_change()\n\n        # 1. Volatility for spread calculation\n        sigma = returns.rolling(20, min_periods=5).std()\n        features['UNI_MM_VOL'] = sigma\n\n        # 2. Optimal spread proxy: volatility-based\n        # spread ~ gamma * sigma^2\n        gamma = 0.1  # Risk aversion parameter\n        optimal_spread = gamma * (sigma ** 2) * 252  # Annualized\n        features['UNI_MM_SPREAD'] = optimal_spread\n\n        # 3. Inventory imbalance: accumulated signed flow\n        signed_returns = np.sign(returns) * returns.abs()\n        inventory = signed_returns.rolling(20, min_periods=1).sum()\n        features['UNI_MM_INV'] = inventory\n\n        # 4. Inventory skew adjustment: quotes adjusted for inventory\n        # When long inventory, lower bid (reduce buying)\n        skew = -inventory / (sigma + 1e-12)\n        features['UNI_MM_SKEW'] = skew\n\n        # 5. Market making profitability: realized spread proxy\n        # Half-spread = (high - low) / (2 * close)\n        half_spread = (high - low) / (2 * close + 1e-12)\n        # Subtract volatility cost\n        mm_pnl = half_spread - sigma\n        features['UNI_MM_PNL'] = mm_pnl.rolling(20, min_periods=1).mean()\n\n        return features",
    "source_file": "core\\features\\universal_math.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "UniversalMathFeatures"
  },
  {
    "name": "_kelly_factors",
    "category": "risk",
    "formula": "b = edge / odds | features",
    "explanation": "Kelly Criterion Position Sizing Features.\n\nKelly (1956) optimal bet sizing:\nf* = (p * b - q) / b = edge / odds\n\nWhere:\n- p: Win probability\n- q: Loss probability (1 - p)\n- b: Win/loss ratio (odds)\n- f*: Optimal fraction of capital\n\nFractional Kelly (25%) is used in practice for smoother equity curves.\n\nReferences:\n- Kelly (1956): \"A New Interpretation of Information Rate\"\n- Thorp (2006): \"The Kelly Criterion in Blackjack\"",
    "python_code": "def _kelly_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Kelly Criterion Position Sizing Features.\n\n        Kelly (1956) optimal bet sizing:\n        f* = (p * b - q) / b = edge / odds\n\n        Where:\n        - p: Win probability\n        - q: Loss probability (1 - p)\n        - b: Win/loss ratio (odds)\n        - f*: Optimal fraction of capital\n\n        Fractional Kelly (25%) is used in practice for smoother equity curves.\n\n        References:\n        - Kelly (1956): \"A New Interpretation of Information Rate\"\n        - Thorp (2006): \"The Kelly Criterion in Blackjack\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Win probability: fraction of positive returns\n        win_prob = (returns > 0).rolling(60, min_periods=20).mean()\n        features['UNI_KELLY_WINP'] = win_prob\n\n        # 2. Win/loss ratio: average win / average loss\n        wins = returns.where(returns > 0, np.nan)\n        losses = returns.where(returns < 0, np.nan).abs()\n        avg_win = wins.rolling(60, min_periods=10).mean()\n        avg_loss = losses.rolling(60, min_periods=10).mean()\n        win_loss_ratio = avg_win / (avg_loss + 1e-12)\n        features['UNI_KELLY_RATIO'] = win_loss_ratio\n\n        # 3. Full Kelly fraction: f* = (p * b - q) / b\n        q = 1 - win_prob\n        full_kelly = (win_prob * win_loss_ratio - q) / (win_loss_ratio + 1e-12)\n        features['UNI_KELLY_FULL'] = full_kelly\n\n        # 4. Fractional Kelly: conservative sizing\n        frac_kelly = full_kelly * self.kelly_frac\n        frac_kelly = frac_kelly.clip(-1, 1)  # Cap at 100% of capital\n        features['UNI_KELLY_FRAC'] = frac_kelly\n\n        # 5. Kelly edge: expected edge per trade\n        edge = win_prob * avg_win - q * avg_loss\n        features['UNI_KELLY_EDGE'] = edge\n\n        return features",
    "source_file": "core\\features\\universal_math.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "UniversalMathFeatures"
  },
  {
    "name": "_attention_factors",
    "category": "reinforcement_learning",
    "formula": "to past returns | pd.Series(result, index=series.index) | pd.Series(result, index=series.index)",
    "explanation": "Attention-Based Feature Weighting.\n\nInspired by Transformer architecture (Vaswani et al 2017) and\nQuantformer (arXiv:2404.00424):\n- Self-attention: relative importance of past observations\n- Feature importance: which features matter most recently\n\nSimplified implementation without full transformer:\n- Attention weights based on similarity to recent pattern\n- Weighted moving averages\n\nReferences:\n- arXiv:2404.00424: \"Quantformer: Transformer-based Trading\" (2024)\n- Vaswani et al (2017): \"Attention Is All You Need\"",
    "python_code": "def _attention_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Attention-Based Feature Weighting.\n\n        Inspired by Transformer architecture (Vaswani et al 2017) and\n        Quantformer (arXiv:2404.00424):\n        - Self-attention: relative importance of past observations\n        - Feature importance: which features matter most recently\n\n        Simplified implementation without full transformer:\n        - Attention weights based on similarity to recent pattern\n        - Weighted moving averages\n\n        References:\n        - arXiv:2404.00424: \"Quantformer: Transformer-based Trading\" (2024)\n        - Vaswani et al (2017): \"Attention Is All You Need\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Self-attention: similarity of current return to past returns\n        def attention_weights(series, query_len=5, key_len=20):\n            \"\"\"Compute attention-weighted average.\"\"\"\n            result = np.zeros(len(series))\n            for i in range(key_len + query_len, len(series)):\n                query = series.iloc[i-query_len:i].values\n                keys = series.iloc[i-key_len-query_len:i-query_len].values\n\n                # Reshape for attention\n                if len(query) < query_len or len(keys) < key_len:\n                    continue\n\n                # Simple dot-product attention\n                # Score = softmax(Q . K / sqrt(d))\n                scores = np.zeros(key_len)\n                for j in range(key_len - query_len + 1):\n                    key_segment = keys[j:j+query_len]\n                    score = np.dot(query, key_segment) / (np.linalg.norm(query) * np.linalg.norm(key_segment) + 1e-12)\n                    scores[j] = score\n\n                # Softmax\n                scores = np.exp(scores - np.max(scores))\n                weights = scores / (scores.sum() + 1e-12)\n\n                # Weighted value\n                values = series.iloc[i-key_len:i].values[-len(weights):]\n                if len(values) == len(weights):\n                    result[i] = np.dot(weights, values)\n\n            return pd.Series(result, index=series.index)\n\n        attention_ret = attention_weights(returns.fillna(0), 5, 20)\n        features['UNI_ATTN_RET'] = attention_ret\n\n        # 2. Attention to volatility regime\n        vol = returns.rolling(5, min_periods=2).std()\n        attention_vol = attention_weights(vol.fillna(0), 5, 20)\n        features['UNI_ATTN_VOL'] = attention_vol\n\n        # 3. Momentum attention: weighted by recency\n        # Exponential weighting (recent matters more)\n        weights = np.exp(-0.1 * np.arange(20))[::-1]\n        weights = weights / weights.sum()\n\n        def exp_weighted_ma(series, w):\n            result = np.zeros(len(series))\n            for i in range(len(w), len(series)):\n                result[i] = np.dot(series.iloc[i-len(w):i].values, w)\n            return pd.Series(result, index=series.index)\n\n        fea",
    "source_file": "core\\features\\universal_math.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "UniversalMathFeatures"
  },
  {
    "name": "_regime_factors",
    "category": "volatility",
    "formula": "pd.Series(result, index=regime_series.index) | features",
    "explanation": "Markov Regime Switching Features.\n\nHamilton (1989) regime switching model:\n- Multiple market regimes (bull/bear, high/low vol)\n- Probability of being in each regime\n- Transition probabilities\n\nSimplified implementation using:\n- Volatility regime detection\n- Trend regime detection\n- Combined regime indicator\n\nReferences:\n- Hamilton (1989): \"A New Approach to Economic Analysis\n  of Nonstationary Time Series and the Business Cycle\"\n- Guidolin (2011): \"Markov Switching Models in Empirical Finance\"",
    "python_code": "def _regime_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Markov Regime Switching Features.\n\n        Hamilton (1989) regime switching model:\n        - Multiple market regimes (bull/bear, high/low vol)\n        - Probability of being in each regime\n        - Transition probabilities\n\n        Simplified implementation using:\n        - Volatility regime detection\n        - Trend regime detection\n        - Combined regime indicator\n\n        References:\n        - Hamilton (1989): \"A New Approach to Economic Analysis\n          of Nonstationary Time Series and the Business Cycle\"\n        - Guidolin (2011): \"Markov Switching Models in Empirical Finance\"\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # 1. Volatility regime: high vs low vol\n        vol = returns.rolling(20, min_periods=5).std()\n        vol_median = vol.rolling(self.regime_lookback, min_periods=20).median()\n        vol_regime = np.where(vol > vol_median, 1, 0)  # 1 = high vol\n        features['UNI_REG_VOL'] = vol_regime\n\n        # 2. Trend regime: bull vs bear\n        ma_short = close.rolling(10, min_periods=1).mean()\n        ma_long = close.rolling(50, min_periods=10).mean()\n        trend_regime = np.where(ma_short > ma_long, 1, -1)  # 1 = bull, -1 = bear\n        features['UNI_REG_TREND'] = trend_regime\n\n        # 3. Regime probability: smooth transition\n        # Use sigmoid of z-score as probability proxy\n        vol_zscore = (vol - vol.rolling(60, min_periods=10).mean()) / (vol.rolling(60, min_periods=10).std() + 1e-12)\n        vol_prob = expit(vol_zscore)  # P(high vol regime)\n        features['UNI_REG_VOL_PROB'] = vol_prob\n\n        # 4. Regime persistence: how long in current regime\n        def regime_duration(regime_series):\n            \"\"\"Count consecutive periods in same regime.\"\"\"\n            result = np.zeros(len(regime_series))\n            count = 1\n            for i in range(1, len(regime_series)):\n                if regime_series.iloc[i] == regime_series.iloc[i-1]:\n                    count += 1\n                else:\n                    count = 1\n                result[i] = count\n            return pd.Series(result, index=regime_series.index)\n\n        duration = regime_duration(pd.Series(trend_regime, index=df.index))\n        features['UNI_REG_DURATION'] = duration\n\n        # 5. Combined regime indicator\n        # 2 = bull + high vol (risky rally)\n        # 1 = bull + low vol (calm uptrend)\n        # -1 = bear + low vol (calm downtrend)\n        # -2 = bear + high vol (crisis)\n        combined = trend_regime * (1 + vol_regime)\n        features['UNI_REG_COMBINED'] = combined\n\n        return features",
    "source_file": "core\\features\\universal_math.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "UniversalMathFeatures"
  },
  {
    "name": "generate_all",
    "category": "reinforcement_learning",
    "formula": "features",
    "explanation": "Generate all Universal Math features.\n\nArgs:\n    df: DataFrame with columns: open, high, low, close, volume\n\nReturns:\n    DataFrame with 30 factor columns",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all Universal Math features.\n\n        Args:\n            df: DataFrame with columns: open, high, low, close, volume\n\n        Returns:\n            DataFrame with 30 factor columns\n        \"\"\"\n        # Ensure required columns\n        if 'close' not in df.columns:\n            raise ValueError(\"Missing required column: 'close'\")\n\n        # Fill missing OHLC from close\n        df = df.copy()\n        if 'open' not in df.columns:\n            df['open'] = df['close'].shift(1).fillna(df['close'])\n        if 'high' not in df.columns:\n            df['high'] = df['close']\n        if 'low' not in df.columns:\n            df['low'] = df['close']\n\n        # Generate all factor groups\n        ou = self._ou_factors(df)\n        coint = self._cointegration_factors(df)\n        mm = self._market_making_factors(df)\n        kelly = self._kelly_factors(df)\n        attention = self._attention_factors(df)\n        regime = self._regime_factors(df)\n\n        # Combine all features\n        features = pd.concat([\n            ou, coint, mm, kelly, attention, regime\n        ], axis=1)\n\n        # Clean up\n        features = features.replace([np.inf, -np.inf], np.nan)\n        features = features.fillna(0)\n\n        return features",
    "source_file": "core\\features\\universal_math.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "UniversalMathFeatures"
  },
  {
    "name": "get_feature_names",
    "category": "feature_engineering",
    "formula": "names",
    "explanation": "Get list of all feature names.",
    "python_code": "def get_feature_names(self) -> List[str]:\n        \"\"\"Get list of all feature names.\"\"\"\n        names = []\n\n        # OU (5)\n        names.extend(['UNI_OU_ZSCORE', 'UNI_OU_THETA', 'UNI_OU_HALFLIFE',\n                      'UNI_OU_ENTRY', 'UNI_OU_EXPECTED'])\n\n        # Cointegration (5)\n        names.extend(['UNI_COINT_ERROR', 'UNI_COINT_ALPHA', 'UNI_COINT_R2',\n                      'UNI_COINT_SIGNAL', 'UNI_COINT_SIGNAL'])\n\n        # Market Making (5)\n        names.extend(['UNI_MM_VOL', 'UNI_MM_SPREAD', 'UNI_MM_INV',\n                      'UNI_MM_SKEW', 'UNI_MM_PNL'])\n\n        # Kelly (5)\n        names.extend(['UNI_KELLY_WINP', 'UNI_KELLY_RATIO', 'UNI_KELLY_FULL',\n                      'UNI_KELLY_FRAC', 'UNI_KELLY_EDGE'])\n\n        # Attention (5)\n        names.extend(['UNI_ATTN_RET', 'UNI_ATTN_VOL', 'UNI_ATTN_MOM',\n                      'UNI_ATTN_EXTREME', 'UNI_ATTN_SIGNAL'])\n\n        # Regime (5)\n        names.extend(['UNI_REG_VOL', 'UNI_REG_TREND', 'UNI_REG_VOL_PROB',\n                      'UNI_REG_DURATION', 'UNI_REG_COMBINED'])\n\n        return names",
    "source_file": "core\\features\\universal_math.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "UniversalMathFeatures"
  },
  {
    "name": "get_factor_category",
    "category": "reinforcement_learning",
    "formula": "'Ornstein-Uhlenbeck' | 'Cointegration' | 'Market Making'",
    "explanation": "Get the category of a factor by name.",
    "python_code": "def get_factor_category(self, factor_name: str) -> str:\n        \"\"\"Get the category of a factor by name.\"\"\"\n        if 'OU_' in factor_name:\n            return 'Ornstein-Uhlenbeck'\n        elif 'COINT' in factor_name:\n            return 'Cointegration'\n        elif 'MM_' in factor_name:\n            return 'Market Making'\n        elif 'KELLY' in factor_name:\n            return 'Kelly Criterion'\n        elif 'ATTN' in factor_name:\n            return 'Attention Weights'\n        elif 'REG_' in factor_name:\n            return 'Regime Detection'\n        return 'Unknown'",
    "source_file": "core\\features\\universal_math.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "UniversalMathFeatures"
  },
  {
    "name": "generate_universal_math_features",
    "category": "technical",
    "formula": "df: OHLCV DataFrame | features.generate_all(df)",
    "explanation": "Generate Universal Mathematical features.\n\nArgs:\n    df: OHLCV DataFrame\n\nReturns:\n    DataFrame with 30 mathematical finance factors",
    "python_code": "def generate_universal_math_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generate Universal Mathematical features.\n\n    Args:\n        df: OHLCV DataFrame\n\n    Returns:\n        DataFrame with 30 mathematical finance factors\n    \"\"\"\n    features = UniversalMathFeatures()\n    return features.generate_all(df)",
    "source_file": "core\\features\\universal_math.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": null
  },
  {
    "name": "estimate_theta",
    "category": "feature_engineering",
    "formula": "pd.Series(result, index=series.index)",
    "explanation": "",
    "python_code": "def estimate_theta(series, window):\n            result = np.zeros(len(series))\n            for i in range(window, len(series)):\n                subseries = series.iloc[i-window:i]\n                if len(subseries) > 10:\n                    autocorr = subseries.autocorr(lag=1)\n                    if autocorr > 0 and autocorr < 1:\n                        result[i] = -np.log(autocorr)\n                    else:\n                        result[i] = 0\n            return pd.Series(result, index=series.index)",
    "source_file": "core\\features\\universal_math.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": null
  },
  {
    "name": "rolling_regression_coef",
    "category": "statistical",
    "formula": "pd.Series(result, index=y.index)",
    "explanation": "",
    "python_code": "def rolling_regression_coef(y, x, window):\n            result = np.zeros(len(y))\n            for i in range(window, len(y)):\n                y_sub = y.iloc[i-window:i].dropna()\n                x_sub = x.iloc[i-window:i].dropna()\n                if len(y_sub) > 10 and len(x_sub) > 10:\n                    try:\n                        slope, _, _, _, _ = stats.linregress(x_sub, y_sub)\n                        result[i] = slope\n                    except:\n                        result[i] = 0\n            return pd.Series(result, index=y.index)",
    "source_file": "core\\features\\universal_math.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": null
  },
  {
    "name": "rolling_r2",
    "category": "statistical",
    "formula": "pd.Series(result, index=y.index)",
    "explanation": "",
    "python_code": "def rolling_r2(y, x, window):\n            result = np.zeros(len(y))\n            for i in range(window, len(y)):\n                y_sub = y.iloc[i-window:i].dropna()\n                x_sub = x.iloc[i-window:i].dropna()\n                if len(y_sub) > 10 and len(x_sub) > 10:\n                    try:\n                        _, _, r, _, _ = stats.linregress(x_sub, y_sub)\n                        result[i] = r ** 2\n                    except:\n                        result[i] = 0\n            return pd.Series(result, index=y.index)",
    "source_file": "core\\features\\universal_math.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": null
  },
  {
    "name": "attention_weights",
    "category": "deep_learning",
    "formula": "pd.Series(result, index=series.index)",
    "explanation": "Compute attention-weighted average.",
    "python_code": "def attention_weights(series, query_len=5, key_len=20):\n            \"\"\"Compute attention-weighted average.\"\"\"\n            result = np.zeros(len(series))\n            for i in range(key_len + query_len, len(series)):\n                query = series.iloc[i-query_len:i].values\n                keys = series.iloc[i-key_len-query_len:i-query_len].values\n\n                # Reshape for attention\n                if len(query) < query_len or len(keys) < key_len:\n                    continue\n\n                # Simple dot-product attention\n                # Score = softmax(Q . K / sqrt(d))\n                scores = np.zeros(key_len)\n                for j in range(key_len - query_len + 1):\n                    key_segment = keys[j:j+query_len]\n                    score = np.dot(query, key_segment) / (np.linalg.norm(query) * np.linalg.norm(key_segment) + 1e-12)\n                    scores[j] = score\n\n                # Softmax\n                scores = np.exp(scores - np.max(scores))\n                weights = scores / (scores.sum() + 1e-12)\n\n                # Weighted value\n                values = series.iloc[i-key_len:i].values[-len(weights):]\n                if len(values) == len(weights):\n                    result[i] = np.dot(weights, values)\n\n            return pd.Series(result, index=series.index)",
    "source_file": "core\\features\\universal_math.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": null
  },
  {
    "name": "exp_weighted_ma",
    "category": "feature_engineering",
    "formula": "pd.Series(result, index=series.index)",
    "explanation": "",
    "python_code": "def exp_weighted_ma(series, w):\n            result = np.zeros(len(series))\n            for i in range(len(w), len(series)):\n                result[i] = np.dot(series.iloc[i-len(w):i].values, w)\n            return pd.Series(result, index=series.index)",
    "source_file": "core\\features\\universal_math.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": null
  },
  {
    "name": "regime_duration",
    "category": "regime",
    "formula": "pd.Series(result, index=regime_series.index)",
    "explanation": "Count consecutive periods in same regime.",
    "python_code": "def regime_duration(regime_series):\n            \"\"\"Count consecutive periods in same regime.\"\"\"\n            result = np.zeros(len(regime_series))\n            count = 1\n            for i in range(1, len(regime_series)):\n                if regime_series.iloc[i] == regime_series.iloc[i-1]:\n                    count += 1\n                else:\n                    count = 1\n                result[i] = count\n            return pd.Series(result, index=regime_series.index)",
    "source_file": "core\\features\\universal_math.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "statistical",
    "formula": "",
    "explanation": "Initialize US Academic Factors.\n\nArgs:\n    windows: Rolling windows (default: [5, 10, 20, 60, 120])",
    "python_code": "def __init__(self, windows: List[int] = None):\n        \"\"\"\n        Initialize US Academic Factors.\n\n        Args:\n            windows: Rolling windows (default: [5, 10, 20, 60, 120])\n        \"\"\"\n        self.windows = windows or [5, 10, 20, 60, 120]",
    "source_file": "core\\features\\us_academic_factors.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "USAcademicFactors"
  },
  {
    "name": "_carry_factors",
    "category": "technical",
    "formula": "(approximates carry) | features",
    "explanation": "CARRY factors - Interest rate differential proxies.\n\nIn real carry trades: Long high yield, short low yield.\nFor single pair: Use forward discount (interest rate differential proxy).\n\nWe approximate using price momentum as carry proxy.",
    "python_code": "def _carry_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        CARRY factors - Interest rate differential proxies.\n\n        In real carry trades: Long high yield, short low yield.\n        For single pair: Use forward discount (interest rate differential proxy).\n\n        We approximate using price momentum as carry proxy.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # Forward discount proxy: rolling mean return (approximates carry)\n        for d in [5, 20, 60]:\n            features[f'CARRY_FWD{d}'] = returns.rolling(d, min_periods=1).mean()\n\n        # Carry momentum: trend in carry\n        carry_proxy = returns.rolling(20, min_periods=1).mean()\n        features['CARRY_MOM5'] = carry_proxy - carry_proxy.shift(5)\n        features['CARRY_MOM20'] = carry_proxy - carry_proxy.shift(20)\n\n        # Carry reversal signal\n        features['CARRY_REV'] = -returns.rolling(5, min_periods=1).mean()\n\n        # Carry volatility adjusted (Sharpe-like)\n        for d in [20, 60]:\n            ret_mean = returns.rolling(d, min_periods=1).mean()\n            ret_std = returns.rolling(d, min_periods=2).std()\n            features[f'CARRY_SR{d}'] = ret_mean / (ret_std + 1e-12)\n\n        return features",
    "source_file": "core\\features\\us_academic_factors.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "USAcademicFactors"
  },
  {
    "name": "_momentum_factors",
    "category": "technical",
    "formula": "TSM = Time series momentum (Moskowitz et al 2012) | * magnitude | features",
    "explanation": "MOMENTUM factors - Cross-sectional and time-series momentum.\n\nMOM(1,1) = 1-month formation, 1-month holding\nTSM = Time series momentum (Moskowitz et al 2012)",
    "python_code": "def _momentum_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        MOMENTUM factors - Cross-sectional and time-series momentum.\n\n        MOM(1,1) = 1-month formation, 1-month holding\n        TSM = Time series momentum (Moskowitz et al 2012)\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # Cross-sectional momentum: past returns\n        for d in [5, 10, 20, 60, 120]:\n            features[f'MOM_XS{d}'] = close / close.shift(d) - 1\n\n        # Time-series momentum (TSM): sign of past return * magnitude\n        for d in [20, 60, 120]:\n            past_ret = close / close.shift(d) - 1\n            features[f'MOM_TS{d}'] = np.sign(past_ret) * np.abs(past_ret)\n\n        # Momentum acceleration\n        mom_20 = close / close.shift(20) - 1\n        mom_60 = close / close.shift(60) - 1\n        features['MOM_ACC'] = mom_20 - (mom_60 / 3)\n\n        # Momentum quality: consistency\n        up_ratio = (returns > 0).rolling(20, min_periods=1).mean()\n        features['MOM_QUAL'] = 2 * up_ratio - 1\n\n        return features",
    "source_file": "core\\features\\us_academic_factors.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "USAcademicFactors"
  },
  {
    "name": "_value_factors",
    "category": "reinforcement_learning",
    "formula": "value = PPP deviation. | deviation | features",
    "explanation": "VALUE factors - PPP deviation proxies.\n\nReal forex value = PPP deviation.\nWe proxy using price deviation from long-term average.",
    "python_code": "def _value_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        VALUE factors - PPP deviation proxies.\n\n        Real forex value = PPP deviation.\n        We proxy using price deviation from long-term average.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n\n        # PPP deviation proxy: distance from moving average\n        for d in [20, 60, 120, 250]:\n            ma = close.rolling(d, min_periods=1).mean()\n            features[f'VAL_PPP{d}'] = (ma - close) / (close + 1e-12)\n\n        # Real exchange rate proxy: cumulative return deviation\n        cum_ret = (close / close.iloc[0] - 1) if len(close) > 0 else close\n        features['VAL_RER'] = cum_ret - cum_ret.rolling(120, min_periods=1).mean()\n\n        # Value momentum: change in value signal\n        val_20 = (close.rolling(20, min_periods=1).mean() - close) / (close + 1e-12)\n        features['VAL_MOM'] = val_20 - val_20.shift(20)\n\n        # Bollinger value (statistical value)\n        ma_60 = close.rolling(60, min_periods=1).mean()\n        std_60 = close.rolling(60, min_periods=2).std()\n        features['VAL_BB'] = (ma_60 - close) / (2 * std_60 + 1e-12)\n\n        # Mean reversion signal\n        features['VAL_MEAN_REV'] = (close.rolling(5, min_periods=1).mean() -\n                                    close.rolling(60, min_periods=1).mean()) / (close + 1e-12)\n\n        return features",
    "source_file": "core\\features\\us_academic_factors.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "USAcademicFactors"
  },
  {
    "name": "_volatility_factors",
    "category": "volatility",
    "formula": "features",
    "explanation": "VOLATILITY factors - FX volatility risk premium.\n\nBased on research showing vol risk premium predicts returns.",
    "python_code": "def _volatility_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        VOLATILITY factors - FX volatility risk premium.\n\n        Based on research showing vol risk premium predicts returns.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        high = df['high']\n        low = df['low']\n        returns = close.pct_change()\n\n        # Realized volatility at different horizons\n        for d in [5, 20, 60]:\n            features[f'VOL_RV{d}'] = returns.rolling(d, min_periods=2).std() * np.sqrt(252)\n\n        # Parkinson volatility (high-low based)\n        log_hl = np.log(high / low + 1e-12)\n        features['VOL_PARK'] = np.sqrt(\n            (log_hl ** 2).rolling(20, min_periods=1).mean() / (4 * np.log(2))\n        ) * np.sqrt(252)\n\n        # Volatility risk premium proxy: short-term vs long-term vol\n        vol_5 = returns.rolling(5, min_periods=2).std()\n        vol_60 = returns.rolling(60, min_periods=2).std()\n        features['VOL_VRP'] = vol_5 - vol_60\n\n        # Volatility trend\n        features['VOL_TREND'] = vol_5 / (vol_60 + 1e-12) - 1\n\n        # Volatility of volatility\n        features['VOL_VOV'] = vol_5.rolling(20, min_periods=2).std()\n\n        # Volatility skew: up vol vs down vol\n        pos_ret = returns.where(returns > 0, 0)\n        neg_ret = returns.where(returns < 0, 0)\n        up_vol = pos_ret.rolling(20, min_periods=2).std()\n        down_vol = np.abs(neg_ret).rolling(20, min_periods=2).std()\n        features['VOL_SKEW'] = (up_vol - down_vol) / (up_vol + down_vol + 1e-12)\n\n        return features",
    "source_file": "core\\features\\us_academic_factors.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "USAcademicFactors"
  },
  {
    "name": "_macro_proxy_factors",
    "category": "reinforcement_learning",
    "formula": "0 | np.polyfit(x, arr, 1)[0] | features",
    "explanation": "MACRO-PROXY factors - Macroeconomic indicator proxies.\n\nWithout actual macro data, we proxy using price patterns that\ncorrelate with macro conditions.",
    "python_code": "def _macro_proxy_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        MACRO-PROXY factors - Macroeconomic indicator proxies.\n\n        Without actual macro data, we proxy using price patterns that\n        correlate with macro conditions.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        returns = close.pct_change()\n\n        # GDP growth proxy: trend strength\n        def calc_slope(arr):\n            if len(arr) < 2:\n                return 0\n            x = np.arange(len(arr))\n            return np.polyfit(x, arr, 1)[0]\n\n        for d in [20, 60]:\n            features[f'MACRO_GDP{d}'] = close.rolling(d, min_periods=2).apply(calc_slope, raw=True)\n\n        # Inflation proxy: price level change\n        features['MACRO_INF20'] = close / close.shift(20) - 1\n        features['MACRO_INF60'] = close / close.shift(60) - 1\n\n        # Economic uncertainty proxy: volatility regime\n        vol_20 = returns.rolling(20, min_periods=2).std()\n        vol_60 = returns.rolling(60, min_periods=2).std()\n        features['MACRO_UNC'] = vol_20 / (vol_60 + 1e-12)\n\n        # Risk appetite proxy: drawdown from high\n        rolling_max = close.rolling(60, min_periods=1).max()\n        features['MACRO_RISK'] = close / rolling_max - 1\n\n        # Growth momentum\n        growth_proxy = close / close.shift(20) - 1\n        features['MACRO_GMOM'] = growth_proxy - growth_proxy.shift(20)\n\n        # Cycle indicator: deviation from trend\n        trend = close.rolling(120, min_periods=1).mean()\n        features['MACRO_CYCLE'] = (close - trend) / (trend + 1e-12)\n\n        return features",
    "source_file": "core\\features\\us_academic_factors.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "USAcademicFactors"
  },
  {
    "name": "_technical_factors",
    "category": "reinforcement_learning",
    "formula": "features",
    "explanation": "TECHNICAL factors - Academic technical analysis.\n\nBased on established technical indicators used in forex research.",
    "python_code": "def _technical_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        TECHNICAL factors - Academic technical analysis.\n\n        Based on established technical indicators used in forex research.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n        close = df['close']\n        high = df['high']\n        low = df['low']\n        returns = close.pct_change()\n\n        # RSI (Relative Strength Index)\n        delta = close.diff()\n        gain = delta.where(delta > 0, 0).rolling(14, min_periods=1).mean()\n        loss = (-delta.where(delta < 0, 0)).rolling(14, min_periods=1).mean()\n        rs = gain / (loss + 1e-12)\n        features['TECH_RSI'] = (100 - (100 / (1 + rs))) / 100 - 0.5\n\n        # MACD\n        ema_12 = close.ewm(span=12, adjust=False).mean()\n        ema_26 = close.ewm(span=26, adjust=False).mean()\n        macd = ema_12 - ema_26\n        signal = macd.ewm(span=9, adjust=False).mean()\n        features['TECH_MACD'] = (macd - signal) / (close + 1e-12)\n\n        # Stochastic Oscillator\n        low_14 = low.rolling(14, min_periods=1).min()\n        high_14 = high.rolling(14, min_periods=1).max()\n        features['TECH_STOCH'] = (close - low_14) / (high_14 - low_14 + 1e-12) - 0.5\n\n        # Williams %R\n        features['TECH_WILLR'] = (high_14 - close) / (high_14 - low_14 + 1e-12) - 0.5\n\n        # Average True Range (normalized)\n        tr = np.maximum(high - low,\n                       np.abs(high - close.shift(1)),\n                       np.abs(low - close.shift(1)))\n        features['TECH_ATR'] = tr.rolling(14, min_periods=1).mean() / (close + 1e-12)\n\n        # Bollinger Band position\n        ma_20 = close.rolling(20, min_periods=1).mean()\n        std_20 = close.rolling(20, min_periods=2).std()\n        features['TECH_BB'] = (close - ma_20) / (2 * std_20 + 1e-12)\n\n        # Moving average crossover signal\n        ma_10 = close.rolling(10, min_periods=1).mean()\n        ma_50 = close.rolling(50, min_periods=1).mean()\n        features['TECH_MAXO'] = (ma_10 - ma_50) / (ma_50 + 1e-12)\n\n        # Price rate of change\n        features['TECH_ROC'] = close / close.shift(10) - 1\n\n        return features",
    "source_file": "core\\features\\us_academic_factors.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "USAcademicFactors"
  },
  {
    "name": "generate_all",
    "category": "reinforcement_learning",
    "formula": "features",
    "explanation": "Generate all US Academic factors.\n\nArgs:\n    df: DataFrame with columns: open, high, low, close, volume\n\nReturns:\n    DataFrame with 50 factor columns",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all US Academic factors.\n\n        Args:\n            df: DataFrame with columns: open, high, low, close, volume\n\n        Returns:\n            DataFrame with 50 factor columns\n        \"\"\"\n        # Ensure required columns\n        required = ['open', 'high', 'low', 'close']\n        for col in required:\n            if col not in df.columns:\n                raise ValueError(f\"Missing required column: {col}\")\n\n        # Generate all factor groups\n        carry = self._carry_factors(df)\n        momentum = self._momentum_factors(df)\n        value = self._value_factors(df)\n        volatility = self._volatility_factors(df)\n        macro = self._macro_proxy_factors(df)\n        technical = self._technical_factors(df)\n\n        # Combine all features\n        features = pd.concat([\n            carry, momentum, value, volatility, macro, technical\n        ], axis=1)\n\n        # Clean up\n        features = features.replace([np.inf, -np.inf], np.nan)\n        features = features.fillna(0)\n\n        return features",
    "source_file": "core\\features\\us_academic_factors.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "USAcademicFactors"
  },
  {
    "name": "get_feature_names",
    "category": "reinforcement_learning",
    "formula": "names",
    "explanation": "Get list of all feature names.",
    "python_code": "def get_feature_names(self) -> List[str]:\n        \"\"\"Get list of all feature names.\"\"\"\n        names = []\n\n        # Carry (8)\n        names.extend(['CARRY_FWD5', 'CARRY_FWD20', 'CARRY_FWD60',\n                      'CARRY_MOM5', 'CARRY_MOM20', 'CARRY_REV',\n                      'CARRY_SR20', 'CARRY_SR60'])\n\n        # Momentum (10)\n        names.extend(['MOM_XS5', 'MOM_XS10', 'MOM_XS20', 'MOM_XS60', 'MOM_XS120',\n                      'MOM_TS20', 'MOM_TS60', 'MOM_TS120', 'MOM_ACC', 'MOM_QUAL'])\n\n        # Value (8)\n        names.extend(['VAL_PPP20', 'VAL_PPP60', 'VAL_PPP120', 'VAL_PPP250',\n                      'VAL_RER', 'VAL_MOM', 'VAL_BB', 'VAL_MEAN_REV'])\n\n        # Volatility (8)\n        names.extend(['VOL_RV5', 'VOL_RV20', 'VOL_RV60', 'VOL_PARK',\n                      'VOL_VRP', 'VOL_TREND', 'VOL_VOV', 'VOL_SKEW'])\n\n        # Macro (8)\n        names.extend(['MACRO_GDP20', 'MACRO_GDP60', 'MACRO_INF20', 'MACRO_INF60',\n                      'MACRO_UNC', 'MACRO_RISK', 'MACRO_GMOM', 'MACRO_CYCLE'])\n\n        # Technical (8)\n        names.extend(['TECH_RSI', 'TECH_MACD', 'TECH_STOCH', 'TECH_WILLR',\n                      'TECH_ATR', 'TECH_BB', 'TECH_MAXO', 'TECH_ROC'])\n\n        return names",
    "source_file": "core\\features\\us_academic_factors.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "USAcademicFactors"
  },
  {
    "name": "get_factor_category",
    "category": "reinforcement_learning",
    "formula": "categories.get(prefix, 'Unknown')",
    "explanation": "Get the category of a factor by name.",
    "python_code": "def get_factor_category(self, factor_name: str) -> str:\n        \"\"\"Get the category of a factor by name.\"\"\"\n        categories = {\n            'CARRY': 'Carry (Interest Rate)',\n            'MOM': 'Momentum',\n            'VAL': 'Value (PPP)',\n            'VOL': 'Volatility',\n            'MACRO': 'Macro-Proxy',\n            'TECH': 'Technical'\n        }\n        prefix = factor_name.split('_')[0]\n        return categories.get(prefix, 'Unknown')",
    "source_file": "core\\features\\us_academic_factors.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "USAcademicFactors"
  },
  {
    "name": "generate_us_academic_factors",
    "category": "reinforcement_learning",
    "formula": "factors.generate_all(df)",
    "explanation": "Generate US Academic forex factors.\n\nArgs:\n    df: OHLCV DataFrame\n\nReturns:\n    DataFrame with 50 academic factors",
    "python_code": "def generate_us_academic_factors(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Generate US Academic forex factors.\n\n    Args:\n        df: OHLCV DataFrame\n\n    Returns:\n        DataFrame with 50 academic factors\n    \"\"\"\n    factors = USAcademicFactors()\n    return factors.generate_all(df)",
    "source_file": "core\\features\\us_academic_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": null
  },
  {
    "name": "calc_slope",
    "category": "reinforcement_learning",
    "formula": "0 | np.polyfit(x, arr, 1)[0]",
    "explanation": "",
    "python_code": "def calc_slope(arr):\n            if len(arr) < 2:\n                return 0\n            x = np.arange(len(arr))\n            return np.polyfit(x, arr, 1)[0]",
    "source_file": "core\\features\\us_academic_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "microstructure",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, config: Optional[VPINConfig] = None):\n        self.config = config or VPINConfig()\n\n        # Current bucket being filled\n        self.current_bucket_volume = 0.0\n        self.current_bucket_buy = 0.0\n        self.current_bucket_sell = 0.0\n\n        # History of completed buckets (buy volume, sell volume)\n        self.bucket_history: deque = deque(maxlen=self.config.n_buckets)\n\n        # BVC: Aggregate trades before classification\n        self.bvc_prices: deque = deque(maxlen=self.config.bvc_window)\n        self.bvc_volumes: deque = deque(maxlen=self.config.bvc_window)\n\n        # Statistics\n        self.total_volume = 0.0\n        self.num_buckets_completed = 0\n\n        logger.info(\n            f\"Initialized VPIN Calculator: \"\n            f\"bucket_size={self.config.bucket_size}, \"\n            f\"n_buckets={self.config.n_buckets}\"\n        )",
    "source_file": "core\\features\\vpin_production.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VPINCalculator"
  },
  {
    "name": "update",
    "category": "microstructure",
    "formula": "VPINResult(",
    "explanation": "Update VPIN with new tick.\n\nArgs:\n    price: Current price\n    prev_price: Previous price\n    volume: Volume of current trade\n\nReturns:\n    VPINResult with current VPIN and toxicity regime",
    "python_code": "def update(\n        self,\n        price: float,\n        prev_price: float,\n        volume: float\n    ) -> VPINResult:\n        \"\"\"\n        Update VPIN with new tick.\n\n        Args:\n            price: Current price\n            prev_price: Previous price\n            volume: Volume of current trade\n\n        Returns:\n            VPINResult with current VPIN and toxicity regime\n        \"\"\"\n        # Add to BVC buffer\n        self.bvc_prices.append(price)\n        self.bvc_volumes.append(volume)\n\n        # Classify trade direction using BVC\n        buy_vol, sell_vol = self._classify_trade_bvc(price, prev_price, volume)\n\n        # Add to current bucket\n        self.current_bucket_volume += volume\n        self.current_bucket_buy += buy_vol\n        self.current_bucket_sell += sell_vol\n        self.total_volume += volume\n\n        # Check if bucket is full\n        if self.current_bucket_volume >= self.config.bucket_size:\n            # Close current bucket\n            self.bucket_history.append((\n                self.current_bucket_buy,\n                self.current_bucket_sell\n            ))\n            self.num_buckets_completed += 1\n\n            # Start new bucket\n            self.current_bucket_volume = 0.0\n            self.current_bucket_buy = 0.0\n            self.current_bucket_sell = 0.0\n\n        # Calculate VPIN\n        vpin, imbalance = self._calculate_vpin()\n\n        # Determine toxicity regime\n        if vpin < self.config.low_threshold:\n            regime = 0  # Low toxicity\n        elif vpin < self.config.high_threshold:\n            regime = 1  # Normal\n        else:\n            regime = 2  # High toxicity\n\n        return VPINResult(\n            vpin=vpin,\n            toxicity_regime=regime,\n            buy_volume=self.current_bucket_buy,\n            sell_volume=self.current_bucket_sell,\n            imbalance=imbalance,\n            num_buckets_filled=len(self.bucket_history)\n        )",
    "source_file": "core\\features\\vpin_production.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VPINCalculator"
  },
  {
    "name": "_classify_trade_bvc",
    "category": "microstructure",
    "formula": "(volume, 0.0) | (0.0, volume) | (volume / 2, volume / 2)",
    "explanation": "Bulk Volume Classification (BVC).\n\nMore robust than tick rule in HFT environments.\n\nBased on Easley et al. (2012) Section 2.2:\n\"The speed and volume of trading in high frequency markets challenges\ntraditional classification schemes. BVC aggregates trades over short\ntime or volume intervals.\"\n\nArgs:\n    price: Current price\n    prev_price: Previous price\n    volume: Trade volume\n\nReturns:\n    (buy_volume, sell_volume)",
    "python_code": "def _classify_trade_bvc(\n        self,\n        price: float,\n        prev_price: float,\n        volume: float\n    ) -> Tuple[float, float]:\n        \"\"\"\n        Bulk Volume Classification (BVC).\n\n        More robust than tick rule in HFT environments.\n\n        Based on Easley et al. (2012) Section 2.2:\n        \"The speed and volume of trading in high frequency markets challenges\n        traditional classification schemes. BVC aggregates trades over short\n        time or volume intervals.\"\n\n        Args:\n            price: Current price\n            prev_price: Previous price\n            volume: Trade volume\n\n        Returns:\n            (buy_volume, sell_volume)\n        \"\"\"\n        if len(self.bvc_prices) < 2:\n            # Not enough data, use simple tick rule\n            return self._tick_rule(price, prev_price, volume)\n\n        # BVC: Use price change from start to end of window\n        start_price = self.bvc_prices[0]\n        end_price = price\n\n        if end_price > start_price:\n            # Net buying pressure\n            return (volume, 0.0)\n        elif end_price < start_price:\n            # Net selling pressure\n            return (0.0, volume)\n        else:\n            # No change, split 50/50\n            return (volume / 2, volume / 2)",
    "source_file": "core\\features\\vpin_production.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VPINCalculator"
  },
  {
    "name": "_tick_rule",
    "category": "microstructure",
    "formula": "(volume, 0.0)  # Uptick = buy | (0.0, volume)  # Downtick = sell | (volume / 2, volume / 2)",
    "explanation": "Simple tick rule classification.\n\nFallback when BVC buffer not full.\n\nArgs:\n    price: Current price\n    prev_price: Previous price\n    volume: Trade volume\n\nReturns:\n    (buy_volume, sell_volume)",
    "python_code": "def _tick_rule(\n        self,\n        price: float,\n        prev_price: float,\n        volume: float\n    ) -> Tuple[float, float]:\n        \"\"\"\n        Simple tick rule classification.\n\n        Fallback when BVC buffer not full.\n\n        Args:\n            price: Current price\n            prev_price: Previous price\n            volume: Trade volume\n\n        Returns:\n            (buy_volume, sell_volume)\n        \"\"\"\n        if price > prev_price:\n            return (volume, 0.0)  # Uptick = buy\n        elif price < prev_price:\n            return (0.0, volume)  # Downtick = sell\n        else:\n            return (volume / 2, volume / 2)",
    "source_file": "core\\features\\vpin_production.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VPINCalculator"
  },
  {
    "name": "_calculate_vpin",
    "category": "microstructure",
    "formula": "VPIN = Average of volume imbalances across buckets | 0.0, 0.0 | 0.0, 0.0",
    "explanation": "Calculate VPIN from bucket history.\n\nVPIN = Average of volume imbalances across buckets\n\nReturns:\n    (vpin, imbalance)",
    "python_code": "def _calculate_vpin(self) -> Tuple[float, float]:\n        \"\"\"\n        Calculate VPIN from bucket history.\n\n        VPIN = Average of volume imbalances across buckets\n\n        Returns:\n            (vpin, imbalance)\n        \"\"\"\n        if len(self.bucket_history) == 0:\n            return 0.0, 0.0\n\n        imbalances = []\n        for buy_vol, sell_vol in self.bucket_history:\n            total_vol = buy_vol + sell_vol\n            if total_vol > 0:\n                imbalance = abs(buy_vol - sell_vol) / total_vol\n                imbalances.append(imbalance)\n\n        if len(imbalances) == 0:\n            return 0.0, 0.0\n\n        vpin = np.mean(imbalances)\n        current_imbalance = imbalances[-1] if imbalances else 0.0\n\n        return float(vpin), float(current_imbalance)",
    "source_file": "core\\features\\vpin_production.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VPINCalculator"
  },
  {
    "name": "get_stats",
    "category": "microstructure",
    "formula": "{",
    "explanation": "Get statistics for monitoring.",
    "python_code": "def get_stats(self) -> Dict[str, float]:\n        \"\"\"Get statistics for monitoring.\"\"\"\n        vpin, imbalance = self._calculate_vpin()\n\n        return {\n            'vpin': vpin,\n            'current_imbalance': imbalance,\n            'num_buckets': len(self.bucket_history),\n            'total_volume': self.total_volume,\n            'buckets_completed': self.num_buckets_completed\n        }",
    "source_file": "core\\features\\vpin_production.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VPINCalculator"
  },
  {
    "name": "reset",
    "category": "microstructure",
    "formula": "",
    "explanation": "Reset calculator to initial state.",
    "python_code": "def reset(self):\n        \"\"\"Reset calculator to initial state.\"\"\"\n        self.current_bucket_volume = 0.0\n        self.current_bucket_buy = 0.0\n        self.current_bucket_sell = 0.0\n        self.bucket_history.clear()\n        self.bvc_prices.clear()\n        self.bvc_volumes.clear()\n        self.total_volume = 0.0\n        self.num_buckets_completed = 0\n        logger.info(\"VPIN calculator reset\")",
    "source_file": "core\\features\\vpin_production.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VPINCalculator"
  },
  {
    "name": "update",
    "category": "volatility",
    "formula": "and volatility | {",
    "explanation": "Update and detect regime.\n\nArgs:\n    price: Current price\n    prev_price: Previous price\n    volume: Trade volume\n\nReturns:\n    Dict with VPIN, volatility, and regime classification",
    "python_code": "def update(\n        self,\n        price: float,\n        prev_price: float,\n        volume: float\n    ) -> Dict[str, any]:\n        \"\"\"\n        Update and detect regime.\n\n        Args:\n            price: Current price\n            prev_price: Previous price\n            volume: Trade volume\n\n        Returns:\n            Dict with VPIN, volatility, and regime classification\n        \"\"\"\n        # Update VPIN\n        vpin_result = self.vpin_calc.update(price, prev_price, volume)\n\n        # Calculate return and volatility\n        if prev_price > 0:\n            ret = (price - prev_price) / prev_price\n            self.returns.append(ret)\n\n        volatility = np.std(self.returns) if len(self.returns) > 10 else 0.0\n\n        # Regime classification\n        # Combines VPIN toxicity with volatility\n        if vpin_result.toxicity_regime == 2 and volatility > 0.001:\n            regime = \"HIGH_TOXICITY_HIGH_VOL\"  # Danger zone\n        elif vpin_result.toxicity_regime == 2:\n            regime = \"HIGH_TOXICITY_LOW_VOL\"   # Informed trading\n        elif vpin_result.toxicity_regime == 0 and volatility < 0.0005:\n            regime = \"LOW_TOXICITY_LOW_VOL\"    # Safe for market making\n        else:\n            regime = \"NORMAL\"\n\n        return {\n            'vpin': vpin_result.vpin,\n            'volatility': volatility,\n            'toxicity_regime': vpin_result.toxicity_regime,\n            'regime': regime,\n            'imbalance': vpin_result.imbalance\n        }",
    "source_file": "core\\features\\vpin_production.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VPINRegimeDetector"
  },
  {
    "name": "create_vpin_calculator",
    "category": "microstructure",
    "formula": "VPINCalculator(config)",
    "explanation": "Create VPIN calculator with standard parameters.\n\nArgs:\n    bucket_size: Volume per bucket (default 50k from Easley et al.)\n    n_buckets: Rolling window size (default 50)\n\nReturns:\n    VPINCalculator instance",
    "python_code": "def create_vpin_calculator(\n    bucket_size: int = 50000,\n    n_buckets: int = 50\n) -> VPINCalculator:\n    \"\"\"\n    Create VPIN calculator with standard parameters.\n\n    Args:\n        bucket_size: Volume per bucket (default 50k from Easley et al.)\n        n_buckets: Rolling window size (default 50)\n\n    Returns:\n        VPINCalculator instance\n    \"\"\"\n    config = VPINConfig(\n        bucket_size=bucket_size,\n        n_buckets=n_buckets\n    )\n    return VPINCalculator(config)",
    "source_file": "core\\features\\vpin_production.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "quantitative",
    "formula": "",
    "explanation": "Initialize MI calculator.\n\nArgs:\n    bins: Number of bins for discretization (if using binning)\n    method: 'binning' or 'knn' (k-nearest neighbors estimator)",
    "python_code": "def __init__(self, bins: int = 50, method: str = 'knn'):\n        \"\"\"\n        Initialize MI calculator.\n\n        Args:\n            bins: Number of bins for discretization (if using binning)\n            method: 'binning' or 'knn' (k-nearest neighbors estimator)\n        \"\"\"\n        self.bins = bins\n        self.method = method",
    "source_file": "core\\information\\mutual_info.py",
    "academic_reference": null,
    "class_name": "MutualInformationCalculator"
  },
  {
    "name": "calculate",
    "category": "statistical",
    "formula": "normalized MI: I(X;Y) / H(Y) | 0.0 | mi",
    "explanation": "Calculate I(X;Y) in bits.\n\nArgs:\n    X: Feature array (1D)\n    Y: Target array (1D)\n    normalize: If True, return normalized MI: I(X;Y) / H(Y)\n\nReturns:\n    Mutual information in bits",
    "python_code": "def calculate(\n        self,\n        X: np.ndarray,\n        Y: np.ndarray,\n        normalize: bool = False,\n    ) -> float:\n        \"\"\"\n        Calculate I(X;Y) in bits.\n\n        Args:\n            X: Feature array (1D)\n            Y: Target array (1D)\n            normalize: If True, return normalized MI: I(X;Y) / H(Y)\n\n        Returns:\n            Mutual information in bits\n        \"\"\"\n        if len(X) != len(Y):\n            raise ValueError(f\"X and Y must have same length, got {len(X)} and {len(Y)}\")\n\n        # Remove NaN values\n        mask = ~(np.isnan(X) | np.isnan(Y))\n        X_clean = X[mask]\n        Y_clean = Y[mask]\n\n        if len(X_clean) < 10:\n            logger.warning(f\"Too few samples after cleaning: {len(X_clean)}\")\n            return 0.0\n\n        if self.method == 'binning':\n            mi = self._calculate_binning(X_clean, Y_clean)\n        elif self.method == 'knn':\n            mi = self._calculate_knn(X_clean, Y_clean)\n        else:\n            raise ValueError(f\"Unknown method: {self.method}\")\n\n        if normalize:\n            h_y = self._entropy(Y_clean)\n            if h_y > 0:\n                mi = mi / h_y\n            else:\n                mi = 0.0\n\n        return mi",
    "source_file": "core\\information\\mutual_info.py",
    "academic_reference": null,
    "class_name": "MutualInformationCalculator"
  },
  {
    "name": "_calculate_binning",
    "category": "quantitative",
    "formula": "mi_bits",
    "explanation": "Calculate MI using binning/discretization.\n\nFast but less accurate for continuous variables.",
    "python_code": "def _calculate_binning(self, X: np.ndarray, Y: np.ndarray) -> float:\n        \"\"\"\n        Calculate MI using binning/discretization.\n\n        Fast but less accurate for continuous variables.\n        \"\"\"\n        # Discretize X and Y\n        X_binned = np.digitize(X, bins=np.linspace(X.min(), X.max(), self.bins))\n        Y_binned = np.digitize(Y, bins=np.linspace(Y.min(), Y.max(), self.bins))\n\n        # Calculate MI using sklearn\n        mi_nats = mutual_info_score(X_binned, Y_binned)\n\n        # Convert to bits (log2 instead of ln)\n        mi_bits = mi_nats / np.log(2)\n\n        return mi_bits",
    "source_file": "core\\information\\mutual_info.py",
    "academic_reference": null,
    "class_name": "MutualInformationCalculator"
  },
  {
    "name": "_calculate_knn",
    "category": "quantitative",
    "formula": "mi_bits",
    "explanation": "Calculate MI using k-nearest neighbors estimator.\n\nMore accurate for continuous variables but slower.\n\nBased on Kraskov et al. (2004): Estimating mutual information.",
    "python_code": "def _calculate_knn(self, X: np.ndarray, Y: np.ndarray, k: int = 3) -> float:\n        \"\"\"\n        Calculate MI using k-nearest neighbors estimator.\n\n        More accurate for continuous variables but slower.\n\n        Based on Kraskov et al. (2004): Estimating mutual information.\n        \"\"\"\n        try:\n            from sklearn.feature_selection import mutual_info_regression\n\n            # Reshape for sklearn\n            X_2d = X.reshape(-1, 1)\n\n            # Calculate MI\n            mi_nats = mutual_info_regression(X_2d, Y, n_neighbors=k)[0]\n\n            # Convert to bits\n            mi_bits = mi_nats / np.log(2)\n\n            return mi_bits\n\n        except ImportError:\n            logger.warning(\"sklearn not available, falling back to binning\")\n            return self._calculate_binning(X, Y)",
    "source_file": "core\\information\\mutual_info.py",
    "academic_reference": null,
    "class_name": "MutualInformationCalculator"
  },
  {
    "name": "_entropy",
    "category": "quantitative",
    "formula": "entropy",
    "explanation": "Calculate Shannon entropy H(X) in bits.\n\nH(X) = - p(x) log2 p(x)",
    "python_code": "def _entropy(self, X: np.ndarray) -> float:\n        \"\"\"\n        Calculate Shannon entropy H(X) in bits.\n\n        H(X) = - p(x) log2 p(x)\n        \"\"\"\n        # Discretize\n        X_binned = np.digitize(X, bins=np.linspace(X.min(), X.max(), self.bins))\n\n        # Calculate probabilities\n        value_counts = np.bincount(X_binned)\n        probabilities = value_counts[value_counts > 0] / len(X_binned)\n\n        # Calculate entropy\n        entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))\n\n        return entropy",
    "source_file": "core\\information\\mutual_info.py",
    "academic_reference": null,
    "class_name": "MutualInformationCalculator"
  },
  {
    "name": "select_features",
    "category": "technical",
    "formula": "selected_indices, mi_scores",
    "explanation": "Select features based on MI with target.\n\nArgs:\n    features: Feature matrix (n_samples, n_features)\n    target: Target array (n_samples,)\n    top_k: Select top K features by MI (if None, use threshold)\n    threshold: Minimum MI threshold in bits\n\nReturns:\n    (selected_indices, mi_scores)",
    "python_code": "def select_features(\n        self,\n        features: np.ndarray,\n        target: np.ndarray,\n        top_k: Optional[int] = None,\n        threshold: Optional[float] = 0.1,\n    ) -> tuple:\n        \"\"\"\n        Select features based on MI with target.\n\n        Args:\n            features: Feature matrix (n_samples, n_features)\n            target: Target array (n_samples,)\n            top_k: Select top K features by MI (if None, use threshold)\n            threshold: Minimum MI threshold in bits\n\n        Returns:\n            (selected_indices, mi_scores)\n        \"\"\"\n        n_features = features.shape[1]\n        mi_scores = np.zeros(n_features)\n\n        logger.info(f\"Calculating MI for {n_features} features...\")\n\n        for i in range(n_features):\n            mi_scores[i] = self.calculate(features[:, i], target)\n\n            if (i + 1) % 100 == 0:\n                logger.info(f\"  Processed {i+1}/{n_features} features\")\n\n        # Select features\n        if top_k is not None:\n            # Select top K\n            selected_indices = np.argsort(mi_scores)[::-1][:top_k]\n        else:\n            # Select by threshold\n            selected_indices = np.where(mi_scores >= threshold)[0]\n\n        logger.info(f\"Selected {len(selected_indices)} features (from {n_features})\")\n        logger.info(f\"Total information: {mi_scores[selected_indices].sum():.3f} bits\")\n\n        return selected_indices, mi_scores",
    "source_file": "core\\information\\mutual_info.py",
    "academic_reference": null,
    "class_name": "MutualInformationCalculator"
  },
  {
    "name": "calculate_mutual_information",
    "category": "deep_learning",
    "formula": "feature = np.array([1, 2, 3, 4, 5]) | target = np.array([2, 4, 6, 8, 10]) | mi = calculate_mutual_information(feature, target)",
    "explanation": "Convenience function to calculate MI.\n\nArgs:\n    X: Feature array\n    Y: Target array\n    bins: Number of bins for discretization\n    method: 'binning' or 'knn'\n\nReturns:\n    Mutual information in bits\n\nExample:\n    >>> feature = np.array([1, 2, 3, 4, 5])\n    >>> target = np.array([2, 4, 6, 8, 10])\n    >>> mi = calculate_mutual_information(feature, target)\n    >>> print(f\"I(X;Y) = {mi:.3f} bits\")",
    "python_code": "def calculate_mutual_information(\n    X: np.ndarray,\n    Y: np.ndarray,\n    bins: int = 50,\n    method: str = 'knn',\n) -> float:\n    \"\"\"\n    Convenience function to calculate MI.\n\n    Args:\n        X: Feature array\n        Y: Target array\n        bins: Number of bins for discretization\n        method: 'binning' or 'knn'\n\n    Returns:\n        Mutual information in bits\n\n    Example:\n        >>> feature = np.array([1, 2, 3, 4, 5])\n        >>> target = np.array([2, 4, 6, 8, 10])\n        >>> mi = calculate_mutual_information(feature, target)\n        >>> print(f\"I(X;Y) = {mi:.3f} bits\")\n    \"\"\"\n    calc = MutualInformationCalculator(bins=bins, method=method)\n    return calc.calculate(X, Y)",
    "source_file": "core\\information\\mutual_info.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "quantitative",
    "formula": "",
    "explanation": "Initialize TE calculator.\n\nArgs:\n    bins: Number of bins for discretization\n    k: History length (number of past values to condition on)",
    "python_code": "def __init__(self, bins: int = 10, k: int = 1):\n        \"\"\"\n        Initialize TE calculator.\n\n        Args:\n            bins: Number of bins for discretization\n            k: History length (number of past values to condition on)\n        \"\"\"\n        self.bins = bins\n        self.k = k",
    "source_file": "core\\information\\transfer_entropy.py",
    "academic_reference": null,
    "class_name": "TransferEntropyCalculator"
  },
  {
    "name": "calculate",
    "category": "quantitative",
    "formula": "variable_lag=True) | {'te': 0.0, 'optimal_lag': lag, 'te_per_lag': {}} | 0.0",
    "explanation": "Calculate TE(XY) in bits.\n\nArgs:\n    X: Source time series\n    Y: Target time series\n    lag: Time lag (how many steps in past does X affect Y)\n    variable_lag: If True, search for optimal lag\n    max_lag: Maximum lag to search (if variable_lag=True)\n\nReturns:\n    Transfer entropy in bits (or dict with optimal lag if variable_lag)",
    "python_code": "def calculate(\n        self,\n        X: np.ndarray,\n        Y: np.ndarray,\n        lag: int = 1,\n        variable_lag: bool = False,\n        max_lag: int = 20,\n    ) -> Union[float, Dict]:\n        \"\"\"\n        Calculate TE(XY) in bits.\n\n        Args:\n            X: Source time series\n            Y: Target time series\n            lag: Time lag (how many steps in past does X affect Y)\n            variable_lag: If True, search for optimal lag\n            max_lag: Maximum lag to search (if variable_lag=True)\n\n        Returns:\n            Transfer entropy in bits (or dict with optimal lag if variable_lag)\n        \"\"\"\n        if len(X) != len(Y):\n            raise ValueError(f\"X and Y must have same length, got {len(X)} and {len(Y)}\")\n\n        # Remove NaN\n        mask = ~(np.isnan(X) | np.isnan(Y))\n        X_clean = X[mask]\n        Y_clean = Y[mask]\n\n        if len(X_clean) < max(lag, self.k) + 10:\n            logger.warning(f\"Too few samples: {len(X_clean)}\")\n            if variable_lag:\n                return {'te': 0.0, 'optimal_lag': lag, 'te_per_lag': {}}\n            return 0.0\n\n        if variable_lag:\n            # Search for optimal lag\n            te_per_lag = {}\n            for test_lag in range(1, min(max_lag + 1, len(X_clean) // 10)):\n                te_per_lag[test_lag] = self._calculate_fixed_lag(X_clean, Y_clean, test_lag)\n\n            # Find lag with maximum TE\n            optimal_lag = max(te_per_lag, key=te_per_lag.get)\n            optimal_te = te_per_lag[optimal_lag]\n\n            return {\n                'te': optimal_te,\n                'optimal_lag': optimal_lag,\n                'te_per_lag': te_per_lag,\n            }\n        else:\n            return self._calculate_fixed_lag(X_clean, Y_clean, lag)",
    "source_file": "core\\information\\transfer_entropy.py",
    "academic_reference": null,
    "class_name": "TransferEntropyCalculator"
  },
  {
    "name": "_calculate_fixed_lag",
    "category": "quantitative",
    "formula": "te",
    "explanation": "Calculate TE with fixed lag.\n\nTE(XY) = I(Y_t; X_{t-lag} | Y_{t-1:t-k})\n        = H(Y_t | Y_{t-1:t-k}) - H(Y_t | Y_{t-1:t-k}, X_{t-lag})\n\nWhere H is conditional entropy.",
    "python_code": "def _calculate_fixed_lag(\n        self,\n        X: np.ndarray,\n        Y: np.ndarray,\n        lag: int,\n    ) -> float:\n        \"\"\"\n        Calculate TE with fixed lag.\n\n        TE(XY) = I(Y_t; X_{t-lag} | Y_{t-1:t-k})\n                = H(Y_t | Y_{t-1:t-k}) - H(Y_t | Y_{t-1:t-k}, X_{t-lag})\n\n        Where H is conditional entropy.\n        \"\"\"\n        # Create lagged series\n        # Y_t (current Y)\n        Y_t = Y[max(lag, self.k):]\n\n        # Y_{t-1:t-k} (past Y)\n        Y_past = self._create_history_matrix(Y, self.k, lag)\n\n        # X_{t-lag} (past X at lag)\n        X_lag = X[max(lag, self.k) - lag:-lag] if lag < len(X) else X[:len(Y_t)]\n\n        # Ensure same length\n        min_len = min(len(Y_t), len(Y_past), len(X_lag))\n        Y_t = Y_t[:min_len]\n        Y_past = Y_past[:min_len]\n        X_lag = X_lag[:min_len]\n\n        # Discretize\n        Y_t_binned = self._discretize(Y_t)\n        Y_past_binned = self._discretize_multivariate(Y_past)\n        X_lag_binned = self._discretize(X_lag)\n\n        # Calculate conditional entropies\n        # H(Y_t | Y_past)\n        h_y_given_ypast = self._conditional_entropy(Y_t_binned, Y_past_binned)\n\n        # H(Y_t | Y_past, X_lag)\n        joint_condition = np.column_stack([Y_past_binned, X_lag_binned])\n        h_y_given_ypast_xlag = self._conditional_entropy(Y_t_binned, joint_condition)\n\n        # TE = H(Y_t | Y_past) - H(Y_t | Y_past, X_lag)\n        te = h_y_given_ypast - h_y_given_ypast_xlag\n\n        # Ensure non-negative (numerical errors can cause small negative)\n        te = max(0.0, te)\n\n        return te",
    "source_file": "core\\information\\transfer_entropy.py",
    "academic_reference": null,
    "class_name": "TransferEntropyCalculator"
  },
  {
    "name": "_create_history_matrix",
    "category": "technical",
    "formula": "history",
    "explanation": "Create matrix of past k values.\n\nReturns matrix where each row is [Y_{t-1}, Y_{t-2}, ..., Y_{t-k}]",
    "python_code": "def _create_history_matrix(\n        self,\n        series: np.ndarray,\n        k: int,\n        offset: int,\n    ) -> np.ndarray:\n        \"\"\"\n        Create matrix of past k values.\n\n        Returns matrix where each row is [Y_{t-1}, Y_{t-2}, ..., Y_{t-k}]\n        \"\"\"\n        n = len(series) - offset\n        history = np.zeros((n - k, k))\n\n        for i in range(k):\n            history[:, i] = series[offset + i:n - k + i]\n\n        return history",
    "source_file": "core\\information\\transfer_entropy.py",
    "academic_reference": null,
    "class_name": "TransferEntropyCalculator"
  },
  {
    "name": "_discretize",
    "category": "quantitative",
    "formula": "np.array([]) | np.digitize(X, bins=bins[:-1])",
    "explanation": "Discretize 1D array into bins.",
    "python_code": "def _discretize(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Discretize 1D array into bins.\"\"\"\n        if len(X) == 0:\n            return np.array([])\n\n        # Use quantile-based bins for better distribution\n        try:\n            bins = np.percentile(X, np.linspace(0, 100, self.bins + 1))\n            # Ensure bins are unique\n            bins = np.unique(bins)\n            if len(bins) < 2:\n                bins = np.array([X.min(), X.max()])\n        except:\n            bins = np.linspace(X.min(), X.max(), self.bins + 1)\n\n        return np.digitize(X, bins=bins[:-1])",
    "source_file": "core\\information\\transfer_entropy.py",
    "academic_reference": null,
    "class_name": "TransferEntropyCalculator"
  },
  {
    "name": "_discretize_multivariate",
    "category": "quantitative",
    "formula": "labels",
    "explanation": "Discretize multivariate array.\n\nFor 2D array, create composite bins.",
    "python_code": "def _discretize_multivariate(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Discretize multivariate array.\n\n        For 2D array, create composite bins.\n        \"\"\"\n        if X.ndim == 1:\n            return self._discretize(X)\n\n        # Create composite labels (each combination is unique state)\n        n_samples, n_dims = X.shape\n        labels = np.zeros(n_samples, dtype=int)\n\n        for dim in range(n_dims):\n            dim_binned = self._discretize(X[:, dim])\n            labels += dim_binned * (self.bins ** dim)\n\n        return labels",
    "source_file": "core\\information\\transfer_entropy.py",
    "academic_reference": null,
    "class_name": "TransferEntropyCalculator"
  },
  {
    "name": "_conditional_entropy",
    "category": "quantitative",
    "formula": "h_y_given_x",
    "explanation": "Calculate H(Y|X) = H(Y,X) - H(X)\n\nWhere H is Shannon entropy.",
    "python_code": "def _conditional_entropy(\n        self,\n        Y: np.ndarray,\n        X: np.ndarray,\n    ) -> float:\n        \"\"\"\n        Calculate H(Y|X) = H(Y,X) - H(X)\n\n        Where H is Shannon entropy.\n        \"\"\"\n        # Handle multivariate X\n        if X.ndim > 1:\n            X = self._discretize_multivariate(X)\n\n        # Joint entropy H(Y,X)\n        joint = np.column_stack([Y, X])\n        h_joint = self._joint_entropy(joint)\n\n        # Marginal entropy H(X)\n        h_x = self._entropy(X)\n\n        # Conditional entropy\n        h_y_given_x = h_joint - h_x\n\n        return h_y_given_x",
    "source_file": "core\\information\\transfer_entropy.py",
    "academic_reference": null,
    "class_name": "TransferEntropyCalculator"
  },
  {
    "name": "_entropy",
    "category": "quantitative",
    "formula": "entropy",
    "explanation": "Shannon entropy H(X) in bits.",
    "python_code": "def _entropy(self, X: np.ndarray) -> float:\n        \"\"\"Shannon entropy H(X) in bits.\"\"\"\n        # Count occurrences\n        unique, counts = np.unique(X, return_counts=True)\n        probabilities = counts / len(X)\n\n        # H(X) = - p(x) log2 p(x)\n        entropy = -np.sum(probabilities * np.log2(probabilities + 1e-10))\n\n        return entropy",
    "source_file": "core\\information\\transfer_entropy.py",
    "academic_reference": null,
    "class_name": "TransferEntropyCalculator"
  },
  {
    "name": "_joint_entropy",
    "category": "quantitative",
    "formula": "",
    "explanation": "Joint entropy H(X1, X2, ..., Xn) in bits.\n\nFor multivariate X.",
    "python_code": "def _joint_entropy(self, X: np.ndarray) -> float:\n        \"\"\"\n        Joint entropy H(X1, X2, ..., Xn) in bits.\n\n        For multivariate X.\n        \"\"\"\n        if X.ndim == 1:\n            return self._entropy(X)\n\n        # Create unique states from all dimensions\n        states = self._discretize_multivariate(X)\n\n        # Calculate entropy of joint states\n        return self._entropy(states)",
    "source_file": "core\\information\\transfer_entropy.py",
    "academic_reference": null,
    "class_name": "TransferEntropyCalculator"
  },
  {
    "name": "detect_causality",
    "category": "quantitative",
    "formula": "{",
    "explanation": "Detect causal direction between X and Y.\n\nArgs:\n    X: First time series\n    Y: Second time series\n    max_lag: Maximum lag to test\n    significance_threshold: Minimum TE in bits to consider significant\n\nReturns:\n    Dictionary with causality results",
    "python_code": "def detect_causality(\n        self,\n        X: np.ndarray,\n        Y: np.ndarray,\n        max_lag: int = 20,\n        significance_threshold: float = 0.05,\n    ) -> Dict:\n        \"\"\"\n        Detect causal direction between X and Y.\n\n        Args:\n            X: First time series\n            Y: Second time series\n            max_lag: Maximum lag to test\n            significance_threshold: Minimum TE in bits to consider significant\n\n        Returns:\n            Dictionary with causality results\n        \"\"\"\n        # Calculate TE in both directions\n        te_x_to_y_result = self.calculate(X, Y, variable_lag=True, max_lag=max_lag)\n        te_y_to_x_result = self.calculate(Y, X, variable_lag=True, max_lag=max_lag)\n\n        te_x_to_y = te_x_to_y_result['te']\n        te_y_to_x = te_y_to_x_result['te']\n\n        # Determine causality\n        if te_x_to_y > significance_threshold and te_y_to_x > significance_threshold:\n            if abs(te_x_to_y - te_y_to_x) < significance_threshold:\n                direction = 'bidirectional'\n            elif te_x_to_y > te_y_to_x:\n                direction = 'X  Y'\n            else:\n                direction = 'Y  X'\n        elif te_x_to_y > significance_threshold:\n            direction = 'X  Y'\n        elif te_y_to_x > significance_threshold:\n            direction = 'Y  X'\n        else:\n            direction = 'no causality'\n\n        return {\n            'direction': direction,\n            'te_X_to_Y': te_x_to_y,\n            'te_Y_to_X': te_y_to_x,\n            'optimal_lag_X_to_Y': te_x_to_y_result['optimal_lag'],\n            'optimal_lag_Y_to_X': te_y_to_x_result['optimal_lag'],\n            'net_causality': te_x_to_y - te_y_to_x,\n            'interpretation': self._interpret_causality(direction, te_x_to_y, te_y_to_x),\n        }",
    "source_file": "core\\information\\transfer_entropy.py",
    "academic_reference": null,
    "class_name": "TransferEntropyCalculator"
  },
  {
    "name": "_interpret_causality",
    "category": "quantitative",
    "formula": "\"No significant information flow detected\" | f\"Bidirectional causality (TE forward: {te_forward:.3f}, reverse: {te_reverse:.3f} bits)\" | f\"X causes Y (TE: {te_forward:.3f} bits, {te_forward - te_reverse:.3f} bits stronger)\"",
    "explanation": "Generate human-readable interpretation.",
    "python_code": "def _interpret_causality(self, direction: str, te_forward: float, te_reverse: float) -> str:\n        \"\"\"Generate human-readable interpretation.\"\"\"\n        if direction == 'no causality':\n            return \"No significant information flow detected\"\n        elif direction == 'bidirectional':\n            return f\"Bidirectional causality (TE forward: {te_forward:.3f}, reverse: {te_reverse:.3f} bits)\"\n        elif direction == 'X  Y':\n            return f\"X causes Y (TE: {te_forward:.3f} bits, {te_forward - te_reverse:.3f} bits stronger)\"\n        else:  # Y  X\n            return f\"Y causes X (TE: {te_reverse:.3f} bits, {te_reverse - te_forward:.3f} bits stronger)\"",
    "source_file": "core\\information\\transfer_entropy.py",
    "academic_reference": null,
    "class_name": "TransferEntropyCalculator"
  },
  {
    "name": "calculate_transfer_entropy",
    "category": "deep_learning",
    "formula": "eurusd_returns = np.diff(eurusd_prices) / eurusd_prices[:-1] | gbpusd_returns = np.diff(gbpusd_prices) / gbpusd_prices[:-1] | te = calculate_transfer_entropy(eurusd_returns, gbpusd_returns, lag=5)",
    "explanation": "Convenience function to calculate TE.\n\nArgs:\n    X: Source time series\n    Y: Target time series\n    lag: Time lag\n    bins: Number of bins for discretization\n    k: History length\n\nReturns:\n    Transfer entropy in bits\n\nExample:\n    >>> # Does EUR/USD lead GBP/USD?\n    >>> eurusd_returns = np.diff(eurusd_prices) / eurusd_prices[:-1]\n    >>> gbpusd_returns = np.diff(gbpusd_prices) / gbpusd_prices[:-1]\n    >>> te = calculate_transfer_entropy(eurusd_returns, gbpusd_returns, lag=5)\n    >>> print(f\"TE(EURGBP) = {te:.3f} bits\")",
    "python_code": "def calculate_transfer_entropy(\n    X: np.ndarray,\n    Y: np.ndarray,\n    lag: int = 1,\n    bins: int = 10,\n    k: int = 1,\n) -> float:\n    \"\"\"\n    Convenience function to calculate TE.\n\n    Args:\n        X: Source time series\n        Y: Target time series\n        lag: Time lag\n        bins: Number of bins for discretization\n        k: History length\n\n    Returns:\n        Transfer entropy in bits\n\n    Example:\n        >>> # Does EUR/USD lead GBP/USD?\n        >>> eurusd_returns = np.diff(eurusd_prices) / eurusd_prices[:-1]\n        >>> gbpusd_returns = np.diff(gbpusd_prices) / gbpusd_prices[:-1]\n        >>> te = calculate_transfer_entropy(eurusd_returns, gbpusd_returns, lag=5)\n        >>> print(f\"TE(EURGBP) = {te:.3f} bits\")\n    \"\"\"\n    calc = TransferEntropyCalculator(bins=bins, k=k)\n    return calc.calculate(X, Y, lag=lag)",
    "source_file": "core\\information\\transfer_entropy.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "machine_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(\n        self,\n        model_dir: Path = None,\n        online_model_dir: Path = None,\n        enable_online_learning: bool = True,\n        update_interval: int = 60,  # seconds\n        min_samples_for_update: int = 500,\n        live_weight: float = 3.0,\n    ):\n        self.model_dir = model_dir or Path(\"models/production\")\n        self.online_model_dir = online_model_dir or Path(\"models/production/online\")\n\n        # Settings\n        self.enable_online = enable_online_learning\n        self.update_interval = update_interval\n        self.min_samples = min_samples_for_update\n        self.live_weight = live_weight\n\n        # Static models (from training)\n        self.static_models: Dict[str, Dict] = {}\n        self.feature_names: List[str] = []\n\n        # Online learners (one per symbol)\n        self.online_learners: Dict[str, ChineseQuantOnlineLearner] = {}\n\n        # Shared detectors\n        self.regime_detector = RegimeDetector()\n        self.current_regime = RegimeState()\n\n        # Data buffers\n        self.observation_buffer: Dict[str, deque] = {}\n\n        # State\n        self.loaded = False\n        self.online_enabled = False\n        self.last_update_times: Dict[str, float] = {}\n\n        # Thread safety\n        self._lock = threading.Lock()\n        self._update_thread: Optional[threading.Thread] = None\n        self._running = False\n\n        # Metrics\n        self.prediction_count = 0\n        self.update_count = 0",
    "source_file": "core\\ml\\adaptive_ensemble.py",
    "academic_reference": null,
    "class_name": "AdaptiveMLEnsemble"
  },
  {
    "name": "load_models",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Load pre-trained static models.",
    "python_code": "def load_models(self, symbols: List[str] = None):\n        \"\"\"Load pre-trained static models.\"\"\"\n        if symbols is None:\n            symbols = ['EURUSD', 'GBPUSD', 'USDJPY']\n\n        for symbol in symbols:\n            model_path = self.model_dir / f\"{symbol}_models.pkl\"\n\n            if model_path.exists():\n                try:\n                    with open(model_path, 'rb') as f:\n                        data = pickle.load(f)\n\n                    # Support multiple model formats\n                    adapted = None\n\n                    # Format 1: train_models.py format (target_direction_N/xgboost/etc)\n                    # Supports target_direction_1, target_direction_5, target_direction_10, etc.\n                    target_keys = [k for k in data.keys() if k.startswith('target_direction_')]\n                    if target_keys:\n                        # Use first available target\n                        td_key = target_keys[0]\n                        td = data[td_key]\n                        adapted = {\n                            'models': {\n                                'xgboost': td.get('xgboost'),\n                                'lightgbm': td.get('lightgbm'),\n                                'catboost': td.get('catboost')\n                            },\n                            'feature_names': td.get('features', []),\n                            'n_features': len(td.get('features', [])),\n                        }\n\n                    # Format 2: New train_parallel_max.py format (xgb/lgb/cb)\n                    elif 'xgb' in data or 'lgb' in data or 'cb' in data:\n                        adapted = {\n                            'models': {\n                                'xgboost': data.get('xgb'),\n                                'lightgbm': data.get('lgb'),\n                                'catboost': data.get('cb')\n                            },\n                            'feature_names': [],\n                            'n_features': 0  # Track expected feature count\n                        }\n                        # Try to get feature count from XGBoost Booster\n                        xgb_model = data.get('xgb')\n                        if xgb_model and hasattr(xgb_model, 'num_features'):\n                            adapted['n_features'] = xgb_model.num_features()\n                        # Try to get feature names from CatBoost\n                        cb = data.get('cb')\n                        if cb and hasattr(cb, 'feature_names_'):\n                            adapted['feature_names'] = list(cb.feature_names_)\n                            adapted['n_features'] = len(cb.feature_names_)\n\n                    if adapted:\n                        # Filter out None models\n                        adapted['models'] = {k: v for k, v in adapted['models'].items() if v is not None}\n                        self.static_models[symbol] = adapted\n\n                        if not self.feature_names and adapted['feature_names']:\n                            se",
    "source_file": "core\\ml\\adaptive_ensemble.py",
    "academic_reference": null,
    "class_name": "AdaptiveMLEnsemble"
  },
  {
    "name": "init_online_learning",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Initialize online learners from static models.\n\nThis transfers the pre-trained models to incremental learners.",
    "python_code": "def init_online_learning(self, symbols: List[str] = None):\n        \"\"\"\n        Initialize online learners from static models.\n\n        This transfers the pre-trained models to incremental learners.\n        \"\"\"\n        if symbols is None:\n            symbols = list(self.static_models.keys())\n\n        self.online_model_dir.mkdir(parents=True, exist_ok=True)\n\n        for symbol in symbols:\n            if symbol not in self.static_models:\n                logger.warning(f\"[{symbol}] No static model to initialize from\")\n                continue\n\n            # Create online learner\n            learner = ChineseQuantOnlineLearner(\n                symbol=symbol,\n                model_dir=self.online_model_dir,\n                update_interval=self.update_interval,\n                min_samples_for_update=self.min_samples,\n                live_weight=self.live_weight,\n            )\n\n            # Transfer static models to incremental learners\n            static = self.static_models[symbol]['models']\n\n            if 'xgboost' in static and static['xgboost'] is not None:\n                learner.xgb.model = static['xgboost']\n                learner.xgb.version.version = 1\n                logger.info(f\"[{symbol}] Transferred XGBoost to online learner\")\n\n            if 'lightgbm' in static and static['lightgbm'] is not None:\n                learner.lgb.model = static['lightgbm']\n                learner.lgb.version.version = 1\n                logger.info(f\"[{symbol}] Transferred LightGBM to online learner\")\n\n            if 'catboost' in static and static['catboost'] is not None:\n                learner.cb.model = static['catboost']\n                learner.cb.version.version = 1\n                logger.info(f\"[{symbol}] Transferred CatBoost to online learner\")\n\n            self.online_learners[symbol] = learner\n\n        self.online_enabled = len(self.online_learners) > 0\n        logger.info(f\"Initialized {len(self.online_learners)} online learners\")",
    "source_file": "core\\ml\\adaptive_ensemble.py",
    "academic_reference": null,
    "class_name": "AdaptiveMLEnsemble"
  },
  {
    "name": "start_background_updates",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Start background thread for periodic model updates.",
    "python_code": "def start_background_updates(self):\n        \"\"\"Start background thread for periodic model updates.\"\"\"\n        if not self.online_enabled:\n            logger.warning(\"Online learning not enabled, skipping background updates\")\n            return\n\n        self._running = True\n        self._update_thread = threading.Thread(target=self._update_loop, daemon=True)\n        self._update_thread.start()\n        logger.info(\"Started background update thread\")",
    "source_file": "core\\ml\\adaptive_ensemble.py",
    "academic_reference": null,
    "class_name": "AdaptiveMLEnsemble"
  },
  {
    "name": "stop_background_updates",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Stop background update thread.",
    "python_code": "def stop_background_updates(self):\n        \"\"\"Stop background update thread.\"\"\"\n        self._running = False\n        if self._update_thread:\n            self._update_thread.join(timeout=5)\n        logger.info(\"Stopped background update thread\")",
    "source_file": "core\\ml\\adaptive_ensemble.py",
    "academic_reference": null,
    "class_name": "AdaptiveMLEnsemble"
  },
  {
    "name": "_update_loop",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Background loop for periodic model updates.",
    "python_code": "def _update_loop(self):\n        \"\"\"Background loop for periodic model updates.\"\"\"\n        while self._running:\n            try:\n                # Update each symbol\n                for symbol, learner in self.online_learners.items():\n                    if learner.should_update():\n                        result = learner.incremental_update()\n                        if result.get(\"status\") == \"success\":\n                            self.update_count += 1\n                            logger.info(f\"[{symbol}] Online update #{result['total_updates']}: \"\n                                       f\"acc={result['accuracy']:.4f}, regime={result['regime']}\")\n\n                # Sleep before next check\n                time.sleep(10)\n\n            except Exception as e:\n                logger.error(f\"Update loop error: {e}\")\n                time.sleep(30)",
    "source_file": "core\\ml\\adaptive_ensemble.py",
    "academic_reference": null,
    "class_name": "AdaptiveMLEnsemble"
  },
  {
    "name": "add_observation",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Add observation for online learning.\n\nCalled after each tick with the actual outcome.",
    "python_code": "def add_observation(\n        self,\n        symbol: str,\n        features: np.ndarray,\n        actual_direction: Optional[int],\n        price: float,\n    ):\n        \"\"\"\n        Add observation for online learning.\n\n        Called after each tick with the actual outcome.\n        \"\"\"\n        with self._lock:\n            # Add to buffer\n            if symbol in self.observation_buffer:\n                self.observation_buffer[symbol].append({\n                    'features': features,\n                    'label': actual_direction,\n                    'price': price,\n                    'timestamp': datetime.now(),\n                })\n\n            # Update online learner\n            if self.online_enabled and symbol in self.online_learners:\n                self.online_learners[symbol].add_tick(features, actual_direction, price)\n\n            # Update regime detector\n            self.regime_detector.update(price, self._last_price.get(symbol, price))\n            self._last_price[symbol] = price",
    "source_file": "core\\ml\\adaptive_ensemble.py",
    "academic_reference": null,
    "class_name": "AdaptiveMLEnsemble"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Generate trading signal with adaptive ensemble.\n\nUses online learners if enabled, falls back to static models.",
    "python_code": "def predict(self, symbol: str, features: Dict[str, float]) -> Optional[Signal]:\n        \"\"\"\n        Generate trading signal with adaptive ensemble.\n\n        Uses online learners if enabled, falls back to static models.\n        \"\"\"\n        self.prediction_count += 1\n\n        # Convert features to array\n        if self.feature_names:\n            X = np.array([features.get(f, 0.0) for f in self.feature_names])\n        else:\n            X = np.array(list(features.values()))\n\n        # Debug: log feature count periodically\n        if self.prediction_count % 500 == 1:\n            logger.info(f\"[{symbol}] Predict: features={len(X)}, feature_names={len(self.feature_names)}\")\n\n        # Get current regime\n        self.current_regime = self.regime_detector.detect_regime()\n\n        predictions = {}\n        probabilities = {}\n        drift_detected = False\n\n        # Try online learners first\n        if self.online_enabled and symbol in self.online_learners:\n            learner = self.online_learners[symbol]\n            try:\n                prob, conf = learner.predict(X)\n                probabilities['online_ensemble'] = prob\n                predictions['online_ensemble'] = 1 if prob > 0.5 else 0\n\n                # Check for drift\n                if learner.drift_detector:\n                    drift_metrics = learner.drift_detector.check_drift(X.reshape(1, -1))\n                    drift_detected = drift_metrics.should_retrain\n\n            except Exception as e:\n                logger.warning(f\"[{symbol}] Online prediction failed: {e}\")\n\n        # Fallback or supplement with static models\n        if symbol in self.static_models:\n            static_data = self.static_models[symbol]\n            static = static_data['models']\n            n_features = static_data.get('n_features', 0)\n\n            # Truncate features to match model's expected count\n            X_static = X\n            if n_features > 0 and len(X) > n_features:\n                X_static = X[:n_features]\n            elif n_features > 0 and len(X) < n_features:\n                # Pad with zeros if we have fewer features\n                X_static = np.pad(X, (0, n_features - len(X)), 'constant')\n\n            for name, model in static.items():\n                try:\n                    X_input = X_static.reshape(1, -1)\n                    proba = 0.5\n\n                    # Handle different model types\n                    if isinstance(model, xgb.Booster):\n                        # XGBoost Booster - use DMatrix\n                        dmat = xgb.DMatrix(X_input)\n                        proba = model.predict(dmat)[0]\n                    elif hasattr(model, 'predict_proba'):\n                        # Sklearn-style classifier (XGBClassifier, LGBMClassifier, CatBoost)\n                        proba = model.predict_proba(X_input)[0]\n                        proba = proba[1] if len(proba) > 1 else proba[0]\n                    elif isinstance(model, lgb.Booster):\n                        # LightGBM Booster\n           ",
    "source_file": "core\\ml\\adaptive_ensemble.py",
    "academic_reference": null,
    "class_name": "AdaptiveMLEnsemble"
  },
  {
    "name": "force_update",
    "category": "machine_learning",
    "formula": "{\"status\": \"error\", \"reason\": \"no online learner\"}",
    "explanation": "Force an immediate model update for a symbol.",
    "python_code": "def force_update(self, symbol: str) -> Dict[str, Any]:\n        \"\"\"Force an immediate model update for a symbol.\"\"\"\n        if symbol not in self.online_learners:\n            return {\"status\": \"error\", \"reason\": \"no online learner\"}\n\n        return self.online_learners[symbol].incremental_update(force=True)",
    "source_file": "core\\ml\\adaptive_ensemble.py",
    "academic_reference": null,
    "class_name": "AdaptiveMLEnsemble"
  },
  {
    "name": "get_status",
    "category": "machine_learning",
    "formula": "status",
    "explanation": "Get ensemble status.",
    "python_code": "def get_status(self) -> Dict[str, Any]:\n        \"\"\"Get ensemble status.\"\"\"\n        status = {\n            \"loaded\": self.loaded,\n            \"online_enabled\": self.online_enabled,\n            \"static_models\": list(self.static_models.keys()),\n            \"online_learners\": list(self.online_learners.keys()),\n            \"predictions\": self.prediction_count,\n            \"updates\": self.update_count,\n            \"current_regime\": self.current_regime.state_name,\n        }\n\n        # Per-symbol stats\n        for symbol, learner in self.online_learners.items():\n            status[f\"{symbol}_updates\"] = learner.total_updates\n            status[f\"{symbol}_buffer_size\"] = len(learner.feature_buffer)\n\n        return status",
    "source_file": "core\\ml\\adaptive_ensemble.py",
    "academic_reference": null,
    "class_name": "AdaptiveMLEnsemble"
  },
  {
    "name": "save_all",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Save all online learners.",
    "python_code": "def save_all(self):\n        \"\"\"Save all online learners.\"\"\"\n        for symbol, learner in self.online_learners.items():\n            learner.save_models()\n        logger.info(f\"Saved {len(self.online_learners)} online learners\")",
    "source_file": "core\\ml\\adaptive_ensemble.py",
    "academic_reference": null,
    "class_name": "AdaptiveMLEnsemble"
  },
  {
    "name": "create_adaptive_ensemble",
    "category": "machine_learning",
    "formula": "ensemble",
    "explanation": "Create an adaptive ensemble with online learning.",
    "python_code": "def create_adaptive_ensemble(\n    model_dir: str = \"models/production\",\n    enable_online: bool = True,\n    **kwargs\n) -> AdaptiveMLEnsemble:\n    \"\"\"Create an adaptive ensemble with online learning.\"\"\"\n    ensemble = AdaptiveMLEnsemble(\n        model_dir=Path(model_dir),\n        enable_online_learning=enable_online,\n        **kwargs\n    )\n    return ensemble",
    "source_file": "core\\ml\\adaptive_ensemble.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "machine_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(\n        self,\n        capacity: int = 5000,\n        importance_threshold: float = 0.5,\n    ):\n        self.capacity = capacity\n        self.importance_threshold = importance_threshold\n        self.buffer: deque = deque(maxlen=capacity)\n        self.importance_scores: deque = deque(maxlen=capacity)\n        self._lock = threading.Lock()",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "ReplayBuffer"
  },
  {
    "name": "add",
    "category": "machine_learning",
    "formula": "Higher = more likely to be replayed",
    "explanation": "Add sample to replay buffer.\n\nArgs:\n    features: Feature vector\n    label: Target label\n    importance: Importance score (0-1). Higher = more likely to be replayed\n               Score based on: drift detection, regime change, prediction error\n    metadata: Optional metadata (timestamp, regime, etc.)",
    "python_code": "def add(\n        self,\n        features: np.ndarray,\n        label: int,\n        importance: float = 0.5,\n        metadata: Optional[Dict] = None,\n    ):\n        \"\"\"\n        Add sample to replay buffer.\n\n        Args:\n            features: Feature vector\n            label: Target label\n            importance: Importance score (0-1). Higher = more likely to be replayed\n                       Score based on: drift detection, regime change, prediction error\n            metadata: Optional metadata (timestamp, regime, etc.)\n        \"\"\"\n        with self._lock:\n            self.buffer.append({\n                'features': features.copy() if isinstance(features, np.ndarray) else features,\n                'label': label,\n                'importance': importance,\n                'metadata': metadata or {},\n                'timestamp': time.time(),\n            })\n            self.importance_scores.append(importance)",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "ReplayBuffer"
  },
  {
    "name": "add_batch",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Add batch of samples.",
    "python_code": "def add_batch(self, X: np.ndarray, y: np.ndarray, importances: Optional[np.ndarray] = None):\n        \"\"\"Add batch of samples.\"\"\"\n        if importances is None:\n            importances = np.ones(len(y)) * 0.5\n\n        for i in range(len(y)):\n            self.add(X[i], y[i], importances[i])",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "ReplayBuffer"
  },
  {
    "name": "sample",
    "category": "machine_learning",
    "formula": "prioritized: If True, sample weighted by importance (Prioritized Experience Replay) | np.array([]), np.array([]) | X, y",
    "explanation": "Sample from replay buffer.\n\nArgs:\n    batch_size: Number of samples to return\n    prioritized: If True, sample weighted by importance (Prioritized Experience Replay)\n\nReturns:\n    (features_array, labels_array)",
    "python_code": "def sample(self, batch_size: int, prioritized: bool = True) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Sample from replay buffer.\n\n        Args:\n            batch_size: Number of samples to return\n            prioritized: If True, sample weighted by importance (Prioritized Experience Replay)\n\n        Returns:\n            (features_array, labels_array)\n        \"\"\"\n        with self._lock:\n            if len(self.buffer) == 0:\n                return np.array([]), np.array([])\n\n            n_samples = min(batch_size, len(self.buffer))\n            buffer_list = list(self.buffer)\n\n            if prioritized:\n                # Prioritized sampling weighted by importance\n                importances = np.array([s['importance'] for s in buffer_list])\n                probs = importances / importances.sum()\n                indices = np.random.choice(len(buffer_list), size=n_samples, replace=False, p=probs)\n            else:\n                # Uniform random sampling\n                indices = np.random.choice(len(buffer_list), size=n_samples, replace=False)\n\n            samples = [buffer_list[i] for i in indices]\n            X = np.array([s['features'] for s in samples])\n            y = np.array([s['label'] for s in samples])\n\n            return X, y",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "ReplayBuffer"
  },
  {
    "name": "get_high_importance_samples",
    "category": "machine_learning",
    "formula": "np.array([]), np.array([]) | X, y",
    "explanation": "Get the top-k most important samples.",
    "python_code": "def get_high_importance_samples(self, top_k: int = 500) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Get the top-k most important samples.\"\"\"\n        with self._lock:\n            if len(self.buffer) == 0:\n                return np.array([]), np.array([])\n\n            buffer_list = list(self.buffer)\n            sorted_samples = sorted(buffer_list, key=lambda x: x['importance'], reverse=True)\n            top_samples = sorted_samples[:top_k]\n\n            X = np.array([s['features'] for s in top_samples])\n            y = np.array([s['label'] for s in top_samples])\n\n            return X, y",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "ReplayBuffer"
  },
  {
    "name": "update_importance",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Update importance scores based on prediction errors.\n\nSamples where model was wrong get higher importance (hard examples).",
    "python_code": "def update_importance(self, prediction_errors: np.ndarray):\n        \"\"\"\n        Update importance scores based on prediction errors.\n\n        Samples where model was wrong get higher importance (hard examples).\n        \"\"\"\n        with self._lock:\n            for i, error in enumerate(prediction_errors[-len(self.buffer):]):\n                if i < len(self.buffer):\n                    old_importance = self.buffer[i]['importance']\n                    # Increase importance for misclassified samples\n                    new_importance = min(1.0, old_importance + 0.1 * error)\n                    self.buffer[i]['importance'] = new_importance",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ReplayBuffer"
  },
  {
    "name": "should_full_retrain",
    "category": "machine_learning",
    "formula": "True, f\"sample_count ({self.samples_since_full_retrain} >= {self.full_retrain_interval})\" | True, f\"model_age ({hours_since_retrain:.1f}h >= {self.max_model_age_hours}h)\" | False, \"conditions not met\"",
    "explanation": "Check if full retrain is needed.\n\nReturns:\n    (should_retrain, reason)",
    "python_code": "def should_full_retrain(self) -> Tuple[bool, str]:\n        \"\"\"\n        Check if full retrain is needed.\n\n        Returns:\n            (should_retrain, reason)\n        \"\"\"\n        # Check sample count\n        if self.samples_since_full_retrain >= self.full_retrain_interval:\n            return True, f\"sample_count ({self.samples_since_full_retrain} >= {self.full_retrain_interval})\"\n\n        # Check model age\n        hours_since_retrain = (time.time() - self.last_full_retrain_time) / 3600\n        if hours_since_retrain >= self.max_model_age_hours:\n            return True, f\"model_age ({hours_since_retrain:.1f}h >= {self.max_model_age_hours}h)\"\n\n        return False, \"conditions not met\"",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "PeriodicFullRetrainer"
  },
  {
    "name": "record_samples",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Record that N new samples were processed.",
    "python_code": "def record_samples(self, n_samples: int):\n        \"\"\"Record that N new samples were processed.\"\"\"\n        self.samples_since_full_retrain += n_samples",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "PeriodicFullRetrainer"
  },
  {
    "name": "record_full_retrain",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Record that full retrain was completed.",
    "python_code": "def record_full_retrain(self, new_accuracy: float):\n        \"\"\"Record that full retrain was completed.\"\"\"\n        self.samples_since_full_retrain = 0\n        self.last_full_retrain_time = time.time()\n        self.full_retrain_count += 1\n        self.current_accuracy = new_accuracy\n        logger.info(f\"[FullRetrain] #{self.full_retrain_count} completed, accuracy={new_accuracy:.4f}\")",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "PeriodicFullRetrainer"
  },
  {
    "name": "get_status",
    "category": "machine_learning",
    "formula": "{",
    "explanation": "Get retrainer status.",
    "python_code": "def get_status(self) -> Dict[str, Any]:\n        \"\"\"Get retrainer status.\"\"\"\n        return {\n            \"samples_since_full_retrain\": self.samples_since_full_retrain,\n            \"hours_since_full_retrain\": (time.time() - self.last_full_retrain_time) / 3600,\n            \"full_retrain_count\": self.full_retrain_count,\n            \"current_accuracy\": self.current_accuracy,\n        }",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "PeriodicFullRetrainer"
  },
  {
    "name": "fit",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Fit meta-learner on base model predictions.\n\nArgs:\n    base_predictions: Dict of {model_name: predictions_array}\n    y_true: True labels\n    X_original: Original features (optional, for top features)\n    feature_names: Feature names (optional)",
    "python_code": "def fit(\n        self,\n        base_predictions: Dict[str, np.ndarray],\n        y_true: np.ndarray,\n        X_original: Optional[np.ndarray] = None,\n        feature_names: Optional[List[str]] = None,\n    ):\n        \"\"\"\n        Fit meta-learner on base model predictions.\n\n        Args:\n            base_predictions: Dict of {model_name: predictions_array}\n            y_true: True labels\n            X_original: Original features (optional, for top features)\n            feature_names: Feature names (optional)\n        \"\"\"\n        with self._lock:\n            # Stack base predictions\n            X_meta = np.column_stack([\n                base_predictions.get('xgb', np.zeros(len(y_true))),\n                base_predictions.get('lgb', np.zeros(len(y_true))),\n                base_predictions.get('cb', np.zeros(len(y_true))),\n            ])\n\n            # Add agreement/confidence features\n            pred_std = np.std(X_meta, axis=1, keepdims=True)  # Disagreement\n            pred_mean = np.mean(X_meta, axis=1, keepdims=True)  # Average\n            X_meta = np.hstack([X_meta, pred_std, pred_mean])\n\n            # Optionally add top original features\n            if self.use_top_features and X_original is not None:\n                # Use first top_k features (in production, would rank by importance)\n                top_features = X_original[:, :min(self.top_k_features, X_original.shape[1])]\n                X_meta = np.hstack([X_meta, top_features])\n\n            # Train meta-model\n            if self.meta_model_type == \"xgboost\":\n                self.meta_model = xgb.XGBClassifier(\n                    n_estimators=100,\n                    max_depth=4,\n                    learning_rate=0.1,\n                    tree_method='hist',\n                    device='cuda',\n                    verbosity=0,\n                )\n            elif self.meta_model_type == \"lightgbm\":\n                self.meta_model = lgb.LGBMClassifier(\n                    n_estimators=100,\n                    max_depth=4,\n                    learning_rate=0.1,\n                    device='gpu',\n                    verbose=-1,\n                )\n            else:\n                from sklearn.linear_model import LogisticRegression\n                self.meta_model = LogisticRegression(max_iter=1000)\n\n            self.meta_model.fit(X_meta, y_true)\n            self.is_fitted = True\n            logger.info(f\"[MetaLearner] Fitted on {len(y_true)} samples, features={X_meta.shape[1]}\")",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "StackingMetaLearner"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "sum(base_predictions.get(k, 0.5) * v for k, v in weights.items()) | float(proba)",
    "explanation": "Predict using meta-learner.\n\nArgs:\n    base_predictions: Dict of {model_name: prediction_value}\n    X_original: Original features (optional)\n\nReturns:\n    Meta-learner prediction probability",
    "python_code": "def predict(\n        self,\n        base_predictions: Dict[str, float],\n        X_original: Optional[np.ndarray] = None,\n    ) -> float:\n        \"\"\"\n        Predict using meta-learner.\n\n        Args:\n            base_predictions: Dict of {model_name: prediction_value}\n            X_original: Original features (optional)\n\n        Returns:\n            Meta-learner prediction probability\n        \"\"\"\n        if not self.is_fitted:\n            # Fallback to weighted average if not fitted\n            weights = {'xgb': 0.4, 'lgb': 0.4, 'cb': 0.2}\n            return sum(base_predictions.get(k, 0.5) * v for k, v in weights.items())\n\n        # Build meta-features\n        X_meta = np.array([\n            base_predictions.get('xgb', 0.5),\n            base_predictions.get('lgb', 0.5),\n            base_predictions.get('cb', 0.5),\n        ])\n\n        # Add agreement features\n        pred_std = np.std(X_meta)\n        pred_mean = np.mean(X_meta)\n        X_meta = np.append(X_meta, [pred_std, pred_mean])\n\n        # Add top features if available\n        if self.use_top_features and X_original is not None:\n            top_features = X_original[:min(self.top_k_features, len(X_original))]\n            X_meta = np.append(X_meta, top_features)\n\n        X_meta = X_meta.reshape(1, -1)\n\n        try:\n            proba = self.meta_model.predict_proba(X_meta)[0, 1]\n        except:\n            proba = self.meta_model.predict(X_meta)[0]\n\n        return float(proba)",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "StackingMetaLearner"
  },
  {
    "name": "update_incremental",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Incremental update of meta-learner.\n\nFor XGBoost/LightGBM, uses warm-start continuation.",
    "python_code": "def update_incremental(\n        self,\n        base_predictions: Dict[str, np.ndarray],\n        y_true: np.ndarray,\n        X_original: Optional[np.ndarray] = None,\n    ):\n        \"\"\"\n        Incremental update of meta-learner.\n\n        For XGBoost/LightGBM, uses warm-start continuation.\n        \"\"\"\n        # For now, just refit (in production, would use warm-start)\n        self.fit(base_predictions, y_true, X_original)",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "StackingMetaLearner"
  },
  {
    "name": "state_name",
    "category": "regime",
    "formula": "{0: \"bull\", 1: \"bear\", 2: \"sideways\"}.get(self.state, \"unknown\")",
    "explanation": "",
    "python_code": "def state_name(self) -> str:\n        return {0: \"bull\", 1: \"bear\", 2: \"sideways\"}.get(self.state, \"unknown\")",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "RegimeState"
  },
  {
    "name": "update",
    "category": "regime",
    "formula": "",
    "explanation": "Update with new price.",
    "python_code": "def update(self, price: float, prev_price: float):\n        \"\"\"Update with new price.\"\"\"\n        if prev_price > 0:\n            ret = (price - prev_price) / prev_price\n            self.returns_buffer.append(ret)\n\n            if len(self.returns_buffer) >= 20:\n                vol = np.std(list(self.returns_buffer)[-20:])\n                self.volatility_buffer.append(vol)",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "HMMRegimeDetector"
  },
  {
    "name": "fit",
    "category": "machine_learning",
    "formula": "False | True | False",
    "explanation": "Fit HMM on buffered data.",
    "python_code": "def fit(self):\n        \"\"\"Fit HMM on buffered data.\"\"\"\n        if not HMM_AVAILABLE or len(self.returns_buffer) < 50:\n            return False\n\n        try:\n            returns = np.array(list(self.returns_buffer))\n            volatility = np.array(list(self.volatility_buffer))\n\n            # Ensure same length\n            min_len = min(len(returns), len(volatility))\n            features = np.column_stack([returns[-min_len:], volatility[-min_len:]])\n\n            self.model.fit(features)\n            self.is_fitted = True\n            logger.info(\"[HMM] Regime detector fitted\")\n            return True\n        except Exception as e:\n            logger.warning(f\"[HMM] Fit failed: {e}\")\n            return False",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "HMMRegimeDetector"
  },
  {
    "name": "detect_regime",
    "category": "regime",
    "formula": "RegimeState() | = np.mean(recent_returns) | RegimeState(",
    "explanation": "Detect current market regime.",
    "python_code": "def detect_regime(self) -> RegimeState:\n        \"\"\"Detect current market regime.\"\"\"\n        if len(self.returns_buffer) < 20:\n            return RegimeState()\n\n        recent_returns = list(self.returns_buffer)[-20:]\n        avg_return = np.mean(recent_returns)\n        volatility = np.std(recent_returns)\n        trend_strength = abs(avg_return) / (volatility + 1e-10)\n\n        # Use HMM if fitted\n        if HMM_AVAILABLE and self.is_fitted and len(self.volatility_buffer) > 0:\n            try:\n                features = np.array([[avg_return, volatility]])\n                state = int(self.model.predict(features)[0])\n                prob = float(np.max(self.model.predict_proba(features)))\n            except:\n                # Fallback to rule-based\n                state, prob = self._rule_based_detect(avg_return, volatility)\n        else:\n            state, prob = self._rule_based_detect(avg_return, volatility)\n\n        return RegimeState(\n            state=state,\n            probability=prob,\n            volatility=volatility,\n            trend_strength=trend_strength,\n        )",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "HMMRegimeDetector"
  },
  {
    "name": "_rule_based_detect",
    "category": "regime",
    "formula": "> bull_threshold and volatility < vol_high_threshold: | 0, 0.7  # Bull | < bear_threshold or volatility > vol_high_threshold:",
    "explanation": "Rule-based fallback detection.",
    "python_code": "def _rule_based_detect(self, avg_return: float, volatility: float) -> Tuple[int, float]:\n        \"\"\"Rule-based fallback detection.\"\"\"\n        bull_threshold = 0.0002\n        bear_threshold = -0.0002\n        vol_high_threshold = 0.002\n\n        if avg_return > bull_threshold and volatility < vol_high_threshold:\n            return 0, 0.7  # Bull\n        elif avg_return < bear_threshold or volatility > vol_high_threshold:\n            return 1, 0.7  # Bear\n        else:\n            return 2, 0.6",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "HMMRegimeDetector"
  },
  {
    "name": "initial_train",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Initial training from scratch.",
    "python_code": "def initial_train(\n        self,\n        X: np.ndarray,\n        y: np.ndarray,\n        num_boost_round: int = 500,\n    ) -> xgb.Booster:\n        \"\"\"Initial training from scratch.\"\"\"\n        self.n_features = X.shape[1]  # Track feature count\n        dtrain = xgb.DMatrix(X, label=y)\n\n        self.model = xgb.train(\n            self.base_params,\n            dtrain,\n            num_boost_round=num_boost_round,\n            verbose_eval=False,\n        )\n\n        self.version = ModelVersion(\n            version=1,\n            trees_added=num_boost_round,\n        )\n\n        logger.info(f\"[XGB] Initial training: {num_boost_round} trees, {self.n_features} features\")\n        return self.model",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "IncrementalXGBoost"
  },
  {
    "name": "incremental_update",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Incremental update with new data.\n\nUses xgb_model parameter for warm start.\nReference: XGBoost train() xgb_model parameter",
    "python_code": "def incremental_update(\n        self,\n        X_new: np.ndarray,\n        y_new: np.ndarray,\n        num_boost_round: int = 50,\n        sample_weight: Optional[np.ndarray] = None,\n    ) -> xgb.Booster:\n        \"\"\"\n        Incremental update with new data.\n\n        Uses xgb_model parameter for warm start.\n        Reference: XGBoost train() xgb_model parameter\n        \"\"\"\n        if self.model is None:\n            return self.initial_train(X_new, y_new, num_boost_round)\n\n        with self._lock:\n            # Check if we need to prune old trees\n            current_trees = len(self.model.get_dump())\n            if current_trees + num_boost_round > self.max_trees:\n                logger.warning(f\"[XGB] Tree limit reached ({current_trees}), pruning old trees\")\n                # In production, you'd implement tree pruning here\n                # For now, just limit new trees\n                num_boost_round = max(10, self.max_trees - current_trees)\n\n            dtrain = xgb.DMatrix(X_new, label=y_new, weight=sample_weight)\n\n            if self.mode == \"hot_update\":\n                # Hot update mode: keep structure, update weights\n                params = self.base_params.copy()\n                params.update({\n                    'process_type': 'update',\n                    'updater': 'refresh',\n                    'refresh_leaf': True,\n                })\n\n                self.model = xgb.train(\n                    params,\n                    dtrain,\n                    num_boost_round=num_boost_round,\n                    xgb_model=self.model,\n                    verbose_eval=False,\n                )\n                logger.info(f\"[XGB] Hot update: refreshed leaf weights\")\n            else:\n                # Add trees mode: continue training\n                self.model = xgb.train(\n                    self.base_params,\n                    dtrain,\n                    num_boost_round=num_boost_round,\n                    xgb_model=self.model,\n                    verbose_eval=False,\n                )\n                logger.info(f\"[XGB] Added {num_boost_round} trees (total: {len(self.model.get_dump())})\")\n\n            self.version.version += 1\n            self.version.update_count += 1\n            self.version.trees_added += num_boost_round\n\n            return self.model",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "IncrementalXGBoost"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Predict probabilities with feature alignment.",
    "python_code": "def predict(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Predict probabilities with feature alignment.\"\"\"\n        if self.model is None:\n            raise ValueError(\"Model not trained\")\n\n        # Get expected feature count from model if not set\n        if self.n_features == 0 and hasattr(self.model, 'num_features'):\n            self.n_features = self.model.num_features()\n\n        # Align features if count is known\n        X_aligned = X\n        if self.n_features > 0:\n            if X.ndim == 1:\n                if len(X) > self.n_features:\n                    X_aligned = X[:self.n_features]\n                elif len(X) < self.n_features:\n                    X_aligned = np.pad(X, (0, self.n_features - len(X)), 'constant')\n            else:\n                if X.shape[1] > self.n_features:\n                    X_aligned = X[:, :self.n_features]\n                elif X.shape[1] < self.n_features:\n                    X_aligned = np.pad(X, ((0, 0), (0, self.n_features - X.shape[1])), 'constant')\n\n        dtest = xgb.DMatrix(X_aligned)\n        return self.model.predict(dtest)",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "IncrementalXGBoost"
  },
  {
    "name": "save",
    "category": "technical",
    "formula": "",
    "explanation": "Save model and version info.",
    "python_code": "def save(self, path: Path):\n        \"\"\"Save model and version info.\"\"\"\n        if self.model:\n            self.model.save_model(str(path / \"xgb_incremental.json\"))\n            with open(path / \"xgb_version.json\", \"w\") as f:\n                json.dump({\n                    \"version\": self.version.version,\n                    \"update_count\": self.version.update_count,\n                    \"trees_added\": self.version.trees_added,\n                    \"mode\": self.mode,\n                }, f)",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "IncrementalXGBoost"
  },
  {
    "name": "load",
    "category": "technical",
    "formula": "",
    "explanation": "Load model and version info.",
    "python_code": "def load(self, path: Path):\n        \"\"\"Load model and version info.\"\"\"\n        model_path = path / \"xgb_incremental.json\"\n        if model_path.exists():\n            self.model = xgb.Booster()\n            self.model.load_model(str(model_path))\n\n            version_path = path / \"xgb_version.json\"\n            if version_path.exists():\n                with open(version_path) as f:\n                    data = json.load(f)\n                    self.version.version = data.get(\"version\", 1)\n                    self.version.update_count = data.get(\"update_count\", 0)",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "IncrementalXGBoost"
  },
  {
    "name": "incremental_update",
    "category": "machine_learning",
    "formula": "keep_training_booster=True",
    "explanation": "Incremental update with new data.\n\nKey: init_model parameter + keep_training_booster=True",
    "python_code": "def incremental_update(\n        self,\n        X_new: np.ndarray,\n        y_new: np.ndarray,\n        num_boost_round: int = 50,\n        sample_weight: Optional[np.ndarray] = None,\n    ) -> lgb.Booster:\n        \"\"\"\n        Incremental update with new data.\n\n        Key: init_model parameter + keep_training_booster=True\n        \"\"\"\n        if self.model is None:\n            return self.initial_train(X_new, y_new, num_boost_round)\n\n        with self._lock:\n            dtrain = lgb.Dataset(X_new, label=y_new, weight=sample_weight)\n\n            # Continue training from existing model\n            self.model = lgb.train(\n                self.base_params,\n                dtrain,\n                num_boost_round=num_boost_round,\n                init_model=self.model,  # Warm start from existing model\n                keep_training_booster=True,  # Keep for future incremental updates\n            )\n\n            self.version.version += 1\n            self.version.update_count += 1\n            self.version.trees_added += num_boost_round\n\n            logger.info(f\"[LGB] Incremental update: +{num_boost_round} trees (total: {self.model.num_trees()})\")\n            return self.model",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "IncrementalLightGBM"
  },
  {
    "name": "predict_proba",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Predict probabilities with feature alignment.",
    "python_code": "def predict_proba(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Predict probabilities with feature alignment.\"\"\"\n        if self.model is None:\n            raise ValueError(\"Model not trained\")\n\n        # Get expected feature count from model if not set\n        if self.n_features == 0 and hasattr(self.model, 'feature_count_'):\n            self.n_features = self.model.feature_count_\n\n        # Align features if count is known\n        X_aligned = X\n        if self.n_features > 0:\n            if X.ndim == 1:\n                if len(X) > self.n_features:\n                    X_aligned = X[:self.n_features]\n                elif len(X) < self.n_features:\n                    X_aligned = np.pad(X, (0, self.n_features - len(X)), 'constant')\n            else:\n                if X.shape[1] > self.n_features:\n                    X_aligned = X[:, :self.n_features]\n                elif X.shape[1] < self.n_features:\n                    X_aligned = np.pad(X, ((0, 0), (0, self.n_features - X.shape[1])), 'constant')\n\n        return self.model.predict_proba(X_aligned)[:, 1]",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "IncrementalCatBoost"
  },
  {
    "name": "set_reference",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Set reference distribution for drift detection.",
    "python_code": "def set_reference(self, features: np.ndarray, accuracy: float):\n        \"\"\"Set reference distribution for drift detection.\"\"\"\n        self.reference_features = features\n        self.reference_accuracy = accuracy\n        logger.info(f\"[Drift] Reference set: accuracy={accuracy:.4f}\")",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "DriftDetector"
  },
  {
    "name": "add_observation",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Add prediction/actual pair for accuracy monitoring.",
    "python_code": "def add_observation(self, prediction: float, actual: int):\n        \"\"\"Add prediction/actual pair for accuracy monitoring.\"\"\"\n        self.recent_predictions.append(prediction)\n        self.recent_actuals.append(actual)",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "DriftDetector"
  },
  {
    "name": "check_drift",
    "category": "machine_learning",
    "formula": "metrics | metrics",
    "explanation": "Check for concept drift.\n\nReturns DriftMetrics with drift indicators.",
    "python_code": "def check_drift(self, current_features: np.ndarray) -> DriftMetrics:\n        \"\"\"\n        Check for concept drift.\n\n        Returns DriftMetrics with drift indicators.\n        \"\"\"\n        metrics = DriftMetrics()\n\n        if self.reference_features is None:\n            return metrics\n\n        # 1. KL Divergence for feature distribution\n        try:\n            # Compare histograms for each feature (sample a few)\n            n_features = min(10, current_features.shape[1])\n            kl_values = []\n\n            for i in range(n_features):\n                ref_hist, bins = np.histogram(self.reference_features[:, i], bins=50, density=True)\n                cur_hist, _ = np.histogram(current_features[:, i], bins=bins, density=True)\n\n                # Add small epsilon to avoid division by zero\n                ref_hist = ref_hist + 1e-10\n                cur_hist = cur_hist + 1e-10\n\n                kl = np.sum(rel_entr(cur_hist, ref_hist))\n                kl_values.append(kl)\n\n            metrics.kl_divergence = np.mean(kl_values)\n        except Exception as e:\n            logger.warning(f\"[Drift] KL calculation failed: {e}\")\n\n        # 2. KS Test for distribution shift\n        try:\n            ks_values = []\n            for i in range(min(10, current_features.shape[1])):\n                ks_stat, _ = stats.ks_2samp(\n                    self.reference_features[:, i],\n                    current_features[:, i]\n                )\n                ks_values.append(ks_stat)\n            metrics.ks_statistic = np.mean(ks_values)\n        except Exception as e:\n            logger.warning(f\"[Drift] KS test failed: {e}\")\n\n        # 3. Accuracy drop monitoring\n        if len(self.recent_predictions) >= 100:\n            preds = np.array(self.recent_predictions) > 0.5\n            actuals = np.array(self.recent_actuals)\n            current_accuracy = np.mean(preds == actuals)\n            metrics.accuracy_drop = self.reference_accuracy - current_accuracy\n\n        # 4. Determine if retraining needed\n        if metrics.kl_divergence > self.kl_threshold:\n            metrics.should_retrain = True\n            metrics.drift_type = \"sudden\" if metrics.kl_divergence > 2 * self.kl_threshold else \"gradual\"\n        elif metrics.accuracy_drop > self.accuracy_drop_threshold:\n            metrics.should_retrain = True\n            metrics.drift_type = \"performance_decay\"\n\n        if metrics.should_retrain:\n            logger.warning(f\"[Drift] Detected! Type: {metrics.drift_type}, KL={metrics.kl_divergence:.4f}, Acc drop={metrics.accuracy_drop:.4f}\")\n\n        return metrics",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "DriftDetector"
  },
  {
    "name": "detect_regime",
    "category": "regime",
    "formula": "RegimeState() | = np.mean(recent_returns) | > self.bull_threshold and volatility < self.vol_high_threshold:",
    "explanation": "Detect current market regime.",
    "python_code": "def detect_regime(self) -> RegimeState:\n        \"\"\"Detect current market regime.\"\"\"\n        if len(self.returns_buffer) < 20:\n            return RegimeState()\n\n        # Calculate metrics\n        recent_returns = list(self.returns_buffer)[-20:]\n        avg_return = np.mean(recent_returns)\n        volatility = np.std(recent_returns)\n        trend_strength = abs(avg_return) / (volatility + 1e-10)\n\n        # Simple rule-based regime detection\n        # In production, use HMM from hmmlearn\n        if avg_return > self.bull_threshold and volatility < self.vol_high_threshold:\n            state = 0  # Bull\n            prob = min(0.9, 0.5 + trend_strength)\n        elif avg_return < self.bear_threshold or volatility > self.vol_high_threshold:\n            state = 1  # Bear\n            prob = min(0.9, 0.5 + abs(avg_return) / 0.001)\n        else:\n            state = 2  # Sideways\n            prob = 0.6\n\n        return RegimeState(\n            state=state,\n            probability=prob,\n            volatility=volatility,\n            trend_strength=trend_strength,\n        )",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "RegimeDetector"
  },
  {
    "name": "add_tick",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Add new tick data to buffers.\n\nCalled for each incoming tick during live trading.\nGold standard: Also populates replay buffer with important samples.",
    "python_code": "def add_tick(self, features: np.ndarray, label: Optional[int], price: float):\n        \"\"\"\n        Add new tick data to buffers.\n\n        Called for each incoming tick during live trading.\n        Gold standard: Also populates replay buffer with important samples.\n        \"\"\"\n        with self._lock:\n            self.feature_buffer.append(features)\n            if label is not None:\n                self.label_buffer.append(label)\n\n            # Update regime detector\n            prev_regime = self.current_regime.state if self.current_regime else None\n            if len(self.price_buffer) > 0:\n                self.regime_detector.update(price, self.price_buffer[-1])\n            self.price_buffer.append(price)\n\n            # Update current regime\n            new_regime_state = self.regime_detector.detect_regime()\n\n            # =====================================================================\n            # GOLD STANDARD: Replay Buffer Population\n            # Add important samples for catastrophic forgetting mitigation\n            # =====================================================================\n            if self.enable_replay_buffer and self.replay_buffer is not None and label is not None:\n                # Calculate importance score\n                importance = 0.5  # Default\n\n                # Higher importance for regime change samples\n                if prev_regime is not None and new_regime_state.state != prev_regime:\n                    importance = 0.9\n                    logger.debug(f\"[{self.symbol}] Regime change sample added to replay buffer\")\n\n                # Higher importance for high volatility periods\n                if new_regime_state.volatility > 0.002:\n                    importance = max(importance, 0.7)\n\n                # Add to replay buffer\n                self.replay_buffer.add(\n                    features=features,\n                    label=label,\n                    importance=importance,\n                    metadata={\n                        'regime': new_regime_state.state_name,\n                        'volatility': new_regime_state.volatility,\n                        'price': price,\n                    }\n                )\n\n            self.current_regime = new_regime_state\n\n            # Track samples for periodic full retrain\n            if self.enable_periodic_full_retrain and self.full_retrainer is not None:\n                self.full_retrainer.record_samples(1)",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "ChineseQuantOnlineLearner"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "0.5, 0.0 | 0.5, 0.0 | float(ensemble_pred), float(confidence)",
    "explanation": "Ensemble prediction with regime-adaptive weights.\n\nGOLD STANDARD: Uses stacking meta-learner when available.\n\nReturns: (probability, confidence)",
    "python_code": "def predict(self, features: np.ndarray) -> Tuple[float, float]:\n        \"\"\"\n        Ensemble prediction with regime-adaptive weights.\n\n        GOLD STANDARD: Uses stacking meta-learner when available.\n\n        Returns: (probability, confidence)\n        \"\"\"\n        if self.xgb.model is None:\n            return 0.5, 0.0\n\n        # Get predictions from all models\n        X = features.reshape(1, -1) if features.ndim == 1 else features\n\n        try:\n            xgb_pred = self.xgb.predict(X)[0]\n            lgb_pred = self.lgb.predict(X)[0]\n            cb_pred = self.cb.predict_proba(X)[0]\n        except Exception as e:\n            logger.warning(f\"[{self.symbol}] Prediction error: {e}\")\n            return 0.5, 0.0\n\n        # Confidence based on model agreement\n        preds = [xgb_pred, lgb_pred, cb_pred]\n        confidence = 1.0 - np.std(preds)  # Higher agreement = higher confidence\n\n        # =====================================================================\n        # GOLD STANDARD: Use Stacking Meta-Learner if available\n        # =====================================================================\n        if self.enable_stacking_meta_learner and self.meta_learner is not None and self.meta_learner.is_fitted:\n            base_preds = {\n                'xgb': xgb_pred,\n                'lgb': lgb_pred,\n                'cb': cb_pred,\n            }\n            ensemble_pred = self.meta_learner.predict(base_preds, X.flatten())\n        else:\n            # Fallback: Weighted average\n            weights = self.ensemble_weights\n            ensemble_pred = (\n                weights[\"xgb\"] * xgb_pred +\n                weights[\"lgb\"] * lgb_pred +\n                weights[\"cb\"] * cb_pred\n            )\n\n        return float(ensemble_pred), float(confidence)",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "ChineseQuantOnlineLearner"
  },
  {
    "name": "should_update",
    "category": "machine_learning",
    "formula": "False | False | True",
    "explanation": "Check if incremental update should run.",
    "python_code": "def should_update(self) -> bool:\n        \"\"\"Check if incremental update should run.\"\"\"\n        # Time-based check\n        if time.time() - self.last_update_time < self.update_interval:\n            return False\n\n        # Data availability check\n        if len(self.label_buffer) < self.min_samples_for_update:\n            return False\n\n        return True",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "ChineseQuantOnlineLearner"
  },
  {
    "name": "incremental_update",
    "category": "machine_learning",
    "formula": "{\"status\": \"skipped\", \"reason\": \"conditions not met\"} | {\"status\": \"skipped\", \"reason\": \"insufficient samples\"} | {\"status\": \"skipped\", \"reason\": \"inconsistent feature shapes\"}",
    "explanation": "Perform incremental update with buffered data.\n\nGOLD STANDARD IMPLEMENTATION (Audit 2026-01-18):\n1. Exponential weighting for recent data\n2. Drift-triggered retraining\n3. Regime-adaptive model selection\n4. **NEW** Replay buffer mixing (catastrophic forgetting mitigation)\n5. **NEW** Periodic full retrain check\n6. **NEW** Meta-learner update",
    "python_code": "def incremental_update(self, force: bool = False) -> Dict[str, Any]:\n        \"\"\"\n        Perform incremental update with buffered data.\n\n        GOLD STANDARD IMPLEMENTATION (Audit 2026-01-18):\n        1. Exponential weighting for recent data\n        2. Drift-triggered retraining\n        3. Regime-adaptive model selection\n        4. **NEW** Replay buffer mixing (catastrophic forgetting mitigation)\n        5. **NEW** Periodic full retrain check\n        6. **NEW** Meta-learner update\n        \"\"\"\n        if not force and not self.should_update():\n            return {\"status\": \"skipped\", \"reason\": \"conditions not met\"}\n\n        with self._lock:\n            # =====================================================================\n            # GOLD STANDARD: Check for Periodic Full Retrain\n            # \"\" - BigQuant\n            # =====================================================================\n            if self.enable_periodic_full_retrain and self.full_retrainer is not None:\n                should_full, reason = self.full_retrainer.should_full_retrain()\n                if should_full:\n                    logger.info(f\"[{self.symbol}] Triggering FULL RETRAIN: {reason}\")\n                    return self._full_retrain()\n\n            # Prepare live data\n            n_samples = min(len(self.feature_buffer), len(self.label_buffer))\n            if n_samples < self.min_samples_for_update:\n                return {\"status\": \"skipped\", \"reason\": \"insufficient samples\"}\n\n            # Get raw features and labels\n            raw_features = list(self.feature_buffer)[-n_samples:]\n            raw_labels = list(self.label_buffer)[-n_samples:]\n\n            # Filter for consistent feature shapes\n            if len(raw_features) > 0:\n                expected_shape = None\n                valid_indices = []\n                for i, f in enumerate(raw_features):\n                    if hasattr(f, 'shape'):\n                        if expected_shape is None:\n                            expected_shape = f.shape\n                        if f.shape == expected_shape:\n                            valid_indices.append(i)\n                    elif hasattr(f, '__len__'):\n                        if expected_shape is None:\n                            expected_shape = len(f)\n                        if len(f) == expected_shape:\n                            valid_indices.append(i)\n\n                if len(valid_indices) < self.min_samples_for_update:\n                    return {\"status\": \"skipped\", \"reason\": \"inconsistent feature shapes\"}\n\n                raw_features = [raw_features[i] for i in valid_indices]\n                raw_labels = [raw_labels[i] for i in valid_indices]\n\n            try:\n                X_live = np.array(raw_features)\n                y_live = np.array(raw_labels)\n            except ValueError as e:\n                logger.warning(f\"[{self.symbol}] Feature array error: {e}, skipping update\")\n                return {\"status\": \"skipped\", \"reason\": f\"array",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "ChineseQuantOnlineLearner"
  },
  {
    "name": "_full_retrain",
    "category": "machine_learning",
    "formula": "{\"status\": \"error\", \"reason\": \"no data for full retrain\"} | {\"status\": \"error\", \"error\": str(e)} | result",
    "explanation": "Perform full retrain from scratch.\n\nGold Standard: \"\"\nCombines historical data from replay buffer with recent live data.",
    "python_code": "def _full_retrain(self) -> Dict[str, Any]:\n        \"\"\"\n        Perform full retrain from scratch.\n\n        Gold Standard: \"\"\n        Combines historical data from replay buffer with recent live data.\n        \"\"\"\n        logger.info(f\"[{self.symbol}] Starting FULL RETRAIN...\")\n\n        # Collect all available data\n        X_parts = []\n        y_parts = []\n        weight_parts = []\n\n        # 1. Recent live data (highest weight)\n        n_live = min(len(self.feature_buffer), len(self.label_buffer))\n        if n_live > 0:\n            X_live = np.array(list(self.feature_buffer)[-n_live:])\n            y_live = np.array(list(self.label_buffer)[-n_live:])\n            w_live = np.exp(np.linspace(-0.5, 0, n_live)) * self.live_weight\n            X_parts.append(X_live)\n            y_parts.append(y_live)\n            weight_parts.append(w_live)\n\n        # 2. Replay buffer (important historical samples)\n        if self.replay_buffer is not None and len(self.replay_buffer) > 0:\n            X_replay, y_replay = self.replay_buffer.get_high_importance_samples(top_k=2000)\n            if len(X_replay) > 0:\n                w_replay = np.ones(len(y_replay)) * 0.8  # Slightly lower weight than live\n                X_parts.append(X_replay)\n                y_parts.append(y_replay)\n                weight_parts.append(w_replay)\n\n        if not X_parts:\n            return {\"status\": \"error\", \"reason\": \"no data for full retrain\"}\n\n        # Combine all data\n        X = np.vstack(X_parts)\n        y = np.concatenate(y_parts)\n        weights = np.concatenate(weight_parts)\n\n        logger.info(f\"[{self.symbol}] Full retrain data: {len(y)} samples \"\n                   f\"(live={len(y_parts[0]) if y_parts else 0}, replay={len(y) - len(y_parts[0]) if len(y_parts) > 1 else 0})\")\n\n        # Train fresh models\n        try:\n            # Reset and retrain XGBoost\n            self.xgb = IncrementalXGBoost(mode=\"add_trees\")\n            self.xgb.initial_train(X, y, num_boost_round=500)\n\n            # Reset and retrain LightGBM\n            self.lgb = IncrementalLightGBM()\n            self.lgb.initial_train(X, y, num_boost_round=500)\n\n            # Reset and retrain CatBoost\n            self.cb = IncrementalCatBoost()\n            self.cb.initial_train(X, y, iterations=500)\n\n        except Exception as e:\n            logger.error(f\"[{self.symbol}] Full retrain failed: {e}\")\n            return {\"status\": \"error\", \"error\": str(e)}\n\n        # Update meta-learner\n        if self.meta_learner is not None:\n            try:\n                xgb_preds = self.xgb.predict(X)\n                lgb_preds = self.lgb.predict(X)\n                cb_preds = self.cb.predict_proba(X)\n\n                base_preds = {'xgb': xgb_preds, 'lgb': lgb_preds, 'cb': cb_preds}\n                self.meta_learner.fit(base_preds, y, X)\n            except Exception as e:\n                logger.warning(f\"[{self.symbol}] Meta-learner fit failed: {e}\")\n\n        # Evaluate\n        new_accuracy = self._evaluate_ensemble(",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "ChineseQuantOnlineLearner"
  },
  {
    "name": "_evaluate_ensemble",
    "category": "machine_learning",
    "formula": "accuracy | 0.5",
    "explanation": "Evaluate ensemble accuracy.",
    "python_code": "def _evaluate_ensemble(self, X: np.ndarray, y: np.ndarray) -> float:\n        \"\"\"Evaluate ensemble accuracy.\"\"\"\n        try:\n            probs, _ = self.predict(X[0])  # Get shape\n            preds = []\n            for i in range(len(X)):\n                prob, _ = self.predict(X[i])\n                preds.append(prob > 0.5)\n            accuracy = np.mean(np.array(preds) == y)\n            return accuracy\n        except:\n            return 0.5",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "ChineseQuantOnlineLearner"
  },
  {
    "name": "_adapt_ensemble_weights",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Adapt ensemble weights based on market regime.",
    "python_code": "def _adapt_ensemble_weights(self):\n        \"\"\"Adapt ensemble weights based on market regime.\"\"\"\n        regime = self.current_regime.state\n\n        if regime == 0:  # Bull - favor momentum (XGBoost)\n            self.ensemble_weights = {\"xgb\": 0.45, \"lgb\": 0.35, \"cb\": 0.20}\n        elif regime == 1:  # Bear - favor robustness (CatBoost)\n            self.ensemble_weights = {\"xgb\": 0.30, \"lgb\": 0.35, \"cb\": 0.35}\n        else:  # Sideways - balanced\n            self.ensemble_weights = {\"xgb\": 0.35, \"lgb\": 0.40, \"cb\": 0.25}\n\n        logger.debug(f\"[{self.symbol}] Weights adapted for {self.current_regime.state_name}: {self.ensemble_weights}\")",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "ChineseQuantOnlineLearner"
  },
  {
    "name": "save_models",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Save all models to disk.",
    "python_code": "def save_models(self):\n        \"\"\"Save all models to disk.\"\"\"\n        symbol_dir = self.model_dir / self.symbol\n        symbol_dir.mkdir(parents=True, exist_ok=True)\n\n        self.xgb.save(symbol_dir)\n        self.lgb.save(symbol_dir)\n        self.cb.save(symbol_dir)\n\n        # Save state\n        state = {\n            \"total_updates\": self.total_updates,\n            \"last_update_time\": self.last_update_time,\n            \"ensemble_weights\": self.ensemble_weights,\n            \"symbol\": self.symbol,\n        }\n        with open(symbol_dir / \"state.json\", \"w\") as f:\n            json.dump(state, f, indent=2)\n\n        logger.info(f\"[{self.symbol}] Models saved to {symbol_dir}\")",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "ChineseQuantOnlineLearner"
  },
  {
    "name": "load_models",
    "category": "machine_learning",
    "formula": "False | True",
    "explanation": "Load models from disk.",
    "python_code": "def load_models(self):\n        \"\"\"Load models from disk.\"\"\"\n        symbol_dir = self.model_dir / self.symbol\n        if not symbol_dir.exists():\n            return False\n\n        self.xgb.load(symbol_dir)\n        self.lgb.load(symbol_dir)\n        self.cb.load(symbol_dir)\n\n        state_path = symbol_dir / \"state.json\"\n        if state_path.exists():\n            with open(state_path) as f:\n                state = json.load(f)\n                self.total_updates = state.get(\"total_updates\", 0)\n                self.ensemble_weights = state.get(\"ensemble_weights\", self.ensemble_weights)\n\n        logger.info(f\"[{self.symbol}] Models loaded from {symbol_dir}\")\n        return True",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": "ChineseQuantOnlineLearner"
  },
  {
    "name": "create_online_learner",
    "category": "reinforcement_learning",
    "formula": "ChineseQuantOnlineLearner(",
    "explanation": "Factory function to create an online learner.",
    "python_code": "def create_online_learner(\n    symbol: str,\n    model_dir: str = \"models/production/online\",\n    **kwargs\n) -> ChineseQuantOnlineLearner:\n    \"\"\"Factory function to create an online learner.\"\"\"\n    return ChineseQuantOnlineLearner(\n        symbol=symbol,\n        model_dir=Path(model_dir),\n        **kwargs\n    )",
    "source_file": "core\\ml\\chinese_online_learning.py",
    "academic_reference": "Rabiner (1989) 'HMM Tutorial' IEEE Proceedings",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Args:\n    symbol: Currency pair (e.g., 'EURUSD')\n    model_dir: Directory to save/load models",
    "python_code": "def __init__(self, symbol: str, model_dir: Path = None):\n        \"\"\"\n        Args:\n            symbol: Currency pair (e.g., 'EURUSD')\n            model_dir: Directory to save/load models\n        \"\"\"\n        self.symbol = symbol\n        self.model_dir = model_dir or Path(\"models/production\")\n\n        # Current active models (read by predictions)\n        self._current: Optional[ModelVersion] = None\n\n        # Previous version (fallback)\n        self._previous: Optional[ModelVersion] = None\n\n        # Version counter\n        self._version = 0\n\n        # Lock for atomic swaps\n        self._lock = threading.RLock()\n\n        # Load initial models if available\n        self._load_initial_models()",
    "source_file": "core\\ml\\ensemble.py",
    "academic_reference": null,
    "class_name": "HotSwappableEnsemble"
  },
  {
    "name": "_load_initial_models",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Load models from disk if available.",
    "python_code": "def _load_initial_models(self):\n        \"\"\"Load models from disk if available.\"\"\"\n        model_file = self.model_dir / f\"{self.symbol}_models.pkl\"\n        if model_file.exists():\n            try:\n                with open(model_file, 'rb') as f:\n                    models_dict = pickle.load(f)\n\n                # Find the first target's models\n                for target, model_data in models_dict.items():\n                    self._current = ModelVersion(\n                        version=0,\n                        timestamp=datetime.now(),\n                        accuracy=0.0,  # Unknown for loaded models\n                        auc=0.0,\n                        n_samples=0,\n                        models=model_data,\n                        feature_names=model_data.get('features', [])\n                    )\n                    self._version = 1\n                    logger.info(f\"Loaded initial models for {self.symbol}\")\n                    break\n            except Exception as e:\n                logger.error(f\"Failed to load models: {e}\")",
    "source_file": "core\\ml\\ensemble.py",
    "academic_reference": null,
    "class_name": "HotSwappableEnsemble"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "0.5, 0.0  # No model = no prediction | 0.5, 0.0 | 0.5, 0.0",
    "explanation": "Make ensemble prediction using current models.\n\nArgs:\n    features: Feature dict from HFTFeatureEngine\n\nReturns:\n    (probability, confidence) - probability of up move, confidence",
    "python_code": "def predict(self, features: Dict[str, float]) -> Tuple[float, float]:\n        \"\"\"\n        Make ensemble prediction using current models.\n\n        Args:\n            features: Feature dict from HFTFeatureEngine\n\n        Returns:\n            (probability, confidence) - probability of up move, confidence\n        \"\"\"\n        with self._lock:\n            if self._current is None:\n                return 0.5, 0.0  # No model = no prediction\n\n            models = self._current.models\n            feature_names = self._current.feature_names\n\n        # Extract features in correct order\n        try:\n            X = np.array([[features.get(f, 0.0) for f in feature_names]])\n        except Exception as e:\n            logger.error(f\"Feature extraction error: {e}\")\n            return 0.5, 0.0\n\n        # Get predictions from each model\n        predictions = []\n\n        try:\n            import xgboost as xgb\n            if 'xgboost' in models:\n                dmatrix = xgb.DMatrix(X)\n                pred = models['xgboost'].predict(dmatrix)[0]\n                predictions.append(pred)\n        except Exception as e:\n            logger.debug(f\"XGBoost prediction error: {e}\")\n\n        try:\n            if 'lightgbm' in models:\n                pred = models['lightgbm'].predict(X)[0]\n                predictions.append(pred)\n        except Exception as e:\n            logger.debug(f\"LightGBM prediction error: {e}\")\n\n        try:\n            if 'catboost' in models:\n                pred = models['catboost'].predict_proba(X)[0, 1]\n                predictions.append(pred)\n        except Exception as e:\n            logger.debug(f\"CatBoost prediction error: {e}\")\n\n        if not predictions:\n            return 0.5, 0.0\n\n        # Ensemble average\n        prob = np.mean(predictions)\n        # Confidence = agreement between models\n        confidence = 1.0 - np.std(predictions) * 2 if len(predictions) > 1 else 0.5\n\n        return float(prob), float(confidence)",
    "source_file": "core\\ml\\ensemble.py",
    "academic_reference": null,
    "class_name": "HotSwappableEnsemble"
  },
  {
    "name": "hot_swap",
    "category": "machine_learning",
    "formula": "False | True",
    "explanation": "Atomically swap in new models.\n\nArgs:\n    new_models: Dict with 'xgboost', 'lightgbm', 'catboost' models\n    feature_names: List of feature names\n    accuracy: Validation accuracy\n    auc: Validation AUC\n    n_samples: Number of training samples\n\nReturns:\n    True if swap successful",
    "python_code": "def hot_swap(self, new_models: Dict[str, Any], feature_names: list,\n                 accuracy: float, auc: float, n_samples: int) -> bool:\n        \"\"\"\n        Atomically swap in new models.\n\n        Args:\n            new_models: Dict with 'xgboost', 'lightgbm', 'catboost' models\n            feature_names: List of feature names\n            accuracy: Validation accuracy\n            auc: Validation AUC\n            n_samples: Number of training samples\n\n        Returns:\n            True if swap successful\n        \"\"\"\n        with self._lock:\n            # Check if new models are better\n            if self._current is not None:\n                improvement = accuracy - self._current.accuracy\n                if improvement < 0.005:  # Require 0.5% improvement\n                    logger.info(f\"[{self.symbol}] No improvement: {accuracy:.4f} vs {self._current.accuracy:.4f}\")\n                    return False\n\n            # Create new version\n            new_version = ModelVersion(\n                version=self._version,\n                timestamp=datetime.now(),\n                accuracy=accuracy,\n                auc=auc,\n                n_samples=n_samples,\n                models=new_models,\n                feature_names=feature_names\n            )\n\n            # Atomic swap\n            self._previous = self._current\n            self._current = new_version\n            self._version += 1\n\n            logger.info(f\"[{self.symbol}] HOT-SWAP: v{new_version.version} \"\n                       f\"acc={accuracy:.4f} auc={auc:.4f} samples={n_samples}\")\n\n            # Save to disk\n            self._save_models()\n\n            return True",
    "source_file": "core\\ml\\ensemble.py",
    "academic_reference": null,
    "class_name": "HotSwappableEnsemble"
  },
  {
    "name": "_save_models",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Save current models to disk.",
    "python_code": "def _save_models(self):\n        \"\"\"Save current models to disk.\"\"\"\n        if self._current is None:\n            return\n\n        self.model_dir.mkdir(parents=True, exist_ok=True)\n\n        # Save in format compatible with existing code\n        model_file = self.model_dir / f\"{self.symbol}_models.pkl\"\n        models_dict = {\n            'target_direction_10': {\n                'xgboost': self._current.models.get('xgboost'),\n                'lightgbm': self._current.models.get('lightgbm'),\n                'catboost': self._current.models.get('catboost'),\n                'features': self._current.feature_names\n            }\n        }\n\n        try:\n            with open(model_file, 'wb') as f:\n                pickle.dump(models_dict, f)\n            logger.info(f\"Saved models to {model_file}\")\n        except Exception as e:\n            logger.error(f\"Failed to save models: {e}\")",
    "source_file": "core\\ml\\ensemble.py",
    "academic_reference": null,
    "class_name": "HotSwappableEnsemble"
  },
  {
    "name": "rollback",
    "category": "technical",
    "formula": "False | True",
    "explanation": "Rollback to previous model version.",
    "python_code": "def rollback(self) -> bool:\n        \"\"\"Rollback to previous model version.\"\"\"\n        with self._lock:\n            if self._previous is None:\n                logger.warning(\"No previous version to rollback to\")\n                return False\n\n            self._current = self._previous\n            self._previous = None\n            logger.info(f\"[{self.symbol}] Rolled back to v{self._current.version}\")\n            return True",
    "source_file": "core\\ml\\ensemble.py",
    "academic_reference": null,
    "class_name": "HotSwappableEnsemble"
  },
  {
    "name": "get_stats",
    "category": "machine_learning",
    "formula": "{'status': 'no_model'} | {",
    "explanation": "Get current model stats.",
    "python_code": "def get_stats(self) -> Dict:\n        \"\"\"Get current model stats.\"\"\"\n        with self._lock:\n            if self._current is None:\n                return {'status': 'no_model'}\n\n            return {\n                'symbol': self.symbol,\n                'version': self._current.version,\n                'accuracy': self._current.accuracy,\n                'auc': self._current.auc,\n                'n_samples': self._current.n_samples,\n                'timestamp': self._current.timestamp.isoformat(),\n                'has_fallback': self._previous is not None\n            }",
    "source_file": "core\\ml\\ensemble.py",
    "academic_reference": null,
    "class_name": "HotSwappableEnsemble"
  },
  {
    "name": "get_ensemble",
    "category": "machine_learning",
    "formula": "_ensembles[symbol]",
    "explanation": "Get or create ensemble manager for symbol.",
    "python_code": "def get_ensemble(symbol: str) -> HotSwappableEnsemble:\n    \"\"\"Get or create ensemble manager for symbol.\"\"\"\n    global _ensembles\n    with _registry_lock:\n        if symbol not in _ensembles:\n            _ensembles[symbol] = HotSwappableEnsemble(symbol)\n        return _ensembles[symbol]",
    "source_file": "core\\ml\\ensemble.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Initialize feature selector with configuration.",
    "python_code": "def __init__(self, config: Optional[FeatureSelectionConfig] = None):\n        \"\"\"Initialize feature selector with configuration.\"\"\"\n        self.config = config or FeatureSelectionConfig()\n\n        # State (set after fit)\n        self.selected_features_: Optional[List[str]] = None\n        self.selected_indices_: Optional[np.ndarray] = None\n        self.feature_scores_: Optional[Dict[str, float]] = None\n        self.stage_results_: Optional[Dict[str, Dict]] = None",
    "source_file": "core\\ml\\feature_selector.py",
    "academic_reference": null,
    "class_name": "FeatureSelector"
  },
  {
    "name": "fit",
    "category": "technical",
    "formula": "",
    "explanation": "Fit the feature selector.\n\nArgs:\n    X: Feature matrix (n_samples, n_features)\n    y: Target vector (n_samples,)\n    feature_names: List of feature names\n\nReturns:\n    self",
    "python_code": "def fit(\n        self,\n        X: np.ndarray,\n        y: np.ndarray,\n        feature_names: List[str]\n    ) -> 'FeatureSelector':\n        \"\"\"\n        Fit the feature selector.\n\n        Args:\n            X: Feature matrix (n_samples, n_features)\n            y: Target vector (n_samples,)\n            feature_names: List of feature names\n\n        Returns:\n            self\n        \"\"\"\n        logger.info(f\"Starting feature selection on {X.shape[1]} features...\")\n\n        self.stage_results_ = {}\n        current_features = np.arange(len(feature_names))\n        current_names = list(feature_names)\n\n        # Stage 1: Variance Filter\n        logger.info(\"Stage 1: Variance Filter\")\n        var_mask = self._variance_filter(X[:, current_features])\n        current_features = current_features[var_mask]\n        current_names = [n for n, m in zip(current_names, var_mask) if m]\n        self.stage_results_['variance'] = {\n            'input_features': len(feature_names),\n            'output_features': len(current_features),\n            'removed': len(feature_names) - len(current_features)\n        }\n        logger.info(f\"  {len(feature_names)} -> {len(current_features)} features\")\n\n        # Stage 2: Correlation Filter\n        logger.info(\"Stage 2: Correlation Filter\")\n        corr_mask = self._correlation_filter(X[:, current_features])\n        current_features = current_features[corr_mask]\n        current_names = [n for n, m in zip(current_names, corr_mask) if m]\n        self.stage_results_['correlation'] = {\n            'input_features': self.stage_results_['variance']['output_features'],\n            'output_features': len(current_features),\n            'removed': self.stage_results_['variance']['output_features'] - len(current_features)\n        }\n        logger.info(f\"  {self.stage_results_['variance']['output_features']} -> {len(current_features)} features\")\n\n        # Stage 3: Mutual Information\n        logger.info(\"Stage 3: Mutual Information Ranking\")\n        mi_scores = self._mutual_information(X[:, current_features], y)\n        mi_threshold = np.percentile(mi_scores, 100 - self.config.mi_percentile)\n        mi_mask = mi_scores >= mi_threshold\n        current_features = current_features[mi_mask]\n        current_names = [n for n, m in zip(current_names, mi_mask) if m]\n        mi_scores = mi_scores[mi_mask]\n        self.stage_results_['mutual_info'] = {\n            'input_features': self.stage_results_['correlation']['output_features'],\n            'output_features': len(current_features),\n            'removed': self.stage_results_['correlation']['output_features'] - len(current_features)\n        }\n        logger.info(f\"  {self.stage_results_['correlation']['output_features']} -> {len(current_features)} features\")\n\n        # Stage 4: Tree Importance\n        logger.info(\"Stage 4: Tree Importance (LightGBM)\")\n        tree_scores = self._tree_importance(X[:, current_features], y)\n\n        # Combine MI and tree scores\n        combined_scores = 0.5 * self._normali",
    "source_file": "core\\ml\\feature_selector.py",
    "academic_reference": null,
    "class_name": "FeatureSelector"
  },
  {
    "name": "transform",
    "category": "technical",
    "formula": "X[:, self.selected_indices_]",
    "explanation": "Transform features using fitted selector.\n\nArgs:\n    X: Feature matrix (n_samples, n_features)\n\nReturns:\n    Selected features (n_samples, n_selected)",
    "python_code": "def transform(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Transform features using fitted selector.\n\n        Args:\n            X: Feature matrix (n_samples, n_features)\n\n        Returns:\n            Selected features (n_samples, n_selected)\n        \"\"\"\n        if self.selected_indices_ is None:\n            raise RuntimeError(\"FeatureSelector not fitted. Call fit() first.\")\n\n        return X[:, self.selected_indices_]",
    "source_file": "core\\ml\\feature_selector.py",
    "academic_reference": null,
    "class_name": "FeatureSelector"
  },
  {
    "name": "fit_transform",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Fit and transform in one call.",
    "python_code": "def fit_transform(\n        self,\n        X: np.ndarray,\n        y: np.ndarray,\n        feature_names: List[str]\n    ) -> np.ndarray:\n        \"\"\"Fit and transform in one call.\"\"\"\n        self.fit(X, y, feature_names)\n        return self.transform(X)",
    "source_file": "core\\ml\\feature_selector.py",
    "academic_reference": null,
    "class_name": "FeatureSelector"
  },
  {
    "name": "_variance_filter",
    "category": "filtering",
    "formula": "mask",
    "explanation": "Remove features with low variance.\n\nFeatures with variance below threshold are constant or near-constant\nand provide no discriminative power.",
    "python_code": "def _variance_filter(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Remove features with low variance.\n\n        Features with variance below threshold are constant or near-constant\n        and provide no discriminative power.\n        \"\"\"\n        variances = np.nanvar(X, axis=0)\n        mask = variances >= self.config.variance_threshold\n        return mask",
    "source_file": "core\\ml\\feature_selector.py",
    "academic_reference": null,
    "class_name": "FeatureSelector"
  },
  {
    "name": "_correlation_filter",
    "category": "statistical",
    "formula": "mask",
    "explanation": "Remove highly correlated features.\n\nWhen two features are highly correlated (>0.95), we keep the first\nand remove the second to reduce redundancy.",
    "python_code": "def _correlation_filter(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Remove highly correlated features.\n\n        When two features are highly correlated (>0.95), we keep the first\n        and remove the second to reduce redundancy.\n        \"\"\"\n        n_features = X.shape[1]\n\n        # Handle NaN values\n        X_clean = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n\n        # Compute correlation matrix efficiently\n        # Standardize first\n        means = np.mean(X_clean, axis=0)\n        stds = np.std(X_clean, axis=0)\n        stds[stds == 0] = 1  # Avoid division by zero\n        X_std = (X_clean - means) / stds\n\n        # Correlation matrix\n        corr_matrix = np.abs(np.corrcoef(X_std.T))\n\n        # Find highly correlated pairs\n        mask = np.ones(n_features, dtype=bool)\n        for i in range(n_features):\n            if not mask[i]:\n                continue\n            for j in range(i + 1, n_features):\n                if not mask[j]:\n                    continue\n                if abs(corr_matrix[i, j]) > self.config.correlation_threshold:\n                    mask[j] = False  # Remove the second feature\n\n        return mask",
    "source_file": "core\\ml\\feature_selector.py",
    "academic_reference": null,
    "class_name": "FeatureSelector"
  },
  {
    "name": "_mutual_information",
    "category": "feature_engineering",
    "formula": "mi_scores | np.nanvar(X, axis=0)",
    "explanation": "Compute mutual information between features and target.\n\nUses sklearn's mutual_info_classif for classification tasks.",
    "python_code": "def _mutual_information(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute mutual information between features and target.\n\n        Uses sklearn's mutual_info_classif for classification tasks.\n        \"\"\"\n        try:\n            from sklearn.feature_selection import mutual_info_classif\n\n            # Handle NaN values\n            X_clean = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n\n            mi_scores = mutual_info_classif(\n                X_clean, y,\n                discrete_features=False,\n                random_state=self.config.random_state,\n                n_neighbors=5\n            )\n\n            return mi_scores\n\n        except Exception as e:\n            logger.warning(f\"MI computation failed: {e}. Using variance as fallback.\")\n            return np.nanvar(X, axis=0)",
    "source_file": "core\\ml\\feature_selector.py",
    "academic_reference": null,
    "class_name": "FeatureSelector"
  },
  {
    "name": "_tree_importance",
    "category": "feature_engineering",
    "formula": "model.feature_importances_ | np.nanvar(X, axis=0)",
    "explanation": "Compute feature importance using LightGBM.\n\nTree-based importance captures non-linear relationships\nthat linear methods miss.",
    "python_code": "def _tree_importance(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute feature importance using LightGBM.\n\n        Tree-based importance captures non-linear relationships\n        that linear methods miss.\n        \"\"\"\n        try:\n            import lightgbm as lgb\n\n            # Handle NaN values\n            X_clean = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n\n            # Train a quick LightGBM\n            model = lgb.LGBMClassifier(\n                n_estimators=self.config.n_estimators,\n                max_depth=6,\n                learning_rate=0.1,\n                subsample=0.8,\n                colsample_bytree=0.8,\n                n_jobs=-1,\n                verbose=-1,\n                random_state=self.config.random_state\n            )\n\n            model.fit(X_clean, y)\n\n            return model.feature_importances_\n\n        except Exception as e:\n            logger.warning(f\"Tree importance failed: {e}. Using variance as fallback.\")\n            return np.nanvar(X, axis=0)",
    "source_file": "core\\ml\\feature_selector.py",
    "academic_reference": null,
    "class_name": "FeatureSelector"
  },
  {
    "name": "_normalize",
    "category": "statistical",
    "formula": "np.ones_like(scores) | (scores - min_val) / (max_val - min_val)",
    "explanation": "Normalize scores to 0-1 range.",
    "python_code": "def _normalize(scores: np.ndarray) -> np.ndarray:\n        \"\"\"Normalize scores to 0-1 range.\"\"\"\n        min_val = np.min(scores)\n        max_val = np.max(scores)\n        if max_val - min_val < 1e-10:\n            return np.ones_like(scores)\n        return (scores - min_val) / (max_val - min_val)",
    "source_file": "core\\ml\\feature_selector.py",
    "academic_reference": null,
    "class_name": "FeatureSelector"
  },
  {
    "name": "save",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Save fitted selector to disk.",
    "python_code": "def save(self, path: Path):\n        \"\"\"Save fitted selector to disk.\"\"\"\n        path = Path(path)\n\n        state = {\n            'config': self.config,\n            'selected_features': self.selected_features_,\n            'selected_indices': self.selected_indices_,\n            'feature_scores': self.feature_scores_,\n            'stage_results': self.stage_results_,\n        }\n\n        with open(path, 'wb') as f:\n            pickle.dump(state, f)\n\n        # Also save human-readable summary\n        summary_path = path.with_suffix('.json')\n        summary = {\n            'n_selected': len(self.selected_features_) if self.selected_features_ else 0,\n            'top_20_features': (\n                list(self.selected_features_[:20]) if self.selected_features_ else []\n            ),\n            'stage_results': self.stage_results_,\n        }\n        with open(summary_path, 'w') as f:\n            json.dump(summary, f, indent=2)\n\n        logger.info(f\"Saved selector to {path}\")",
    "source_file": "core\\ml\\feature_selector.py",
    "academic_reference": null,
    "class_name": "FeatureSelector"
  },
  {
    "name": "load",
    "category": "machine_learning",
    "formula": "selector",
    "explanation": "Load fitted selector from disk.",
    "python_code": "def load(cls, path: Path) -> 'FeatureSelector':\n        \"\"\"Load fitted selector from disk.\"\"\"\n        path = Path(path)\n\n        with open(path, 'rb') as f:\n            state = pickle.load(f)\n\n        selector = cls(config=state['config'])\n        selector.selected_features_ = state['selected_features']\n        selector.selected_indices_ = state['selected_indices']\n        selector.feature_scores_ = state['feature_scores']\n        selector.stage_results_ = state['stage_results']\n\n        return selector",
    "source_file": "core\\ml\\feature_selector.py",
    "academic_reference": null,
    "class_name": "FeatureSelector"
  },
  {
    "name": "get_summary",
    "category": "feature_engineering",
    "formula": "{'error': 'Selector not fitted'} | {",
    "explanation": "Get summary of feature selection.",
    "python_code": "def get_summary(self) -> Dict:\n        \"\"\"Get summary of feature selection.\"\"\"\n        if self.selected_features_ is None:\n            return {'error': 'Selector not fitted'}\n\n        return {\n            'n_selected': len(self.selected_features_),\n            'stage_results': self.stage_results_,\n            'top_10_features': self.selected_features_[:10],\n            'top_10_scores': [\n                self.feature_scores_[f] for f in self.selected_features_[:10]\n            ],\n        }",
    "source_file": "core\\ml\\feature_selector.py",
    "academic_reference": null,
    "class_name": "FeatureSelector"
  },
  {
    "name": "transform",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Transform features for a specific symbol.",
    "python_code": "def transform(self, symbol: str, X: np.ndarray) -> np.ndarray:\n        \"\"\"Transform features for a specific symbol.\"\"\"\n        if symbol not in self.selectors_:\n            raise ValueError(f\"Symbol {symbol} not fitted. Call fit() first.\")\n\n        return self.selectors_[symbol].transform(X)",
    "source_file": "core\\ml\\feature_selector.py",
    "academic_reference": null,
    "class_name": "AdaptiveFeatureSelector"
  },
  {
    "name": "_update_shared_features",
    "category": "machine_learning",
    "formula": "# Find features that appear in most selectors",
    "explanation": "Update shared features across all fitted symbols.",
    "python_code": "def _update_shared_features(self):\n        \"\"\"Update shared features across all fitted symbols.\"\"\"\n        if len(self.selectors_) < 2:\n            return\n\n        # Find features that appear in most selectors\n        feature_counts = {}\n        for selector in self.selectors_.values():\n            if selector.selected_features_:\n                for f in selector.selected_features_:\n                    feature_counts[f] = feature_counts.get(f, 0) + 1\n\n        # Sort by count\n        sorted_features = sorted(\n            feature_counts.items(),\n            key=lambda x: x[1],\n            reverse=True\n        )\n\n        # Take top shared_features_pct as shared\n        n_shared = int(len(sorted_features) * self.shared_features_pct)\n        self.shared_features_ = [f for f, _ in sorted_features[:n_shared]]",
    "source_file": "core\\ml\\feature_selector.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AdaptiveFeatureSelector"
  },
  {
    "name": "get_shared_features",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Get features that are important across all symbols.",
    "python_code": "def get_shared_features(self) -> List[str]:\n        \"\"\"Get features that are important across all symbols.\"\"\"\n        return self.shared_features_ or []",
    "source_file": "core\\ml\\feature_selector.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AdaptiveFeatureSelector"
  },
  {
    "name": "load",
    "category": "feature_engineering",
    "formula": "adaptive",
    "explanation": "Load all selectors from disk.",
    "python_code": "def load(cls, path: Path) -> 'AdaptiveFeatureSelector':\n        \"\"\"Load all selectors from disk.\"\"\"\n        path = Path(path)\n\n        adaptive = cls()\n\n        # Load shared features\n        with open(path / \"shared_features.json\", 'r') as f:\n            data = json.load(f)\n            adaptive.shared_features_ = data['shared_features']\n\n        # Load individual selectors\n        for pkl_file in path.glob(\"*_selector.pkl\"):\n            symbol = pkl_file.stem.replace(\"_selector\", \"\")\n            adaptive.selectors_[symbol] = FeatureSelector.load(pkl_file)\n\n        return adaptive",
    "source_file": "core\\ml\\feature_selector.py",
    "academic_reference": null,
    "class_name": "AdaptiveFeatureSelector"
  },
  {
    "name": "select_features_fast",
    "category": "technical",
    "formula": "(",
    "explanation": "Fast feature selection for quick experimentation.\n\nArgs:\n    X: Feature matrix\n    y: Target vector\n    feature_names: Feature names\n    top_k: Number of features to select\n\nReturns:\n    (selected_indices, selected_names, scores)",
    "python_code": "def select_features_fast(\n    X: np.ndarray,\n    y: np.ndarray,\n    feature_names: List[str],\n    top_k: int = 400\n) -> Tuple[np.ndarray, List[str], np.ndarray]:\n    \"\"\"\n    Fast feature selection for quick experimentation.\n\n    Args:\n        X: Feature matrix\n        y: Target vector\n        feature_names: Feature names\n        top_k: Number of features to select\n\n    Returns:\n        (selected_indices, selected_names, scores)\n    \"\"\"\n    config = FeatureSelectionConfig(top_k_features=top_k)\n    selector = FeatureSelector(config)\n    selector.fit(X, y, feature_names)\n\n    return (\n        selector.selected_indices_,\n        selector.selected_features_,\n        np.array([selector.feature_scores_[f] for f in selector.selected_features_])\n    )",
    "source_file": "core\\ml\\feature_selector.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "create_feature_selector",
    "category": "reinforcement_learning",
    "formula": "AdaptiveFeatureSelector(config, min_shared_features=shared_features) | FeatureSelector(config)",
    "explanation": "Factory function to create feature selectors.\n\nArgs:\n    config: Feature selection config (uses defaults if None)\n    adaptive: If True, creates AdaptiveFeatureSelector\n    shared_features: Min shared features for adaptive selector\n\nReturns:\n    FeatureSelector or AdaptiveFeatureSelector instance",
    "python_code": "def create_feature_selector(\n    config: Optional[FeatureSelectionConfig] = None,\n    adaptive: bool = False,\n    shared_features: int = 200\n) -> Union[FeatureSelector, AdaptiveFeatureSelector]:\n    \"\"\"\n    Factory function to create feature selectors.\n\n    Args:\n        config: Feature selection config (uses defaults if None)\n        adaptive: If True, creates AdaptiveFeatureSelector\n        shared_features: Min shared features for adaptive selector\n\n    Returns:\n        FeatureSelector or AdaptiveFeatureSelector instance\n    \"\"\"\n    if config is None:\n        config = FeatureSelectionConfig()\n\n    if adaptive:\n        return AdaptiveFeatureSelector(config, min_shared_features=shared_features)\n    return FeatureSelector(config)",
    "source_file": "core\\ml\\feature_selector.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "configure_gpu",
    "category": "machine_learning",
    "formula": "True | False",
    "explanation": "Configure GPU for maximum ML performance.",
    "python_code": "def configure_gpu():\n    \"\"\"Configure GPU for maximum ML performance.\"\"\"\n    import torch\n\n    if torch.cuda.is_available():\n        # Enable cuDNN auto-tuner for best convolution algorithms\n        torch.backends.cudnn.benchmark = True\n        torch.backends.cudnn.enabled = True\n\n        # Enable TF32 for faster matrix ops on Ampere+ GPUs\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.allow_tf32 = True\n\n        # Set default device\n        torch.cuda.set_device(0)\n\n        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n        print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n        print(f\"cuDNN benchmark: {torch.backends.cudnn.benchmark}\")\n        print(f\"TF32 enabled: {torch.backends.cuda.matmul.allow_tf32}\")\n\n        return True\n    return False",
    "source_file": "core\\ml\\gpu_config.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "get_xgb_gpu_params",
    "category": "machine_learning",
    "formula": "{ | {",
    "explanation": "XGBoost params optimized for RTX 5080 16GB.\n\nNote: XGBoost 2.0+ changed GPU usage:\n- tree_method: 'hist' (not 'gpu_hist')\n- device: 'cuda' enables GPU\n\nArgs:\n    aggressive: If True, use max settings for speed (default)\n               If False, use conservative settings for stability",
    "python_code": "def get_xgb_gpu_params(aggressive: bool = True):\n    \"\"\"XGBoost params optimized for RTX 5080 16GB.\n\n    Note: XGBoost 2.0+ changed GPU usage:\n    - tree_method: 'hist' (not 'gpu_hist')\n    - device: 'cuda' enables GPU\n\n    Args:\n        aggressive: If True, use max settings for speed (default)\n                   If False, use conservative settings for stability\n    \"\"\"\n    if aggressive:\n        return {\n            'tree_method': 'hist',\n            'device': 'cuda',\n            'max_depth': 14,            # Deeper for 16GB VRAM\n            'max_bin': 512,             # More bins = better GPU util\n            'learning_rate': 0.03,      # Lower LR, more trees = better\n            'subsample': 0.8,\n            'colsample_bytree': 0.8,\n            'colsample_bylevel': 0.8,\n            'min_child_weight': 3,\n            'gamma': 0.1,\n            'reg_alpha': 0.1,\n            'reg_lambda': 1.0,\n            'n_jobs': 12,               # Physical cores only\n            'n_estimators': 2000,       # More trees\n            'early_stopping_rounds': 100,\n        }\n    else:\n        return {\n            'tree_method': 'hist',\n            'device': 'cuda',\n            'max_depth': 12,\n            'max_bin': 256,\n            'learning_rate': 0.05,\n            'subsample': 0.8,\n            'colsample_bytree': 0.8,\n            'n_jobs': 12,\n        }",
    "source_file": "core\\ml\\gpu_config.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "get_lgb_gpu_params",
    "category": "machine_learning",
    "formula": "{ | {",
    "explanation": "LightGBM params optimized for RTX 5080 16GB.\n\nArgs:\n    aggressive: If True, use max settings for speed (default)",
    "python_code": "def get_lgb_gpu_params(aggressive: bool = True):\n    \"\"\"LightGBM params optimized for RTX 5080 16GB.\n\n    Args:\n        aggressive: If True, use max settings for speed (default)\n    \"\"\"\n    if aggressive:\n        return {\n            'device': 'gpu',\n            'gpu_platform_id': 0,\n            'gpu_device_id': 0,\n            'max_depth': 14,\n            'learning_rate': 0.03,\n            'num_leaves': 2047,         # 2^11 - 1 (limited by LightGBM)\n            'min_data_in_leaf': 20,\n            'histogram_pool_size': 2048,  # More GPU memory for histograms\n            'gpu_use_dp': False,        # FP32 for speed\n            'feature_fraction': 0.8,\n            'bagging_fraction': 0.8,\n            'bagging_freq': 5,\n            'lambda_l1': 0.1,\n            'lambda_l2': 1.0,\n            'n_estimators': 2000,\n            'n_jobs': 12,               # Physical cores only\n            'verbose': -1,\n            'early_stopping_round': 100,\n        }\n    else:\n        return {\n            'device': 'gpu',\n            'gpu_platform_id': 0,\n            'gpu_device_id': 0,\n            'max_depth': 12,\n            'learning_rate': 0.05,\n            'num_leaves': 511,\n            'histogram_pool_size': 1024,\n            'gpu_use_dp': False,\n            'n_jobs': 12,\n            'verbose': -1,\n        }",
    "source_file": "core\\ml\\gpu_config.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "get_catboost_gpu_params",
    "category": "machine_learning",
    "formula": "{ | {",
    "explanation": "CatBoost params optimized for RTX 5080 16GB.\n\nArgs:\n    aggressive: If True, use max settings for speed (default)",
    "python_code": "def get_catboost_gpu_params(aggressive: bool = True):\n    \"\"\"CatBoost params optimized for RTX 5080 16GB.\n\n    Args:\n        aggressive: If True, use max settings for speed (default)\n    \"\"\"\n    if aggressive:\n        return {\n            'task_type': 'GPU',\n            'devices': '0',\n            'depth': 12,                # Deeper for 16GB VRAM\n            'border_count': 64,         # More bins\n            'learning_rate': 0.03,\n            'iterations': 2000,\n            'boosting_type': 'Plain',   # 2-3x faster than Ordered\n            'l2_leaf_reg': 3.0,\n            'random_strength': 1.0,\n            'bagging_temperature': 0.5,\n            'grow_policy': 'SymmetricTree',\n            'thread_count': 12,         # Physical cores only\n            'early_stopping_rounds': 100,\n            'verbose': 100,\n        }\n    else:\n        return {\n            'task_type': 'GPU',\n            'devices': '0',\n            'depth': 10,\n            'border_count': 32,\n            'learning_rate': 0.05,\n            'iterations': 1000,\n            'boosting_type': 'Plain',\n            'thread_count': 12,\n        }",
    "source_file": "core\\ml\\gpu_config.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "get_xgb_concurrent_params",
    "category": "machine_learning",
    "formula": "{",
    "explanation": "XGBoost params for concurrent training (3 jobs on RTX 5080).\n\nReduced depth and bins to fit 3 models in 16GB VRAM.",
    "python_code": "def get_xgb_concurrent_params():\n    \"\"\"XGBoost params for concurrent training (3 jobs on RTX 5080).\n\n    Reduced depth and bins to fit 3 models in 16GB VRAM.\n    \"\"\"\n    return {\n        'tree_method': 'hist',\n        'device': 'cuda',\n        'max_depth': 8,             # Reduced from 14\n        'max_bin': 128,             # Reduced from 512\n        'learning_rate': 0.05,\n        'subsample': 0.7,\n        'colsample_bytree': 0.6,\n        'objective': 'binary:logistic',\n        'eval_metric': 'auc',\n        'nthread': 4,               # 12 cores / 3 jobs\n    }",
    "source_file": "core\\ml\\gpu_config.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "get_lgb_concurrent_params",
    "category": "machine_learning",
    "formula": "{",
    "explanation": "LightGBM params for concurrent training (3 jobs on RTX 5080).\n\nReduced depth and leaves to fit 3 models in 16GB VRAM.",
    "python_code": "def get_lgb_concurrent_params():\n    \"\"\"LightGBM params for concurrent training (3 jobs on RTX 5080).\n\n    Reduced depth and leaves to fit 3 models in 16GB VRAM.\n    \"\"\"\n    return {\n        'device': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0,\n        'max_depth': 8,             # Reduced from 14\n        'num_leaves': 127,          # Reduced from 2047\n        'feature_fraction': 0.6,\n        'bagging_fraction': 0.7,\n        'bagging_freq': 5,\n        'verbose': -1,\n        'objective': 'binary',\n        'metric': 'auc',\n        'n_jobs': 4,                # 12 cores / 3 jobs\n        'histogram_pool_size': 512, # Reduced from 2048\n    }",
    "source_file": "core\\ml\\gpu_config.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "get_catboost_concurrent_params",
    "category": "machine_learning",
    "formula": "{",
    "explanation": "CatBoost params for concurrent training (3 jobs on RTX 5080).\n\nReduced depth and iterations to fit 3 models in 16GB VRAM.",
    "python_code": "def get_catboost_concurrent_params():\n    \"\"\"CatBoost params for concurrent training (3 jobs on RTX 5080).\n\n    Reduced depth and iterations to fit 3 models in 16GB VRAM.\n    \"\"\"\n    return {\n        'task_type': 'GPU',\n        'devices': '0',\n        'depth': 6,                 # Reduced from 12\n        'border_count': 32,         # Reduced from 64\n        'iterations': 300,          # Reduced from 2000\n        'early_stopping_rounds': 20,\n        'verbose': False,\n        'loss_function': 'Logloss',\n        'thread_count': 4,          # 12 cores / 3 jobs\n    }",
    "source_file": "core\\ml\\gpu_config.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "get_concurrent_training_config",
    "category": "machine_learning",
    "formula": "{",
    "explanation": "Get full config for parallel training.\n\nReturns dict with recommended settings for parallel training.",
    "python_code": "def get_concurrent_training_config():\n    \"\"\"Get full config for parallel training.\n\n    Returns dict with recommended settings for parallel training.\n    \"\"\"\n    return {\n        'cpu_workers': 4,           # Feature generation processes\n        'gpu_workers': 3,           # Concurrent GPU training jobs\n        'max_samples': 50000,       # Max samples per pair\n        'vram_per_job_gb': 5,       # ~5GB VRAM per model set\n        'total_vram_gb': 16,        # RTX 5080\n        'xgb': get_xgb_concurrent_params(),\n        'lgb': get_lgb_concurrent_params(),\n        'cb': get_catboost_concurrent_params(),\n    }",
    "source_file": "core\\ml\\gpu_config.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "set_high_priority",
    "category": "machine_learning",
    "formula": "True | False",
    "explanation": "Set current process to high priority for ML training.",
    "python_code": "def set_high_priority():\n    \"\"\"Set current process to high priority for ML training.\"\"\"\n    import psutil\n    try:\n        p = psutil.Process()\n        p.nice(psutil.HIGH_PRIORITY_CLASS)  # Windows high priority\n        print(f\"Process priority set to HIGH\")\n        return True\n    except Exception as e:\n        print(f\"Could not set high priority: {e}\")\n        return False",
    "source_file": "core\\ml\\gpu_config.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "set_cpu_affinity",
    "category": "machine_learning",
    "formula": "True | False",
    "explanation": "Set CPU affinity to physical cores only (avoids hyperthreading overhead).\n\nArgs:\n    physical_cores_only: If True, use only cores 0-11 (physical)\n                        If False, use all 24 logical processors",
    "python_code": "def set_cpu_affinity(physical_cores_only: bool = True):\n    \"\"\"Set CPU affinity to physical cores only (avoids hyperthreading overhead).\n\n    Args:\n        physical_cores_only: If True, use only cores 0-11 (physical)\n                            If False, use all 24 logical processors\n    \"\"\"\n    import psutil\n    try:\n        p = psutil.Process()\n        if physical_cores_only:\n            # Use first 12 cores (physical cores on Ryzen 9 9900X)\n            p.cpu_affinity(list(range(12)))\n            print(f\"CPU affinity set to physical cores 0-11\")\n        else:\n            p.cpu_affinity(list(range(24)))\n            print(f\"CPU affinity set to all 24 logical processors\")\n        return True\n    except Exception as e:\n        print(f\"Could not set CPU affinity: {e}\")\n        return False",
    "source_file": "core\\ml\\gpu_config.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "optimize_for_training",
    "category": "machine_learning",
    "formula": "gpu_ok and priority_ok and affinity_ok",
    "explanation": "One-call function to optimize everything for ML training.",
    "python_code": "def optimize_for_training():\n    \"\"\"One-call function to optimize everything for ML training.\"\"\"\n    print(\"=\" * 60)\n    print(\"OPTIMIZING FOR ML TRAINING\")\n    print(\"=\" * 60)\n\n    # Configure GPU\n    gpu_ok = configure_gpu()\n\n    # Set process priority\n    priority_ok = set_high_priority()\n\n    # Set CPU affinity to physical cores\n    affinity_ok = set_cpu_affinity(physical_cores_only=True)\n\n    # Verify settings\n    print(\"\\n--- Environment Variables ---\")\n    print(f\"OMP_NUM_THREADS: {os.environ.get('OMP_NUM_THREADS', 'not set')}\")\n    print(f\"MKL_NUM_THREADS: {os.environ.get('MKL_NUM_THREADS', 'not set')}\")\n    print(f\"CUDA_LAUNCH_BLOCKING: {os.environ.get('CUDA_LAUNCH_BLOCKING', 'not set')}\")\n\n    print(\"\\n--- Status ---\")\n    print(f\"GPU configured: {gpu_ok}\")\n    print(f\"High priority: {priority_ok}\")\n    print(f\"CPU affinity: {affinity_ok}\")\n    print(\"=\" * 60)\n\n    return gpu_ok and priority_ok and affinity_ok",
    "source_file": "core\\ml\\gpu_config.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "get_tabnet_gpu_params",
    "category": "deep_learning",
    "formula": "{ | {",
    "explanation": "TabNet params optimized for RTX 5080 16GB.\n\nTabNet uses attention mechanism for feature selection.\nGood for interpretability + competitive with GBMs.\n\nArgs:\n    aggressive: If True, use larger architecture",
    "python_code": "def get_tabnet_gpu_params(aggressive: bool = True):\n    \"\"\"TabNet params optimized for RTX 5080 16GB.\n\n    TabNet uses attention mechanism for feature selection.\n    Good for interpretability + competitive with GBMs.\n\n    Args:\n        aggressive: If True, use larger architecture\n    \"\"\"\n    if aggressive:\n        return {\n            'n_d': 64,                  # Width of decision step\n            'n_a': 64,                  # Width of attention step\n            'n_steps': 5,               # Number of decision steps\n            'gamma': 1.5,               # Feature reusage coefficient\n            'n_independent': 2,         # Independent GLU layers\n            'n_shared': 2,              # Shared GLU layers\n            'momentum': 0.02,\n            'lambda_sparse': 1e-4,      # Sparsity regularization\n            'mask_type': 'sparsemax',   # or 'entmax'\n            'optimizer_params': {\n                'lr': 2e-2,\n                'weight_decay': 1e-5,\n            },\n            'scheduler_params': {\n                'step_size': 10,\n                'gamma': 0.9,\n            },\n            'device_name': 'cuda',\n            'batch_size': 1024,         # Large batch for GPU\n            'virtual_batch_size': 256,  # For ghost batch norm\n            'max_epochs': 100,\n            'patience': 15,\n        }\n    else:\n        return {\n            'n_d': 32,\n            'n_a': 32,\n            'n_steps': 3,\n            'gamma': 1.3,\n            'n_independent': 1,\n            'n_shared': 1,\n            'momentum': 0.02,\n            'device_name': 'cuda',\n            'batch_size': 512,\n            'virtual_batch_size': 128,\n            'max_epochs': 50,\n            'patience': 10,\n        }",
    "source_file": "core\\ml\\gpu_config.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "get_cnn_gpu_params",
    "category": "machine_learning",
    "formula": "{ | {",
    "explanation": "1D-CNN params optimized for RTX 5080 16GB.\n\nTreats features as 1D signal, captures local patterns.\nGood for detecting feature interactions.\n\nArgs:\n    aggressive: If True, use deeper architecture",
    "python_code": "def get_cnn_gpu_params(aggressive: bool = True):\n    \"\"\"1D-CNN params optimized for RTX 5080 16GB.\n\n    Treats features as 1D signal, captures local patterns.\n    Good for detecting feature interactions.\n\n    Args:\n        aggressive: If True, use deeper architecture\n    \"\"\"\n    if aggressive:\n        return {\n            'conv_layers': [\n                {'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n                {'out_channels': 128, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n                {'out_channels': 256, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n            ],\n            'fc_layers': [512, 256, 128],\n            'dropout': 0.3,\n            'batch_norm': True,\n            'activation': 'relu',\n            'pool_size': 2,\n            'optimizer': 'adamw',\n            'learning_rate': 1e-3,\n            'weight_decay': 1e-4,\n            'batch_size': 512,\n            'max_epochs': 100,\n            'patience': 15,\n            'device': 'cuda',\n            'num_workers': 4,\n            'pin_memory': True,\n            'cudnn_benchmark': True,\n        }\n    else:\n        return {\n            'conv_layers': [\n                {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n                {'out_channels': 64, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n            ],\n            'fc_layers': [256, 128],\n            'dropout': 0.2,\n            'batch_norm': True,\n            'activation': 'relu',\n            'learning_rate': 1e-3,\n            'batch_size': 256,\n            'max_epochs': 50,\n            'device': 'cuda',\n        }",
    "source_file": "core\\ml\\gpu_config.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "get_mlp_gpu_params",
    "category": "machine_learning",
    "formula": "{ | {",
    "explanation": "MLP (Multi-Layer Perceptron) params for RTX 5080 16GB.\n\nDeep fully-connected network with batch normalization.\nGood baseline for tabular data.\n\nArgs:\n    aggressive: If True, use deeper/wider architecture",
    "python_code": "def get_mlp_gpu_params(aggressive: bool = True):\n    \"\"\"MLP (Multi-Layer Perceptron) params for RTX 5080 16GB.\n\n    Deep fully-connected network with batch normalization.\n    Good baseline for tabular data.\n\n    Args:\n        aggressive: If True, use deeper/wider architecture\n    \"\"\"\n    if aggressive:\n        return {\n            'hidden_layers': [512, 256, 128, 64],\n            'dropout': 0.3,\n            'batch_norm': True,\n            'activation': 'relu',\n            'use_residual': True,       # Skip connections\n            'optimizer': 'adamw',\n            'learning_rate': 1e-3,\n            'weight_decay': 1e-4,\n            'scheduler': 'cosine',\n            'warmup_epochs': 5,\n            'batch_size': 1024,\n            'max_epochs': 100,\n            'patience': 15,\n            'device': 'cuda',\n            'num_workers': 4,\n            'pin_memory': True,\n            'mixed_precision': True,    # FP16 for speed\n        }\n    else:\n        return {\n            'hidden_layers': [256, 128, 64],\n            'dropout': 0.2,\n            'batch_norm': True,\n            'activation': 'relu',\n            'learning_rate': 1e-3,\n            'batch_size': 512,\n            'max_epochs': 50,\n            'device': 'cuda',\n        }",
    "source_file": "core\\ml\\gpu_config.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "get_transformer_gpu_params",
    "category": "deep_learning",
    "formula": "{ | {",
    "explanation": "Transformer params for time series (iTransformer style).\n\nFor sequence modeling of OHLCV data.\nBest for capturing temporal dependencies.\n\nArgs:\n    aggressive: If True, use larger model",
    "python_code": "def get_transformer_gpu_params(aggressive: bool = True):\n    \"\"\"Transformer params for time series (iTransformer style).\n\n    For sequence modeling of OHLCV data.\n    Best for capturing temporal dependencies.\n\n    Args:\n        aggressive: If True, use larger model\n    \"\"\"\n    if aggressive:\n        return {\n            'd_model': 256,             # Embedding dimension\n            'n_heads': 8,               # Attention heads\n            'n_layers': 4,              # Encoder layers\n            'd_ff': 1024,               # FFN hidden dim\n            'dropout': 0.1,\n            'seq_len': 96,              # Input sequence length\n            'pred_len': 1,              # Prediction length\n            'attention_type': 'full',   # or 'sparse', 'linear'\n            'optimizer': 'adamw',\n            'learning_rate': 1e-4,\n            'weight_decay': 1e-2,\n            'scheduler': 'cosine',\n            'warmup_steps': 1000,\n            'batch_size': 64,           # Smaller for transformers\n            'max_epochs': 100,\n            'patience': 15,\n            'device': 'cuda',\n            'mixed_precision': True,\n            'gradient_checkpointing': False,\n        }\n    else:\n        return {\n            'd_model': 128,\n            'n_heads': 4,\n            'n_layers': 2,\n            'd_ff': 512,\n            'dropout': 0.1,\n            'seq_len': 48,\n            'learning_rate': 1e-4,\n            'batch_size': 32,\n            'max_epochs': 50,\n            'device': 'cuda',\n        }",
    "source_file": "core\\ml\\gpu_config.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": null
  },
  {
    "name": "get_neural_concurrent_params",
    "category": "machine_learning",
    "formula": "{",
    "explanation": "Neural network params for concurrent training with GBMs.\n\nReduced model sizes to fit alongside XGB/LGB/CB training.\nMemory budget: ~2GB for neural nets (vs 5GB each for GBMs).",
    "python_code": "def get_neural_concurrent_params():\n    \"\"\"Neural network params for concurrent training with GBMs.\n\n    Reduced model sizes to fit alongside XGB/LGB/CB training.\n    Memory budget: ~2GB for neural nets (vs 5GB each for GBMs).\n    \"\"\"\n    return {\n        'tabnet': {\n            'n_d': 32,\n            'n_a': 32,\n            'n_steps': 3,\n            'batch_size': 512,\n            'device_name': 'cuda',\n        },\n        'cnn': {\n            'conv_layers': [\n                {'out_channels': 32, 'kernel_size': 3, 'stride': 1, 'padding': 1},\n            ],\n            'fc_layers': [128],\n            'batch_size': 256,\n            'device': 'cuda',\n        },\n        'mlp': {\n            'hidden_layers': [128, 64],\n            'batch_size': 512,\n            'device': 'cuda',\n        },\n    }",
    "source_file": "core\\ml\\gpu_config.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "get_stacking_training_config",
    "category": "machine_learning",
    "formula": "{",
    "explanation": "Get full config for 6-model stacking ensemble.\n\nMemory budget for RTX 5080 16GB:\n- XGBoost: ~3GB\n- LightGBM: ~2GB\n- CatBoost: ~3GB\n- TabNet: ~1.5GB\n- CNN: ~0.5GB\n- MLP: ~0.5GB\n- Total: ~10.5GB (leaves 5.5GB headroom)",
    "python_code": "def get_stacking_training_config():\n    \"\"\"Get full config for 6-model stacking ensemble.\n\n    Memory budget for RTX 5080 16GB:\n    - XGBoost: ~3GB\n    - LightGBM: ~2GB\n    - CatBoost: ~3GB\n    - TabNet: ~1.5GB\n    - CNN: ~0.5GB\n    - MLP: ~0.5GB\n    - Total: ~10.5GB (leaves 5.5GB headroom)\n    \"\"\"\n    return {\n        'xgb': get_xgb_gpu_params(aggressive=False),\n        'lgb': get_lgb_gpu_params(aggressive=False),\n        'catboost': get_catboost_gpu_params(aggressive=False),\n        'tabnet': get_tabnet_gpu_params(aggressive=False),\n        'cnn': get_cnn_gpu_params(aggressive=False),\n        'mlp': get_mlp_gpu_params(aggressive=False),\n        'meta_learner': {\n            'type': 'xgboost',\n            **get_xgb_gpu_params(aggressive=False),\n        },\n        'total_vram_budget_gb': 12.0,\n        'cpu_workers': 4,\n    }",
    "source_file": "core\\ml\\gpu_config.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "get_max_parallel_config",
    "category": "machine_learning",
    "formula": "each = 12 cores) | GB = 5GB | buffer = 4GB",
    "explanation": "Config for MAXIMUM system utilization.\n\nTarget:\n- GPU: 85-95% utilization, 280-320W power draw\n- CPU: 75-85% utilization (all 12 cores)\n- RAM: 20-25 GB used (from 32GB available)\n\nArchitecture:\n- 3 GPU training jobs in parallel (5GB VRAM each)\n- 4 CPU feature workers (3 threads each = 12 cores)\n- Pipeline parallelism: Stage 1 (CPU) feeds Stage 2 (GPU)\n\nMemory Budget (16GB VRAM):\n- Job 1: XGBoost ~3GB + LightGBM ~2GB = 5GB\n- Job 2: CatBoost ~3GB + buffer = 4GB\n- Job 3: TabNet ~1.5GB + CNN ~0.5GB + MLP ~0.5GB + buffer = 4GB\n- Overhead: ~3GB\n- Total: ~16GB",
    "python_code": "def get_max_parallel_config():\n    \"\"\"\n    Config for MAXIMUM system utilization.\n\n    Target:\n    - GPU: 85-95% utilization, 280-320W power draw\n    - CPU: 75-85% utilization (all 12 cores)\n    - RAM: 20-25 GB used (from 32GB available)\n\n    Architecture:\n    - 3 GPU training jobs in parallel (5GB VRAM each)\n    - 4 CPU feature workers (3 threads each = 12 cores)\n    - Pipeline parallelism: Stage 1 (CPU) feeds Stage 2 (GPU)\n\n    Memory Budget (16GB VRAM):\n    - Job 1: XGBoost ~3GB + LightGBM ~2GB = 5GB\n    - Job 2: CatBoost ~3GB + buffer = 4GB\n    - Job 3: TabNet ~1.5GB + CNN ~0.5GB + MLP ~0.5GB + buffer = 4GB\n    - Overhead: ~3GB\n    - Total: ~16GB\n    \"\"\"\n    return {\n        # System utilization targets\n        'gpu_target_utilization': 0.90,  # 90%\n        'cpu_target_utilization': 0.80,  # 80%\n        'ram_target_gb': 22,              # 22 GB of 32 GB\n\n        # Parallel workers\n        'gpu_workers': 3,                 # 3 concurrent GPU training jobs\n        'cpu_workers': 4,                 # 4 feature generation processes\n        'threads_per_cpu_worker': 3,      # 4 * 3 = 12 cores total\n\n        # Data loading\n        'batch_size': 4096,               # Large batch for GPU efficiency\n        'max_samples': 80000,             # More samples (32GB RAM allows it)\n        'prefetch_pairs': 2,              # Prefetch next 2 pairs\n\n        # XGBoost concurrent params (3 jobs fit in 16GB VRAM)\n        'xgb': {\n            'tree_method': 'hist',        # XGBoost 2.0+\n            'device': 'cuda',             # GPU acceleration\n            'max_depth': 10,              # Balanced for concurrent training\n            'max_bin': 256,               # Good GPU efficiency\n            'n_estimators': 1200,         # More trees for accuracy\n            'learning_rate': 0.03,        # Lower LR, more trees\n            'subsample': 0.8,\n            'colsample_bytree': 0.7,\n            'colsample_bylevel': 0.8,\n            'min_child_weight': 3,\n            'gamma': 0.1,\n            'reg_alpha': 0.1,\n            'reg_lambda': 1.0,\n            'nthread': 4,                 # 12 cores / 3 jobs\n            'early_stopping_rounds': 50,\n            'objective': 'binary:logistic',\n            'eval_metric': 'auc',\n        },\n\n        # LightGBM concurrent params\n        'lgb': {\n            'device': 'gpu',\n            'gpu_platform_id': 0,\n            'gpu_device_id': 0,\n            'max_depth': 10,              # Balanced depth\n            'num_leaves': 512,            # 2^9 (moderate for concurrency)\n            'n_estimators': 1200,\n            'learning_rate': 0.03,\n            'feature_fraction': 0.7,\n            'bagging_fraction': 0.8,\n            'bagging_freq': 5,\n            'lambda_l1': 0.1,\n            'lambda_l2': 1.0,\n            'n_jobs': 4,                  # 12 cores / 3 jobs\n            'histogram_pool_size': 512,   # Reduced for concurrent GPU\n            'verbose': -1,\n            'objective': 'binary',\n            'metric': 'auc',\n        }",
    "source_file": "core\\ml\\gpu_config.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "verify_max_parallel_ready",
    "category": "machine_learning",
    "formula": "results",
    "explanation": "Verify system is ready for max parallel training.\n\nReturns dict with status of each component.",
    "python_code": "def verify_max_parallel_ready() -> dict:\n    \"\"\"\n    Verify system is ready for max parallel training.\n\n    Returns dict with status of each component.\n    \"\"\"\n    import torch\n    import psutil\n\n    results = {}\n\n    # GPU check\n    if torch.cuda.is_available():\n        props = torch.cuda.get_device_properties(0)\n        vram_gb = props.total_memory / 1024**3\n        results['gpu'] = {\n            'available': True,\n            'name': torch.cuda.get_device_name(0),\n            'vram_gb': vram_gb,\n            'sufficient': vram_gb >= 14,  # Need 14+ GB for max parallel\n        }\n    else:\n        results['gpu'] = {'available': False, 'sufficient': False}\n\n    # CPU check\n    results['cpu'] = {\n        'physical_cores': psutil.cpu_count(logical=False),\n        'logical_cores': psutil.cpu_count(logical=True),\n        'sufficient': psutil.cpu_count(logical=False) >= 8,\n    }\n\n    # RAM check\n    mem = psutil.virtual_memory()\n    results['ram'] = {\n        'total_gb': mem.total / 1024**3,\n        'available_gb': mem.available / 1024**3,\n        'sufficient': mem.total / 1024**3 >= 24,  # Need 24+ GB\n    }\n\n    # Overall\n    results['ready'] = all([\n        results['gpu'].get('sufficient', False),\n        results['cpu'].get('sufficient', False),\n        results['ram'].get('sufficient', False),\n    ])\n\n    return results",
    "source_file": "core\\ml\\gpu_config.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "print_system_info",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Print current system configuration.",
    "python_code": "def print_system_info():\n    \"\"\"Print current system configuration.\"\"\"\n    import torch\n    import psutil\n\n    print(\"=\" * 60)\n    print(\"SYSTEM CONFIGURATION\")\n    print(\"=\" * 60)\n\n    # CPU\n    print(f\"\\n--- CPU ---\")\n    print(f\"Physical cores: {psutil.cpu_count(logical=False)}\")\n    print(f\"Logical cores: {psutil.cpu_count(logical=True)}\")\n    print(f\"Current frequency: {psutil.cpu_freq().current:.0f} MHz\")\n\n    # Memory\n    mem = psutil.virtual_memory()\n    print(f\"\\n--- Memory ---\")\n    print(f\"Total: {mem.total / 1024**3:.1f} GB\")\n    print(f\"Available: {mem.available / 1024**3:.1f} GB\")\n    print(f\"Used: {mem.percent}%\")\n\n    # GPU\n    if torch.cuda.is_available():\n        print(f\"\\n--- GPU ---\")\n        print(f\"Device: {torch.cuda.get_device_name(0)}\")\n        props = torch.cuda.get_device_properties(0)\n        print(f\"VRAM: {props.total_memory / 1024**3:.1f} GB\")\n        print(f\"Compute capability: {props.major}.{props.minor}\")\n        print(f\"SM count: {props.multi_processor_count}\")\n\n        # Current memory usage\n        allocated = torch.cuda.memory_allocated(0) / 1024**3\n        reserved = torch.cuda.memory_reserved(0) / 1024**3\n        print(f\"Memory allocated: {allocated:.2f} GB\")\n        print(f\"Memory reserved: {reserved:.2f} GB\")\n\n    print(\"=\" * 60)",
    "source_file": "core\\ml\\gpu_config.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Args:\n    symbols: List of symbols to retrain\n    retrain_interval: Seconds between retrains\n    min_samples: Minimum samples required to retrain",
    "python_code": "def __init__(self, symbols: List[str], retrain_interval: int = 60,\n                 min_samples: int = 1000):\n        \"\"\"\n        Args:\n            symbols: List of symbols to retrain\n            retrain_interval: Seconds between retrains\n            min_samples: Minimum samples required to retrain\n        \"\"\"\n        self.symbols = symbols\n        self.retrain_interval = retrain_interval\n        self.min_samples = min_samples\n\n        # Import here to avoid circular imports\n        from core.data.buffer import get_tick_buffer\n        from core.ml.ensemble import get_ensemble\n        from core.ml.gpu_config import (get_xgb_gpu_params, get_lgb_gpu_params,\n                                        get_catboost_gpu_params, configure_gpu)\n\n        self.get_tick_buffer = get_tick_buffer\n        self.get_ensemble = get_ensemble\n        self.get_xgb_params = get_xgb_gpu_params\n        self.get_lgb_params = get_lgb_gpu_params\n        self.get_cb_params = get_catboost_gpu_params\n\n        # Configure GPU once\n        configure_gpu()\n\n        # Threading\n        self._thread: Optional[threading.Thread] = None\n        self._stop_event = threading.Event()\n        self._running = False\n\n        # Stats\n        self.retrain_count = 0\n        self.last_retrain_time = None\n        self.last_gpu_util = 0",
    "source_file": "core\\ml\\live_retrainer.py",
    "academic_reference": null,
    "class_name": "LiveRetrainer"
  },
  {
    "name": "start",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Start background retraining thread.",
    "python_code": "def start(self):\n        \"\"\"Start background retraining thread.\"\"\"\n        if self._running:\n            return\n\n        self._stop_event.clear()\n        self._thread = threading.Thread(target=self._retrain_loop, daemon=True)\n        self._thread.start()\n        self._running = True\n        logger.info(\"[RETRAIN] Background retraining started\")",
    "source_file": "core\\ml\\live_retrainer.py",
    "academic_reference": null,
    "class_name": "LiveRetrainer"
  },
  {
    "name": "stop",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Stop background retraining.",
    "python_code": "def stop(self):\n        \"\"\"Stop background retraining.\"\"\"\n        self._stop_event.set()\n        if self._thread:\n            self._thread.join(timeout=5)\n        self._running = False\n        logger.info(\"[RETRAIN] Background retraining stopped\")",
    "source_file": "core\\ml\\live_retrainer.py",
    "academic_reference": null,
    "class_name": "LiveRetrainer"
  },
  {
    "name": "_retrain_loop",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Main retraining loop.",
    "python_code": "def _retrain_loop(self):\n        \"\"\"Main retraining loop.\"\"\"\n        while not self._stop_event.is_set():\n            try:\n                for symbol in self.symbols:\n                    if self._stop_event.is_set():\n                        break\n                    self._retrain_symbol(symbol)\n\n                self.retrain_count += 1\n                self.last_retrain_time = time.time()\n\n            except Exception as e:\n                logger.error(f\"[RETRAIN] Error: {e}\")\n\n            # Wait for next interval\n            self._stop_event.wait(timeout=self.retrain_interval)",
    "source_file": "core\\ml\\live_retrainer.py",
    "academic_reference": null,
    "class_name": "LiveRetrainer"
  },
  {
    "name": "_retrain_symbol",
    "category": "machine_learning",
    "formula": "# Split data | # Evaluate ensemble",
    "explanation": "Retrain models for a single symbol.",
    "python_code": "def _retrain_symbol(self, symbol: str):\n        \"\"\"Retrain models for a single symbol.\"\"\"\n        tick_buffer = self.get_tick_buffer()\n        ensemble = self.get_ensemble(symbol)\n\n        # Get training data\n        X, y, feature_names = tick_buffer.get_training_data(\n            n_samples=5000, symbol=symbol\n        )\n\n        if X is None or len(X) < self.min_samples:\n            logger.debug(f\"[RETRAIN] {symbol}: Not enough data ({len(X) if X is not None else 0} samples)\")\n            return\n\n        # Split data\n        X_train, X_val, y_train, y_val = train_test_split(\n            X, y, test_size=0.2, shuffle=False  # Keep time order\n        )\n\n        logger.info(f\"[RETRAIN] {symbol}: Training on {len(X_train)} samples...\")\n\n        # Train models on GPU\n        import xgboost as xgb\n        import lightgbm as lgb\n        import catboost as cb\n\n        models = {}\n\n        # XGBoost\n        try:\n            xgb_params = self.get_xgb_params()\n            xgb_params.update({'objective': 'binary:logistic', 'eval_metric': 'auc'})\n\n            dtrain = xgb.DMatrix(X_train, label=y_train)\n            dval = xgb.DMatrix(X_val, label=y_val)\n\n            models['xgboost'] = xgb.train(\n                xgb_params, dtrain, num_boost_round=200,\n                evals=[(dval, 'val')], early_stopping_rounds=20,\n                verbose_eval=False\n            )\n        except Exception as e:\n            logger.error(f\"[RETRAIN] XGBoost error: {e}\")\n\n        # LightGBM\n        try:\n            lgb_params = self.get_lgb_params()\n            lgb_params.update({'objective': 'binary', 'metric': 'auc'})\n\n            train_data = lgb.Dataset(X_train, label=y_train)\n            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n\n            models['lightgbm'] = lgb.train(\n                lgb_params, train_data, num_boost_round=200,\n                valid_sets=[val_data], callbacks=[lgb.early_stopping(20, verbose=False)]\n            )\n        except Exception as e:\n            logger.error(f\"[RETRAIN] LightGBM error: {e}\")\n\n        # CatBoost\n        try:\n            cb_params = self.get_cb_params()\n\n            model = cb.CatBoostClassifier(\n                **cb_params,\n                iterations=200,\n                loss_function='Logloss',\n                eval_metric='AUC',\n                early_stopping_rounds=20,\n                verbose=False\n            )\n            model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)\n            models['catboost'] = model\n        except Exception as e:\n            logger.error(f\"[RETRAIN] CatBoost error: {e}\")\n\n        if not models:\n            logger.error(f\"[RETRAIN] {symbol}: All models failed\")\n            return\n\n        # Evaluate ensemble\n        predictions = []\n        if 'xgboost' in models:\n            predictions.append(models['xgboost'].predict(xgb.DMatrix(X_val)))\n        if 'lightgbm' in models:\n            predictions.append(models['lightgbm'].predict(X_val))\n        if 'catboost'",
    "source_file": "core\\ml\\live_retrainer.py",
    "academic_reference": null,
    "class_name": "LiveRetrainer"
  },
  {
    "name": "get_stats",
    "category": "machine_learning",
    "formula": "{",
    "explanation": "Get retrainer statistics.",
    "python_code": "def get_stats(self) -> Dict:\n        \"\"\"Get retrainer statistics.\"\"\"\n        return {\n            'running': self._running,\n            'retrain_count': self.retrain_count,\n            'last_retrain': self.last_retrain_time,\n            'interval': self.retrain_interval,\n            'symbols': self.symbols\n        }",
    "source_file": "core\\ml\\live_retrainer.py",
    "academic_reference": null,
    "class_name": "LiveRetrainer"
  },
  {
    "name": "get_retrainer",
    "category": "machine_learning",
    "formula": "_retrainer",
    "explanation": "Get or create the global retrainer.",
    "python_code": "def get_retrainer(symbols: List[str] = None) -> LiveRetrainer:\n    \"\"\"Get or create the global retrainer.\"\"\"\n    global _retrainer\n    with _retrainer_lock:\n        if _retrainer is None:\n            if symbols is None:\n                symbols = ['EURUSD', 'GBPUSD', 'USDJPY']\n            _retrainer = LiveRetrainer(symbols=symbols)\n        return _retrainer",
    "source_file": "core\\ml\\live_retrainer.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "machine_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, max_size: int = 100):\n        self.cache = OrderedDict()\n        self.max_size = max_size\n        self.lock = threading.Lock()",
    "source_file": "core\\ml\\llm_reasoner.py",
    "academic_reference": "arXiv:2409.06289",
    "class_name": "LRUCache"
  },
  {
    "name": "get",
    "category": "machine_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def get(self, key: str) -> Optional[str]:\n        with self.lock:\n            if key in self.cache:\n                self.cache.move_to_end(key)\n                return self.cache[key]\n            return None",
    "source_file": "core\\ml\\llm_reasoner.py",
    "academic_reference": "arXiv:2409.06289",
    "class_name": "LRUCache"
  },
  {
    "name": "set",
    "category": "machine_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def set(self, key: str, value: str) -> None:\n        with self.lock:\n            if key in self.cache:\n                self.cache.move_to_end(key)\n            else:\n                if len(self.cache) >= self.max_size:\n                    self.cache.popitem(last=False)\n                self.cache[key] = value",
    "source_file": "core\\ml\\llm_reasoner.py",
    "academic_reference": "arXiv:2409.06289",
    "class_name": "LRUCache"
  },
  {
    "name": "_parse_trader_response",
    "category": "machine_learning",
    "formula": "TradingDecision(",
    "explanation": "Parse the trader agent's response into a TradingDecision.",
    "python_code": "def _parse_trader_response(self, response: str, context: MarketContext,\n                               bull_case: str, bear_case: str,\n                               risk_assessment: str, latency: float) -> TradingDecision:\n        \"\"\"Parse the trader agent's response into a TradingDecision.\"\"\"\n        import re\n\n        # Default values\n        action = \"HOLD\"\n        size = 0.0\n        reasoning = response\n\n        # Parse ACTION\n        action_match = re.search(r'ACTION:\\s*(BUY|SELL|HOLD)', response, re.IGNORECASE)\n        if action_match:\n            action = action_match.group(1).upper()\n\n        # Parse SIZE\n        size_match = re.search(r'SIZE:\\s*(\\d+(?:\\.\\d+)?)\\s*%', response, re.IGNORECASE)\n        if size_match:\n            size = float(size_match.group(1))\n\n        # Parse REASONING\n        reasoning_match = re.search(r'REASONING:\\s*(.+?)(?:\\n|$)', response, re.IGNORECASE | re.DOTALL)\n        if reasoning_match:\n            reasoning = reasoning_match.group(1).strip()\n\n        # Check if LLM wants to override ML prediction\n        override = False\n        if action == \"HOLD\" and context.confidence > 0.7:\n            override = True  # LLM is being more conservative than ML\n        elif action != \"HOLD\" and context.confidence < 0.55:\n            override = True  # LLM is more aggressive than ML\n\n        return TradingDecision(\n            action=action,\n            confidence=context.confidence,\n            position_size_pct=min(size, 50.0),\n            reasoning=reasoning,\n            bull_argument=bull_case[:500],  # Truncate for storage\n            bear_argument=bear_case[:500],\n            risk_assessment=risk_assessment[:500],\n            latency_ms=latency,\n            should_override_ml=override\n        )",
    "source_file": "core\\ml\\llm_reasoner.py",
    "academic_reference": "arXiv:2409.06289",
    "class_name": "MultiAgentTradingReasoner"
  },
  {
    "name": "get_statistics",
    "category": "machine_learning",
    "formula": "{\"total_decisions\": 0} | {",
    "explanation": "Get statistics about LLM decisions.",
    "python_code": "def get_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get statistics about LLM decisions.\"\"\"\n        if not self.decision_history:\n            return {\"total_decisions\": 0}\n\n        actions = [d.action for d in self.decision_history]\n        latencies = [d.latency_ms for d in self.decision_history]\n        overrides = sum(1 for d in self.decision_history if d.should_override_ml)\n\n        return {\n            \"total_decisions\": len(self.decision_history),\n            \"actions\": {\n                \"BUY\": actions.count(\"BUY\"),\n                \"SELL\": actions.count(\"SELL\"),\n                \"HOLD\": actions.count(\"HOLD\")\n            },\n            \"avg_latency_ms\": sum(latencies) / len(latencies),\n            \"max_latency_ms\": max(latencies),\n            \"min_latency_ms\": min(latencies),\n            \"ml_overrides\": overrides,\n            \"override_rate\": overrides / len(self.decision_history)\n        }",
    "source_file": "core\\ml\\llm_reasoner.py",
    "academic_reference": "arXiv:2409.06289",
    "class_name": "MultiAgentTradingReasoner"
  },
  {
    "name": "set_mode",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Change the integration mode.",
    "python_code": "def set_mode(self, mode: Mode) -> None:\n        \"\"\"Change the integration mode.\"\"\"\n        self.mode = mode\n        logger.info(f\"LLM Integration mode changed to: {mode.value}\")",
    "source_file": "core\\ml\\llm_reasoner.py",
    "academic_reference": "arXiv:2409.06289",
    "class_name": "TradingBotLLMIntegration"
  },
  {
    "name": "disable",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Disable LLM integration.",
    "python_code": "def disable(self) -> None:\n        \"\"\"Disable LLM integration.\"\"\"\n        self.enabled = False",
    "source_file": "core\\ml\\llm_reasoner.py",
    "academic_reference": "arXiv:2409.06289",
    "class_name": "TradingBotLLMIntegration"
  },
  {
    "name": "enable",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Enable LLM integration.",
    "python_code": "def enable(self) -> None:\n        \"\"\"Enable LLM integration.\"\"\"\n        self.enabled = True",
    "source_file": "core\\ml\\llm_reasoner.py",
    "academic_reference": "arXiv:2409.06289",
    "class_name": "TradingBotLLMIntegration"
  },
  {
    "name": "create_llm_integration",
    "category": "machine_learning",
    "formula": "TradingBotLLMIntegration(mode=mode_map.get(mode, TradingBotLLMIntegration.Mode.VALIDATION))",
    "explanation": "Create LLM integration with specified mode.",
    "python_code": "def create_llm_integration(mode: str = \"validation\") -> TradingBotLLMIntegration:\n    \"\"\"Create LLM integration with specified mode.\"\"\"\n    mode_map = {\n        \"advisory\": TradingBotLLMIntegration.Mode.ADVISORY,\n        \"validation\": TradingBotLLMIntegration.Mode.VALIDATION,\n        \"autonomous\": TradingBotLLMIntegration.Mode.AUTONOMOUS\n    }\n    return TradingBotLLMIntegration(mode=mode_map.get(mode, TradingBotLLMIntegration.Mode.VALIDATION))",
    "source_file": "core\\ml\\llm_reasoner.py",
    "academic_reference": "arXiv:2409.06289",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "deep_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(\n        self,\n        input_dim: int,\n        conv_layers: List[int] = [64, 128, 256],\n        kernel_size: int = 3,\n        dropout: float = 0.3,\n        fc_layers: List[int] = [256, 128],\n    ):\n        super().__init__()\n\n        self.input_dim = input_dim\n\n        # Reshape input to 1D sequence\n        # Treat each feature as a timestep with 1 channel\n        layers = []\n        in_channels = 1\n\n        for out_channels in conv_layers:\n            layers.extend([\n                nn.Conv1d(in_channels, out_channels, kernel_size, padding=kernel_size // 2),\n                nn.BatchNorm1d(out_channels),\n                nn.ReLU(),\n                nn.Dropout(dropout),\n            ])\n            in_channels = out_channels\n\n        self.conv_layers = nn.Sequential(*layers)\n\n        # Global average pooling\n        self.gap = nn.AdaptiveAvgPool1d(1)\n\n        # Fully connected layers\n        fc = []\n        fc_input = conv_layers[-1]\n        for fc_dim in fc_layers:\n            fc.extend([\n                nn.Linear(fc_input, fc_dim),\n                nn.BatchNorm1d(fc_dim),\n                nn.ReLU(),\n                nn.Dropout(dropout),\n            ])\n            fc_input = fc_dim\n\n        fc.append(nn.Linear(fc_input, 1))\n        self.fc_layers = nn.Sequential(*fc)",
    "source_file": "core\\ml\\neural_models.py",
    "academic_reference": null,
    "class_name": "CNN1D"
  },
  {
    "name": "forward",
    "category": "deep_learning",
    "formula": "x",
    "explanation": "",
    "python_code": "def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # x: (batch, features)\n        # Reshape to (batch, 1, features)\n        x = x.unsqueeze(1)\n\n        # Conv layers\n        x = self.conv_layers(x)  # (batch, channels, features)\n\n        # Global average pooling\n        x = self.gap(x).squeeze(-1)  # (batch, channels)\n\n        # FC layers\n        x = self.fc_layers(x)  # (batch, 1)\n\n        return x",
    "source_file": "core\\ml\\neural_models.py",
    "academic_reference": null,
    "class_name": "CNN1D"
  },
  {
    "name": "fit",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Fit the CNN model.",
    "python_code": "def fit(\n        self,\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_val: Optional[np.ndarray] = None,\n        y_val: Optional[np.ndarray] = None\n    ) -> 'CNN1DClassifier':\n        \"\"\"Fit the CNN model.\"\"\"\n        # Create model\n        self.model = CNN1D(\n            self.input_dim,\n            self.conv_layers,\n            self.kernel_size,\n            self.dropout,\n            self.fc_layers,\n        ).to(self.device)\n\n        # Convert to tensors\n        X_train_t = torch.FloatTensor(X_train).to(self.device)\n        y_train_t = torch.FloatTensor(y_train).unsqueeze(1).to(self.device)\n\n        train_dataset = TensorDataset(X_train_t, y_train_t)\n        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n\n        if X_val is not None and y_val is not None:\n            X_val_t = torch.FloatTensor(X_val).to(self.device)\n            y_val_t = torch.FloatTensor(y_val).unsqueeze(1).to(self.device)\n\n        # Optimizer and loss\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n        criterion = nn.BCEWithLogitsLoss()\n\n        # Training loop\n        best_val_loss = float('inf')\n        patience_counter = 0\n        patience = 10\n\n        for epoch in range(self.max_epochs):\n            self.model.train()\n            train_loss = 0.0\n\n            for batch_x, batch_y in train_loader:\n                optimizer.zero_grad()\n                outputs = self.model(batch_x)\n                loss = criterion(outputs, batch_y)\n                loss.backward()\n                optimizer.step()\n                train_loss += loss.item()\n\n            train_loss /= len(train_loader)\n\n            # Validation\n            if X_val is not None:\n                self.model.eval()\n                with torch.no_grad():\n                    val_outputs = self.model(X_val_t)\n                    val_loss = criterion(val_outputs, y_val_t).item()\n\n                if val_loss < best_val_loss:\n                    best_val_loss = val_loss\n                    patience_counter = 0\n                else:\n                    patience_counter += 1\n                    if patience_counter >= patience:\n                        logger.info(f\"CNN early stopping at epoch {epoch}\")\n                        break\n\n            if (epoch + 1) % 10 == 0:\n                logger.debug(f\"CNN Epoch {epoch+1}: train_loss={train_loss:.4f}\")\n\n        self.is_fitted = True\n        return self",
    "source_file": "core\\ml\\neural_models.py",
    "academic_reference": null,
    "class_name": "CNN1DClassifier"
  },
  {
    "name": "predict_proba",
    "category": "machine_learning",
    "formula": "proba",
    "explanation": "Predict probabilities.",
    "python_code": "def predict_proba(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Predict probabilities.\"\"\"\n        if not self.is_fitted:\n            raise RuntimeError(\"Model not fitted. Call fit() first.\")\n\n        self.model.eval()\n        with torch.no_grad():\n            X_t = torch.FloatTensor(X).to(self.device)\n            outputs = self.model(X_t)\n            proba = torch.sigmoid(outputs).cpu().numpy().squeeze()\n\n        return proba",
    "source_file": "core\\ml\\neural_models.py",
    "academic_reference": null,
    "class_name": "CNN1DClassifier"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "(proba >= threshold).astype(int)",
    "explanation": "Predict class labels.",
    "python_code": "def predict(self, X: np.ndarray, threshold: float = 0.5) -> np.ndarray:\n        \"\"\"Predict class labels.\"\"\"\n        proba = self.predict_proba(X)\n        return (proba >= threshold).astype(int)",
    "source_file": "core\\ml\\neural_models.py",
    "academic_reference": null,
    "class_name": "CNN1DClassifier"
  },
  {
    "name": "save",
    "category": "deep_learning",
    "formula": "",
    "explanation": "Save model to disk.",
    "python_code": "def save(self, path: Path):\n        \"\"\"Save model to disk.\"\"\"\n        path = Path(path)\n        torch.save({\n            'model_state': self.model.state_dict(),\n            'config': {\n                'input_dim': self.input_dim,\n                'conv_layers': self.conv_layers,\n                'kernel_size': self.kernel_size,\n                'dropout': self.dropout,\n                'fc_layers': self.fc_layers,\n            }\n        }, path)",
    "source_file": "core\\ml\\neural_models.py",
    "academic_reference": null,
    "class_name": "CNN1DClassifier"
  },
  {
    "name": "load",
    "category": "deep_learning",
    "formula": "classifier",
    "explanation": "Load model from disk.",
    "python_code": "def load(cls, path: Path, device: Optional[str] = None) -> 'CNN1DClassifier':\n        \"\"\"Load model from disk.\"\"\"\n        path = Path(path)\n        device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n\n        checkpoint = torch.load(path, map_location=device)\n        config = checkpoint['config']\n\n        classifier = cls(\n            input_dim=config['input_dim'],\n            conv_layers=config['conv_layers'],\n            kernel_size=config['kernel_size'],\n            dropout=config['dropout'],\n            fc_layers=config['fc_layers'],\n            device=device,\n        )\n\n        classifier.model = CNN1D(\n            config['input_dim'],\n            config['conv_layers'],\n            config['kernel_size'],\n            config['dropout'],\n            config['fc_layers'],\n        ).to(device)\n        classifier.model.load_state_dict(checkpoint['model_state'])\n        classifier.is_fitted = True\n\n        return classifier",
    "source_file": "core\\ml\\neural_models.py",
    "academic_reference": null,
    "class_name": "CNN1DClassifier"
  },
  {
    "name": "forward",
    "category": "deep_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.layers(x)",
    "source_file": "core\\ml\\neural_models.py",
    "academic_reference": null,
    "class_name": "MLP"
  },
  {
    "name": "create_neural_classifier",
    "category": "reinforcement_learning",
    "formula": "CNN1DClassifier( | MLPClassifier( | EnsembleNeuralClassifier(",
    "explanation": "Factory function to create neural classifiers.\n\nArgs:\n    model_type: 'cnn', 'mlp', or 'ensemble'\n    n_features: Number of input features\n    seq_len: Sequence length for CNN\n    use_gpu: Whether to use GPU\n    **kwargs: Additional model-specific arguments\n\nReturns:\n    Neural classifier instance",
    "python_code": "def create_neural_classifier(\n    model_type: str = 'ensemble',\n    n_features: int = 400,\n    seq_len: int = 20,\n    use_gpu: bool = True,\n    **kwargs\n):\n    \"\"\"\n    Factory function to create neural classifiers.\n\n    Args:\n        model_type: 'cnn', 'mlp', or 'ensemble'\n        n_features: Number of input features\n        seq_len: Sequence length for CNN\n        use_gpu: Whether to use GPU\n        **kwargs: Additional model-specific arguments\n\n    Returns:\n        Neural classifier instance\n    \"\"\"\n    device = 'cuda' if use_gpu and torch.cuda.is_available() else 'cpu'\n\n    if model_type == 'cnn':\n        return CNN1DClassifier(\n            n_features=n_features,\n            seq_len=seq_len,\n            device=device,\n            **kwargs\n        )\n    elif model_type == 'mlp':\n        return MLPClassifier(\n            n_features=n_features,\n            device=device,\n            **kwargs\n        )\n    else:  # ensemble\n        return EnsembleNeuralClassifier(\n            n_features=n_features,\n            seq_len=seq_len,\n            device=device,\n            **kwargs\n        )",
    "source_file": "core\\ml\\neural_models.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "generate_features_for_pair",
    "category": "machine_learning",
    "formula": "(pair, str(features_path), None) | (pair, str(features_path), None) | (pair, None, error_msg)",
    "explanation": "CPU worker: Generate mega features for one pair.\n\nThis function runs in a separate process to parallelize CPU-bound feature generation.\n\nArgs:\n    args: Tuple of (pair, training_dir, output_dir, max_samples)\n\nReturns:\n    Tuple of (pair, features_path, error) - error is None on success",
    "python_code": "def generate_features_for_pair(args: Tuple) -> Tuple[str, Optional[str], Optional[str]]:\n    \"\"\"\n    CPU worker: Generate mega features for one pair.\n\n    This function runs in a separate process to parallelize CPU-bound feature generation.\n\n    Args:\n        args: Tuple of (pair, training_dir, output_dir, max_samples)\n\n    Returns:\n        Tuple of (pair, features_path, error) - error is None on success\n    \"\"\"\n    pair, training_dir, features_cache_dir, max_samples = args\n\n    try:\n        import pandas as pd\n        import numpy as np\n        import gc\n        from pathlib import Path\n        import sys\n\n        # Add project root to path (required for subprocess)\n        project_root = Path(training_dir).parent\n        if str(project_root) not in sys.path:\n            sys.path.insert(0, str(project_root))\n\n        pair_dir = Path(training_dir) / pair\n        features_path = Path(features_cache_dir) / f\"{pair}_features.pkl\"\n\n        # Skip if already cached\n        if features_path.exists():\n            print(f\"[CPU] {pair}: Using cached features\")\n            return (pair, str(features_path), None)\n\n        print(f\"[CPU] {pair}: Loading data...\")\n\n        # Load data\n        train = pd.read_parquet(pair_dir / 'train.parquet')\n        val = pd.read_parquet(pair_dir / 'val.parquet')\n        test = pd.read_parquet(pair_dir / 'test.parquet')\n\n        # Ensure close column\n        for df in [train, val, test]:\n            if 'close' not in df.columns and 'mid' in df.columns:\n                df['close'] = df['mid']\n\n        print(f\"[CPU] {pair}: Generating mega features...\")\n\n        # Import here to avoid subprocess issues\n        from core.features.mega_generator import MegaFeatureGenerator\n\n        generator = MegaFeatureGenerator(verbose=False)\n\n        train_features = generator.generate_all(train)\n        val_features = generator.generate_all(val)\n        test_features = generator.generate_all(test)\n\n        # Filter to numeric only\n        numeric_cols = train_features.select_dtypes(include=[np.number]).columns.tolist()\n        train_features = train_features[numeric_cols]\n        val_features = val_features[numeric_cols]\n        test_features = test_features[numeric_cols]\n\n        feature_names = numeric_cols\n        print(f\"[CPU] {pair}: Generated {len(feature_names)} features\")\n\n        # Get target columns\n        target_cols = [c for c in train.columns if c.startswith('target_direction')]\n\n        # Subsample if needed\n        X_train = train_features.values\n        X_val = val_features.values\n        X_test = test_features.values\n\n        y_train = {t: train[t].values for t in target_cols}\n        y_val = {t: val[t].values for t in target_cols}\n        y_test = {t: test[t].values for t in target_cols}\n\n        # Subsample training data if too large\n        if len(X_train) > max_samples:\n            idx = np.random.choice(len(X_train), max_samples, replace=False)\n            X_train = X_train[idx]\n            y_train = {t: y_train[t][id",
    "source_file": "core\\ml\\parallel_trainer.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Initialize ParallelTrainer.\n\nArgs:\n    cpu_workers: Number of CPU workers for feature generation (default: 4)\n    gpu_workers: Number of concurrent GPU training jobs (default: 3)\n    max_samples: Max training samples per pair (default: 50000)\n    training_dir: Path to training_package directory\n    output_dir: Path to models/production directory\n    features_cache_dir: Path to cache generated features (default: training_package/features_cache)",
    "python_code": "def __init__(\n        self,\n        cpu_workers: int = 4,\n        gpu_workers: int = 3,\n        max_samples: int = 50000,\n        training_dir: Optional[Path] = None,\n        output_dir: Optional[Path] = None,\n        features_cache_dir: Optional[Path] = None,\n    ):\n        \"\"\"\n        Initialize ParallelTrainer.\n\n        Args:\n            cpu_workers: Number of CPU workers for feature generation (default: 4)\n            gpu_workers: Number of concurrent GPU training jobs (default: 3)\n            max_samples: Max training samples per pair (default: 50000)\n            training_dir: Path to training_package directory\n            output_dir: Path to models/production directory\n            features_cache_dir: Path to cache generated features (default: training_package/features_cache)\n        \"\"\"\n        self.cpu_workers = cpu_workers\n        self.gpu_workers = gpu_workers\n        self.max_samples = max_samples\n\n        # Paths\n        project_dir = Path(__file__).parent.parent.parent\n        self.training_dir = training_dir or project_dir / 'training_package'\n        self.output_dir = output_dir or project_dir / 'models' / 'production'\n        self.features_cache_dir = features_cache_dir or self.training_dir / 'features_cache'\n\n        # Create directories\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n        self.features_cache_dir.mkdir(parents=True, exist_ok=True)\n\n        # Training queue and results\n        self.feature_queue = queue.Queue()\n        self.results = {}\n        self.lock = threading.Lock()\n\n        # GPU lock - XGBoost and CatBoost can't handle concurrent GPU access safely\n        # LightGBM seems more robust with concurrent access\n        self.gpu_lock = threading.Lock()\n\n        # Progress tracking\n        self.pairs_total = 0\n        self.pairs_features_done = 0\n        self.pairs_trained = 0\n        self.start_time = None",
    "source_file": "core\\ml\\parallel_trainer.py",
    "academic_reference": null,
    "class_name": "ParallelTrainer"
  },
  {
    "name": "_train_gpu_worker",
    "category": "machine_learning",
    "formula": "",
    "explanation": "GPU worker: Train XGB+LGB+CB for one pair.\n\nThis function runs in a thread pool to allow concurrent GPU training.\nThe GIL is released during GPU operations, so 3 jobs can run concurrently.\n\nArgs:\n    pair: Currency pair symbol\n    features_path: Path to cached features pickle\n\nReturns:\n    Tuple of (pair, results_dict) - results_dict is None on failure",
    "python_code": "def _train_gpu_worker(self, pair: str, features_path: str) -> Tuple[str, Optional[Dict]]:\n        \"\"\"\n        GPU worker: Train XGB+LGB+CB for one pair.\n\n        This function runs in a thread pool to allow concurrent GPU training.\n        The GIL is released during GPU operations, so 3 jobs can run concurrently.\n\n        Args:\n            pair: Currency pair symbol\n            features_path: Path to cached features pickle\n\n        Returns:\n            Tuple of (pair, results_dict) - results_dict is None on failure\n        \"\"\"\n        try:\n            import xgboost as xgb\n            import lightgbm as lgb\n            import catboost as cb\n            from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n\n            print(f\"[GPU] {pair}: Loading features...\")\n\n            # Load cached features\n            with open(features_path, 'rb') as f:\n                data = pickle.load(f)\n\n            X_train = data['X_train']\n            X_val = data['X_val']\n            X_test = data['X_test']\n            y_train = data['y_train']\n            y_val = data['y_val']\n            y_test = data['y_test']\n            feature_names = data['feature_names']\n            target_cols = data['target_cols']\n\n            print(f\"[GPU] {pair}: Training on {len(X_train)} samples, {len(feature_names)} features\")\n\n            results = {}\n            models = {}\n\n            # GPU params optimized for concurrent training (reduced depth to save VRAM)\n            xgb_params = {\n                'tree_method': 'hist',\n                'device': 'cuda',\n                'max_depth': 8,\n                'max_bin': 128,\n                'learning_rate': 0.05,\n                'subsample': 0.7,\n                'colsample_bytree': 0.6,\n                'objective': 'binary:logistic',\n                'eval_metric': 'auc',\n                'nthread': 4,  # Reduce CPU threads per job\n            }\n\n            lgb_params = {\n                'device': 'gpu',\n                'gpu_platform_id': 0,\n                'gpu_device_id': 0,\n                'max_depth': 8,\n                'num_leaves': 127,\n                'feature_fraction': 0.6,\n                'bagging_fraction': 0.7,\n                'bagging_freq': 5,\n                'verbose': -1,\n                'objective': 'binary',\n                'metric': 'auc',\n                'n_jobs': 4,  # Reduce CPU threads per job\n            }\n\n            cb_params = {\n                'task_type': 'GPU',\n                'devices': '0',\n                'depth': 6,\n                'iterations': 300,\n                'early_stopping_rounds': 20,\n                'verbose': False,\n                'loss_function': 'Logloss',\n                'thread_count': 4,  # Reduce CPU threads per job\n            }\n\n            # GPU lock - serialize all GPU training to prevent memory conflicts\n            # Feature generation is still parallel, but GPU training is sequential\n            # This is safer and prevents OOM errors on 16GB VRAM\n            wi",
    "source_file": "core\\ml\\parallel_trainer.py",
    "academic_reference": null,
    "class_name": "ParallelTrainer"
  },
  {
    "name": "_get_already_trained",
    "category": "machine_learning",
    "formula": "trained",
    "explanation": "Get set of pairs that already have results.",
    "python_code": "def _get_already_trained(self) -> set:\n        \"\"\"Get set of pairs that already have results.\"\"\"\n        trained = set()\n        for f in self.output_dir.glob('*_results.json'):\n            pair = f.stem.replace('_results', '')\n            trained.add(pair)\n        return trained",
    "source_file": "core\\ml\\parallel_trainer.py",
    "academic_reference": null,
    "class_name": "ParallelTrainer"
  },
  {
    "name": "_get_pairs_with_data",
    "category": "machine_learning",
    "formula": "sorted(pairs)",
    "explanation": "Get list of pairs that have training data.",
    "python_code": "def _get_pairs_with_data(self) -> List[str]:\n        \"\"\"Get list of pairs that have training data.\"\"\"\n        pairs = []\n        for d in self.training_dir.iterdir():\n            if d.is_dir() and (d / 'train.parquet').exists():\n                if d.name != 'features_cache':\n                    pairs.append(d.name)\n        return sorted(pairs)",
    "source_file": "core\\ml\\parallel_trainer.py",
    "academic_reference": null,
    "class_name": "ParallelTrainer"
  },
  {
    "name": "train_all",
    "category": "machine_learning",
    "formula": "None = all with data) | {} | {}",
    "explanation": "Train all pairs with pipeline parallelism.\n\nArgs:\n    pairs: List of pairs to train (None = all with data)\n    skip_trained: Skip pairs that already have results\n    monitor: Print progress updates\n    dry_run: Only print what would be done, don't train\n\nReturns:\n    Dict mapping pair -> results",
    "python_code": "def train_all(\n        self,\n        pairs: Optional[List[str]] = None,\n        skip_trained: bool = True,\n        monitor: bool = True,\n        dry_run: bool = False,\n    ) -> Dict[str, Dict]:\n        \"\"\"\n        Train all pairs with pipeline parallelism.\n\n        Args:\n            pairs: List of pairs to train (None = all with data)\n            skip_trained: Skip pairs that already have results\n            monitor: Print progress updates\n            dry_run: Only print what would be done, don't train\n\n        Returns:\n            Dict mapping pair -> results\n        \"\"\"\n        self.start_time = time.time()\n\n        # Get pairs to process\n        if pairs is None:\n            pairs = self._get_pairs_with_data()\n\n        if skip_trained:\n            already_trained = self._get_already_trained()\n            pairs = [p for p in pairs if p not in already_trained]\n\n        self.pairs_total = len(pairs)\n\n        if self.pairs_total == 0:\n            print(\"No pairs to train!\")\n            return {}\n\n        print(f\"\\n{'='*60}\")\n        print(f\"PARALLEL TRAINING: {self.pairs_total} pairs\")\n        print(f\"CPU workers: {self.cpu_workers}\")\n        print(f\"GPU workers: {self.gpu_workers}\")\n        print(f\"Max samples: {self.max_samples}\")\n        print(f\"{'='*60}\\n\")\n\n        if dry_run:\n            print(\"DRY RUN - would train:\")\n            for i, p in enumerate(pairs):\n                print(f\"  [{i+1}] {p}\")\n            return {}\n\n        # Stage 1: Feature generation (CPU, parallel processes)\n        print(f\"\\n{'='*40}\")\n        print(\"STAGE 1: Feature Generation (CPU)\")\n        print(f\"{'='*40}\")\n\n        feature_args = [\n            (p, str(self.training_dir), str(self.features_cache_dir), self.max_samples)\n            for p in pairs\n        ]\n\n        feature_results = []\n        with ProcessPoolExecutor(max_workers=self.cpu_workers) as cpu_pool:\n            futures = {cpu_pool.submit(generate_features_for_pair, args): args[0] for args in feature_args}\n\n            for future in as_completed(futures):\n                pair = futures[future]\n                try:\n                    result = future.result()\n                    feature_results.append(result)\n                    self.pairs_features_done += 1\n\n                    if monitor:\n                        elapsed = time.time() - self.start_time\n                        rate = self.pairs_features_done / (elapsed / 60)\n                        print(f\"[Progress] Features: {self.pairs_features_done}/{self.pairs_total} \"\n                              f\"({rate:.1f} pairs/min)\")\n\n                except Exception as e:\n                    print(f\"[ERROR] Feature generation failed for {pair}: {e}\")\n                    feature_results.append((pair, None, str(e)))\n\n        # Filter successful feature generations\n        ready_for_training = [(p, fp) for p, fp, err in feature_results if fp is not None]\n        failed = [(p, err) for p, fp, err in feature_results if fp is None]\n\n        if failed:\n        ",
    "source_file": "core\\ml\\parallel_trainer.py",
    "academic_reference": null,
    "class_name": "ParallelTrainer"
  },
  {
    "name": "clear_feature_cache",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Clear the feature cache to free disk space.",
    "python_code": "def clear_feature_cache(self):\n        \"\"\"Clear the feature cache to free disk space.\"\"\"\n        import shutil\n        if self.features_cache_dir.exists():\n            shutil.rmtree(self.features_cache_dir)\n            self.features_cache_dir.mkdir()\n            print(f\"Cleared feature cache: {self.features_cache_dir}\")",
    "source_file": "core\\ml\\parallel_trainer.py",
    "academic_reference": null,
    "class_name": "ParallelTrainer"
  },
  {
    "name": "__init__",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Args:\n    symbols: Currency pairs to train (optional if tier is provided)\n    tier: Symbol tier from registry (majors, crosses, exotics, all)\n    historical_dir: Path to historical parquet files\n    retrain_interval: Seconds between retrains\n    live_weight: Weight multiplier for live samples",
    "python_code": "def __init__(self, symbols: List[str] = None,\n                 tier: str = None,\n                 historical_dir: Path = None,\n                 retrain_interval: int = 120,  # 2 min (more data per cycle)\n                 live_weight: float = 3.0):    # Live data weighted 3x\n        \"\"\"\n        Args:\n            symbols: Currency pairs to train (optional if tier is provided)\n            tier: Symbol tier from registry (majors, crosses, exotics, all)\n            historical_dir: Path to historical parquet files\n            retrain_interval: Seconds between retrains\n            live_weight: Weight multiplier for live samples\n        \"\"\"\n        # Get symbols from registry if not provided\n        if symbols is None:\n            from core.symbol.registry import SymbolRegistry\n            registry = SymbolRegistry.get()\n            if tier:\n                pairs = registry.get_enabled(tier=tier if tier != 'all' else None)\n            else:\n                pairs = registry.get_enabled(tier='majors')  # Default to majors\n            symbols = [p.symbol for p in pairs]\n\n        self.symbols = symbols\n        self.historical_dir = historical_dir or Path(\"training_package\")\n        self.retrain_interval = retrain_interval\n        self.live_weight = live_weight\n\n        # Load historical data once\n        self.historical_data: Dict[str, pd.DataFrame] = {}\n        self._load_historical_data()\n\n        # Import components\n        from core.data.buffer import get_tick_buffer\n        from core.ml.ensemble import get_ensemble\n        from core.ml.gpu_config import (get_xgb_gpu_params, get_lgb_gpu_params,\n                                        get_catboost_gpu_params, configure_gpu)\n\n        self.get_tick_buffer = get_tick_buffer\n        self.get_ensemble = get_ensemble\n        self.get_xgb_params = get_xgb_gpu_params\n        self.get_lgb_params = get_lgb_gpu_params\n        self.get_cb_params = get_catboost_gpu_params\n\n        # Configure GPU\n        configure_gpu()\n\n        # Threading\n        self._thread: Optional[threading.Thread] = None\n        self._stop_event = threading.Event()\n        self._running = False\n\n        # Stats\n        self.retrain_count = 0\n        self.best_accuracy: Dict[str, float] = {}",
    "source_file": "core\\ml\\retrainer.py",
    "academic_reference": null,
    "class_name": "HybridRetrainer"
  },
  {
    "name": "_load_historical_data",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Load historical training data for each symbol.",
    "python_code": "def _load_historical_data(self):\n        \"\"\"Load historical training data for each symbol.\"\"\"\n        for symbol in self.symbols:\n            symbol_dir = self.historical_dir / symbol\n            if not symbol_dir.exists():\n                logger.warning(f\"No historical data for {symbol}\")\n                continue\n\n            try:\n                train = pd.read_parquet(symbol_dir / \"train.parquet\")\n                val = pd.read_parquet(symbol_dir / \"val.parquet\")\n\n                # Combine train + val for more data\n                combined = pd.concat([train, val], ignore_index=True)\n                self.historical_data[symbol] = combined\n\n                logger.info(f\"[HYBRID] Loaded {len(combined):,} historical samples for {symbol}\")\n            except Exception as e:\n                logger.error(f\"Failed to load historical data for {symbol}: {e}\")",
    "source_file": "core\\ml\\retrainer.py",
    "academic_reference": null,
    "class_name": "HybridRetrainer"
  },
  {
    "name": "_get_feature_columns",
    "category": "machine_learning",
    "formula": "[c for c in df.columns",
    "explanation": "Get feature columns (non-target, non-timestamp columns).",
    "python_code": "def _get_feature_columns(self, df: pd.DataFrame) -> List[str]:\n        \"\"\"Get feature columns (non-target, non-timestamp columns).\"\"\"\n        exclude_patterns = ['target', 'timestamp', 'time', 'date', 'datetime']\n        return [c for c in df.columns\n                if not any(p in c.lower() for p in exclude_patterns)\n                and df[c].dtype in ['float64', 'float32', 'int64', 'int32', 'bool']]",
    "source_file": "core\\ml\\retrainer.py",
    "academic_reference": null,
    "class_name": "HybridRetrainer"
  },
  {
    "name": "_get_combined_data",
    "category": "technical",
    "formula": "X_combined, y_combined, common_features | X_hist, y_hist, feature_cols",
    "explanation": "Combine historical + live data with weighting.\n\nReturns:\n    X: Feature matrix\n    y: Labels\n    feature_names: Column names",
    "python_code": "def _get_combined_data(self, symbol: str) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n        \"\"\"\n        Combine historical + live data with weighting.\n\n        Returns:\n            X: Feature matrix\n            y: Labels\n            feature_names: Column names\n        \"\"\"\n        # Get historical data\n        if symbol not in self.historical_data:\n            return None, None, None\n\n        hist_df = self.historical_data[symbol]\n        feature_cols = self._get_feature_columns(hist_df)\n\n        # Find target column (prefer direction_10 or direction_1)\n        target_col = None\n        for t in ['target_direction_10', 'target_direction_5', 'target_direction_1']:\n            if t in hist_df.columns:\n                target_col = t\n                break\n\n        if target_col is None:\n            logger.error(f\"No target column found for {symbol}\")\n            return None, None, None\n\n        # Get live data from buffer\n        tick_buffer = self.get_tick_buffer()\n        X_live, y_live, live_features = tick_buffer.get_training_data(\n            n_samples=10000, symbol=symbol\n        )\n\n        # Start with historical\n        X_hist = hist_df[feature_cols].values\n        y_hist = hist_df[target_col].values\n\n        # Sample historical to keep training fast (use 50k random samples)\n        n_hist_samples = min(50000, len(X_hist))\n        hist_idx = np.random.choice(len(X_hist), n_hist_samples, replace=False)\n        X_hist = X_hist[hist_idx]\n        y_hist = y_hist[hist_idx]\n\n        # If we have live data, combine with weighting\n        if X_live is not None and len(X_live) > 100:\n            # Match features between historical and live\n            common_features = [f for f in feature_cols if f in live_features]\n\n            if len(common_features) > 50:  # Need enough common features\n                # Re-extract with common features only\n                hist_feat_idx = [feature_cols.index(f) for f in common_features]\n                live_feat_idx = [live_features.index(f) for f in common_features]\n\n                X_hist_common = X_hist[:, hist_feat_idx]\n                X_live_common = X_live[:, live_feat_idx]\n\n                # Duplicate live data to increase its weight\n                n_live_copies = int(self.live_weight)\n                X_live_weighted = np.tile(X_live_common, (n_live_copies, 1))\n                y_live_weighted = np.tile(y_live, n_live_copies)\n\n                # Combine\n                X_combined = np.vstack([X_hist_common, X_live_weighted])\n                y_combined = np.hstack([y_hist, y_live_weighted])\n\n                logger.info(f\"[HYBRID] {symbol}: {n_hist_samples:,} hist + {len(X_live)*n_live_copies:,} live (weighted) = {len(X_combined):,} total\")\n\n                return X_combined, y_combined, common_features\n\n        # Fallback to historical only\n        logger.info(f\"[HYBRID] {symbol}: Using {n_hist_samples:,} historical samples only\")\n        return X_hist, y_hist, feature_cols",
    "source_file": "core\\ml\\retrainer.py",
    "academic_reference": null,
    "class_name": "HybridRetrainer"
  },
  {
    "name": "start",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Start background hybrid retraining.",
    "python_code": "def start(self):\n        \"\"\"Start background hybrid retraining.\"\"\"\n        if self._running:\n            return\n\n        self._stop_event.clear()\n        self._thread = threading.Thread(target=self._retrain_loop, daemon=True)\n        self._thread.start()\n        self._running = True\n        logger.info(\"[HYBRID] Background hybrid retraining started (historical + live)\")",
    "source_file": "core\\ml\\retrainer.py",
    "academic_reference": null,
    "class_name": "HybridRetrainer"
  },
  {
    "name": "stop",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Stop hybrid retraining.",
    "python_code": "def stop(self):\n        \"\"\"Stop hybrid retraining.\"\"\"\n        self._stop_event.set()\n        if self._thread:\n            self._thread.join(timeout=5)\n        self._running = False\n        logger.info(\"[HYBRID] Stopped\")",
    "source_file": "core\\ml\\retrainer.py",
    "academic_reference": null,
    "class_name": "HybridRetrainer"
  },
  {
    "name": "_retrain_loop",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Main retraining loop.",
    "python_code": "def _retrain_loop(self):\n        \"\"\"Main retraining loop.\"\"\"\n        # Initial delay to let live data accumulate\n        self._stop_event.wait(timeout=30)\n\n        while not self._stop_event.is_set():\n            try:\n                for symbol in self.symbols:\n                    if self._stop_event.is_set():\n                        break\n                    self._retrain_symbol(symbol)\n\n                self.retrain_count += 1\n\n            except Exception as e:\n                logger.error(f\"[HYBRID] Error: {e}\")\n\n            self._stop_event.wait(timeout=self.retrain_interval)",
    "source_file": "core\\ml\\retrainer.py",
    "academic_reference": null,
    "class_name": "HybridRetrainer"
  },
  {
    "name": "_retrain_symbol",
    "category": "machine_learning",
    "formula": "# Handle NaN/Inf | # Evaluate ensemble",
    "explanation": "Retrain models for a symbol using hybrid data.",
    "python_code": "def _retrain_symbol(self, symbol: str):\n        \"\"\"Retrain models for a symbol using hybrid data.\"\"\"\n        ensemble = self.get_ensemble(symbol)\n\n        # Get combined historical + live data\n        X, y, feature_names = self._get_combined_data(symbol)\n\n        if X is None or len(X) < 1000:\n            return\n\n        # Handle NaN/Inf\n        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n\n        # Split data (keep time order for recent data)\n        X_train, X_val, y_train, y_val = train_test_split(\n            X, y, test_size=0.2, shuffle=True  # Shuffle since we mixed data\n        )\n\n        logger.info(f\"[HYBRID] {symbol}: Training on {len(X_train):,} samples...\")\n\n        # Train models on GPU\n        import xgboost as xgb\n        import lightgbm as lgb\n        import catboost as cb\n\n        models = {}\n\n        # XGBoost with more trees for larger dataset\n        try:\n            xgb_params = self.get_xgb_params()\n            xgb_params.update({\n                'objective': 'binary:logistic',\n                'eval_metric': 'auc',\n                'max_depth': 14,  # Deeper for more data\n            })\n\n            dtrain = xgb.DMatrix(X_train, label=y_train)\n            dval = xgb.DMatrix(X_val, label=y_val)\n\n            models['xgboost'] = xgb.train(\n                xgb_params, dtrain,\n                num_boost_round=500,  # More trees\n                evals=[(dval, 'val')],\n                early_stopping_rounds=30,\n                verbose_eval=False\n            )\n        except Exception as e:\n            logger.error(f\"[HYBRID] XGBoost error: {e}\")\n\n        # LightGBM\n        try:\n            lgb_params = self.get_lgb_params()\n            lgb_params.update({\n                'objective': 'binary',\n                'metric': 'auc',\n                'max_depth': 14,\n                'num_leaves': 1023,  # More leaves for more data\n            })\n\n            train_data = lgb.Dataset(X_train, label=y_train)\n            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n\n            models['lightgbm'] = lgb.train(\n                lgb_params, train_data,\n                num_boost_round=500,\n                valid_sets=[val_data],\n                callbacks=[lgb.early_stopping(30, verbose=False)]\n            )\n        except Exception as e:\n            logger.error(f\"[HYBRID] LightGBM error: {e}\")\n\n        # CatBoost\n        try:\n            cb_params = self.get_cb_params()\n            # Override for deeper training\n            cb_params['depth'] = 12\n            cb_params['iterations'] = 500\n            # Remove params that conflict with explicit kwargs\n            cb_params.pop('early_stopping_rounds', None)\n            cb_params.pop('verbose', None)\n\n            model = cb.CatBoostClassifier(\n                **cb_params,\n                loss_function='Logloss',\n                eval_metric='AUC',\n                verbose=False\n            )\n            model.fit(X_train, y_train, eval_set=(X_val, y_val),\n                     ",
    "source_file": "core\\ml\\retrainer.py",
    "academic_reference": null,
    "class_name": "HybridRetrainer"
  },
  {
    "name": "get_hybrid_retrainer",
    "category": "machine_learning",
    "formula": "_hybrid_retrainer",
    "explanation": "Get or create hybrid retrainer.\n\nArgs:\n    symbols: Specific symbols to train (optional)\n    tier: Symbol tier from registry (majors, crosses, exotics, all)\n\nReturns:\n    HybridRetrainer instance",
    "python_code": "def get_hybrid_retrainer(symbols: List[str] = None, tier: str = None) -> HybridRetrainer:\n    \"\"\"\n    Get or create hybrid retrainer.\n\n    Args:\n        symbols: Specific symbols to train (optional)\n        tier: Symbol tier from registry (majors, crosses, exotics, all)\n\n    Returns:\n        HybridRetrainer instance\n    \"\"\"\n    global _hybrid_retrainer\n    with _lock:\n        if _hybrid_retrainer is None:\n            _hybrid_retrainer = HybridRetrainer(symbols=symbols, tier=tier)\n        return _hybrid_retrainer",
    "source_file": "core\\ml\\retrainer.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Initialize stacking ensemble.",
    "python_code": "def __init__(self, config: Optional[StackingConfig] = None):\n        \"\"\"Initialize stacking ensemble.\"\"\"\n        self.config = config or StackingConfig()\n\n        # Base learners\n        self.xgb_model = None\n        self.lgb_model = None\n        self.cb_model = None\n        self.tabnet_model = None\n        self.cnn_model = None\n        self.mlp_model = None\n\n        # Meta-learner\n        self.meta_learner = None\n\n        # Feature selection for meta-learner\n        self.top_feature_indices = None\n        self.feature_names = None\n\n        # Training state\n        self.is_fitted = False",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": "StackingEnsemble"
  },
  {
    "name": "fit",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Fit the stacking ensemble.\n\nArgs:\n    X_train: Training features\n    y_train: Training labels\n    X_val: Validation features\n    y_val: Validation labels\n    feature_names: Optional feature names\n\nReturns:\n    self",
    "python_code": "def fit(\n        self,\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_val: np.ndarray,\n        y_val: np.ndarray,\n        feature_names: Optional[List[str]] = None\n    ) -> 'StackingEnsemble':\n        \"\"\"\n        Fit the stacking ensemble.\n\n        Args:\n            X_train: Training features\n            y_train: Training labels\n            X_val: Validation features\n            y_val: Validation labels\n            feature_names: Optional feature names\n\n        Returns:\n            self\n        \"\"\"\n        logger.info(\"Fitting Stacking Ensemble...\")\n        logger.info(f\"Training shape: {X_train.shape}, Validation shape: {X_val.shape}\")\n\n        self.feature_names = feature_names or [f\"f_{i}\" for i in range(X_train.shape[1])]\n\n        # Handle NaN/inf\n        X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)\n        X_val = np.nan_to_num(X_val, nan=0.0, posinf=0.0, neginf=0.0)\n\n        # ===== Level 1: Base Learners =====\n        logger.info(\"Level 1: Training Base Learners...\")\n\n        if self.config.parallel_base_learners:\n            self._fit_base_learners_parallel(X_train, y_train, X_val, y_val)\n        else:\n            self._fit_base_learners_sequential(X_train, y_train, X_val, y_val)\n\n        # Generate out-of-fold predictions for meta-learner training\n        logger.info(\"Generating OOF predictions for meta-learner...\")\n        oof_predictions = self._generate_oof_predictions(X_train, y_train)\n        val_predictions = self._generate_val_predictions(X_val)\n\n        # ===== Level 2: Meta-Learner =====\n        logger.info(\"Level 2: Training Meta-Learner...\")\n\n        # Select top features based on XGBoost importance\n        self._select_top_features(X_train, y_train)\n\n        # Build meta-features\n        X_meta_train = self._build_meta_features(X_train, oof_predictions)\n        X_meta_val = self._build_meta_features(X_val, val_predictions)\n\n        # Train meta-learner\n        self._fit_meta_learner(X_meta_train, y_train, X_meta_val, y_val)\n\n        self.is_fitted = True\n        logger.info(\"Stacking Ensemble training complete!\")\n\n        return self",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": "StackingEnsemble"
  },
  {
    "name": "_fit_base_learners_sequential",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Train base learners sequentially.",
    "python_code": "def _fit_base_learners_sequential(\n        self,\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_val: np.ndarray,\n        y_val: np.ndarray\n    ):\n        \"\"\"Train base learners sequentially.\"\"\"\n        # 1. XGBoost\n        logger.info(\"  [1/6] Training XGBoost...\")\n        self._fit_xgboost(X_train, y_train, X_val, y_val)\n        gc.collect()\n\n        # 2. LightGBM\n        logger.info(\"  [2/6] Training LightGBM...\")\n        self._fit_lightgbm(X_train, y_train, X_val, y_val)\n        gc.collect()\n\n        # 3. CatBoost\n        logger.info(\"  [3/6] Training CatBoost...\")\n        self._fit_catboost(X_train, y_train, X_val, y_val)\n        gc.collect()\n\n        # 4. TabNet (optional)\n        if self.config.enable_tabnet:\n            logger.info(\"  [4/6] Training TabNet...\")\n            self._fit_tabnet(X_train, y_train, X_val, y_val)\n            gc.collect()\n        else:\n            logger.info(\"  [4/6] TabNet disabled\")\n\n        # 5. CNN (optional)\n        if self.config.enable_cnn:\n            logger.info(\"  [5/6] Training 1D-CNN...\")\n            self._fit_cnn(X_train, y_train, X_val, y_val)\n            gc.collect()\n        else:\n            logger.info(\"  [5/6] CNN disabled\")\n\n        # 6. MLP (optional)\n        if self.config.enable_mlp:\n            logger.info(\"  [6/6] Training MLP...\")\n            self._fit_mlp(X_train, y_train, X_val, y_val)\n            gc.collect()\n        else:\n            logger.info(\"  [6/6] MLP disabled\")",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": "StackingEnsemble"
  },
  {
    "name": "_fit_base_learners_parallel",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Train base learners in parallel (experimental).",
    "python_code": "def _fit_base_learners_parallel(\n        self,\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_val: np.ndarray,\n        y_val: np.ndarray\n    ):\n        \"\"\"Train base learners in parallel (experimental).\"\"\"\n        # Note: GPU models can't truly parallelize on single GPU\n        # This is more useful for CPU models or multi-GPU setups\n        with ThreadPoolExecutor(max_workers=3) as executor:\n            futures = [\n                executor.submit(self._fit_xgboost, X_train, y_train, X_val, y_val),\n                executor.submit(self._fit_lightgbm, X_train, y_train, X_val, y_val),\n                executor.submit(self._fit_catboost, X_train, y_train, X_val, y_val),\n            ]\n            for f in futures:\n                f.result()\n\n        # Sequential for neural models (GPU memory)\n        if self.config.enable_tabnet:\n            self._fit_tabnet(X_train, y_train, X_val, y_val)\n        if self.config.enable_cnn:\n            self._fit_cnn(X_train, y_train, X_val, y_val)\n        if self.config.enable_mlp:\n            self._fit_mlp(X_train, y_train, X_val, y_val)",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": "StackingEnsemble"
  },
  {
    "name": "_fit_xgboost",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Fit XGBoost model.",
    "python_code": "def _fit_xgboost(\n        self,\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_val: np.ndarray,\n        y_val: np.ndarray\n    ):\n        \"\"\"Fit XGBoost model.\"\"\"\n        import xgboost as xgb\n\n        params = self.config.xgb_params.copy()\n        n_estimators = params.pop('n_estimators', 1500)\n        early_stopping = params.pop('early_stopping_rounds', 50)\n\n        params['objective'] = 'binary:logistic'\n        params['eval_metric'] = 'auc'\n\n        dtrain = xgb.DMatrix(X_train, label=y_train)\n        dval = xgb.DMatrix(X_val, label=y_val)\n\n        self.xgb_model = xgb.train(\n            params,\n            dtrain,\n            num_boost_round=n_estimators,\n            evals=[(dval, 'val')],\n            early_stopping_rounds=early_stopping,\n            verbose_eval=False\n        )\n\n        # Log validation score\n        val_pred = self.xgb_model.predict(dval)\n        auc = self._compute_auc(y_val, val_pred)\n        logger.info(f\"    XGBoost validation AUC: {auc:.4f}\")",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": "StackingEnsemble"
  },
  {
    "name": "_fit_lightgbm",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Fit LightGBM model.",
    "python_code": "def _fit_lightgbm(\n        self,\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_val: np.ndarray,\n        y_val: np.ndarray\n    ):\n        \"\"\"Fit LightGBM model.\"\"\"\n        import lightgbm as lgb\n\n        params = self.config.lgb_params.copy()\n        n_estimators = params.pop('n_estimators', 1500)\n\n        params['objective'] = 'binary'\n        params['metric'] = 'auc'\n\n        train_data = lgb.Dataset(X_train, label=y_train)\n        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n\n        self.lgb_model = lgb.train(\n            params,\n            train_data,\n            num_boost_round=n_estimators,\n            valid_sets=[val_data],\n            callbacks=[lgb.early_stopping(50, verbose=False)]\n        )\n\n        # Log validation score\n        val_pred = self.lgb_model.predict(X_val)\n        auc = self._compute_auc(y_val, val_pred)\n        logger.info(f\"    LightGBM validation AUC: {auc:.4f}\")",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": "StackingEnsemble"
  },
  {
    "name": "_fit_catboost",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Fit CatBoost model.",
    "python_code": "def _fit_catboost(\n        self,\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_val: np.ndarray,\n        y_val: np.ndarray\n    ):\n        \"\"\"Fit CatBoost model.\"\"\"\n        import catboost as cb\n\n        params = self.config.cb_params.copy()\n\n        self.cb_model = cb.CatBoostClassifier(**params)\n        self.cb_model.fit(\n            X_train, y_train,\n            eval_set=(X_val, y_val),\n            use_best_model=True\n        )\n\n        # Log validation score\n        val_pred = self.cb_model.predict_proba(X_val)[:, 1]\n        auc = self._compute_auc(y_val, val_pred)\n        logger.info(f\"    CatBoost validation AUC: {auc:.4f}\")",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": "StackingEnsemble"
  },
  {
    "name": "_fit_tabnet",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Fit TabNet model.",
    "python_code": "def _fit_tabnet(\n        self,\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_val: np.ndarray,\n        y_val: np.ndarray\n    ):\n        \"\"\"Fit TabNet model.\"\"\"\n        try:\n            from pytorch_tabnet.tab_model import TabNetClassifier\n\n            params = self.config.tabnet_params.copy()\n            max_epochs = params.pop('max_epochs', 100)\n            patience = params.pop('patience', 10)\n            batch_size = params.pop('batch_size', 1024)\n            virtual_batch_size = params.pop('virtual_batch_size', 128)\n\n            self.tabnet_model = TabNetClassifier(**params, verbose=0)\n\n            self.tabnet_model.fit(\n                X_train, y_train,\n                eval_set=[(X_val, y_val)],\n                eval_metric=['auc'],\n                max_epochs=max_epochs,\n                patience=patience,\n                batch_size=batch_size,\n                virtual_batch_size=virtual_batch_size,\n            )\n\n            # Log validation score\n            val_pred = self.tabnet_model.predict_proba(X_val)[:, 1]\n            auc = self._compute_auc(y_val, val_pred)\n            logger.info(f\"    TabNet validation AUC: {auc:.4f}\")\n\n        except ImportError:\n            logger.warning(\"TabNet not installed. Skipping. Install with: pip install pytorch-tabnet\")\n            self.tabnet_model = None",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": "StackingEnsemble"
  },
  {
    "name": "_fit_cnn",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Fit 1D-CNN model.",
    "python_code": "def _fit_cnn(\n        self,\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_val: np.ndarray,\n        y_val: np.ndarray\n    ):\n        \"\"\"Fit 1D-CNN model.\"\"\"\n        try:\n            from .neural_models import CNN1DClassifier\n\n            params = self.config.cnn_params.copy()\n\n            self.cnn_model = CNN1DClassifier(\n                input_dim=X_train.shape[1],\n                **params\n            )\n\n            self.cnn_model.fit(X_train, y_train, X_val, y_val)\n\n            # Log validation score\n            val_pred = self.cnn_model.predict_proba(X_val)\n            auc = self._compute_auc(y_val, val_pred)\n            logger.info(f\"    CNN validation AUC: {auc:.4f}\")\n\n        except ImportError as e:\n            logger.warning(f\"CNN model failed to import: {e}\")\n            self.cnn_model = None",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": "StackingEnsemble"
  },
  {
    "name": "_fit_mlp",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Fit MLP model.",
    "python_code": "def _fit_mlp(\n        self,\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_val: np.ndarray,\n        y_val: np.ndarray\n    ):\n        \"\"\"Fit MLP model.\"\"\"\n        try:\n            from .neural_models import MLPClassifier\n\n            params = self.config.mlp_params.copy()\n\n            self.mlp_model = MLPClassifier(\n                input_dim=X_train.shape[1],\n                **params\n            )\n\n            self.mlp_model.fit(X_train, y_train, X_val, y_val)\n\n            # Log validation score\n            val_pred = self.mlp_model.predict_proba(X_val)\n            auc = self._compute_auc(y_val, val_pred)\n            logger.info(f\"    MLP validation AUC: {auc:.4f}\")\n\n        except ImportError as e:\n            logger.warning(f\"MLP model failed to import: {e}\")\n            self.mlp_model = None",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": "StackingEnsemble"
  },
  {
    "name": "_select_top_features",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Select top features for meta-learner.",
    "python_code": "def _select_top_features(self, X: np.ndarray, y: np.ndarray):\n        \"\"\"Select top features for meta-learner.\"\"\"\n        # Use XGBoost feature importance\n        if self.xgb_model is not None:\n            import xgboost as xgb\n\n            importance = self.xgb_model.get_score(importance_type='gain')\n\n            # Map feature names to indices\n            feature_importance = []\n            for i in range(X.shape[1]):\n                feat_name = f'f{i}'\n                imp = importance.get(feat_name, 0.0)\n                feature_importance.append((i, imp))\n\n            # Sort by importance and take top K\n            feature_importance.sort(key=lambda x: x[1], reverse=True)\n            self.top_feature_indices = [\n                idx for idx, _ in feature_importance[:self.config.top_features_for_meta]\n            ]\n        else:\n            # Fallback: use first K features\n            self.top_feature_indices = list(range(min(self.config.top_features_for_meta, X.shape[1])))",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": "StackingEnsemble"
  },
  {
    "name": "_generate_oof_predictions",
    "category": "machine_learning",
    "formula": "oof_preds",
    "explanation": "Generate out-of-fold predictions for meta-learner training.",
    "python_code": "def _generate_oof_predictions(self, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n        \"\"\"Generate out-of-fold predictions for meta-learner training.\"\"\"\n        from sklearn.model_selection import KFold\n        import xgboost as xgb\n\n        n_samples = len(X)\n        n_models = self._count_active_models()\n        oof_preds = np.zeros((n_samples, n_models))\n\n        kf = KFold(n_splits=self.config.n_folds, shuffle=False)\n\n        for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n            X_tr, y_tr = X[train_idx], y[train_idx]\n            X_va = X[val_idx]\n\n            col_idx = 0\n\n            # XGBoost\n            if self.xgb_model is not None:\n                dval = xgb.DMatrix(X_va)\n                oof_preds[val_idx, col_idx] = self.xgb_model.predict(dval)\n                col_idx += 1\n\n            # LightGBM\n            if self.lgb_model is not None:\n                oof_preds[val_idx, col_idx] = self.lgb_model.predict(X_va)\n                col_idx += 1\n\n            # CatBoost\n            if self.cb_model is not None:\n                oof_preds[val_idx, col_idx] = self.cb_model.predict_proba(X_va)[:, 1]\n                col_idx += 1\n\n            # TabNet\n            if self.tabnet_model is not None:\n                oof_preds[val_idx, col_idx] = self.tabnet_model.predict_proba(X_va)[:, 1]\n                col_idx += 1\n\n            # CNN\n            if self.cnn_model is not None:\n                oof_preds[val_idx, col_idx] = self.cnn_model.predict_proba(X_va)\n                col_idx += 1\n\n            # MLP\n            if self.mlp_model is not None:\n                oof_preds[val_idx, col_idx] = self.mlp_model.predict_proba(X_va)\n                col_idx += 1\n\n        return oof_preds",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": "StackingEnsemble"
  },
  {
    "name": "_generate_val_predictions",
    "category": "machine_learning",
    "formula": "np.column_stack(predictions)",
    "explanation": "Generate predictions from all base learners for validation set.",
    "python_code": "def _generate_val_predictions(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"Generate predictions from all base learners for validation set.\"\"\"\n        import xgboost as xgb\n\n        predictions = []\n\n        # XGBoost\n        if self.xgb_model is not None:\n            dval = xgb.DMatrix(X)\n            predictions.append(self.xgb_model.predict(dval))\n\n        # LightGBM\n        if self.lgb_model is not None:\n            predictions.append(self.lgb_model.predict(X))\n\n        # CatBoost\n        if self.cb_model is not None:\n            predictions.append(self.cb_model.predict_proba(X)[:, 1])\n\n        # TabNet\n        if self.tabnet_model is not None:\n            predictions.append(self.tabnet_model.predict_proba(X)[:, 1])\n\n        # CNN\n        if self.cnn_model is not None:\n            predictions.append(self.cnn_model.predict_proba(X))\n\n        # MLP\n        if self.mlp_model is not None:\n            predictions.append(self.mlp_model.predict_proba(X))\n\n        return np.column_stack(predictions)",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": "StackingEnsemble"
  },
  {
    "name": "_build_meta_features",
    "category": "machine_learning",
    "formula": "np.hstack([stacked_preds, top_features])",
    "explanation": "Build meta-features for level 2.",
    "python_code": "def _build_meta_features(self, X: np.ndarray, stacked_preds: np.ndarray) -> np.ndarray:\n        \"\"\"Build meta-features for level 2.\"\"\"\n        # Combine stacked predictions with top original features\n        top_features = X[:, self.top_feature_indices]\n        return np.hstack([stacked_preds, top_features])",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": "StackingEnsemble"
  },
  {
    "name": "_fit_meta_learner",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Fit the meta-learner.",
    "python_code": "def _fit_meta_learner(\n        self,\n        X_train: np.ndarray,\n        y_train: np.ndarray,\n        X_val: np.ndarray,\n        y_val: np.ndarray\n    ):\n        \"\"\"Fit the meta-learner.\"\"\"\n        import xgboost as xgb\n\n        params = self.config.meta_learner_params.copy()\n        n_estimators = params.pop('n_estimators', 500)\n\n        params['objective'] = 'binary:logistic'\n        params['eval_metric'] = 'auc'\n\n        dtrain = xgb.DMatrix(X_train, label=y_train)\n        dval = xgb.DMatrix(X_val, label=y_val)\n\n        self.meta_learner = xgb.train(\n            params,\n            dtrain,\n            num_boost_round=n_estimators,\n            evals=[(dval, 'val')],\n            early_stopping_rounds=30,\n            verbose_eval=False\n        )\n\n        # Log final score\n        val_pred = self.meta_learner.predict(dval)\n        auc = self._compute_auc(y_val, val_pred)\n        acc = ((val_pred > 0.5) == y_val).mean()\n        logger.info(f\"    Meta-Learner validation AUC: {auc:.4f}, Accuracy: {acc:.4f}\")",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": "StackingEnsemble"
  },
  {
    "name": "_count_active_models",
    "category": "machine_learning",
    "formula": "count",
    "explanation": "Count number of active base learners.",
    "python_code": "def _count_active_models(self) -> int:\n        \"\"\"Count number of active base learners.\"\"\"\n        count = 0\n        if self.xgb_model is not None:\n            count += 1\n        if self.lgb_model is not None:\n            count += 1\n        if self.cb_model is not None:\n            count += 1\n        if self.tabnet_model is not None:\n            count += 1\n        if self.cnn_model is not None:\n            count += 1\n        if self.mlp_model is not None:\n            count += 1\n        return count",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": "StackingEnsemble"
  },
  {
    "name": "_compute_auc",
    "category": "machine_learning",
    "formula": "roc_auc_score(y_true, y_pred) | 0.5",
    "explanation": "Compute AUC score.",
    "python_code": "def _compute_auc(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n        \"\"\"Compute AUC score.\"\"\"\n        from sklearn.metrics import roc_auc_score\n        try:\n            return roc_auc_score(y_true, y_pred)\n        except:\n            return 0.5",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": "StackingEnsemble"
  },
  {
    "name": "predict_proba",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Predict class probabilities.\n\nArgs:\n    X: Features\n\nReturns:\n    Probability of positive class",
    "python_code": "def predict_proba(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Predict class probabilities.\n\n        Args:\n            X: Features\n\n        Returns:\n            Probability of positive class\n        \"\"\"\n        if not self.is_fitted:\n            raise RuntimeError(\"Ensemble not fitted. Call fit() first.\")\n\n        import xgboost as xgb\n\n        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n\n        # Get base learner predictions\n        stacked_preds = self._generate_val_predictions(X)\n\n        # Build meta-features\n        meta_features = self._build_meta_features(X, stacked_preds)\n\n        # Predict with meta-learner\n        dmeta = xgb.DMatrix(meta_features)\n        return self.meta_learner.predict(dmeta)",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": "StackingEnsemble"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "(proba >= threshold).astype(int)",
    "explanation": "Predict class labels.\n\nArgs:\n    X: Features\n    threshold: Classification threshold\n\nReturns:\n    Predicted class labels (0 or 1)",
    "python_code": "def predict(self, X: np.ndarray, threshold: float = 0.5) -> np.ndarray:\n        \"\"\"\n        Predict class labels.\n\n        Args:\n            X: Features\n            threshold: Classification threshold\n\n        Returns:\n            Predicted class labels (0 or 1)\n        \"\"\"\n        proba = self.predict_proba(X)\n        return (proba >= threshold).astype(int)",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": "StackingEnsemble"
  },
  {
    "name": "get_base_learner_predictions",
    "category": "machine_learning",
    "formula": "predictions",
    "explanation": "Get predictions from each base learner.",
    "python_code": "def get_base_learner_predictions(self, X: np.ndarray) -> Dict[str, np.ndarray]:\n        \"\"\"Get predictions from each base learner.\"\"\"\n        import xgboost as xgb\n\n        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n        predictions = {}\n\n        if self.xgb_model is not None:\n            dval = xgb.DMatrix(X)\n            predictions['xgboost'] = self.xgb_model.predict(dval)\n\n        if self.lgb_model is not None:\n            predictions['lightgbm'] = self.lgb_model.predict(X)\n\n        if self.cb_model is not None:\n            predictions['catboost'] = self.cb_model.predict_proba(X)[:, 1]\n\n        if self.tabnet_model is not None:\n            predictions['tabnet'] = self.tabnet_model.predict_proba(X)[:, 1]\n\n        if self.cnn_model is not None:\n            predictions['cnn'] = self.cnn_model.predict_proba(X)\n\n        if self.mlp_model is not None:\n            predictions['mlp'] = self.mlp_model.predict_proba(X)\n\n        return predictions",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": "StackingEnsemble"
  },
  {
    "name": "save",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Save ensemble to disk.",
    "python_code": "def save(self, path: Path):\n        \"\"\"Save ensemble to disk.\"\"\"\n        path = Path(path)\n        path.mkdir(parents=True, exist_ok=True)\n\n        # Save gradient boosting models\n        if self.xgb_model is not None:\n            self.xgb_model.save_model(str(path / 'xgb_model.json'))\n\n        if self.lgb_model is not None:\n            self.lgb_model.save_model(str(path / 'lgb_model.txt'))\n\n        if self.cb_model is not None:\n            self.cb_model.save_model(str(path / 'cb_model.cbm'))\n\n        if self.meta_learner is not None:\n            self.meta_learner.save_model(str(path / 'meta_learner.json'))\n\n        # Save neural models\n        if self.tabnet_model is not None:\n            self.tabnet_model.save_model(str(path / 'tabnet_model'))\n\n        if self.cnn_model is not None:\n            self.cnn_model.save(path / 'cnn_model.pt')\n\n        if self.mlp_model is not None:\n            self.mlp_model.save(path / 'mlp_model.pt')\n\n        # Save state\n        state = {\n            'top_feature_indices': self.top_feature_indices,\n            'feature_names': self.feature_names,\n            'config': self.config,\n            'is_fitted': self.is_fitted,\n        }\n        with open(path / 'state.pkl', 'wb') as f:\n            pickle.dump(state, f)\n\n        logger.info(f\"Saved ensemble to {path}\")",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": "StackingEnsemble"
  },
  {
    "name": "load",
    "category": "machine_learning",
    "formula": "ensemble",
    "explanation": "Load ensemble from disk.",
    "python_code": "def load(cls, path: Path) -> 'StackingEnsemble':\n        \"\"\"Load ensemble from disk.\"\"\"\n        import xgboost as xgb\n        import lightgbm as lgb\n        import catboost as cb\n\n        path = Path(path)\n\n        # Load state\n        with open(path / 'state.pkl', 'rb') as f:\n            state = pickle.load(f)\n\n        ensemble = cls(config=state['config'])\n        ensemble.top_feature_indices = state['top_feature_indices']\n        ensemble.feature_names = state['feature_names']\n        ensemble.is_fitted = state['is_fitted']\n\n        # Load gradient boosting models\n        if (path / 'xgb_model.json').exists():\n            ensemble.xgb_model = xgb.Booster()\n            ensemble.xgb_model.load_model(str(path / 'xgb_model.json'))\n\n        if (path / 'lgb_model.txt').exists():\n            ensemble.lgb_model = lgb.Booster(model_file=str(path / 'lgb_model.txt'))\n\n        if (path / 'cb_model.cbm').exists():\n            ensemble.cb_model = cb.CatBoostClassifier()\n            ensemble.cb_model.load_model(str(path / 'cb_model.cbm'))\n\n        if (path / 'meta_learner.json').exists():\n            ensemble.meta_learner = xgb.Booster()\n            ensemble.meta_learner.load_model(str(path / 'meta_learner.json'))\n\n        # Load neural models\n        try:\n            from pytorch_tabnet.tab_model import TabNetClassifier\n            if (path / 'tabnet_model.zip').exists():\n                ensemble.tabnet_model = TabNetClassifier()\n                ensemble.tabnet_model.load_model(str(path / 'tabnet_model.zip'))\n        except:\n            pass\n\n        try:\n            from .neural_models import CNN1DClassifier, MLPClassifier\n            if (path / 'cnn_model.pt').exists():\n                ensemble.cnn_model = CNN1DClassifier.load(path / 'cnn_model.pt')\n            if (path / 'mlp_model.pt').exists():\n                ensemble.mlp_model = MLPClassifier.load(path / 'mlp_model.pt')\n        except:\n            pass\n\n        return ensemble",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": "StackingEnsemble"
  },
  {
    "name": "create_stacking_ensemble",
    "category": "reinforcement_learning",
    "formula": "StackingEnsemble(config)",
    "explanation": "Factory function to create stacking ensemble.\n\nArgs:\n    config: Stacking config (uses defaults if None)\n    use_gpu: Whether to use GPU acceleration\n    enable_tabnet: Whether to include TabNet\n    enable_neural: Whether to include CNN/MLP\n\nReturns:\n    StackingEnsemble instance",
    "python_code": "def create_stacking_ensemble(\n    config: Optional[StackingConfig] = None,\n    use_gpu: bool = True,\n    enable_tabnet: bool = True,\n    enable_neural: bool = True\n) -> StackingEnsemble:\n    \"\"\"\n    Factory function to create stacking ensemble.\n\n    Args:\n        config: Stacking config (uses defaults if None)\n        use_gpu: Whether to use GPU acceleration\n        enable_tabnet: Whether to include TabNet\n        enable_neural: Whether to include CNN/MLP\n\n    Returns:\n        StackingEnsemble instance\n    \"\"\"\n    if config is None:\n        config = StackingConfig()\n\n    # Disable GPU if requested\n    if not use_gpu:\n        config.xgb_params['device'] = 'cpu'\n        config.xgb_params['tree_method'] = 'hist'\n        config.lgb_params['device'] = 'cpu'\n        config.cb_params['task_type'] = 'CPU'\n\n    # Disable optional models\n    if not enable_tabnet:\n        config.tabnet_params = None\n    if not enable_neural:\n        config.cnn_params = None\n        config.mlp_params = None\n\n    return StackingEnsemble(config)",
    "source_file": "core\\ml\\stacking_ensemble.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "quantitative",
    "formula": "",
    "explanation": "Initialize cache.\n\nArgs:\n    max_models: Maximum models to keep in RAM (default 10 = ~700MB)",
    "python_code": "def __init__(self, max_models: int = 10):\n        \"\"\"\n        Initialize cache.\n\n        Args:\n            max_models: Maximum models to keep in RAM (default 10 = ~700MB)\n        \"\"\"\n        self.max_models = max_models\n        self._cache: OrderedDict[str, Dict[str, Any]] = OrderedDict()\n        self._lock = threading.RLock()\n        self._hits = 0\n        self._misses = 0",
    "source_file": "core\\models\\cache.py",
    "academic_reference": null,
    "class_name": "ModelCache"
  },
  {
    "name": "get",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get a model from cache.\n\nIf found, moves to most recently used position.\n\nArgs:\n    symbol: Trading symbol\n\nReturns:\n    Model data dict or None if not cached",
    "python_code": "def get(self, symbol: str) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Get a model from cache.\n\n        If found, moves to most recently used position.\n\n        Args:\n            symbol: Trading symbol\n\n        Returns:\n            Model data dict or None if not cached\n        \"\"\"\n        with self._lock:\n            if symbol in self._cache:\n                # Move to end (most recently used)\n                self._cache.move_to_end(symbol)\n                self._hits += 1\n                return self._cache[symbol]\n\n            self._misses += 1\n            return None",
    "source_file": "core\\models\\cache.py",
    "academic_reference": null,
    "class_name": "ModelCache"
  },
  {
    "name": "put",
    "category": "quantitative",
    "formula": "# Evict LRU if at capacity",
    "explanation": "Add a model to cache.\n\nEvicts least recently used if at capacity.\n\nArgs:\n    symbol: Trading symbol\n    model: Model data dictionary",
    "python_code": "def put(self, symbol: str, model: Dict[str, Any]):\n        \"\"\"\n        Add a model to cache.\n\n        Evicts least recently used if at capacity.\n\n        Args:\n            symbol: Trading symbol\n            model: Model data dictionary\n        \"\"\"\n        with self._lock:\n            # If already in cache, update and move to end\n            if symbol in self._cache:\n                self._cache[symbol] = model\n                self._cache.move_to_end(symbol)\n                return\n\n            # Evict LRU if at capacity\n            while len(self._cache) >= self.max_models:\n                evicted_symbol, _ = self._cache.popitem(last=False)\n                logger.debug(f\"Evicted {evicted_symbol} from model cache\")\n\n            self._cache[symbol] = model\n            logger.debug(f\"Cached model for {symbol} ({len(self._cache)}/{self.max_models})\")",
    "source_file": "core\\models\\cache.py",
    "academic_reference": null,
    "class_name": "ModelCache"
  },
  {
    "name": "remove",
    "category": "quantitative",
    "formula": "True | False",
    "explanation": "Remove a model from cache.",
    "python_code": "def remove(self, symbol: str) -> bool:\n        \"\"\"Remove a model from cache.\"\"\"\n        with self._lock:\n            if symbol in self._cache:\n                del self._cache[symbol]\n                return True\n            return False",
    "source_file": "core\\models\\cache.py",
    "academic_reference": null,
    "class_name": "ModelCache"
  },
  {
    "name": "clear",
    "category": "quantitative",
    "formula": "",
    "explanation": "Clear all cached models.",
    "python_code": "def clear(self):\n        \"\"\"Clear all cached models.\"\"\"\n        with self._lock:\n            self._cache.clear()\n            logger.info(\"Model cache cleared\")",
    "source_file": "core\\models\\cache.py",
    "academic_reference": null,
    "class_name": "ModelCache"
  },
  {
    "name": "contains",
    "category": "quantitative",
    "formula": "symbol in self._cache",
    "explanation": "Check if symbol is in cache.",
    "python_code": "def contains(self, symbol: str) -> bool:\n        \"\"\"Check if symbol is in cache.\"\"\"\n        with self._lock:\n            return symbol in self._cache",
    "source_file": "core\\models\\cache.py",
    "academic_reference": null,
    "class_name": "ModelCache"
  },
  {
    "name": "size",
    "category": "quantitative",
    "formula": "len(self._cache)",
    "explanation": "Current number of cached models.",
    "python_code": "def size(self) -> int:\n        \"\"\"Current number of cached models.\"\"\"\n        return len(self._cache)",
    "source_file": "core\\models\\cache.py",
    "academic_reference": null,
    "class_name": "ModelCache"
  },
  {
    "name": "hit_rate",
    "category": "quantitative",
    "formula": "0.0",
    "explanation": "Cache hit rate.",
    "python_code": "def hit_rate(self) -> float:\n        \"\"\"Cache hit rate.\"\"\"\n        total = self._hits + self._misses\n        if total == 0:\n            return 0.0\n        return self._hits / total",
    "source_file": "core\\models\\cache.py",
    "academic_reference": null,
    "class_name": "ModelCache"
  },
  {
    "name": "stats",
    "category": "quantitative",
    "formula": "{",
    "explanation": "Get cache statistics.",
    "python_code": "def stats(self) -> Dict[str, Any]:\n        \"\"\"Get cache statistics.\"\"\"\n        return {\n            'size': len(self._cache),\n            'max_size': self.max_models,\n            'hits': self._hits,\n            'misses': self._misses,\n            'hit_rate': self.hit_rate,\n            'cached_symbols': list(self._cache.keys()),\n        }",
    "source_file": "core\\models\\cache.py",
    "academic_reference": null,
    "class_name": "ModelCache"
  },
  {
    "name": "__init__",
    "category": "quantitative",
    "formula": "",
    "explanation": "Initialize model loader.\n\nArgs:\n    model_dir: Directory containing model files\n    max_cached: Maximum models to keep in RAM",
    "python_code": "def __init__(\n        self,\n        model_dir: Path = None,\n        max_cached: int = 10\n    ):\n        \"\"\"\n        Initialize model loader.\n\n        Args:\n            model_dir: Directory containing model files\n            max_cached: Maximum models to keep in RAM\n        \"\"\"\n        self.model_dir = model_dir or Path(\"models/production\")\n        self.cache = ModelCache(max_models=max_cached)\n        self._lock = threading.RLock()\n        self._index: Dict[str, Dict] = {}\n        self._load_index()",
    "source_file": "core\\models\\loader.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "ModelLoader"
  },
  {
    "name": "_load_index",
    "category": "quantitative",
    "formula": "",
    "explanation": "Load model index from disk.",
    "python_code": "def _load_index(self):\n        \"\"\"Load model index from disk.\"\"\"\n        index_path = self.model_dir / \"index.json\"\n        if index_path.exists():\n            try:\n                with open(index_path) as f:\n                    self._index = json.load(f)\n                logger.info(f\"Loaded model index: {len(self._index)} models\")\n            except Exception as e:\n                logger.warning(f\"Failed to load model index: {e}\")",
    "source_file": "core\\models\\loader.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "ModelLoader"
  },
  {
    "name": "_save_index",
    "category": "quantitative",
    "formula": "",
    "explanation": "Save model index to disk.",
    "python_code": "def _save_index(self):\n        \"\"\"Save model index to disk.\"\"\"\n        index_path = self.model_dir / \"index.json\"\n        try:\n            with open(index_path, 'w') as f:\n                json.dump(self._index, f, indent=2)\n        except Exception as e:\n            logger.warning(f\"Failed to save model index: {e}\")",
    "source_file": "core\\models\\loader.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "ModelLoader"
  },
  {
    "name": "load",
    "category": "quantitative",
    "formula": "cached | model_data",
    "explanation": "Load a model for a symbol.\n\nFirst checks cache, then loads from disk if needed.\n\nArgs:\n    symbol: Trading symbol\n\nReturns:\n    Model data dictionary or None if not found",
    "python_code": "def load(self, symbol: str) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Load a model for a symbol.\n\n        First checks cache, then loads from disk if needed.\n\n        Args:\n            symbol: Trading symbol\n\n        Returns:\n            Model data dictionary or None if not found\n        \"\"\"\n        # Check cache first\n        cached = self.cache.get(symbol)\n        if cached is not None:\n            return cached\n\n        # Load from disk\n        with self._lock:\n            model_data = self._load_from_disk(symbol)\n            if model_data is not None:\n                self.cache.put(symbol, model_data)\n                return model_data\n\n        return None",
    "source_file": "core\\models\\loader.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "ModelLoader"
  },
  {
    "name": "_load_from_disk",
    "category": "quantitative",
    "formula": "normalized",
    "explanation": "Load model from disk.",
    "python_code": "def _load_from_disk(self, symbol: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Load model from disk.\"\"\"\n        # Try different file patterns\n        patterns = [\n            f\"{symbol}_models.pkl\",\n            f\"{symbol}_target_direction_10_models.pkl\",\n        ]\n\n        for pattern in patterns:\n            model_path = self.model_dir / pattern\n            if model_path.exists():\n                try:\n                    with open(model_path, 'rb') as f:\n                        data = pickle.load(f)\n\n                    # Normalize format\n                    normalized = self._normalize_model_data(data, symbol)\n                    logger.info(f\"Loaded model for {symbol} from {model_path.name}\")\n                    return normalized\n\n                except Exception as e:\n                    logger.error(f\"Failed to load {symbol} from {model_path}: {e}\")\n\n        logger.debug(f\"No model found for {symbol}\")\n        return None",
    "source_file": "core\\models\\loader.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "ModelLoader"
  },
  {
    "name": "_normalize_model_data",
    "category": "statistical",
    "formula": "{ | data | {'models': {}, 'feature_names': [], 'symbol': symbol}",
    "explanation": "Normalize model data to consistent format.",
    "python_code": "def _normalize_model_data(self, data: Dict, symbol: str) -> Dict[str, Any]:\n        \"\"\"Normalize model data to consistent format.\"\"\"\n        # Handle old format: {'target_direction_1': {'xgboost': ...}}\n        if 'target_direction_1' in data or 'target_direction_10' in data:\n            # Find best target\n            for target in ['target_direction_10', 'target_direction_5', 'target_direction_1']:\n                if target in data:\n                    td = data[target]\n                    return {\n                        'models': {\n                            'xgboost': td.get('xgboost'),\n                            'lightgbm': td.get('lightgbm'),\n                            'catboost': td.get('catboost'),\n                        },\n                        'feature_names': td.get('features', []),\n                        'target': target,\n                        'symbol': symbol,\n                    }\n\n        # Handle new format: {'models': {...}, 'feature_names': [...]}\n        if 'models' in data:\n            data['symbol'] = symbol\n            return data\n\n        # Unknown format\n        logger.warning(f\"Unknown model format for {symbol}\")\n        return {'models': {}, 'feature_names': [], 'symbol': symbol}",
    "source_file": "core\\models\\loader.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "ModelLoader"
  },
  {
    "name": "get_available",
    "category": "quantitative",
    "formula": "[] | sorted(symbols)",
    "explanation": "Get list of symbols with available models.\n\nReturns:\n    List of symbol names",
    "python_code": "def get_available(self) -> List[str]:\n        \"\"\"\n        Get list of symbols with available models.\n\n        Returns:\n            List of symbol names\n        \"\"\"\n        if not self.model_dir.exists():\n            return []\n\n        symbols = set()\n        for f in self.model_dir.glob(\"*_models.pkl\"):\n            # Extract symbol from filename\n            name = f.stem.replace('_models', '')\n            name = name.replace('_target_direction_10', '')\n            name = name.replace('_target_direction_5', '')\n            name = name.replace('_target_direction_1', '')\n            symbols.add(name)\n\n        return sorted(symbols)",
    "source_file": "core\\models\\loader.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "ModelLoader"
  },
  {
    "name": "is_available",
    "category": "quantitative",
    "formula": "symbol in self.get_available()",
    "explanation": "Check if a model exists for a symbol.",
    "python_code": "def is_available(self, symbol: str) -> bool:\n        \"\"\"Check if a model exists for a symbol.\"\"\"\n        return symbol in self.get_available()",
    "source_file": "core\\models\\loader.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "ModelLoader"
  },
  {
    "name": "unload",
    "category": "quantitative",
    "formula": "",
    "explanation": "Remove a model from cache.",
    "python_code": "def unload(self, symbol: str) -> bool:\n        \"\"\"Remove a model from cache.\"\"\"\n        return self.cache.remove(symbol)",
    "source_file": "core\\models\\loader.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "ModelLoader"
  },
  {
    "name": "preload",
    "category": "quantitative",
    "formula": "",
    "explanation": "Preload models for a list of symbols.",
    "python_code": "def preload(self, symbols: List[str]):\n        \"\"\"Preload models for a list of symbols.\"\"\"\n        for symbol in symbols:\n            self.load(symbol)\n        logger.info(f\"Preloaded {len(symbols)} models\")",
    "source_file": "core\\models\\loader.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "ModelLoader"
  },
  {
    "name": "get_model_info",
    "category": "quantitative",
    "formula": "json.load(f)",
    "explanation": "Get model metadata without loading full model.",
    "python_code": "def get_model_info(self, symbol: str) -> Optional[Dict]:\n        \"\"\"Get model metadata without loading full model.\"\"\"\n        # Check index\n        if symbol in self._index:\n            return self._index[symbol]\n\n        # Check results.json\n        results_path = self.model_dir / f\"{symbol}_results.json\"\n        if results_path.exists():\n            try:\n                with open(results_path) as f:\n                    return json.load(f)\n            except Exception:\n                pass\n\n        return None",
    "source_file": "core\\models\\loader.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "ModelLoader"
  },
  {
    "name": "stats",
    "category": "quantitative",
    "formula": "{",
    "explanation": "Get loader statistics.",
    "python_code": "def stats(self) -> Dict:\n        \"\"\"Get loader statistics.\"\"\"\n        return {\n            'model_dir': str(self.model_dir),\n            'available_models': len(self.get_available()),\n            'cache': self.cache.stats(),\n        }",
    "source_file": "core\\models\\loader.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "ModelLoader"
  },
  {
    "name": "from_dict",
    "category": "risk",
    "formula": "cls(",
    "explanation": "Create RiskLimits from a configuration dict.",
    "python_code": "def from_dict(cls, config: Dict[str, Any]) -> 'RiskLimits':\n        \"\"\"Create RiskLimits from a configuration dict.\"\"\"\n        return cls(\n            max_position_pct=config.get('max_position_pct', 0.02),\n            kelly_fraction=config.get('kelly_fraction', 0.25),\n            max_drawdown_pct=config.get('max_drawdown_pct', 0.05),\n            max_loss_per_trade_pct=config.get('max_loss_per_trade_pct', 0.01),\n            daily_trade_limit=config.get('daily_trade_limit', 50),\n            max_open_positions=config.get('max_open_positions', 10),\n            max_correlation=config.get('max_correlation', 0.7),\n            min_confidence=config.get('min_confidence', 0.15),\n            spread_limit_pips=config.get('spread_limit_pips', 3.0),\n        )",
    "source_file": "core\\risk\\limits.py",
    "academic_reference": null,
    "class_name": "RiskLimits"
  },
  {
    "name": "to_dict",
    "category": "deep_learning",
    "formula": "{",
    "explanation": "Convert to dictionary.",
    "python_code": "def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        return {\n            'max_position_pct': self.max_position_pct,\n            'kelly_fraction': self.kelly_fraction,\n            'max_drawdown_pct': self.max_drawdown_pct,\n            'max_loss_per_trade_pct': self.max_loss_per_trade_pct,\n            'daily_trade_limit': self.daily_trade_limit,\n            'max_open_positions': self.max_open_positions,\n            'max_correlation': self.max_correlation,\n            'min_confidence': self.min_confidence,\n            'spread_limit_pips': self.spread_limit_pips,\n        }",
    "source_file": "core\\risk\\limits.py",
    "academic_reference": null,
    "class_name": "RiskLimits"
  },
  {
    "name": "__init__",
    "category": "risk",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, symbol: str, limits: RiskLimits = None):\n        self.symbol = symbol\n        self.limits = limits or RiskLimits()\n        self.metrics = SymbolMetrics()\n        self._position_size: float = 0.0",
    "source_file": "core\\risk\\per_symbol.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "PerSymbolRiskManager"
  },
  {
    "name": "can_trade",
    "category": "risk",
    "formula": "False, f\"{self.symbol}: Daily trade limit ({self.limits.daily_trade_limit})\" | False, f\"{self.symbol}: Max drawdown ({self.metrics.current_drawdown:.1%})\" | False, f\"{self.symbol}: Spread too wide ({spread_pips:.1f} > {self.limits.spread_limit_pips})\"",
    "explanation": "Check if trading is allowed for this symbol.\n\nReturns:\n    (can_trade, reason)",
    "python_code": "def can_trade(self, spread_pips: float = 0.0) -> Tuple[bool, str]:\n        \"\"\"\n        Check if trading is allowed for this symbol.\n\n        Returns:\n            (can_trade, reason)\n        \"\"\"\n        # Reset daily counters if new day\n        self._check_daily_reset()\n\n        # Daily trade limit\n        if self.metrics.daily_trades >= self.limits.daily_trade_limit:\n            return False, f\"{self.symbol}: Daily trade limit ({self.limits.daily_trade_limit})\"\n\n        # Drawdown limit\n        if self.metrics.current_drawdown >= self.limits.max_drawdown_pct:\n            return False, f\"{self.symbol}: Max drawdown ({self.metrics.current_drawdown:.1%})\"\n\n        # Spread check\n        if spread_pips > self.limits.spread_limit_pips:\n            return False, f\"{self.symbol}: Spread too wide ({spread_pips:.1f} > {self.limits.spread_limit_pips})\"\n\n        return True, \"OK\"",
    "source_file": "core\\risk\\per_symbol.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "PerSymbolRiskManager"
  },
  {
    "name": "calculate_position_size",
    "category": "risk",
    "formula": "position",
    "explanation": "Calculate position size using fractional Kelly.\n\nArgs:\n    signal_strength: Signal confidence (-1 to 1)\n    account_balance: Total account balance\n    win_rate: Historical win rate for this symbol\n\nReturns:\n    Position size in base currency units",
    "python_code": "def calculate_position_size(\n        self,\n        signal_strength: float,\n        account_balance: float,\n        win_rate: float = 0.55\n    ) -> float:\n        \"\"\"\n        Calculate position size using fractional Kelly.\n\n        Args:\n            signal_strength: Signal confidence (-1 to 1)\n            account_balance: Total account balance\n            win_rate: Historical win rate for this symbol\n\n        Returns:\n            Position size in base currency units\n        \"\"\"\n        # Allocated capital for this symbol\n        allocated = account_balance * self.limits.max_position_pct\n\n        # Kelly criterion: f* = (bp - q) / b\n        # where b = win/loss ratio, p = win prob, q = lose prob\n        # Simplified: use signal strength as confidence multiplier\n        kelly_bet = abs(signal_strength) * self.limits.kelly_fraction\n\n        # Final position size\n        position = allocated * kelly_bet\n\n        self._position_size = position\n        return position",
    "source_file": "core\\risk\\per_symbol.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "PerSymbolRiskManager"
  },
  {
    "name": "record_trade",
    "category": "execution",
    "formula": "",
    "explanation": "Record a trade execution.",
    "python_code": "def record_trade(self, pnl: float = 0.0):\n        \"\"\"Record a trade execution.\"\"\"\n        self._check_daily_reset()\n        self.metrics.daily_trades += 1\n        self.metrics.last_trade_time = datetime.now()\n\n        if pnl != 0:\n            self.record_pnl(pnl)",
    "source_file": "core\\risk\\per_symbol.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "PerSymbolRiskManager"
  },
  {
    "name": "record_pnl",
    "category": "risk",
    "formula": "",
    "explanation": "Record realized PnL.",
    "python_code": "def record_pnl(self, pnl: float):\n        \"\"\"Record realized PnL.\"\"\"\n        self.metrics.daily_pnl += pnl\n        self.metrics.current_equity += pnl\n\n        if pnl > 0:\n            self.metrics.win_count += 1\n        else:\n            self.metrics.loss_count += 1\n\n        # Update peak and drawdown\n        if self.metrics.current_equity > self.metrics.peak_equity:\n            self.metrics.peak_equity = self.metrics.current_equity\n\n        if self.metrics.peak_equity > 0:\n            self.metrics.current_drawdown = (\n                (self.metrics.peak_equity - self.metrics.current_equity)\n                / self.metrics.peak_equity\n            )",
    "source_file": "core\\risk\\per_symbol.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "PerSymbolRiskManager"
  },
  {
    "name": "_check_daily_reset",
    "category": "risk",
    "formula": "",
    "explanation": "Reset daily counters if new day.",
    "python_code": "def _check_daily_reset(self):\n        \"\"\"Reset daily counters if new day.\"\"\"\n        today = date.today()\n        if self.metrics.last_reset_date != today:\n            self.metrics.daily_trades = 0\n            self.metrics.daily_pnl = 0.0\n            self.metrics.last_reset_date = today\n            logger.debug(f\"{self.symbol}: Daily counters reset\")",
    "source_file": "core\\risk\\per_symbol.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "PerSymbolRiskManager"
  },
  {
    "name": "win_rate",
    "category": "risk",
    "formula": "0.5  # Default assumption",
    "explanation": "Calculate win rate.",
    "python_code": "def win_rate(self) -> float:\n        \"\"\"Calculate win rate.\"\"\"\n        total = self.metrics.win_count + self.metrics.loss_count\n        if total == 0:\n            return 0.5  # Default assumption\n        return self.metrics.win_count / total",
    "source_file": "core\\risk\\per_symbol.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "PerSymbolRiskManager"
  },
  {
    "name": "trades_remaining",
    "category": "technical",
    "formula": "max(0, self.limits.daily_trade_limit - self.metrics.daily_trades)",
    "explanation": "Get remaining trades for today.",
    "python_code": "def trades_remaining(self) -> int:\n        \"\"\"Get remaining trades for today.\"\"\"\n        return max(0, self.limits.daily_trade_limit - self.metrics.daily_trades)",
    "source_file": "core\\risk\\per_symbol.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "PerSymbolRiskManager"
  },
  {
    "name": "summary",
    "category": "risk",
    "formula": "{",
    "explanation": "Get summary statistics.",
    "python_code": "def summary(self) -> dict:\n        \"\"\"Get summary statistics.\"\"\"\n        return {\n            'symbol': self.symbol,\n            'daily_trades': self.metrics.daily_trades,\n            'trades_remaining': self.trades_remaining,\n            'daily_pnl': self.metrics.daily_pnl,\n            'current_drawdown': self.metrics.current_drawdown,\n            'win_rate': self.win_rate,\n            'can_trade': self.can_trade()[0],\n        }",
    "source_file": "core\\risk\\per_symbol.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "PerSymbolRiskManager"
  },
  {
    "name": "__init__",
    "category": "risk",
    "formula": "",
    "explanation": "Initialize portfolio risk manager.\n\nArgs:\n    total_capital: Total trading capital\n    max_portfolio_risk: Max % of capital at risk (default 10%)",
    "python_code": "def __init__(self, total_capital: float, max_portfolio_risk: float = 0.10):\n        \"\"\"\n        Initialize portfolio risk manager.\n\n        Args:\n            total_capital: Total trading capital\n            max_portfolio_risk: Max % of capital at risk (default 10%)\n        \"\"\"\n        self.total_capital = total_capital\n        self.max_portfolio_risk = max_portfolio_risk\n\n        self._symbol_managers: Dict[str, PerSymbolRiskManager] = {}\n        self._lock = threading.RLock()\n\n        # Portfolio metrics\n        self.total_pnl: float = 0.0\n        self.daily_pnl: float = 0.0\n        self.peak_equity: float = total_capital\n        self.current_equity: float = total_capital\n\n        # Correlation matrix (populated externally)\n        self.correlations: Optional[Dict[str, Dict[str, float]]] = None",
    "source_file": "core\\risk\\portfolio.py",
    "academic_reference": null,
    "class_name": "PortfolioRiskManager"
  },
  {
    "name": "get_manager",
    "category": "risk",
    "formula": "",
    "explanation": "Get or create a risk manager for a symbol.\n\nArgs:\n    symbol: Trading symbol\n\nReturns:\n    PerSymbolRiskManager for the symbol",
    "python_code": "def get_manager(self, symbol: str) -> PerSymbolRiskManager:\n        \"\"\"\n        Get or create a risk manager for a symbol.\n\n        Args:\n            symbol: Trading symbol\n\n        Returns:\n            PerSymbolRiskManager for the symbol\n        \"\"\"\n        with self._lock:\n            if symbol not in self._symbol_managers:\n                # Get config from registry\n                try:\n                    from core.symbol import SymbolRegistry\n                    config = SymbolRegistry.get().get_config(symbol)\n                    limits = RiskLimits.from_dict(config)\n                except Exception:\n                    limits = RiskLimits()\n\n                self._symbol_managers[symbol] = PerSymbolRiskManager(symbol, limits)\n                logger.debug(f\"Created risk manager for {symbol}\")\n\n            return self._symbol_managers[symbol]",
    "source_file": "core\\risk\\portfolio.py",
    "academic_reference": null,
    "class_name": "PortfolioRiskManager"
  },
  {
    "name": "can_trade",
    "category": "risk",
    "formula": "can, reason | portfolio_check | True, \"OK\"",
    "explanation": "Check if trading is allowed for a symbol.\n\nChecks both symbol-level and portfolio-level limits.\n\nReturns:\n    (can_trade, reason)",
    "python_code": "def can_trade(self, symbol: str, spread_pips: float = 0.0) -> Tuple[bool, str]:\n        \"\"\"\n        Check if trading is allowed for a symbol.\n\n        Checks both symbol-level and portfolio-level limits.\n\n        Returns:\n            (can_trade, reason)\n        \"\"\"\n        # Symbol-level check\n        manager = self.get_manager(symbol)\n        can, reason = manager.can_trade(spread_pips)\n        if not can:\n            return can, reason\n\n        # Portfolio-level checks\n        portfolio_check = self._check_portfolio_risk(symbol)\n        if not portfolio_check[0]:\n            return portfolio_check\n\n        return True, \"OK\"",
    "source_file": "core\\risk\\portfolio.py",
    "academic_reference": null,
    "class_name": "PortfolioRiskManager"
  },
  {
    "name": "_check_portfolio_risk",
    "category": "risk",
    "formula": "False, f\"Portfolio drawdown limit ({drawdown:.1%})\" | False, f\"Max open positions ({max_positions})\" | high_corr_exposure",
    "explanation": "Check portfolio-level risk limits.",
    "python_code": "def _check_portfolio_risk(self, symbol: str) -> Tuple[bool, str]:\n        \"\"\"Check portfolio-level risk limits.\"\"\"\n        # Total drawdown check\n        if self.peak_equity > 0:\n            drawdown = (self.peak_equity - self.current_equity) / self.peak_equity\n            if drawdown >= self.max_portfolio_risk:\n                return False, f\"Portfolio drawdown limit ({drawdown:.1%})\"\n\n        # Max open positions\n        open_positions = len([m for m in self._symbol_managers.values()\n                            if getattr(m, '_position_size', 0) > 0])\n        max_positions = RiskLimits().max_open_positions\n\n        if open_positions >= max_positions:\n            return False, f\"Max open positions ({max_positions})\"\n\n        # Correlation check (avoid concentrated exposure)\n        if self.correlations and symbol in self.correlations:\n            high_corr_exposure = self._check_correlation_risk(symbol)\n            if not high_corr_exposure[0]:\n                return high_corr_exposure\n\n        return True, \"OK\"",
    "source_file": "core\\risk\\portfolio.py",
    "academic_reference": null,
    "class_name": "PortfolioRiskManager"
  },
  {
    "name": "_check_correlation_risk",
    "category": "statistical",
    "formula": "True, \"OK\" | False, f\"High correlation with {other} ({corr:.2f})\" | True, \"OK\"",
    "explanation": "Check if adding position would create correlation risk.",
    "python_code": "def _check_correlation_risk(self, symbol: str) -> Tuple[bool, str]:\n        \"\"\"Check if adding position would create correlation risk.\"\"\"\n        if not self.correlations or symbol not in self.correlations:\n            return True, \"OK\"\n\n        # Get symbols with open positions\n        open_symbols = [s for s, m in self._symbol_managers.items()\n                       if getattr(m, '_position_size', 0) > 0]\n\n        # Check correlation with existing positions\n        max_corr = RiskLimits().max_correlation\n        for other in open_symbols:\n            if other in self.correlations.get(symbol, {}):\n                corr = self.correlations[symbol][other]\n                if abs(corr) > max_corr:\n                    return False, f\"High correlation with {other} ({corr:.2f})\"\n\n        return True, \"OK\"",
    "source_file": "core\\risk\\portfolio.py",
    "academic_reference": null,
    "class_name": "PortfolioRiskManager"
  },
  {
    "name": "calculate_position_size",
    "category": "risk",
    "formula": "manager.calculate_position_size(",
    "explanation": "Calculate position size for a symbol.\n\nArgs:\n    symbol: Trading symbol\n    signal_strength: Signal confidence (-1 to 1)\n    win_rate: Historical win rate\n\nReturns:\n    Position size",
    "python_code": "def calculate_position_size(\n        self,\n        symbol: str,\n        signal_strength: float,\n        win_rate: float = 0.55\n    ) -> float:\n        \"\"\"\n        Calculate position size for a symbol.\n\n        Args:\n            symbol: Trading symbol\n            signal_strength: Signal confidence (-1 to 1)\n            win_rate: Historical win rate\n\n        Returns:\n            Position size\n        \"\"\"\n        manager = self.get_manager(symbol)\n        return manager.calculate_position_size(\n            signal_strength,\n            self.total_capital,\n            win_rate\n        )",
    "source_file": "core\\risk\\portfolio.py",
    "academic_reference": null,
    "class_name": "PortfolioRiskManager"
  },
  {
    "name": "record_trade",
    "category": "risk",
    "formula": "",
    "explanation": "Record a trade for a symbol.",
    "python_code": "def record_trade(self, symbol: str, pnl: float = 0.0):\n        \"\"\"Record a trade for a symbol.\"\"\"\n        manager = self.get_manager(symbol)\n        manager.record_trade(pnl)\n\n        if pnl != 0:\n            self._update_portfolio_pnl(pnl)",
    "source_file": "core\\risk\\portfolio.py",
    "academic_reference": null,
    "class_name": "PortfolioRiskManager"
  },
  {
    "name": "_update_portfolio_pnl",
    "category": "risk",
    "formula": "",
    "explanation": "Update portfolio-level P&L tracking.",
    "python_code": "def _update_portfolio_pnl(self, pnl: float):\n        \"\"\"Update portfolio-level P&L tracking.\"\"\"\n        with self._lock:\n            self.total_pnl += pnl\n            self.daily_pnl += pnl\n            self.current_equity += pnl\n\n            if self.current_equity > self.peak_equity:\n                self.peak_equity = self.current_equity",
    "source_file": "core\\risk\\portfolio.py",
    "academic_reference": null,
    "class_name": "PortfolioRiskManager"
  },
  {
    "name": "reset_daily",
    "category": "risk",
    "formula": "",
    "explanation": "Reset daily metrics.",
    "python_code": "def reset_daily(self):\n        \"\"\"Reset daily metrics.\"\"\"\n        with self._lock:\n            self.daily_pnl = 0.0\n            for manager in self._symbol_managers.values():\n                manager._check_daily_reset()",
    "source_file": "core\\risk\\portfolio.py",
    "academic_reference": null,
    "class_name": "PortfolioRiskManager"
  },
  {
    "name": "set_correlations",
    "category": "technical",
    "formula": "",
    "explanation": "Set correlation matrix for risk checks.",
    "python_code": "def set_correlations(self, corr_matrix: Dict[str, Dict[str, float]]):\n        \"\"\"Set correlation matrix for risk checks.\"\"\"\n        self.correlations = corr_matrix",
    "source_file": "core\\risk\\portfolio.py",
    "academic_reference": null,
    "class_name": "PortfolioRiskManager"
  },
  {
    "name": "summary",
    "category": "risk",
    "formula": "{",
    "explanation": "Get portfolio summary.",
    "python_code": "def summary(self) -> Dict:\n        \"\"\"Get portfolio summary.\"\"\"\n        drawdown = 0.0\n        if self.peak_equity > 0:\n            drawdown = (self.peak_equity - self.current_equity) / self.peak_equity\n\n        symbol_summaries = {\n            symbol: manager.summary()\n            for symbol, manager in self._symbol_managers.items()\n        }\n\n        return {\n            'total_capital': self.total_capital,\n            'current_equity': self.current_equity,\n            'total_pnl': self.total_pnl,\n            'daily_pnl': self.daily_pnl,\n            'drawdown': drawdown,\n            'active_symbols': len(self._symbol_managers),\n            'symbols': symbol_summaries,\n        }",
    "source_file": "core\\risk\\portfolio.py",
    "academic_reference": null,
    "class_name": "PortfolioRiskManager"
  },
  {
    "name": "get_tradeable_symbols",
    "category": "risk",
    "formula": "tradeable",
    "explanation": "Get list of symbols that can currently trade.",
    "python_code": "def get_tradeable_symbols(self) -> List[str]:\n        \"\"\"Get list of symbols that can currently trade.\"\"\"\n        tradeable = []\n        for symbol in self._symbol_managers:\n            can, _ = self.can_trade(symbol)\n            if can:\n                tradeable.append(symbol)\n        return tradeable",
    "source_file": "core\\risk\\portfolio.py",
    "academic_reference": null,
    "class_name": "PortfolioRiskManager"
  },
  {
    "name": "__init__",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(\n        self,\n        input_dim: int,\n        output_dim: int,\n        hidden_dims: List[int] = [256, 256],\n        activation: str = 'relu',\n        output_activation: Optional[str] = None,\n    ):\n        super().__init__()\n\n        activations = {\n            'relu': nn.ReLU,\n            'tanh': nn.Tanh,\n            'leaky_relu': nn.LeakyReLU,\n            'elu': nn.ELU,\n        }\n\n        act_fn = activations.get(activation, nn.ReLU)\n\n        layers = []\n        prev_dim = input_dim\n        for hidden_dim in hidden_dims:\n            layers.append(nn.Linear(prev_dim, hidden_dim))\n            layers.append(act_fn())\n            prev_dim = hidden_dim\n\n        layers.append(nn.Linear(prev_dim, output_dim))\n\n        if output_activation:\n            layers.append(activations.get(output_activation, nn.Identity)())\n\n        self.net = nn.Sequential(*layers)",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "MLP"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.net(x)",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "MLP"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "mean, log_std",
    "explanation": "Forward pass returning mean and log_std.",
    "python_code": "def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Forward pass returning mean and log_std.\"\"\"\n        features = self.backbone(state)\n        mean = self.mean_head(features)\n        log_std = self.log_std_head(features)\n        log_std = torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n        return mean, log_std",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "GaussianActor"
  },
  {
    "name": "sample",
    "category": "reinforcement_learning",
    "formula": "action_tanh, log_prob",
    "explanation": "Sample action and compute log probability.",
    "python_code": "def sample(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Sample action and compute log probability.\"\"\"\n        mean, log_std = self.forward(state)\n        std = log_std.exp()\n        dist = Normal(mean, std)\n        action = dist.rsample()  # Reparameterization trick\n        log_prob = dist.log_prob(action).sum(-1, keepdim=True)\n\n        # Squash to [-1, 1] (for SAC)\n        action_tanh = torch.tanh(action)\n        log_prob -= torch.log(1 - action_tanh.pow(2) + 1e-6).sum(-1, keepdim=True)\n\n        return action_tanh, log_prob",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "GaussianActor"
  },
  {
    "name": "get_action",
    "category": "execution",
    "formula": "torch.tanh(mean) | torch.tanh(action)",
    "explanation": "Get action for execution.",
    "python_code": "def get_action(self, state: torch.Tensor, deterministic: bool = False) -> torch.Tensor:\n        \"\"\"Get action for execution.\"\"\"\n        mean, log_std = self.forward(state)\n        if deterministic:\n            return torch.tanh(mean)\n        std = log_std.exp()\n        dist = Normal(mean, std)\n        action = dist.sample()\n        return torch.tanh(action)",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "GaussianActor"
  },
  {
    "name": "q1_forward",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def q1_forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n        return self.q1(state, action)",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "TwinCritic"
  },
  {
    "name": "add",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def add(self, state, action, reward, next_state, done):\n        self.states[self.ptr] = state\n        self.actions[self.ptr] = action\n        self.rewards[self.ptr] = reward\n        self.next_states[self.ptr] = next_state\n        self.dones[self.ptr] = done\n\n        self.ptr = (self.ptr + 1) % self.max_size\n        self.size = min(self.size + 1, self.max_size)",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "ReplayBuffer"
  },
  {
    "name": "sample",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "",
    "python_code": "def sample(self, batch_size: int, device: str = 'cpu') -> Dict[str, torch.Tensor]:\n        idx = np.random.randint(0, self.size, size=batch_size)\n        return {\n            'states': torch.FloatTensor(self.states[idx]).to(device),\n            'actions': torch.FloatTensor(self.actions[idx]).to(device),\n            'rewards': torch.FloatTensor(self.rewards[idx]).to(device),\n            'next_states': torch.FloatTensor(self.next_states[idx]).to(device),\n            'dones': torch.FloatTensor(self.dones[idx]).to(device),\n        }",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "ReplayBuffer"
  },
  {
    "name": "select_action",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Select action given state.",
    "python_code": "def select_action(self, state: np.ndarray, deterministic: bool = False) -> np.ndarray:\n        \"\"\"Select action given state.\"\"\"\n        pass",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BaseRLAgent"
  },
  {
    "name": "train",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Train on a batch of data.",
    "python_code": "def train(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n        \"\"\"Train on a batch of data.\"\"\"\n        pass",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BaseRLAgent"
  },
  {
    "name": "learn",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Main training loop.",
    "python_code": "def learn(self, total_timesteps: int, callback=None) -> 'BaseRLAgent':\n        \"\"\"Main training loop.\"\"\"\n        state, _ = self.env.reset()\n        episode_reward = 0\n        episode_length = 0\n\n        for step in range(total_timesteps):\n            action = self.select_action(state)\n\n            next_state, reward, terminated, truncated, info = self.env.step(action)\n            done = terminated or truncated\n\n            self.buffer.add(state, action, reward, next_state, float(done))\n\n            state = next_state\n            episode_reward += reward\n            episode_length += 1\n            self.total_steps += 1\n\n            if done:\n                state, _ = self.env.reset()\n                if callback:\n                    callback({\n                        'episode_reward': episode_reward,\n                        'episode_length': episode_length,\n                        'total_steps': self.total_steps,\n                    })\n                episode_reward = 0\n                episode_length = 0\n\n            if self.buffer.size >= self.config.batch_size:\n                batch = self.buffer.sample(self.config.batch_size, self.device)\n                self.train(batch)\n\n        return self",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BaseRLAgent"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Predict action for given state.",
    "python_code": "def predict(self, state: np.ndarray, deterministic: bool = True) -> np.ndarray:\n        \"\"\"Predict action for given state.\"\"\"\n        return self.select_action(state, deterministic=deterministic)",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BaseRLAgent"
  },
  {
    "name": "save",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Save model to path.",
    "python_code": "def save(self, path: str):\n        \"\"\"Save model to path.\"\"\"\n        torch.save(self.state_dict(), path)",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BaseRLAgent"
  },
  {
    "name": "load",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Load model from path.",
    "python_code": "def load(self, path: str):\n        \"\"\"Load model from path.\"\"\"\n        self.load_state_dict(torch.load(path, map_location=self.device))",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BaseRLAgent"
  },
  {
    "name": "state_dict",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Get state dict for saving.",
    "python_code": "def state_dict(self) -> Dict[str, Any]:\n        \"\"\"Get state dict for saving.\"\"\"\n        pass",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BaseRLAgent"
  },
  {
    "name": "load_state_dict",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Load state dict.",
    "python_code": "def load_state_dict(self, state_dict: Dict[str, Any]):\n        \"\"\"Load state dict.\"\"\"\n        pass",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BaseRLAgent"
  },
  {
    "name": "select_action",
    "category": "reinforcement_learning",
    "formula": "action.cpu().numpy()[0]",
    "explanation": "",
    "python_code": "def select_action(self, state: np.ndarray, deterministic: bool = False) -> np.ndarray:\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n            action = self.actor.get_action(state_tensor, deterministic)\n            return action.cpu().numpy()[0]",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "PPOAgent"
  },
  {
    "name": "compute_gae",
    "category": "reinforcement_learning",
    "formula": "advantages, returns",
    "explanation": "Compute Generalized Advantage Estimation.\n\nReference: Schulman et al. (2016)\n\"High-Dimensional Continuous Control Using GAE\"",
    "python_code": "def compute_gae(\n        self,\n        rewards: torch.Tensor,\n        values: torch.Tensor,\n        dones: torch.Tensor,\n        next_value: torch.Tensor,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Compute Generalized Advantage Estimation.\n\n        Reference: Schulman et al. (2016)\n        \"High-Dimensional Continuous Control Using GAE\"\n        \"\"\"\n        advantages = torch.zeros_like(rewards)\n        last_gae = 0\n\n        for t in reversed(range(len(rewards))):\n            if t == len(rewards) - 1:\n                next_val = next_value\n            else:\n                next_val = values[t + 1]\n\n            delta = rewards[t] + self.config.gamma * next_val * (1 - dones[t]) - values[t]\n            advantages[t] = last_gae = delta + self.config.gamma * self.config.gae_lambda * (1 - dones[t]) * last_gae\n\n        returns = advantages + values\n        return advantages, returns",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "PPOAgent"
  },
  {
    "name": "train",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "Train PPO agent on rollout data.",
    "python_code": "def train(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n        \"\"\"Train PPO agent on rollout data.\"\"\"\n        states = batch['states']\n        actions = batch['actions']\n        old_log_probs = batch['log_probs']\n        advantages = batch['advantages']\n        returns = batch['returns']\n\n        # Normalize advantages\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n        # Multiple epochs of updates\n        total_loss = 0.0\n        total_pg_loss = 0.0\n        total_value_loss = 0.0\n        total_entropy = 0.0\n\n        for _ in range(self.config.ppo_epochs):\n            # Get current policy distribution\n            mean, log_std = self.actor(states)\n            std = log_std.exp()\n            dist = Normal(mean, std)\n\n            new_log_probs = dist.log_prob(actions).sum(-1, keepdim=True)\n            entropy = dist.entropy().sum(-1).mean()\n\n            # PPO clipped objective (Schulman et al. 2017)\n            ratio = (new_log_probs - old_log_probs).exp()\n            surr1 = ratio * advantages\n            surr2 = torch.clamp(ratio, 1 - self.config.ppo_clip, 1 + self.config.ppo_clip) * advantages\n            pg_loss = -torch.min(surr1, surr2).mean()\n\n            # Value loss\n            values = self.critic(states)\n            value_loss = F.mse_loss(values, returns)\n\n            # Total loss\n            loss = pg_loss + self.config.value_coef * value_loss - self.config.entropy_coef * entropy\n\n            # Update networks\n            self.actor_optimizer.zero_grad()\n            self.critic_optimizer.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(self.actor.parameters(), self.config.max_grad_norm)\n            nn.utils.clip_grad_norm_(self.critic.parameters(), self.config.max_grad_norm)\n            self.actor_optimizer.step()\n            self.critic_optimizer.step()\n\n            total_loss += loss.item()\n            total_pg_loss += pg_loss.item()\n            total_value_loss += value_loss.item()\n            total_entropy += entropy.item()\n\n        n_epochs = self.config.ppo_epochs\n        return {\n            'loss': total_loss / n_epochs,\n            'pg_loss': total_pg_loss / n_epochs,\n            'value_loss': total_value_loss / n_epochs,\n            'entropy': total_entropy / n_epochs,\n        }",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "PPOAgent"
  },
  {
    "name": "state_dict",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "",
    "python_code": "def state_dict(self) -> Dict[str, Any]:\n        return {\n            'actor': self.actor.state_dict(),\n            'critic': self.critic.state_dict(),\n            'actor_optimizer': self.actor_optimizer.state_dict(),\n            'critic_optimizer': self.critic_optimizer.state_dict(),\n        }",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "PPOAgent"
  },
  {
    "name": "get",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "",
    "python_code": "def get(self, device: str = 'cpu') -> Dict[str, torch.Tensor]:\n        return {\n            'states': torch.FloatTensor(self.states[:self.size]).to(device),\n            'actions': torch.FloatTensor(self.actions[:self.size]).to(device),\n            'rewards': torch.FloatTensor(self.rewards[:self.size]).to(device),\n            'log_probs': torch.FloatTensor(self.log_probs[:self.size]).to(device),\n            'dones': torch.FloatTensor(self.dones[:self.size]).to(device),\n            'values': torch.FloatTensor(self.values[:self.size]).to(device),\n        }",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "RolloutBuffer"
  },
  {
    "name": "clear",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def clear(self):\n        self.ptr = 0\n        self.size = 0",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "RolloutBuffer"
  },
  {
    "name": "state_dict",
    "category": "reinforcement_learning",
    "formula": "state",
    "explanation": "",
    "python_code": "def state_dict(self) -> Dict[str, Any]:\n        state = {\n            'actor': self.actor.state_dict(),\n            'critic': self.critic.state_dict(),\n            'critic_target': self.critic_target.state_dict(),\n            'actor_optimizer': self.actor_optimizer.state_dict(),\n            'critic_optimizer': self.critic_optimizer.state_dict(),\n        }\n        if self.config.auto_alpha:\n            state['log_alpha'] = self.log_alpha\n            state['alpha_optimizer'] = self.alpha_optimizer.state_dict()\n        return state",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "SACAgent"
  },
  {
    "name": "select_action",
    "category": "reinforcement_learning",
    "formula": "action",
    "explanation": "",
    "python_code": "def select_action(self, state: np.ndarray, deterministic: bool = False) -> np.ndarray:\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n            action = self.actor(state_tensor).cpu().numpy()[0]\n\n            if not deterministic:\n                noise = np.random.normal(0, self.config.policy_noise, size=action.shape)\n                action = np.clip(action + noise, -1.0, 1.0)\n\n            return action",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "TD3Agent"
  },
  {
    "name": "select_action",
    "category": "reinforcement_learning",
    "formula": "np.array([np.random.randint(self.action_dim)]) | action",
    "explanation": "",
    "python_code": "def select_action(self, state: np.ndarray, deterministic: bool = False) -> np.ndarray:\n        if not deterministic and np.random.random() < self.epsilon:\n            return np.array([np.random.randint(self.action_dim)])\n\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n            q_values = self.q_network(state_tensor)\n            action = q_values.argmax(dim=-1).cpu().numpy()\n            return action",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "DQNAgent"
  },
  {
    "name": "create_agent",
    "category": "reinforcement_learning",
    "formula": "agents[agent_type](env, config)",
    "explanation": "Factory function to create RL agents.\n\nArgs:\n    agent_type: One of 'ppo', 'sac', 'a2c', 'td3', 'dqn', 'ddpg'\n    env: Gym environment\n    config: Agent configuration\n\nReturns:\n    RL agent instance",
    "python_code": "def create_agent(\n    agent_type: str,\n    env: gym.Env,\n    config: Optional[AgentConfig] = None,\n) -> BaseRLAgent:\n    \"\"\"\n    Factory function to create RL agents.\n\n    Args:\n        agent_type: One of 'ppo', 'sac', 'a2c', 'td3', 'dqn', 'ddpg'\n        env: Gym environment\n        config: Agent configuration\n\n    Returns:\n        RL agent instance\n    \"\"\"\n    agents = {\n        'ppo': PPOAgent,\n        'sac': SACAgent,\n        'a2c': A2CAgent,\n        'td3': TD3Agent,\n        'dqn': DQNAgent,\n        'ddpg': DDPGAgent,\n    }\n\n    agent_type = agent_type.lower()\n    if agent_type not in agents:\n        raise ValueError(f\"Unknown agent type: {agent_type}. Available: {list(agents.keys())}\")\n\n    return agents[agent_type](env, config)",
    "source_file": "core\\rl\\agents.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "microstructure",
    "formula": "",
    "explanation": "Initialize order execution environment.\n\nArgs:\n    price_data: Array of prices (T,)\n    config: MARL configuration\n    volume_data: Optional volume data for market impact",
    "python_code": "def __init__(\n        self,\n        price_data: np.ndarray,\n        config: Optional[MARLConfig] = None,\n        volume_data: Optional[np.ndarray] = None,\n    ):\n        \"\"\"\n        Initialize order execution environment.\n\n        Args:\n            price_data: Array of prices (T,)\n            config: MARL configuration\n            volume_data: Optional volume data for market impact\n        \"\"\"\n        super().__init__()\n\n        self.config = config or MARLConfig()\n        self.prices = price_data\n        self.volumes = volume_data if volume_data is not None else np.ones_like(price_data)\n        self.n_steps = len(price_data)\n        self.n_agents = self.config.n_agents\n\n        # State dimension per agent\n        self.state_dim_per_agent = 10  # inventory, time, price features, etc.\n        self.state_dim = self.state_dim_per_agent * self.n_agents\n\n        # Action space: each agent chooses execution rate [0, 1]\n        self.action_space = spaces.Box(\n            low=0.0,\n            high=1.0,\n            shape=(self.n_agents,),\n            dtype=np.float32\n        )\n\n        self.observation_space = spaces.Box(\n            low=-np.inf,\n            high=np.inf,\n            shape=(self.state_dim,),\n            dtype=np.float32\n        )\n\n        # Episode state\n        self.reset()",
    "source_file": "core\\rl\\chinese_marl.py",
    "academic_reference": "arXiv:2009.11189",
    "class_name": "OrderExecutionEnv"
  },
  {
    "name": "reset",
    "category": "execution",
    "formula": "",
    "explanation": "Reset environment for new episode.",
    "python_code": "def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, Dict]:\n        \"\"\"Reset environment for new episode.\"\"\"\n        super().reset(seed=seed)\n\n        self.current_step = 0\n        self.inventories = np.ones(self.n_agents) * self.config.max_inventory\n        self.arrival_prices = np.full(self.n_agents, self.prices[0])\n        self.vwap_prices = np.zeros(self.n_agents)\n        self.executed_volumes = np.zeros(self.n_agents)\n        self.total_costs = np.zeros(self.n_agents)\n\n        return self._get_state(), {}",
    "source_file": "core\\rl\\chinese_marl.py",
    "academic_reference": "arXiv:2009.11189",
    "class_name": "OrderExecutionEnv"
  },
  {
    "name": "_get_state",
    "category": "execution",
    "formula": "= (self.prices[self.current_step] - self.prices[self.current_step - 1]) / self.prices[self.current_step - 1] | = 0.0 | np.array(states, dtype=np.float32)",
    "explanation": "Get observation for all agents.",
    "python_code": "def _get_state(self) -> np.ndarray:\n        \"\"\"Get observation for all agents.\"\"\"\n        states = []\n\n        for i in range(self.n_agents):\n            # Remaining inventory (normalized)\n            inv_remaining = self.inventories[i] / self.config.max_inventory\n\n            # Time remaining (normalized)\n            time_remaining = 1.0 - self.current_step / self.config.time_horizon\n\n            # Price features\n            if self.current_step > 0:\n                price_return = (self.prices[self.current_step] - self.prices[self.current_step - 1]) / self.prices[self.current_step - 1]\n            else:\n                price_return = 0.0\n\n            # Volatility (rolling)\n            lookback = min(20, self.current_step + 1)\n            if lookback > 1:\n                returns = np.diff(np.log(self.prices[self.current_step - lookback + 1:self.current_step + 1]))\n                volatility = np.std(returns) if len(returns) > 0 else 0.0\n            else:\n                volatility = 0.0\n\n            # Volume ratio\n            vol_ratio = self.volumes[self.current_step] / np.mean(self.volumes[:self.current_step + 1])\n\n            # Implementation shortfall so far\n            if self.executed_volumes[i] > 0:\n                is_so_far = (self.vwap_prices[i] - self.arrival_prices[i]) / self.arrival_prices[i]\n            else:\n                is_so_far = 0.0\n\n            # Other agents' average inventory (communication)\n            other_inv = np.mean([self.inventories[j] for j in range(self.n_agents) if j != i])\n\n            agent_state = [\n                inv_remaining,\n                time_remaining,\n                price_return,\n                volatility,\n                vol_ratio,\n                is_so_far,\n                other_inv / self.config.max_inventory,\n                self.prices[self.current_step] / self.arrival_prices[i] - 1,  # Price deviation\n                0.0,  # Placeholder for spread\n                0.0,  # Placeholder for imbalance\n            ]\n\n            states.extend(agent_state)\n\n        return np.array(states, dtype=np.float32)",
    "source_file": "core\\rl\\chinese_marl.py",
    "academic_reference": "arXiv:2009.11189",
    "class_name": "OrderExecutionEnv"
  },
  {
    "name": "_calculate_market_impact",
    "category": "microstructure",
    "formula": "Impact =  * sign(order) * sqrt(volume / ADV) | impact",
    "explanation": "Calculate market impact using Kyle's Lambda model.\n\nReference: Kyle (1985)\n\"Continuous Auctions and Insider Trading\"\n\nImpact =  * sign(order) * sqrt(volume / ADV)",
    "python_code": "def _calculate_market_impact(self, execution_rate: float, agent_idx: int) -> float:\n        \"\"\"\n        Calculate market impact using Kyle's Lambda model.\n\n        Reference: Kyle (1985)\n        \"Continuous Auctions and Insider Trading\"\n\n        Impact =  * sign(order) * sqrt(volume / ADV)\n        \"\"\"\n        volume = execution_rate * self.inventories[agent_idx]\n        adv = np.mean(self.volumes)  # Average daily volume\n\n        # Simplified Kyle's Lambda\n        impact = self.config.market_impact_coef * np.sqrt(volume / (adv + 1e-8))\n\n        return impact",
    "source_file": "core\\rl\\chinese_marl.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "OrderExecutionEnv"
  },
  {
    "name": "step",
    "category": "execution",
    "formula": "state, total_reward, terminated, truncated, info",
    "explanation": "Execute one step for all agents.\n\nArgs:\n    actions: Array of execution rates for each agent\n\nReturns:\n    state, reward, terminated, truncated, info",
    "python_code": "def step(self, actions: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n        \"\"\"\n        Execute one step for all agents.\n\n        Args:\n            actions: Array of execution rates for each agent\n\n        Returns:\n            state, reward, terminated, truncated, info\n        \"\"\"\n        actions = np.clip(actions, 0.0, 1.0)\n        current_price = self.prices[self.current_step]\n\n        total_reward = 0.0\n        agent_rewards = []\n\n        for i in range(self.n_agents):\n            if self.inventories[i] <= 0:\n                agent_rewards.append(0.0)\n                continue\n\n            # Execute order\n            exec_volume = actions[i] * self.inventories[i]\n\n            # Calculate market impact\n            impact = self._calculate_market_impact(actions[i], i)\n            exec_price = current_price * (1 + impact)  # Adverse price movement\n\n            # Update VWAP\n            old_total = self.vwap_prices[i] * self.executed_volumes[i]\n            self.executed_volumes[i] += exec_volume\n            if self.executed_volumes[i] > 0:\n                self.vwap_prices[i] = (old_total + exec_price * exec_volume) / self.executed_volumes[i]\n\n            # Update inventory\n            self.inventories[i] -= exec_volume\n\n            # Calculate reward (negative implementation shortfall)\n            is_cost = exec_volume * (exec_price - self.arrival_prices[i])\n            self.total_costs[i] += is_cost\n\n            # Reward = negative cost (we want to minimize cost)\n            reward = -is_cost / (self.arrival_prices[i] * self.config.max_inventory)\n\n            # Penalty for not completing (if time is running out)\n            time_pressure = 1.0 - (self.config.time_horizon - self.current_step) / self.config.time_horizon\n            if self.inventories[i] > 0.1 * self.config.max_inventory:\n                reward -= time_pressure * 0.01\n\n            agent_rewards.append(reward)\n            total_reward += reward\n\n        # Move to next step\n        self.current_step += 1\n\n        # Check termination\n        terminated = np.all(self.inventories <= 0)\n        truncated = self.current_step >= min(self.config.time_horizon, self.n_steps - 1)\n\n        # Final reward for completing execution\n        if terminated:\n            total_reward += 0.1 * self.n_agents  # Bonus for completion\n\n        state = self._get_state() if not (terminated or truncated) else np.zeros(self.state_dim, dtype=np.float32)\n\n        info = {\n            'inventories': self.inventories.copy(),\n            'vwap_prices': self.vwap_prices.copy(),\n            'total_costs': self.total_costs.copy(),\n            'implementation_shortfall': np.sum(self.total_costs),\n            'agent_rewards': agent_rewards,\n        }\n\n        return state, total_reward, terminated, truncated, info",
    "source_file": "core\\rl\\chinese_marl.py",
    "academic_reference": "arXiv:2009.11189",
    "class_name": "OrderExecutionEnv"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "q_tot.squeeze(-1).squeeze(-1)",
    "explanation": "Mix individual Q-values into total Q-value.\n\nArgs:\n    agent_qs: Individual agent Q-values (batch, n_agents)\n    state: Global state (batch, state_dim)\n\nReturns:\n    Q_tot: Total Q-value (batch, 1)",
    "python_code": "def forward(\n        self,\n        agent_qs: torch.Tensor,  # (batch, n_agents)\n        state: torch.Tensor,  # (batch, state_dim)\n    ) -> torch.Tensor:\n        \"\"\"\n        Mix individual Q-values into total Q-value.\n\n        Args:\n            agent_qs: Individual agent Q-values (batch, n_agents)\n            state: Global state (batch, state_dim)\n\n        Returns:\n            Q_tot: Total Q-value (batch, 1)\n        \"\"\"\n        batch_size = agent_qs.size(0)\n\n        # Generate mixing weights (ensure non-negative with abs)\n        w1 = torch.abs(self.hyper_w1(state))\n        w1 = w1.view(batch_size, self.n_agents, self.mixing_hidden_dim)\n\n        b1 = self.hyper_b1(state).view(batch_size, 1, self.mixing_hidden_dim)\n\n        # First layer\n        hidden = F.elu(torch.bmm(agent_qs.unsqueeze(1), w1) + b1)\n\n        # Second layer\n        w2 = torch.abs(self.hyper_w2(state))\n        w2 = w2.view(batch_size, self.mixing_hidden_dim, 1)\n\n        b2 = self.hyper_b2(state).view(batch_size, 1, 1)\n\n        # Output\n        q_tot = torch.bmm(hidden, w2) + b2\n\n        return q_tot.squeeze(-1).squeeze(-1)",
    "source_file": "core\\rl\\chinese_marl.py",
    "academic_reference": "arXiv:2009.11189",
    "class_name": "QMIXMixer"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "mean, log_std, value | logits, value",
    "explanation": "Forward pass returning action distribution params and value.",
    "python_code": "def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Forward pass returning action distribution params and value.\"\"\"\n        features = self.encoder(state)\n        value = self.value_head(features)\n\n        if self.continuous:\n            mean = torch.sigmoid(self.mean_head(features))  # Action in [0, 1]\n            log_std = torch.clamp(self.log_std_head(features), -5, 0)\n            return mean, log_std, value\n        else:\n            logits = self.action_head(features)\n            return logits, value",
    "source_file": "core\\rl\\chinese_marl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MARLAgent"
  },
  {
    "name": "get_action",
    "category": "execution",
    "formula": "mean | torch.clamp(action, 0.0, 1.0) | logits.argmax(dim=-1)",
    "explanation": "Get action for execution.",
    "python_code": "def get_action(self, state: torch.Tensor, deterministic: bool = False) -> torch.Tensor:\n        \"\"\"Get action for execution.\"\"\"\n        if self.continuous:\n            mean, log_std, _ = self.forward(state)\n            if deterministic:\n                return mean\n            std = log_std.exp()\n            dist = Normal(mean, std)\n            action = dist.sample()\n            return torch.clamp(action, 0.0, 1.0)\n        else:\n            logits, _ = self.forward(state)\n            if deterministic:\n                return logits.argmax(dim=-1)\n            dist = Categorical(logits=logits)\n            return dist.sample()",
    "source_file": "core\\rl\\chinese_marl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MARLAgent"
  },
  {
    "name": "select_actions",
    "category": "execution",
    "formula": "np.array(actions).flatten()",
    "explanation": "Select actions for all agents.",
    "python_code": "def select_actions(\n        self,\n        state: np.ndarray,\n        deterministic: bool = False,\n    ) -> np.ndarray:\n        \"\"\"Select actions for all agents.\"\"\"\n        actions = []\n\n        with torch.no_grad():\n            for i in range(self.n_agents):\n                # Extract agent's local observation\n                start_idx = i * self.state_dim_per_agent\n                end_idx = start_idx + self.state_dim_per_agent\n                agent_state = state[start_idx:end_idx]\n\n                agent_state_tensor = torch.FloatTensor(agent_state).unsqueeze(0).to(self.device)\n                action = self.agents[i].get_action(agent_state_tensor, deterministic)\n                actions.append(action.cpu().numpy()[0])\n\n        return np.array(actions).flatten()",
    "source_file": "core\\rl\\chinese_marl.py",
    "academic_reference": "arXiv:2009.11189",
    "class_name": "MARLOrderExecution"
  },
  {
    "name": "train_step",
    "category": "execution",
    "formula": "{'loss': loss.item()}",
    "explanation": "Train MARL agents with QMIX.\n\nReference: Rashid et al. (2018) QMIX",
    "python_code": "def train_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n        \"\"\"\n        Train MARL agents with QMIX.\n\n        Reference: Rashid et al. (2018) QMIX\n        \"\"\"\n        states = batch['states']\n        actions = batch['actions']\n        rewards = batch['rewards']\n        next_states = batch['next_states']\n        dones = batch['dones']\n\n        batch_size = states.size(0)\n\n        # Get individual Q-values\n        agent_qs = []\n        next_agent_qs = []\n\n        for i in range(self.n_agents):\n            start_idx = i * self.state_dim_per_agent\n            end_idx = start_idx + self.state_dim_per_agent\n\n            agent_state = states[:, start_idx:end_idx]\n            next_agent_state = next_states[:, start_idx:end_idx]\n\n            _, _, value = self.agents[i](agent_state)\n            agent_qs.append(value)\n\n            with torch.no_grad():\n                _, _, next_value = self.agents[i](next_agent_state)\n                next_agent_qs.append(next_value)\n\n        agent_qs = torch.cat(agent_qs, dim=-1)\n        next_agent_qs = torch.cat(next_agent_qs, dim=-1)\n\n        # QMIX\n        if self.config.use_qmix:\n            q_tot = self.mixer(agent_qs, states)\n\n            with torch.no_grad():\n                next_q_tot = self.target_mixer(next_agent_qs, next_states)\n                target_q_tot = rewards + self.config.gamma * (1 - dones) * next_q_tot\n\n            loss = F.mse_loss(q_tot, target_q_tot)\n        else:\n            # Simple sum\n            q_tot = agent_qs.sum(dim=-1)\n            with torch.no_grad():\n                next_q_tot = next_agent_qs.sum(dim=-1)\n                target_q_tot = rewards + self.config.gamma * (1 - dones) * next_q_tot\n            loss = F.mse_loss(q_tot, target_q_tot)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        nn.utils.clip_grad_norm_(self.agents.parameters(), 1.0)\n        self.optimizer.step()\n\n        # Soft update target mixer\n        if self.config.use_qmix:\n            for param, target_param in zip(self.mixer.parameters(), self.target_mixer.parameters()):\n                target_param.data.copy_(self.config.tau * param.data + (1 - self.config.tau) * target_param.data)\n\n        return {'loss': loss.item()}",
    "source_file": "core\\rl\\chinese_marl.py",
    "academic_reference": "arXiv:2009.11189",
    "class_name": "MARLOrderExecution"
  },
  {
    "name": "learn",
    "category": "execution",
    "formula": "",
    "explanation": "Main training loop.",
    "python_code": "def learn(self, total_timesteps: int, callback=None):\n        \"\"\"Main training loop.\"\"\"\n        state, _ = self.env.reset()\n        episode_reward = 0.0\n\n        for step in range(total_timesteps):\n            actions = self.select_actions(state)\n            next_state, reward, terminated, truncated, info = self.env.step(actions)\n            done = terminated or truncated\n\n            self.buffer.add(state, actions, reward, next_state, float(done))\n\n            state = next_state\n            episode_reward += reward\n\n            if done:\n                if callback:\n                    callback({\n                        'episode_reward': episode_reward,\n                        'implementation_shortfall': info.get('implementation_shortfall', 0),\n                    })\n                state, _ = self.env.reset()\n                episode_reward = 0.0\n\n            if self.buffer.size >= self.config.batch_size:\n                batch = self.buffer.sample(self.config.batch_size, self.device)\n                self.train_step(batch)",
    "source_file": "core\\rl\\chinese_marl.py",
    "academic_reference": "arXiv:2009.11189",
    "class_name": "MARLOrderExecution"
  },
  {
    "name": "save",
    "category": "execution",
    "formula": "",
    "explanation": "Save model.",
    "python_code": "def save(self, path: str):\n        \"\"\"Save model.\"\"\"\n        torch.save({\n            'agents': self.agents.state_dict(),\n            'mixer': self.mixer.state_dict() if self.config.use_qmix else None,\n        }, path)",
    "source_file": "core\\rl\\chinese_marl.py",
    "academic_reference": "arXiv:2009.11189",
    "class_name": "MARLOrderExecution"
  },
  {
    "name": "load",
    "category": "execution",
    "formula": "",
    "explanation": "Load model.",
    "python_code": "def load(self, path: str):\n        \"\"\"Load model.\"\"\"\n        checkpoint = torch.load(path, map_location=self.device)\n        self.agents.load_state_dict(checkpoint['agents'])\n        if self.config.use_qmix and checkpoint['mixer']:\n            self.mixer.load_state_dict(checkpoint['mixer'])",
    "source_file": "core\\rl\\chinese_marl.py",
    "academic_reference": "arXiv:2009.11189",
    "class_name": "MARLOrderExecution"
  },
  {
    "name": "add",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def add(self, state, actions, reward, next_state, done):\n        self.states[self.ptr] = state\n        self.actions[self.ptr] = actions\n        self.rewards[self.ptr] = reward\n        self.next_states[self.ptr] = next_state\n        self.dones[self.ptr] = done\n\n        self.ptr = (self.ptr + 1) % self.max_size\n        self.size = min(self.size + 1, self.max_size)",
    "source_file": "core\\rl\\chinese_marl.py",
    "academic_reference": "arXiv:2009.11189",
    "class_name": "MARLReplayBuffer"
  },
  {
    "name": "sample",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "",
    "python_code": "def sample(self, batch_size: int, device: str = 'cpu') -> Dict[str, torch.Tensor]:\n        idx = np.random.randint(0, self.size, size=batch_size)\n        return {\n            'states': torch.FloatTensor(self.states[idx]).to(device),\n            'actions': torch.FloatTensor(self.actions[idx]).to(device),\n            'rewards': torch.FloatTensor(self.rewards[idx]).to(device),\n            'next_states': torch.FloatTensor(self.next_states[idx]).to(device),\n            'dones': torch.FloatTensor(self.dones[idx]).to(device),\n        }",
    "source_file": "core\\rl\\chinese_marl.py",
    "academic_reference": "arXiv:2009.11189",
    "class_name": "MARLReplayBuffer"
  },
  {
    "name": "create_marl_order_execution",
    "category": "execution",
    "formula": "MARLOrderExecution(env, config)",
    "explanation": "Factory function to create MARL order execution system.\n\nArgs:\n    price_data: Price time series\n    n_agents: Number of agents (orders)\n    config: MARL configuration\n\nReturns:\n    MARLOrderExecution instance",
    "python_code": "def create_marl_order_execution(\n    price_data: np.ndarray,\n    n_agents: int = 5,\n    config: Optional[MARLConfig] = None,\n) -> MARLOrderExecution:\n    \"\"\"\n    Factory function to create MARL order execution system.\n\n    Args:\n        price_data: Price time series\n        n_agents: Number of agents (orders)\n        config: MARL configuration\n\n    Returns:\n        MARLOrderExecution instance\n    \"\"\"\n    config = config or MARLConfig(n_agents=n_agents)\n    env = OrderExecutionEnv(price_data, config)\n    return MARLOrderExecution(env, config)",
    "source_file": "core\\rl\\chinese_marl.py",
    "academic_reference": "arXiv:2009.11189",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, state_dim: int, action_dim: int, num_quantiles: int = 200, hidden_dim: int = 256):\n        super().__init__()\n\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.num_quantiles = num_quantiles\n\n        # Feature extraction\n        self.feature_net = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n        )\n\n        # Quantile value heads (one per action)\n        self.quantile_heads = nn.ModuleList([\n            nn.Linear(hidden_dim, num_quantiles) for _ in range(action_dim)\n        ])",
    "source_file": "core\\rl\\distributional_qrdqn.py",
    "academic_reference": "Mnih (2015) 'Human-level Control' Nature",
    "class_name": "QuantileNetwork"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "quantiles",
    "explanation": "Args:\n    state: (batch, state_dim)\n\nReturns:\n    quantiles: (batch, action_dim, num_quantiles)",
    "python_code": "def forward(self, state: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            state: (batch, state_dim)\n\n        Returns:\n            quantiles: (batch, action_dim, num_quantiles)\n        \"\"\"\n        features = self.feature_net(state)\n\n        # Compute quantile values for each action\n        quantiles = torch.stack([head(features) for head in self.quantile_heads], dim=1)\n\n        return quantiles",
    "source_file": "core\\rl\\distributional_qrdqn.py",
    "academic_reference": "Mnih (2015) 'Human-level Control' Nature",
    "class_name": "QuantileNetwork"
  },
  {
    "name": "act",
    "category": "reinforcement_learning",
    "formula": "value = mean of quantile values (center of distribution) | random.randint(0, self.action_dim - 1) | action",
    "explanation": "Select action using epsilon-greedy policy based on expected Q-value.\n\nExpected Q-value = mean of quantile values (center of distribution)\n\nArgs:\n    state: (state_dim,) or (1, state_dim)\n    epsilon: Exploration rate\n\nReturns:\n    action: Integer action index",
    "python_code": "def act(self, state: torch.Tensor, epsilon: float = 0.0) -> int:\n        \"\"\"\n        Select action using epsilon-greedy policy based on expected Q-value.\n\n        Expected Q-value = mean of quantile values (center of distribution)\n\n        Args:\n            state: (state_dim,) or (1, state_dim)\n            epsilon: Exploration rate\n\n        Returns:\n            action: Integer action index\n        \"\"\"\n        if random.random() < epsilon:\n            return random.randint(0, self.action_dim - 1)\n\n        with torch.no_grad():\n            if len(state.shape) == 1:\n                state = state.unsqueeze(0)\n\n            # Get quantile values for all actions\n            quantiles = self.q_network(state.to(self.device))  # (1, action_dim, num_quantiles)\n\n            # Expected Q-value = mean of quantiles\n            q_values = quantiles.mean(dim=2)  # (1, action_dim)\n\n            action = q_values.argmax(dim=1).item()\n\n        return action",
    "source_file": "core\\rl\\distributional_qrdqn.py",
    "academic_reference": "Mnih (2015) 'Human-level Control' Nature",
    "class_name": "QR_DQN_Agent"
  },
  {
    "name": "get_risk_sensitive_action",
    "category": "risk",
    "formula": "action",
    "explanation": "Select action using CVaR (Conditional Value at Risk).\n\nInstead of maximizing expected return, maximize CVaR_\nwhere  is the risk level (e.g., 0.95 for 95th percentile).\n\nArgs:\n    state: (state_dim,) or (1, state_dim)\n    risk_level: Confidence level (0.95 = optimize for worst 5% outcomes)\n\nReturns:\n    action: Integer action index",
    "python_code": "def get_risk_sensitive_action(\n        self,\n        state: torch.Tensor,\n        risk_level: float = 0.95\n    ) -> int:\n        \"\"\"\n        Select action using CVaR (Conditional Value at Risk).\n\n        Instead of maximizing expected return, maximize CVaR_\n        where  is the risk level (e.g., 0.95 for 95th percentile).\n\n        Args:\n            state: (state_dim,) or (1, state_dim)\n            risk_level: Confidence level (0.95 = optimize for worst 5% outcomes)\n\n        Returns:\n            action: Integer action index\n        \"\"\"\n        with torch.no_grad():\n            if len(state.shape) == 1:\n                state = state.unsqueeze(0)\n\n            quantiles = self.q_network(state.to(self.device))  # (1, action_dim, num_quantiles)\n\n            # Compute CVaR for each action\n            cutoff_idx = int(self.num_quantiles * (1 - risk_level))\n            cvar_values = quantiles[:, :, :cutoff_idx].mean(dim=2)  # (1, action_dim)\n\n            action = cvar_values.argmax(dim=1).item()\n\n        return action",
    "source_file": "core\\rl\\distributional_qrdqn.py",
    "academic_reference": "Mnih (2015) 'Human-level Control' Nature",
    "class_name": "QR_DQN_Agent"
  },
  {
    "name": "quantile_huber_loss",
    "category": "technical",
    "formula": "_^(u) = | - 1_{u<0}| * _(u) /  | _^(u) = | - 1_{u<0}| * _(u) /  | loss",
    "explanation": "Quantile Huber loss for QR-DQN.\n\nMathematical Formula:\n    _^(u) = | - 1_{u<0}| * _(u) / \n\n    where _(u) = {\n        0.5 * u^2,           if |u|  \n         * (|u| - 0.5*),  otherwise\n    }\n\nArgs:\n    pred: Predicted quantile values (batch, num_quantiles)\n    target: Target quantile values (batch, num_quantiles)\n    tau: Quantile fractions (num_quantiles,)\n\nReturns:\n    loss: Scalar loss value",
    "python_code": "def quantile_huber_loss(\n        self,\n        pred: torch.Tensor,\n        target: torch.Tensor,\n        tau: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"\n        Quantile Huber loss for QR-DQN.\n\n        Mathematical Formula:\n            _^(u) = | - 1_{u<0}| * _(u) / \n\n            where _(u) = {\n                0.5 * u^2,           if |u|  \n                 * (|u| - 0.5*),  otherwise\n            }\n\n        Args:\n            pred: Predicted quantile values (batch, num_quantiles)\n            target: Target quantile values (batch, num_quantiles)\n            tau: Quantile fractions (num_quantiles,)\n\n        Returns:\n            loss: Scalar loss value\n        \"\"\"\n        # Temporal difference error\n        error = target - pred  # (batch, num_quantiles)\n\n        # Huber loss component\n        huber_loss = torch.where(\n            torch.abs(error) <= self.kappa,\n            0.5 * error ** 2,\n            self.kappa * (torch.abs(error) - 0.5 * self.kappa)\n        )\n\n        # Quantile weighting: | - 1_{u<0}|\n        quantile_weight = torch.abs(tau.unsqueeze(0) - (error < 0).float())\n\n        # Combined quantile Huber loss\n        loss = (quantile_weight * huber_loss).mean()\n\n        return loss",
    "source_file": "core\\rl\\distributional_qrdqn.py",
    "academic_reference": "Mnih (2015) 'Human-level Control' Nature",
    "class_name": "QR_DQN_Agent"
  },
  {
    "name": "update",
    "category": "reinforcement_learning",
    "formula": "# Sample mini-batch",
    "explanation": "Store transition and perform learning update.\n\nArgs:\n    state: Current state\n    action: Action taken\n    reward: Reward received\n    next_state: Next state\n    done: Whether episode ended",
    "python_code": "def update(\n        self,\n        state: torch.Tensor,\n        action: int,\n        reward: float,\n        next_state: torch.Tensor,\n        done: bool\n    ):\n        \"\"\"\n        Store transition and perform learning update.\n\n        Args:\n            state: Current state\n            action: Action taken\n            reward: Reward received\n            next_state: Next state\n            done: Whether episode ended\n        \"\"\"\n        # Store transition\n        self.replay_buffer.append((state, action, reward, next_state, done))\n\n        # Only update if enough samples\n        if len(self.replay_buffer) < self.batch_size:\n            return\n\n        # Sample mini-batch\n        batch = random.sample(self.replay_buffer, self.batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n\n        states = torch.stack(states).to(self.device)\n        actions = torch.LongTensor(actions).to(self.device)\n        rewards = torch.FloatTensor(rewards).to(self.device)\n        next_states = torch.stack(next_states).to(self.device)\n        dones = torch.FloatTensor(dones).to(self.device)\n\n        # Compute current quantile values\n        current_quantiles = self.q_network(states)  # (batch, action_dim, num_quantiles)\n        current_quantiles = current_quantiles[torch.arange(self.batch_size), actions]  # (batch, num_quantiles)\n\n        # Compute target quantile values\n        with torch.no_grad():\n            next_quantiles = self.target_network(next_states)  # (batch, action_dim, num_quantiles)\n\n            # Greedy action selection based on expected Q-value\n            next_q_values = next_quantiles.mean(dim=2)  # (batch, action_dim)\n            next_actions = next_q_values.argmax(dim=1)  # (batch,)\n\n            next_quantiles = next_quantiles[torch.arange(self.batch_size), next_actions]  # (batch, num_quantiles)\n\n            # Bellman update for quantiles\n            target_quantiles = rewards.unsqueeze(1) + self.gamma * (1 - dones.unsqueeze(1)) * next_quantiles\n\n        # Compute loss\n        loss = self.quantile_huber_loss(current_quantiles, target_quantiles, self.quantile_tau)\n\n        # Optimize\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        # Soft update target network\n        self._soft_update()",
    "source_file": "core\\rl\\distributional_qrdqn.py",
    "academic_reference": "Mnih (2015) 'Human-level Control' Nature",
    "class_name": "QR_DQN_Agent"
  },
  {
    "name": "_soft_update",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Soft update of target network: '   + (1-)'",
    "python_code": "def _soft_update(self):\n        \"\"\"Soft update of target network: '   + (1-)'\"\"\"\n        for target_param, param in zip(self.target_network.parameters(), self.q_network.parameters()):\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)",
    "source_file": "core\\rl\\distributional_qrdqn.py",
    "academic_reference": "Mnih (2015) 'Human-level Control' Nature",
    "class_name": "QR_DQN_Agent"
  },
  {
    "name": "save",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Save model weights.",
    "python_code": "def save(self, path: str):\n        \"\"\"Save model weights.\"\"\"\n        torch.save({\n            'q_network': self.q_network.state_dict(),\n            'target_network': self.target_network.state_dict(),\n            'optimizer': self.optimizer.state_dict()\n        }, path)",
    "source_file": "core\\rl\\distributional_qrdqn.py",
    "academic_reference": "Mnih (2015) 'Human-level Control' Nature",
    "class_name": "QR_DQN_Agent"
  },
  {
    "name": "load",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Load model weights.",
    "python_code": "def load(self, path: str):\n        \"\"\"Load model weights.\"\"\"\n        checkpoint = torch.load(path, map_location=self.device)\n        self.q_network.load_state_dict(checkpoint['q_network'])\n        self.target_network.load_state_dict(checkpoint['target_network'])\n        self.optimizer.load_state_dict(checkpoint['optimizer'])",
    "source_file": "core\\rl\\distributional_qrdqn.py",
    "academic_reference": "Mnih (2015) 'Human-level Control' Nature",
    "class_name": "QR_DQN_Agent"
  },
  {
    "name": "predict",
    "category": "risk",
    "formula": "signal, action_name",
    "explanation": "Generate trading signal from features.\n\nArgs:\n    features: (575,) array of features\n    risk_level: If provided, use CVaR-based action selection\n\nReturns:\n    signal: -1 (sell), 0 (hold), +1 (buy)\n    action_name: 'SELL', 'HOLD', or 'BUY'",
    "python_code": "def predict(self, features: np.ndarray, risk_level: Optional[float] = None) -> Tuple[int, str]:\n        \"\"\"\n        Generate trading signal from features.\n\n        Args:\n            features: (575,) array of features\n            risk_level: If provided, use CVaR-based action selection\n\n        Returns:\n            signal: -1 (sell), 0 (hold), +1 (buy)\n            action_name: 'SELL', 'HOLD', or 'BUY'\n        \"\"\"\n        state = torch.FloatTensor(features)\n\n        if risk_level is not None:\n            action_idx = self.agent.get_risk_sensitive_action(state, risk_level)\n        else:\n            action_idx = self.agent.act(state, epsilon=0.0)\n\n        signal = self.action_map[action_idx]\n        action_name = self.action_names[action_idx]\n\n        return signal, action_name",
    "source_file": "core\\rl\\distributional_qrdqn.py",
    "academic_reference": "Mnih (2015) 'Human-level Control' Nature",
    "class_name": "QR_DQN_TradingWrapper"
  },
  {
    "name": "learn",
    "category": "reinforcement_learning",
    "formula": "next_features: Next state features",
    "explanation": "Update agent from trading experience.\n\nArgs:\n    features: Current state features\n    action: Action taken (-1, 0, or 1)\n    reward: Realized PnL or return\n    next_features: Next state features\n    done: Whether episode ended",
    "python_code": "def learn(\n        self,\n        features: np.ndarray,\n        action: int,\n        reward: float,\n        next_features: np.ndarray,\n        done: bool\n    ):\n        \"\"\"\n        Update agent from trading experience.\n\n        Args:\n            features: Current state features\n            action: Action taken (-1, 0, or 1)\n            reward: Realized PnL or return\n            next_features: Next state features\n            done: Whether episode ended\n        \"\"\"\n        # Map trading signal to DQN action\n        action_idx = {-1: 0, 0: 1, 1: 2}[action]\n\n        state = torch.FloatTensor(features)\n        next_state = torch.FloatTensor(next_features)\n\n        self.agent.update(state, action_idx, reward, next_state, done)",
    "source_file": "core\\rl\\distributional_qrdqn.py",
    "academic_reference": "Mnih (2015) 'Human-level Control' Nature",
    "class_name": "QR_DQN_TradingWrapper"
  },
  {
    "name": "__init__",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Initialize ensemble.\n\nArgs:\n    agents: List of trained RL agents\n    config: Ensemble configuration",
    "python_code": "def __init__(\n        self,\n        agents: List[BaseRLAgent],\n        config: Optional[EnsembleConfig] = None,\n    ):\n        \"\"\"\n        Initialize ensemble.\n\n        Args:\n            agents: List of trained RL agents\n            config: Ensemble configuration\n        \"\"\"\n        self.agents = agents\n        self.config = config or EnsembleConfig()\n        self.n_agents = len(agents)\n\n        # Track agent performance for UCB\n        self.agent_rewards = [[] for _ in range(self.n_agents)]\n        self.agent_selections = np.zeros(self.n_agents)",
    "source_file": "core\\rl\\ensemble.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RLEnsemble"
  },
  {
    "name": "select_action",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Select action using ensemble voting.\n\nArgs:\n    state: Current state\n    deterministic: If True, use deterministic actions\n\nReturns:\n    Ensemble action",
    "python_code": "def select_action(\n        self,\n        state: np.ndarray,\n        deterministic: bool = False,\n    ) -> np.ndarray:\n        \"\"\"\n        Select action using ensemble voting.\n\n        Args:\n            state: Current state\n            deterministic: If True, use deterministic actions\n\n        Returns:\n            Ensemble action\n        \"\"\"\n        # Get actions from all agents\n        actions = []\n        for agent in self.agents:\n            action = agent.select_action(state, deterministic=deterministic)\n            actions.append(action)\n\n        actions = np.array(actions)\n\n        # Apply voting method\n        if self.config.voting_method == 'mean':\n            return self._mean_vote(actions)\n        elif self.config.voting_method == 'median':\n            return self._median_vote(actions)\n        elif self.config.voting_method == 'vote':\n            return self._majority_vote(actions)\n        elif self.config.voting_method == 'ucb':\n            return self._ucb_select(actions)\n        else:\n            return self._mean_vote(actions)",
    "source_file": "core\\rl\\ensemble.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RLEnsemble"
  },
  {
    "name": "_mean_vote",
    "category": "machine_learning",
    "formula": "np.mean(actions, axis=0)",
    "explanation": "Average actions across agents.",
    "python_code": "def _mean_vote(self, actions: np.ndarray) -> np.ndarray:\n        \"\"\"Average actions across agents.\"\"\"\n        return np.mean(actions, axis=0)",
    "source_file": "core\\rl\\ensemble.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RLEnsemble"
  },
  {
    "name": "_median_vote",
    "category": "machine_learning",
    "formula": "np.median(actions, axis=0)",
    "explanation": "Median action (robust to outliers).",
    "python_code": "def _median_vote(self, actions: np.ndarray) -> np.ndarray:\n        \"\"\"Median action (robust to outliers).\"\"\"\n        return np.median(actions, axis=0)",
    "source_file": "core\\rl\\ensemble.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RLEnsemble"
  },
  {
    "name": "_majority_vote",
    "category": "machine_learning",
    "formula": "mode_result.mode",
    "explanation": "Majority vote for discrete actions.",
    "python_code": "def _majority_vote(self, actions: np.ndarray) -> np.ndarray:\n        \"\"\"Majority vote for discrete actions.\"\"\"\n        # Round to nearest discrete action\n        discrete_actions = np.round(actions)\n        # Mode across agents\n        from scipy import stats\n        mode_result = stats.mode(discrete_actions, axis=0, keepdims=False)\n        return mode_result.mode",
    "source_file": "core\\rl\\ensemble.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RLEnsemble"
  },
  {
    "name": "_ucb_select",
    "category": "reinforcement_learning",
    "formula": "UCB = mean_reward + c * sqrt(log(N) / n_i) | actions[best_agent]",
    "explanation": "UCB selection across agents.\n\nReference: Chen et al. (2017)\n\"UCB Exploration via Q-Ensembles\"\n\nUCB = mean_reward + c * sqrt(log(N) / n_i)",
    "python_code": "def _ucb_select(self, actions: np.ndarray) -> np.ndarray:\n        \"\"\"\n        UCB selection across agents.\n\n        Reference: Chen et al. (2017)\n        \"UCB Exploration via Q-Ensembles\"\n\n        UCB = mean_reward + c * sqrt(log(N) / n_i)\n        \"\"\"\n        total_selections = np.sum(self.agent_selections) + 1\n\n        ucb_values = []\n        for i in range(self.n_agents):\n            if len(self.agent_rewards[i]) > 0:\n                mean_reward = np.mean(self.agent_rewards[i])\n            else:\n                mean_reward = 0.0\n\n            n_i = self.agent_selections[i] + 1\n            exploration_bonus = self.config.ucb_c * np.sqrt(np.log(total_selections) / n_i)\n            ucb_values.append(mean_reward + exploration_bonus)\n\n        # Select agent with highest UCB\n        best_agent = np.argmax(ucb_values)\n        self.agent_selections[best_agent] += 1\n\n        return actions[best_agent]",
    "source_file": "core\\rl\\ensemble.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RLEnsemble"
  },
  {
    "name": "update_rewards",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Update reward tracking for UCB.",
    "python_code": "def update_rewards(self, agent_idx: int, reward: float):\n        \"\"\"Update reward tracking for UCB.\"\"\"\n        self.agent_rewards[agent_idx].append(reward)",
    "source_file": "core\\rl\\ensemble.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RLEnsemble"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Predict action (alias for select_action).",
    "python_code": "def predict(self, state: np.ndarray, deterministic: bool = True) -> np.ndarray:\n        \"\"\"Predict action (alias for select_action).\"\"\"\n        return self.select_action(state, deterministic=deterministic)",
    "source_file": "core\\rl\\ensemble.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RLEnsemble"
  },
  {
    "name": "evaluate_agent",
    "category": "machine_learning",
    "formula": "total_reward / n_episodes",
    "explanation": "Evaluate agent performance.\n\nReturns average episode return.",
    "python_code": "def evaluate_agent(self, agent: BaseRLAgent, n_episodes: int = 5) -> float:\n        \"\"\"\n        Evaluate agent performance.\n\n        Returns average episode return.\n        \"\"\"\n        total_reward = 0.0\n\n        for _ in range(n_episodes):\n            state, _ = self.env.reset()\n            episode_reward = 0.0\n            done = False\n\n            while not done:\n                action = agent.select_action(state, deterministic=True)\n                state, reward, terminated, truncated, _ = self.env.step(action)\n                episode_reward += reward\n                done = terminated or truncated\n\n            total_reward += episode_reward\n\n        return total_reward / n_episodes",
    "source_file": "core\\rl\\ensemble.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TournamentEnsemble"
  },
  {
    "name": "tournament_selection",
    "category": "machine_learning",
    "formula": "keep_indices",
    "explanation": "Perform tournament selection.\n\nReturns indices of agents to keep.",
    "python_code": "def tournament_selection(self) -> List[int]:\n        \"\"\"\n        Perform tournament selection.\n\n        Returns indices of agents to keep.\n        \"\"\"\n        # Evaluate all agents\n        for i, agent in enumerate(self.agents):\n            self.agent_scores[i] = self.evaluate_agent(agent)\n\n        # Sort by score (descending)\n        sorted_indices = np.argsort(self.agent_scores)[::-1]\n\n        # Keep top performers\n        n_keep = self.config.n_agents - self.config.tournament_size\n        keep_indices = sorted_indices[:n_keep].tolist()\n\n        return keep_indices",
    "source_file": "core\\rl\\ensemble.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TournamentEnsemble"
  },
  {
    "name": "mutate_agent",
    "category": "machine_learning",
    "formula": "new_agent",
    "explanation": "Create mutated copy of agent.\n\nAdds Gaussian noise to network weights.",
    "python_code": "def mutate_agent(self, agent: BaseRLAgent, mutation_rate: float = 0.1) -> BaseRLAgent:\n        \"\"\"\n        Create mutated copy of agent.\n\n        Adds Gaussian noise to network weights.\n        \"\"\"\n        new_agent = create_agent(\n            self.config.agent_type,\n            self.env,\n            self.agent_config,\n        )\n\n        # Copy weights with mutation\n        state_dict = agent.state_dict()\n        for key, value in state_dict.items():\n            if isinstance(value, torch.Tensor) and value.dtype == torch.float32:\n                noise = torch.randn_like(value) * mutation_rate\n                state_dict[key] = value + noise\n\n        new_agent.load_state_dict(state_dict)\n        return new_agent",
    "source_file": "core\\rl\\ensemble.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TournamentEnsemble"
  },
  {
    "name": "evolve",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Perform one evolution step.\n\n1. Tournament selection\n2. Replace losers with mutations of winners",
    "python_code": "def evolve(self):\n        \"\"\"\n        Perform one evolution step.\n\n        1. Tournament selection\n        2. Replace losers with mutations of winners\n        \"\"\"\n        # Get indices to keep\n        keep_indices = self.tournament_selection()\n\n        # Replace worst agents\n        new_agents = []\n        for i in range(self.config.n_agents):\n            if i in keep_indices:\n                new_agents.append(self.agents[i])\n            else:\n                # Mutate a random winner\n                parent_idx = np.random.choice(keep_indices)\n                mutated = self.mutate_agent(self.agents[parent_idx])\n                new_agents.append(mutated)\n\n        self.agents = new_agents\n        self.generation += 1",
    "source_file": "core\\rl\\ensemble.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TournamentEnsemble"
  },
  {
    "name": "train",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Train ensemble with tournament selection.\n\nArgs:\n    total_timesteps: Total training steps\n    callback: Optional callback function",
    "python_code": "def train(self, total_timesteps: int, callback=None):\n        \"\"\"\n        Train ensemble with tournament selection.\n\n        Args:\n            total_timesteps: Total training steps\n            callback: Optional callback function\n        \"\"\"\n        steps_per_agent = total_timesteps // self.config.n_agents\n\n        while self.total_steps < total_timesteps:\n            # Train each agent for a batch of steps\n            for agent in self.agents:\n                agent.learn(\n                    self.config.selection_frequency // self.config.n_agents,\n                    callback=callback,\n                )\n                self.total_steps += self.config.selection_frequency // self.config.n_agents\n\n            # Perform tournament selection\n            self.evolve()\n\n            if callback:\n                callback({\n                    'generation': self.generation,\n                    'total_steps': self.total_steps,\n                    'best_score': np.max(self.agent_scores),\n                    'mean_score': np.mean(self.agent_scores),\n                })",
    "source_file": "core\\rl\\ensemble.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TournamentEnsemble"
  },
  {
    "name": "get_best_agent",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Get the best performing agent.",
    "python_code": "def get_best_agent(self) -> BaseRLAgent:\n        \"\"\"Get the best performing agent.\"\"\"\n        # Evaluate all agents\n        for i, agent in enumerate(self.agents):\n            self.agent_scores[i] = self.evaluate_agent(agent)\n\n        best_idx = np.argmax(self.agent_scores)\n        return self.agents[best_idx]",
    "source_file": "core\\rl\\ensemble.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TournamentEnsemble"
  },
  {
    "name": "select_action",
    "category": "machine_learning",
    "formula": "best_agent.select_action(state, deterministic=deterministic)",
    "explanation": "Select action using best agent.",
    "python_code": "def select_action(self, state: np.ndarray, deterministic: bool = False) -> np.ndarray:\n        \"\"\"Select action using best agent.\"\"\"\n        best_agent = self.get_best_agent()\n        return best_agent.select_action(state, deterministic=deterministic)",
    "source_file": "core\\rl\\ensemble.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TournamentEnsemble"
  },
  {
    "name": "distill_step",
    "category": "reinforcement_learning",
    "formula": "Loss = KL(student || teacher_ensemble) | distillation_loss",
    "explanation": "Perform one distillation step.\n\nReference: Hinton et al. (2015)\n\"Distilling the Knowledge in a Neural Network\"\n\nLoss = KL(student || teacher_ensemble)",
    "python_code": "def distill_step(\n        self,\n        states: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"\n        Perform one distillation step.\n\n        Reference: Hinton et al. (2015)\n        \"Distilling the Knowledge in a Neural Network\"\n\n        Loss = KL(student || teacher_ensemble)\n        \"\"\"\n        # Get teacher actions\n        teacher_actions = []\n        for teacher in self.teachers:\n            with torch.no_grad():\n                action = teacher.actor.get_action(states, deterministic=False)\n                teacher_actions.append(action)\n\n        # Average teacher actions (soft labels)\n        teacher_mean = torch.stack(teacher_actions).mean(dim=0)\n\n        # Get student action distribution\n        mean, log_std = self.student.actor(states)\n        std = log_std.exp()\n\n        # KL divergence loss (simplified for Gaussian)\n        # KL(student || teacher)  0.5 * ||mean_s - mean_t||^2 / T^2\n        T = self.config.distillation_temperature\n        distillation_loss = 0.5 * ((mean - teacher_mean) ** 2).mean() / (T ** 2)\n\n        return distillation_loss",
    "source_file": "core\\rl\\ensemble.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DistilledEnsemble"
  },
  {
    "name": "create_rl_ensemble",
    "category": "reinforcement_learning",
    "formula": "TournamentEnsemble(env, config, agent_config) | RLEnsemble(agents, config)",
    "explanation": "Factory function to create RL ensemble.\n\nArgs:\n    env: Training environment\n    config: Ensemble configuration\n    agent_config: Agent configuration\n    ensemble_type: 'basic', 'tournament', or 'distilled'\n\nReturns:\n    RL ensemble",
    "python_code": "def create_rl_ensemble(\n    env: gym.Env,\n    config: Optional[EnsembleConfig] = None,\n    agent_config: Optional[AgentConfig] = None,\n    ensemble_type: str = 'tournament',\n) -> Union[RLEnsemble, TournamentEnsemble]:\n    \"\"\"\n    Factory function to create RL ensemble.\n\n    Args:\n        env: Training environment\n        config: Ensemble configuration\n        agent_config: Agent configuration\n        ensemble_type: 'basic', 'tournament', or 'distilled'\n\n    Returns:\n        RL ensemble\n    \"\"\"\n    config = config or EnsembleConfig()\n    agent_config = agent_config or AgentConfig()\n\n    if ensemble_type == 'tournament':\n        return TournamentEnsemble(env, config, agent_config)\n    elif ensemble_type == 'basic':\n        # Create basic ensemble with pre-trained agents\n        agents = []\n        for _ in range(config.n_agents):\n            agent = create_agent(config.agent_type, env, agent_config)\n            agents.append(agent)\n        return RLEnsemble(agents, config)\n    else:\n        raise ValueError(f\"Unknown ensemble type: {ensemble_type}\")",
    "source_file": "core\\rl\\ensemble.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "act",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Select action given state.\n\nArgs:\n    state: Current state (features)\n    deterministic: If True, select greedy action\n\nReturns:\n    action: Continuous action in [-1, 1] (position size)",
    "python_code": "def act(self, state: np.ndarray, deterministic: bool = True) -> float:\n        \"\"\"\n        Select action given state.\n\n        Args:\n            state: Current state (features)\n            deterministic: If True, select greedy action\n\n        Returns:\n            action: Continuous action in [-1, 1] (position size)\n        \"\"\"\n        raise NotImplementedError",
    "source_file": "core\\rl\\ensemble_dynamic.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RLAgentBase"
  },
  {
    "name": "update",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Update agent from experience.",
    "python_code": "def update(self, state, action, reward, next_state, done):\n        \"\"\"Update agent from experience.\"\"\"\n        raise NotImplementedError",
    "source_file": "core\\rl\\ensemble_dynamic.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RLAgentBase"
  },
  {
    "name": "__init__",
    "category": "reinforcement_learning",
    "formula": "higher = more selective)",
    "explanation": "Args:\n    agents: List of RL agents (PPO, SAC, TD3, A2C, DDPG)\n    beta: Temperature parameter for softmax weighting (higher = more selective)\n    window_size: Number of recent returns to track per agent\n    min_samples: Minimum samples before using performance-based weighting",
    "python_code": "def __init__(\n        self,\n        agents: List[RLAgentBase],\n        beta: float = 2.0,\n        window_size: int = 100,\n        min_samples: int = 20\n    ):\n        \"\"\"\n        Args:\n            agents: List of RL agents (PPO, SAC, TD3, A2C, DDPG)\n            beta: Temperature parameter for softmax weighting (higher = more selective)\n            window_size: Number of recent returns to track per agent\n            min_samples: Minimum samples before using performance-based weighting\n        \"\"\"\n        self.agents = agents\n        self.beta = beta\n        self.window_size = window_size\n        self.min_samples = min_samples\n\n        # Track performance for each agent\n        self.performance = [AgentPerformance() for _ in agents]\n\n        # Track ensemble performance\n        self.ensemble_returns = deque(maxlen=window_size)",
    "source_file": "core\\rl\\ensemble_dynamic.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DynamicEnsemble"
  },
  {
    "name": "_compute_sharpe_ratio",
    "category": "risk",
    "formula": "Sharpe = mean(returns) / std(returns) * sqrt(252)  # Annualized | 0.0 | = np.mean(returns_array)",
    "explanation": "Compute Sharpe ratio from returns.\n\nSharpe = mean(returns) / std(returns) * sqrt(252)  # Annualized",
    "python_code": "def _compute_sharpe_ratio(self, returns: List[float]) -> float:\n        \"\"\"\n        Compute Sharpe ratio from returns.\n\n        Sharpe = mean(returns) / std(returns) * sqrt(252)  # Annualized\n        \"\"\"\n        if len(returns) < 2:\n            return 0.0\n\n        returns_array = np.array(returns)\n        mean_return = np.mean(returns_array)\n        std_return = np.std(returns_array)\n\n        if std_return == 0:\n            return 0.0\n\n        # Annualized Sharpe (assuming daily returns)\n        sharpe = (mean_return / std_return) * np.sqrt(252)\n\n        return sharpe",
    "source_file": "core\\rl\\ensemble_dynamic.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DynamicEnsemble"
  },
  {
    "name": "_compute_weights",
    "category": "risk",
    "formula": "w_i = exp(Sharpe_i) / _j exp(Sharpe_j) | w_i = exp(Sharpe_i) / _j exp(Sharpe_j) | w_i = exp(Sharpe_i) / _j exp(Sharpe_j)",
    "explanation": "Compute agent weights using softmax over Sharpe ratios.\n\nMathematical Formula:\n    w_i = exp(Sharpe_i) / _j exp(Sharpe_j)\n\nReturns:\n    weights: (num_agents,) array of weights summing to 1",
    "python_code": "def _compute_weights(self) -> np.ndarray:\n        \"\"\"\n        Compute agent weights using softmax over Sharpe ratios.\n\n        Mathematical Formula:\n            w_i = exp(Sharpe_i) / _j exp(Sharpe_j)\n\n        Returns:\n            weights: (num_agents,) array of weights summing to 1\n        \"\"\"\n        sharpes = []\n        for perf in self.performance:\n            if len(perf.recent_returns) >= self.min_samples:\n                sharpe = self._compute_sharpe_ratio(list(perf.recent_returns))\n            else:\n                sharpe = 0.0\n            sharpes.append(sharpe)\n\n        sharpes = np.array(sharpes)\n\n        # Softmax with temperature \n        exp_sharpes = np.exp(self.beta * sharpes)\n        weights = exp_sharpes / np.sum(exp_sharpes)\n\n        return weights",
    "source_file": "core\\rl\\ensemble_dynamic.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DynamicEnsemble"
  },
  {
    "name": "act",
    "category": "machine_learning",
    "formula": "agent weights | ensemble_action, weights | ensemble_action",
    "explanation": "Get ensemble action as weighted average of individual agents.\n\nArgs:\n    state: Current state (features)\n    deterministic: If True, use greedy actions\n    return_weights: If True, return agent weights\n\nReturns:\n    action: Continuous action in [-1, 1]\n    weights: (optional) Agent weights used",
    "python_code": "def act(\n        self,\n        state: np.ndarray,\n        deterministic: bool = True,\n        return_weights: bool = False\n    ) -> Tuple[float, Optional[np.ndarray]]:\n        \"\"\"\n        Get ensemble action as weighted average of individual agents.\n\n        Args:\n            state: Current state (features)\n            deterministic: If True, use greedy actions\n            return_weights: If True, return agent weights\n\n        Returns:\n            action: Continuous action in [-1, 1]\n            weights: (optional) Agent weights used\n        \"\"\"\n        # Get actions from all agents\n        actions = np.array([agent.act(state, deterministic) for agent in self.agents])\n\n        # Compute weights\n        weights = self._compute_weights()\n\n        # Weighted average\n        ensemble_action = np.sum(weights * actions)\n\n        # Clip to [-1, 1]\n        ensemble_action = np.clip(ensemble_action, -1.0, 1.0)\n\n        if return_weights:\n            return ensemble_action, weights\n        return ensemble_action",
    "source_file": "core\\rl\\ensemble_dynamic.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DynamicEnsemble"
  },
  {
    "name": "get_best_agent_action",
    "category": "machine_learning",
    "formula": "action, best_idx, agent_name",
    "explanation": "Get action from currently best-performing agent.\n\nReturns:\n    action: Action from best agent\n    agent_idx: Index of best agent\n    agent_name: Name of best agent",
    "python_code": "def get_best_agent_action(self, state: np.ndarray) -> Tuple[float, int, str]:\n        \"\"\"\n        Get action from currently best-performing agent.\n\n        Returns:\n            action: Action from best agent\n            agent_idx: Index of best agent\n            agent_name: Name of best agent\n        \"\"\"\n        weights = self._compute_weights()\n        best_idx = np.argmax(weights)\n        best_agent = self.agents[best_idx]\n\n        action = best_agent.act(state, deterministic=True)\n\n        agent_name = type(best_agent).__name__\n\n        return action, best_idx, agent_name",
    "source_file": "core\\rl\\ensemble_dynamic.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DynamicEnsemble"
  },
  {
    "name": "get_performance_summary",
    "category": "machine_learning",
    "formula": "summary",
    "explanation": "Get performance summary for ensemble and individual agents.\n\nReturns:\n    summary: Dict with performance metrics",
    "python_code": "def get_performance_summary(self) -> Dict:\n        \"\"\"\n        Get performance summary for ensemble and individual agents.\n\n        Returns:\n            summary: Dict with performance metrics\n        \"\"\"\n        weights = self._compute_weights()\n\n        ensemble_sharpe = self._compute_sharpe_ratio(list(self.ensemble_returns))\n\n        summary = {\n            'ensemble': {\n                'sharpe_ratio': ensemble_sharpe,\n                'total_return': sum(self.ensemble_returns),\n                'num_trades': len(self.ensemble_returns)\n            },\n            'agents': []\n        }\n\n        for i, (agent, perf, weight) in enumerate(zip(self.agents, self.performance, weights)):\n            agent_summary = {\n                'name': type(agent).__name__,\n                'sharpe_ratio': perf.sharpe_ratio,\n                'weight': weight,\n                'total_return': sum(perf.recent_returns),\n                'num_samples': len(perf.recent_returns)\n            }\n            summary['agents'].append(agent_summary)\n\n        return summary",
    "source_file": "core\\rl\\ensemble_dynamic.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DynamicEnsemble"
  },
  {
    "name": "save",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Save ensemble and all agents.",
    "python_code": "def save(self, path: str):\n        \"\"\"Save ensemble and all agents.\"\"\"\n        torch.save({\n            'agents': [agent.save_dict() for agent in self.agents],\n            'performance': self.performance,\n            'ensemble_returns': list(self.ensemble_returns)\n        }, path)",
    "source_file": "core\\rl\\ensemble_dynamic.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DynamicEnsemble"
  },
  {
    "name": "load",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Load ensemble and all agents.",
    "python_code": "def load(self, path: str):\n        \"\"\"Load ensemble and all agents.\"\"\"\n        checkpoint = torch.load(path)\n        for agent, agent_state in zip(self.agents, checkpoint['agents']):\n            agent.load_dict(agent_state)\n        self.performance = checkpoint['performance']\n        self.ensemble_returns = deque(checkpoint['ensemble_returns'], maxlen=self.window_size)",
    "source_file": "core\\rl\\ensemble_dynamic.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DynamicEnsemble"
  },
  {
    "name": "__init__",
    "category": "machine_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(\n        self,\n        agents: List[RLAgentBase],\n        beta: float = 2.0,\n        sentiment_window: int = 50\n    ):\n        super().__init__(agents, beta)\n        self.sentiment_window = sentiment_window\n        self.sentiment_history = deque(maxlen=sentiment_window)",
    "source_file": "core\\rl\\ensemble_dynamic.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "SentimentBasedEnsemble"
  },
  {
    "name": "update_sentiment",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Update market sentiment score.\n\nArgs:\n    market_sentiment: Score in [-1, 1] where:\n        -1 = very bearish\n         0 = neutral\n        +1 = very bullish",
    "python_code": "def update_sentiment(self, market_sentiment: float):\n        \"\"\"\n        Update market sentiment score.\n\n        Args:\n            market_sentiment: Score in [-1, 1] where:\n                -1 = very bearish\n                 0 = neutral\n                +1 = very bullish\n        \"\"\"\n        self.sentiment_history.append(market_sentiment)",
    "source_file": "core\\rl\\ensemble_dynamic.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "SentimentBasedEnsemble"
  },
  {
    "name": "_get_sentiment_match_score",
    "category": "machine_learning",
    "formula": "Higher = better match to sentiment | 1.0  # Neutral if not enough data | match",
    "explanation": "Compute how well agent's style matches current sentiment.\n\nThis is a heuristic - in practice, learn this from data.\n\nArgs:\n    agent_idx: Index of agent\n\nReturns:\n    match_score: Higher = better match to sentiment",
    "python_code": "def _get_sentiment_match_score(self, agent_idx: int) -> float:\n        \"\"\"\n        Compute how well agent's style matches current sentiment.\n\n        This is a heuristic - in practice, learn this from data.\n\n        Args:\n            agent_idx: Index of agent\n\n        Returns:\n            match_score: Higher = better match to sentiment\n        \"\"\"\n        if len(self.sentiment_history) < 10:\n            return 1.0  # Neutral if not enough data\n\n        recent_sentiment = np.mean(list(self.sentiment_history))\n\n        # Example heuristics:\n        # - SAC (entropy-seeking) good in neutral/choppy markets (sentiment ~ 0)\n        # - PPO (stable) good in all markets\n        # - TD3 (deterministic) good in trending markets (|sentiment| > 0.5)\n\n        agent_type = type(self.agents[agent_idx]).__name__\n\n        if 'SAC' in agent_type:\n            match = 1.0 - abs(recent_sentiment)  # Higher when sentiment near 0\n        elif 'TD3' in agent_type or 'DDPG' in agent_type:\n            match = abs(recent_sentiment)  # Higher in trending markets\n        else:  # PPO, A2C - stable in all conditions\n            match = 1.0\n\n        return match",
    "source_file": "core\\rl\\ensemble_dynamic.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "SentimentBasedEnsemble"
  },
  {
    "name": "_compute_weights",
    "category": "risk",
    "formula": "w_i = exp((Sharpe_i + Sentiment_match_i)) / _j exp(...) | w_i = exp((Sharpe_i + Sentiment_match_i)) / _j exp(...) | w_i = exp((Sharpe_i + Sentiment_match_i)) / _j exp(...)",
    "explanation": "Compute weights with sentiment adjustment.\n\nMathematical Formula:\n    w_i = exp((Sharpe_i + Sentiment_match_i)) / _j exp(...)\n\nwhere  controls sentiment influence",
    "python_code": "def _compute_weights(self) -> np.ndarray:\n        \"\"\"\n        Compute weights with sentiment adjustment.\n\n        Mathematical Formula:\n            w_i = exp((Sharpe_i + Sentiment_match_i)) / _j exp(...)\n\n        where  controls sentiment influence\n        \"\"\"\n        gamma = 0.5  # Sentiment weight\n\n        sharpes = []\n        for i, perf in enumerate(self.performance):\n            if len(perf.recent_returns) >= self.min_samples:\n                sharpe = self._compute_sharpe_ratio(list(perf.recent_returns))\n                sentiment_match = self._get_sentiment_match_score(i)\n                combined_score = sharpe + gamma * sentiment_match\n            else:\n                combined_score = 0.0\n            sharpes.append(combined_score)\n\n        sharpes = np.array(sharpes)\n\n        # Softmax\n        exp_sharpes = np.exp(self.beta * sharpes)\n        weights = exp_sharpes / np.sum(exp_sharpes)\n\n        return weights",
    "source_file": "core\\rl\\ensemble_dynamic.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "SentimentBasedEnsemble"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "signal, confidence",
    "explanation": "Generate trading signal from features.\n\nArgs:\n    features: (575,) array\n\nReturns:\n    signal: -1 (sell), 0 (hold), 1 (buy)\n    confidence: Strength of signal [0, 1]",
    "python_code": "def predict(self, features: np.ndarray) -> Tuple[int, float]:\n        \"\"\"\n        Generate trading signal from features.\n\n        Args:\n            features: (575,) array\n\n        Returns:\n            signal: -1 (sell), 0 (hold), 1 (buy)\n            confidence: Strength of signal [0, 1]\n        \"\"\"\n        action, weights = self.ensemble.act(features, return_weights=True)\n\n        # Map continuous action to discrete signal\n        if action > 0.2:\n            signal = 1  # BUY\n        elif action < -0.2:\n            signal = -1  # SELL\n        else:\n            signal = 0  # HOLD\n\n        # Confidence = magnitude of action\n        confidence = abs(action)\n\n        return signal, confidence",
    "source_file": "core\\rl\\ensemble_dynamic.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "EnsembleRLTrader"
  },
  {
    "name": "learn",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Update ensemble from experience.",
    "python_code": "def learn(self, features, action, reward, next_features, done):\n        \"\"\"Update ensemble from experience.\"\"\"\n        self.ensemble.update(features, action, reward, next_features, done)",
    "source_file": "core\\rl\\ensemble_dynamic.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "EnsembleRLTrader"
  },
  {
    "name": "get_summary",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Get performance summary.",
    "python_code": "def get_summary(self) -> Dict:\n        \"\"\"Get performance summary.\"\"\"\n        return self.ensemble.get_performance_summary()",
    "source_file": "core\\rl\\ensemble_dynamic.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "EnsembleRLTrader"
  },
  {
    "name": "__init__",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Initialize Forex Trading Environment.\n\nArgs:\n    df: DataFrame with OHLCV + features, index should be datetime\n    config: Environment configuration\n    feature_columns: List of feature columns to use (default: all except OHLCV)\n    render_mode: Rendering mode ('human' or 'rgb_array')",
    "python_code": "def __init__(\n        self,\n        df: pd.DataFrame,\n        config: Optional[ForexTradingEnvConfig] = None,\n        feature_columns: Optional[List[str]] = None,\n        render_mode: Optional[str] = None,\n    ):\n        \"\"\"\n        Initialize Forex Trading Environment.\n\n        Args:\n            df: DataFrame with OHLCV + features, index should be datetime\n            config: Environment configuration\n            feature_columns: List of feature columns to use (default: all except OHLCV)\n            render_mode: Rendering mode ('human' or 'rgb_array')\n        \"\"\"\n        super().__init__()\n\n        self.config = config or ForexTradingEnvConfig()\n        self.render_mode = render_mode\n\n        # Store data\n        self.df = df.copy()\n        self.prices = df['close'].values\n\n        # Determine feature columns\n        ohlcv_cols = ['open', 'high', 'low', 'close', 'volume']\n        if feature_columns is None:\n            self.feature_columns = [c for c in df.columns if c not in ohlcv_cols]\n        else:\n            self.feature_columns = feature_columns\n\n        # Precompute features matrix\n        if self.feature_columns:\n            self.features = df[self.feature_columns].values\n        else:\n            # Use returns if no features\n            self.features = np.diff(np.log(self.prices), prepend=0).reshape(-1, 1)\n\n        # Normalize features (z-score)\n        self.feature_mean = np.nanmean(self.features, axis=0)\n        self.feature_std = np.nanstd(self.features, axis=0) + 1e-8\n        self.features_normalized = (self.features - self.feature_mean) / self.feature_std\n\n        # State dimension\n        self.n_features = self.features.shape[1]\n        extra_features = 0\n        if self.config.include_position:\n            extra_features += 1\n        if self.config.include_pnl:\n            extra_features += 1\n        if self.config.include_time:\n            extra_features += 2  # hour, day_of_week\n\n        self.state_dim = self.n_features + extra_features\n\n        # Define spaces\n        self.observation_space = spaces.Box(\n            low=-np.inf,\n            high=np.inf,\n            shape=(self.state_dim,),\n            dtype=np.float32\n        )\n\n        if self.config.action_type == 'continuous':\n            self.action_space = spaces.Box(\n                low=-1.0,\n                high=1.0,\n                shape=(1,),\n                dtype=np.float32\n            )\n        else:\n            self.action_space = spaces.Discrete(self.config.discrete_actions)\n\n        # Episode state\n        self.current_step = 0\n        self.position = 0.0\n        self.capital = self.config.initial_capital\n        self.portfolio_value = self.config.initial_capital\n        self.entry_price = 0.0\n        self.returns_history = []\n        self.max_portfolio_value = self.config.initial_capital\n\n        # Metrics\n        self.trades = 0\n        self.wins = 0\n        self.total_pnl = 0.0",
    "source_file": "core\\rl\\environments.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ForexTradingEnv"
  },
  {
    "name": "_get_state",
    "category": "reinforcement_learning",
    "formula": "state.astype(np.float32)",
    "explanation": "Get current state observation.",
    "python_code": "def _get_state(self) -> np.ndarray:\n        \"\"\"Get current state observation.\"\"\"\n        # Base features (normalized)\n        state = self.features_normalized[self.current_step].copy()\n\n        # Replace NaN with 0\n        state = np.nan_to_num(state, nan=0.0, posinf=0.0, neginf=0.0)\n\n        extra = []\n\n        if self.config.include_position:\n            extra.append(self.position)\n\n        if self.config.include_pnl:\n            # Unrealized PnL as fraction of capital\n            if self.position != 0 and self.entry_price > 0:\n                unrealized = self.position * (self.prices[self.current_step] - self.entry_price)\n                unrealized_pct = unrealized / self.config.initial_capital\n            else:\n                unrealized_pct = 0.0\n            extra.append(unrealized_pct)\n\n        if self.config.include_time:\n            # Extract time features if index is datetime\n            if hasattr(self.df.index, 'hour'):\n                hour = self.df.index[self.current_step].hour / 24.0\n                dow = self.df.index[self.current_step].dayofweek / 7.0\n            else:\n                hour = 0.0\n                dow = 0.0\n            extra.extend([hour, dow])\n\n        if extra:\n            state = np.concatenate([state, extra])\n\n        return state.astype(np.float32)",
    "source_file": "core\\rl\\environments.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ForexTradingEnv"
  },
  {
    "name": "_calculate_reward",
    "category": "risk",
    "formula": "Reward = R_t -  * transaction_cost | and  penalizes trading costs | ret = (new_value - old_value) / old_value if old_value > 0 else 0",
    "explanation": "Calculate reward using Differential Sharpe Ratio.\n\nReference:\n- Moody & Saffell (2001): \"Learning to Trade via Direct Reinforcement\"\n  Management Science, 47(8), 1066-1085\n\nReward = R_t -  * transaction_cost\nwhere R_t is the return and  penalizes trading costs",
    "python_code": "def _calculate_reward(self, old_value: float, new_value: float, action: float) -> float:\n        \"\"\"\n        Calculate reward using Differential Sharpe Ratio.\n\n        Reference:\n        - Moody & Saffell (2001): \"Learning to Trade via Direct Reinforcement\"\n          Management Science, 47(8), 1066-1085\n\n        Reward = R_t -  * transaction_cost\n        where R_t is the return and  penalizes trading costs\n        \"\"\"\n        # Portfolio return\n        ret = (new_value - old_value) / old_value if old_value > 0 else 0\n\n        # Track returns for Sharpe calculation\n        self.returns_history.append(ret)\n        if len(self.returns_history) > self.config.sharpe_window:\n            self.returns_history.pop(0)\n\n        # Differential Sharpe Ratio component\n        if len(self.returns_history) >= 2:\n            returns = np.array(self.returns_history)\n            excess_returns = returns - self.config.risk_free_rate\n            mean_ret = np.mean(excess_returns)\n            std_ret = np.std(excess_returns) + 1e-8\n            sharpe_component = mean_ret / std_ret\n        else:\n            sharpe_component = ret\n\n        # Transaction cost penalty\n        cost_penalty = abs(action) * self.config.transaction_cost_pct\n\n        # Combined reward\n        reward = sharpe_component - cost_penalty\n\n        # Scale reward\n        reward *= self.config.reward_scaling\n\n        return reward",
    "source_file": "core\\rl\\environments.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ForexTradingEnv"
  },
  {
    "name": "_execute_trade",
    "category": "microstructure",
    "formula": "0.0, 0.0 | transaction_cost, slippage",
    "explanation": "Execute trade with transaction costs and slippage.\n\nReference:\n- Almgren & Chriss (2001): Optimal Execution\n- Kyle (1985): Market Impact\n\nReturns:\n    Tuple of (transaction_cost, slippage_cost)",
    "python_code": "def _execute_trade(self, action: float) -> Tuple[float, float]:\n        \"\"\"\n        Execute trade with transaction costs and slippage.\n\n        Reference:\n        - Almgren & Chriss (2001): Optimal Execution\n        - Kyle (1985): Market Impact\n\n        Returns:\n            Tuple of (transaction_cost, slippage_cost)\n        \"\"\"\n        current_price = self.prices[self.current_step]\n\n        # Convert discrete action to continuous\n        if self.config.action_type == 'discrete':\n            action = (action - (self.config.discrete_actions - 1) / 2) / ((self.config.discrete_actions - 1) / 2)\n\n        # Target position\n        target_position = action * self.config.max_position\n\n        # Position change\n        position_change = target_position - self.position\n\n        if abs(position_change) < 1e-6:\n            return 0.0, 0.0\n\n        # Transaction cost (Almgren & Chriss)\n        trade_value = abs(position_change) * current_price * self.capital / self.config.initial_capital\n        transaction_cost = trade_value * self.config.transaction_cost_pct\n\n        # Slippage (Kyle's Lambda simplified)\n        slippage = trade_value * self.config.slippage_pct * np.sign(position_change)\n\n        # Update position\n        if position_change > 0:\n            # Buying\n            self.entry_price = current_price * (1 + self.config.slippage_pct)\n        elif position_change < 0 and self.position > 0:\n            # Selling long position\n            pnl = self.position * (current_price * (1 - self.config.slippage_pct) - self.entry_price)\n            self.total_pnl += pnl\n            self.trades += 1\n            if pnl > 0:\n                self.wins += 1\n            self.entry_price = 0.0 if target_position <= 0 else current_price\n        elif position_change < 0:\n            # Going short\n            self.entry_price = current_price * (1 - self.config.slippage_pct)\n\n        self.position = target_position\n\n        return transaction_cost, slippage",
    "source_file": "core\\rl\\environments.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ForexTradingEnv"
  },
  {
    "name": "step",
    "category": "reinforcement_learning",
    "formula": "state, reward, terminated, truncated, info",
    "explanation": "Execute one step in the environment.\n\nArgs:\n    action: Trading action (position target)\n\nReturns:\n    observation, reward, terminated, truncated, info",
    "python_code": "def step(self, action) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n        \"\"\"\n        Execute one step in the environment.\n\n        Args:\n            action: Trading action (position target)\n\n        Returns:\n            observation, reward, terminated, truncated, info\n        \"\"\"\n        # Convert action\n        if isinstance(action, np.ndarray):\n            action = action[0]\n\n        # Store old portfolio value\n        old_value = self.portfolio_value\n\n        # Execute trade\n        transaction_cost, slippage = self._execute_trade(action)\n\n        # Move to next step\n        self.current_step += 1\n\n        # Check if episode is done\n        terminated = False\n        truncated = False\n\n        if self.current_step >= len(self.prices) - 1:\n            truncated = True\n\n        # Calculate new portfolio value\n        if self.current_step < len(self.prices):\n            current_price = self.prices[self.current_step]\n\n            # Mark-to-market PnL\n            if self.position != 0 and self.entry_price > 0:\n                unrealized_pnl = self.position * (current_price - self.entry_price)\n            else:\n                unrealized_pnl = 0.0\n\n            self.portfolio_value = self.capital + unrealized_pnl - transaction_cost\n\n            # Update max portfolio value for drawdown\n            self.max_portfolio_value = max(self.max_portfolio_value, self.portfolio_value)\n\n            # Check drawdown constraint\n            drawdown = (self.max_portfolio_value - self.portfolio_value) / self.max_portfolio_value\n            if drawdown > self.config.max_drawdown:\n                terminated = True\n\n        # Calculate reward\n        reward = self._calculate_reward(old_value, self.portfolio_value, action)\n\n        # Get new state\n        state = self._get_state() if not (terminated or truncated) else np.zeros(self.state_dim, dtype=np.float32)\n\n        # Info dict\n        info = {\n            'portfolio_value': self.portfolio_value,\n            'position': self.position,\n            'total_pnl': self.total_pnl,\n            'trades': self.trades,\n            'win_rate': self.wins / max(1, self.trades),\n            'drawdown': (self.max_portfolio_value - self.portfolio_value) / self.max_portfolio_value,\n        }\n\n        return state, reward, terminated, truncated, info",
    "source_file": "core\\rl\\environments.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ForexTradingEnv"
  },
  {
    "name": "reset",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Reset environment to initial state.",
    "python_code": "def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, Dict]:\n        \"\"\"Reset environment to initial state.\"\"\"\n        super().reset(seed=seed)\n\n        self.current_step = 0\n        self.position = 0.0\n        self.capital = self.config.initial_capital\n        self.portfolio_value = self.config.initial_capital\n        self.entry_price = 0.0\n        self.returns_history = []\n        self.max_portfolio_value = self.config.initial_capital\n        self.trades = 0\n        self.wins = 0\n        self.total_pnl = 0.0\n\n        return self._get_state(), {}",
    "source_file": "core\\rl\\environments.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ForexTradingEnv"
  },
  {
    "name": "render",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Render environment state.",
    "python_code": "def render(self):\n        \"\"\"Render environment state.\"\"\"\n        if self.render_mode == 'human':\n            print(f\"Step: {self.current_step}, Position: {self.position:.2f}, \"\n                  f\"Portfolio: ${self.portfolio_value:.2f}, PnL: ${self.total_pnl:.2f}\")",
    "source_file": "core\\rl\\environments.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ForexTradingEnv"
  },
  {
    "name": "_compute_correlation",
    "category": "technical",
    "formula": "np.zeros(self.n_assets * (self.n_assets - 1) // 2) | np.nan_to_num(upper_tri, nan=0.0)",
    "explanation": "Compute rolling correlation matrix.",
    "python_code": "def _compute_correlation(self, lookback: int = 20) -> np.ndarray:\n        \"\"\"Compute rolling correlation matrix.\"\"\"\n        if self.current_step < lookback:\n            return np.zeros(self.n_assets * (self.n_assets - 1) // 2)\n\n        returns = np.zeros((lookback, self.n_assets))\n        for i, sym in enumerate(self.symbols):\n            prices = self.prices[sym][self.current_step - lookback:self.current_step]\n            returns[:, i] = np.diff(np.log(prices), prepend=0)[1:]\n\n        corr = np.corrcoef(returns.T)\n        # Extract upper triangle (excluding diagonal)\n        upper_tri = corr[np.triu_indices(self.n_assets, k=1)]\n        return np.nan_to_num(upper_tri, nan=0.0)",
    "source_file": "core\\rl\\environments.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "MultiAssetForexEnv"
  },
  {
    "name": "_get_state",
    "category": "reinforcement_learning",
    "formula": "np.concatenate(state_parts).astype(np.float32)",
    "explanation": "Get current state observation.",
    "python_code": "def _get_state(self) -> np.ndarray:\n        \"\"\"Get current state observation.\"\"\"\n        state_parts = []\n\n        # Asset features\n        for sym in self.symbols:\n            feats = self.features[sym][self.current_step]\n            feats = np.nan_to_num(feats, nan=0.0)\n            state_parts.append(feats)\n\n        # Current positions\n        state_parts.append(self.positions)\n\n        # Correlation matrix\n        state_parts.append(self._compute_correlation())\n\n        # Portfolio metrics\n        pv_normalized = (self.portfolio_value - self.config.initial_capital) / self.config.initial_capital\n        dd = (self.max_portfolio_value - self.portfolio_value) / self.max_portfolio_value\n        state_parts.append(np.array([pv_normalized, dd]))\n\n        return np.concatenate(state_parts).astype(np.float32)",
    "source_file": "core\\rl\\environments.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "MultiAssetForexEnv"
  },
  {
    "name": "create_forex_env",
    "category": "reinforcement_learning",
    "formula": "ForexTradingEnv(",
    "explanation": "Factory function to create Forex Trading Environment.\n\nArgs:\n    df: DataFrame with OHLCV + features\n    config: Environment configuration\n    feature_columns: Feature columns to use\n    **kwargs: Additional arguments passed to environment\n\nReturns:\n    ForexTradingEnv instance",
    "python_code": "def create_forex_env(\n    df: pd.DataFrame,\n    config: Optional[ForexTradingEnvConfig] = None,\n    feature_columns: Optional[List[str]] = None,\n    **kwargs\n) -> ForexTradingEnv:\n    \"\"\"\n    Factory function to create Forex Trading Environment.\n\n    Args:\n        df: DataFrame with OHLCV + features\n        config: Environment configuration\n        feature_columns: Feature columns to use\n        **kwargs: Additional arguments passed to environment\n\n    Returns:\n        ForexTradingEnv instance\n    \"\"\"\n    return ForexTradingEnv(\n        df=df,\n        config=config,\n        feature_columns=feature_columns,\n        **kwargs\n    )",
    "source_file": "core\\rl\\environments.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "deep_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        n_heads: int = 4,\n        dropout: float = 0.1,\n        concat: bool = True,\n    ):\n        super().__init__()\n\n        self.in_features = in_features\n        self.out_features = out_features\n        self.n_heads = n_heads\n        self.concat = concat\n\n        # Linear transformation for each head\n        self.W = nn.Parameter(torch.empty(n_heads, in_features, out_features))\n        nn.init.xavier_uniform_(self.W)\n\n        # Attention parameters\n        self.a = nn.Parameter(torch.empty(n_heads, 2 * out_features, 1))\n        nn.init.xavier_uniform_(self.a)\n\n        self.leaky_relu = nn.LeakyReLU(0.2)\n        self.dropout = nn.Dropout(dropout)",
    "source_file": "core\\rl\\graph_rl.py",
    "academic_reference": null,
    "class_name": "GraphAttentionLayer"
  },
  {
    "name": "forward",
    "category": "technical",
    "formula": "out",
    "explanation": "Forward pass with attention.\n\nArgs:\n    x: Node features (batch, n_nodes, in_features)\n    adj: Adjacency matrix\n\nReturns:\n    Updated node features",
    "python_code": "def forward(\n        self,\n        x: torch.Tensor,  # (batch, n_nodes, in_features)\n        adj: torch.Tensor,  # (batch, n_nodes, n_nodes) or (n_nodes, n_nodes)\n    ) -> torch.Tensor:\n        \"\"\"\n        Forward pass with attention.\n\n        Args:\n            x: Node features (batch, n_nodes, in_features)\n            adj: Adjacency matrix\n\n        Returns:\n            Updated node features\n        \"\"\"\n        batch_size, n_nodes, _ = x.shape\n\n        # Linear transformation: (batch, n_nodes, n_heads, out_features)\n        h = torch.einsum('bni,hio->bnho', x, self.W)\n\n        # Compute attention scores\n        # Self-attention: concat [Wh_i || Wh_j] for all pairs\n        h_repeat = h.unsqueeze(2).repeat(1, 1, n_nodes, 1, 1)  # (batch, n_nodes, n_nodes, n_heads, out)\n        h_repeat_t = h.unsqueeze(1).repeat(1, n_nodes, 1, 1, 1)  # (batch, n_nodes, n_nodes, n_heads, out)\n\n        concat_features = torch.cat([h_repeat, h_repeat_t], dim=-1)  # (batch, n, n, heads, 2*out)\n\n        # Attention logits\n        e = torch.einsum('bnmho,hok->bnmh', concat_features, self.a).squeeze(-1)  # (batch, n, n, heads)\n        e = self.leaky_relu(e)\n\n        # Mask with adjacency matrix\n        if adj.dim() == 2:\n            adj = adj.unsqueeze(0).expand(batch_size, -1, -1)\n\n        mask = (adj == 0).unsqueeze(-1).expand_as(e)\n        e = e.masked_fill(mask, float('-inf'))\n\n        # Softmax attention\n        attention = F.softmax(e, dim=2)\n        attention = self.dropout(attention)\n\n        # Apply attention to values\n        out = torch.einsum('bnmh,bmho->bnho', attention, h)\n\n        if self.concat:\n            out = out.reshape(batch_size, n_nodes, -1)  # (batch, n, heads * out)\n        else:\n            out = out.mean(dim=2)  # (batch, n, out)\n\n        return out",
    "source_file": "core\\rl\\graph_rl.py",
    "academic_reference": null,
    "class_name": "GraphAttentionLayer"
  },
  {
    "name": "forward",
    "category": "technical",
    "formula": "q_tot",
    "explanation": "Forward pass with GNN communication.\n\nArgs:\n    agent_obs: Individual agent observations\n    global_state: Global state\n    adj: Agent communication adjacency matrix\n\nReturns:\n    Q_tot: Total Q-value",
    "python_code": "def forward(\n        self,\n        agent_obs: torch.Tensor,  # (batch, n_agents, obs_dim)\n        global_state: torch.Tensor,  # (batch, state_dim)\n        adj: torch.Tensor,  # (n_agents, n_agents) communication graph\n    ) -> torch.Tensor:\n        \"\"\"\n        Forward pass with GNN communication.\n\n        Args:\n            agent_obs: Individual agent observations\n            global_state: Global state\n            adj: Agent communication adjacency matrix\n\n        Returns:\n            Q_tot: Total Q-value\n        \"\"\"\n        batch_size = agent_obs.size(0)\n\n        # GNN message passing for agent communication\n        agent_features = self.agent_gnn(agent_obs, adj)  # (batch, n_agents, hidden * heads)\n\n        # Individual Q-values\n        agent_qs = []\n        for i in range(self.n_agents):\n            q_i = self.q_networks[i](agent_features[:, i, :])\n            agent_qs.append(q_i)\n\n        agent_qs = torch.cat(agent_qs, dim=-1)  # (batch, n_agents)\n\n        # Mixing with state + GNN features\n        gnn_flat = agent_features.view(batch_size, -1)\n        mixing_input = torch.cat([global_state, gnn_flat], dim=-1)\n\n        # Generate mixing weights (non-negative)\n        w = torch.abs(self.hyper_w(mixing_input))\n        w = w.view(batch_size, self.n_agents, -1)\n\n        b = self.hyper_b(mixing_input)\n\n        # Mix Q-values\n        hidden = F.elu(torch.bmm(agent_qs.unsqueeze(1), w).squeeze(1) + b)\n\n        q_tot = self.final_layer(hidden)\n\n        return q_tot",
    "source_file": "core\\rl\\graph_rl.py",
    "academic_reference": null,
    "class_name": "GraphMIX"
  },
  {
    "name": "update_correlation_graph",
    "category": "technical",
    "formula": "# Compute correlation matrix",
    "explanation": "Update adjacency matrix based on asset correlations.\n\nArgs:\n    returns: Asset returns (T, n_assets)",
    "python_code": "def update_correlation_graph(self, returns: np.ndarray):\n        \"\"\"\n        Update adjacency matrix based on asset correlations.\n\n        Args:\n            returns: Asset returns (T, n_assets)\n        \"\"\"\n        if not self.config.use_correlation_graph:\n            return\n\n        # Compute correlation matrix\n        corr = np.corrcoef(returns.T)\n\n        # Threshold to create adjacency\n        adj = (np.abs(corr) > self.config.correlation_threshold).astype(np.float32)\n\n        self.adj = torch.FloatTensor(adj).to(self.device)",
    "source_file": "core\\rl\\graph_rl.py",
    "academic_reference": null,
    "class_name": "GraphRLAgent"
  },
  {
    "name": "select_action",
    "category": "reinforcement_learning",
    "formula": "torch.tanh(mean).cpu().numpy()[0] | torch.tanh(action).cpu().numpy()[0]",
    "explanation": "Select trading actions using GNN policy.",
    "python_code": "def select_action(self, state: np.ndarray, deterministic: bool = False) -> np.ndarray:\n        \"\"\"Select trading actions using GNN policy.\"\"\"\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n\n            # Reshape to (batch, n_assets, features_per_asset)\n            features_per_asset = state.shape[0] // self.n_assets\n            x = state_tensor.view(1, self.n_assets, features_per_asset)\n\n            # GNN forward\n            gnn_out = x\n            for layer in self.gnn:\n                if isinstance(layer, (GraphAttentionLayer, GraphConvLayer)):\n                    gnn_out = layer(gnn_out, self.adj)\n                else:\n                    gnn_out = layer(gnn_out)\n\n            # Flatten and policy\n            gnn_flat = gnn_out.view(1, -1)\n            features = self.policy(gnn_flat)\n\n            mean = self.mean_head(features)\n            log_std = torch.clamp(self.log_std_head(features), -5, 2)\n\n            if deterministic:\n                return torch.tanh(mean).cpu().numpy()[0]\n\n            std = log_std.exp()\n            dist = Normal(mean, std)\n            action = dist.sample()\n            return torch.tanh(action).cpu().numpy()[0]",
    "source_file": "core\\rl\\graph_rl.py",
    "academic_reference": null,
    "class_name": "GraphRLAgent"
  },
  {
    "name": "get_value",
    "category": "reinforcement_learning",
    "formula": "value.item()",
    "explanation": "Get state value estimate.",
    "python_code": "def get_value(self, state: np.ndarray) -> float:\n        \"\"\"Get state value estimate.\"\"\"\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n\n            features_per_asset = state.shape[0] // self.n_assets\n            x = state_tensor.view(1, self.n_assets, features_per_asset)\n\n            gnn_out = x\n            for layer in self.gnn:\n                if isinstance(layer, (GraphAttentionLayer, GraphConvLayer)):\n                    gnn_out = layer(gnn_out, self.adj)\n                else:\n                    gnn_out = layer(gnn_out)\n\n            gnn_flat = gnn_out.view(1, -1)\n            features = self.policy(gnn_flat)\n            value = self.value_head(features)\n\n            return value.item()",
    "source_file": "core\\rl\\graph_rl.py",
    "academic_reference": null,
    "class_name": "GraphRLAgent"
  },
  {
    "name": "create_graph_rl_agent",
    "category": "reinforcement_learning",
    "formula": "GraphRLAgent(n_assets, state_dim, config)",
    "explanation": "Factory function to create Graph RL agent.\n\nArgs:\n    n_assets: Number of assets\n    state_dim: State dimension\n    config: Graph RL configuration\n\nReturns:\n    GraphRLAgent instance",
    "python_code": "def create_graph_rl_agent(\n    n_assets: int,\n    state_dim: int,\n    config: Optional[GraphRLConfig] = None,\n) -> GraphRLAgent:\n    \"\"\"\n    Factory function to create Graph RL agent.\n\n    Args:\n        n_assets: Number of assets\n        state_dim: State dimension\n        config: Graph RL configuration\n\n    Returns:\n        GraphRLAgent instance\n    \"\"\"\n    return GraphRLAgent(n_assets, state_dim, config)",
    "source_file": "core\\rl\\graph_rl.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, config: GRPOConfig):\n        super().__init__()\n        self.config = config\n\n        # Build MLP policy\n        layers = []\n        prev_dim = config.state_dim\n        for hidden_dim in config.hidden_dims:\n            layers.extend([\n                nn.Linear(prev_dim, hidden_dim),\n                nn.ReLU(),\n                nn.LayerNorm(hidden_dim)  # Stabilize training\n            ])\n            prev_dim = hidden_dim\n\n        self.backbone = nn.Sequential(*layers)\n\n        # Action head (Gaussian policy for continuous actions)\n        self.action_mean = nn.Linear(prev_dim, config.action_dim)\n        self.action_logstd = nn.Parameter(torch.zeros(config.action_dim))",
    "source_file": "core\\rl\\grpo_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GRPOPolicyNetwork"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "mean, std",
    "explanation": "Forward pass.\n\nArgs:\n    state: (batch_size, state_dim)\n\nReturns:\n    mean: (batch_size, action_dim)\n    std: (batch_size, action_dim)",
    "python_code": "def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward pass.\n\n        Args:\n            state: (batch_size, state_dim)\n\n        Returns:\n            mean: (batch_size, action_dim)\n            std: (batch_size, action_dim)\n        \"\"\"\n        features = self.backbone(state)\n        mean = self.action_mean(features)\n        std = torch.exp(self.action_logstd).expand_as(mean)\n        return mean, std",
    "source_file": "core\\rl\\grpo_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GRPOPolicyNetwork"
  },
  {
    "name": "get_action_and_log_prob",
    "category": "reinforcement_learning",
    "formula": "action, log_prob",
    "explanation": "Sample action and compute log probability.\n\nArgs:\n    state: (batch_size, state_dim)\n\nReturns:\n    action: (batch_size, action_dim)\n    log_prob: (batch_size,)",
    "python_code": "def get_action_and_log_prob(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Sample action and compute log probability.\n\n        Args:\n            state: (batch_size, state_dim)\n\n        Returns:\n            action: (batch_size, action_dim)\n            log_prob: (batch_size,)\n        \"\"\"\n        mean, std = self.forward(state)\n        dist = Normal(mean, std)\n        action = dist.sample()\n        log_prob = dist.log_prob(action).sum(dim=-1)\n        return action, log_prob",
    "source_file": "core\\rl\\grpo_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GRPOPolicyNetwork"
  },
  {
    "name": "sample_group",
    "category": "reinforcement_learning",
    "formula": "list(actions), list(log_probs)",
    "explanation": "Sample K actions from current policy for same state.\n\nThis is the key GRPO operation: sample a GROUP of actions,\nthen use relative rewards for advantage estimation.\n\nArgs:\n    state: Market state (features)\n    k: Group size (default: config.group_size)\n\nReturns:\n    actions: List of K actions\n    log_probs: List of K log probabilities",
    "python_code": "def sample_group(\n        self,\n        state: np.ndarray,\n        k: Optional[int] = None\n    ) -> Tuple[List[np.ndarray], List[float]]:\n        \"\"\"\n        Sample K actions from current policy for same state.\n\n        This is the key GRPO operation: sample a GROUP of actions,\n        then use relative rewards for advantage estimation.\n\n        Args:\n            state: Market state (features)\n            k: Group size (default: config.group_size)\n\n        Returns:\n            actions: List of K actions\n            log_probs: List of K log probabilities\n        \"\"\"\n        if k is None:\n            k = self.config.group_size\n\n        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n        state_tensor = state_tensor.expand(k, -1)  # (k, state_dim)\n\n        with torch.no_grad():\n            actions, log_probs = self.policy.get_action_and_log_prob(state_tensor)\n\n        actions = actions.cpu().numpy()\n        log_probs = log_probs.cpu().numpy()\n\n        return list(actions), list(log_probs)",
    "source_file": "core\\rl\\grpo_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GRPOAgent"
  },
  {
    "name": "update",
    "category": "reinforcement_learning",
    "formula": "final_metrics",
    "explanation": "GRPO update step.\n\nKey difference from PPO: Advantage is group-relative, no critic.\n\nArgs:\n    states: (batch_size, state_dim)\n    actions: (batch_size, action_dim)\n    rewards: (batch_size,)\n    old_log_probs: (batch_size,)\n    num_epochs: Number of update epochs\n\nReturns:\n    metrics: Dict with loss, kl, etc.",
    "python_code": "def update(\n        self,\n        states: np.ndarray,\n        actions: np.ndarray,\n        rewards: np.ndarray,\n        old_log_probs: np.ndarray,\n        num_epochs: int = 1\n    ) -> Dict[str, float]:\n        \"\"\"\n        GRPO update step.\n\n        Key difference from PPO: Advantage is group-relative, no critic.\n\n        Args:\n            states: (batch_size, state_dim)\n            actions: (batch_size, action_dim)\n            rewards: (batch_size,)\n            old_log_probs: (batch_size,)\n            num_epochs: Number of update epochs\n\n        Returns:\n            metrics: Dict with loss, kl, etc.\n        \"\"\"\n        states = torch.FloatTensor(states).to(self.device)\n        actions = torch.FloatTensor(actions).to(self.device)\n        rewards = torch.FloatTensor(rewards).to(self.device)\n        old_log_probs = torch.FloatTensor(old_log_probs).to(self.device)\n\n        # Group relative advantage (key GRPO innovation)\n        # Reshape rewards into groups of size K\n        batch_size = len(rewards)\n        k = self.config.group_size\n\n        if batch_size % k != 0:\n            logger.warning(f\"Batch size {batch_size} not divisible by group size {k}\")\n            # Pad or truncate\n            remainder = batch_size % k\n            if remainder > 0:\n                rewards = rewards[:-remainder]\n                states = states[:-remainder]\n                actions = actions[:-remainder]\n                old_log_probs = old_log_probs[:-remainder]\n                batch_size = len(rewards)\n\n        # Compute group-relative advantages\n        num_groups = batch_size // k\n        rewards_grouped = rewards.view(num_groups, k)\n        group_means = rewards_grouped.mean(dim=1, keepdim=True)\n        advantages = (rewards_grouped - group_means).view(-1)\n\n        # Normalize advantages (optional but helps stability)\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n        metrics = {}\n\n        for epoch in range(num_epochs):\n            # Get current log probs\n            mean, std = self.policy(states)\n            dist = Normal(mean, std)\n            log_probs = dist.log_prob(actions).sum(dim=-1)\n\n            # Importance ratio\n            ratio = torch.exp(log_probs - old_log_probs)\n\n            # Clipped surrogate objective (like PPO)\n            clip_ratio = self.config.clip_ratio\n            surr1 = ratio * advantages\n            surr2 = torch.clamp(ratio, 1 - clip_ratio, 1 + clip_ratio) * advantages\n            policy_loss = -torch.min(surr1, surr2).mean()\n\n            # Entropy bonus (encourage exploration)\n            entropy = dist.entropy().sum(dim=-1).mean()\n            entropy_loss = -self.config.entropy_coef * entropy\n\n            # KL penalty (keep policy close to old policy)\n            kl_div = (old_log_probs - log_probs).mean()\n            kl_loss = self.config.kl_coefficient * kl_div\n\n            # Total loss\n            loss = policy_loss + entropy_loss + kl_loss\n\n            # Optimization step\n            se",
    "source_file": "core\\rl\\grpo_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GRPOAgent"
  },
  {
    "name": "save",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Save model checkpoint.",
    "python_code": "def save(self, path: str):\n        \"\"\"Save model checkpoint.\"\"\"\n        torch.save({\n            'policy_state_dict': self.policy.state_dict(),\n            'old_policy_state_dict': self.old_policy.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'config': self.config\n        }, path)\n        logger.info(f\"Saved GRPO agent to {path}\")",
    "source_file": "core\\rl\\grpo_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GRPOAgent"
  },
  {
    "name": "load",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Load model checkpoint.",
    "python_code": "def load(self, path: str):\n        \"\"\"Load model checkpoint.\"\"\"\n        checkpoint = torch.load(path, map_location=self.device)\n        self.policy.load_state_dict(checkpoint['policy_state_dict'])\n        self.old_policy.load_state_dict(checkpoint['old_policy_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        logger.info(f\"Loaded GRPO agent from {path}\")",
    "source_file": "core\\rl\\grpo_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GRPOAgent"
  },
  {
    "name": "create_grpo_agent",
    "category": "reinforcement_learning",
    "formula": "GRPOAgent(config, device=device)",
    "explanation": "Create GRPO agent with DeepSeek-R1 hyperparameters.\n\nArgs:\n    state_dim: Number of input features\n    action_dim: Number of action dimensions\n    device: 'cuda' or 'cpu'\n\nReturns:\n    GRPOAgent instance",
    "python_code": "def create_grpo_agent(state_dim: int, action_dim: int, device: str = 'cuda') -> GRPOAgent:\n    \"\"\"\n    Create GRPO agent with DeepSeek-R1 hyperparameters.\n\n    Args:\n        state_dim: Number of input features\n        action_dim: Number of action dimensions\n        device: 'cuda' or 'cpu'\n\n    Returns:\n        GRPOAgent instance\n    \"\"\"\n    config = GRPOConfig(\n        state_dim=state_dim,\n        action_dim=action_dim,\n        learning_rate=3e-6,  # DeepSeek-R1 setting\n        kl_coefficient=0.001,\n        clip_ratio=10.0,\n        group_size=16\n    )\n    return GRPOAgent(config, device=device)",
    "source_file": "core\\rl\\grpo_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(\n        self,\n        state_dim: int,\n        goal_dim: int,\n        hidden_dims: List[int] = [256, 256],\n    ):\n        super().__init__()\n\n        # State encoder\n        layers = []\n        prev_dim = state_dim\n        for hidden_dim in hidden_dims:\n            layers.append(nn.Linear(prev_dim, hidden_dim))\n            layers.append(nn.ReLU())\n            prev_dim = hidden_dim\n        self.encoder = nn.Sequential(*layers)\n\n        # Goal/option output\n        self.goal_head = nn.Linear(hidden_dims[-1], goal_dim)\n\n        # Value head\n        self.value_head = nn.Linear(hidden_dims[-1], 1)",
    "source_file": "core\\rl\\hierarchical_rl.py",
    "academic_reference": null,
    "class_name": "ManagerNetwork"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "goal, value",
    "explanation": "Forward pass.\n\nArgs:\n    state: Current state\n\nReturns:\n    goal: Goal/option selection logits\n    value: State value estimate",
    "python_code": "def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward pass.\n\n        Args:\n            state: Current state\n\n        Returns:\n            goal: Goal/option selection logits\n            value: State value estimate\n        \"\"\"\n        features = self.encoder(state)\n        goal = self.goal_head(features)\n        value = self.value_head(features)\n        return goal, value",
    "source_file": "core\\rl\\hierarchical_rl.py",
    "academic_reference": null,
    "class_name": "ManagerNetwork"
  },
  {
    "name": "get_goal",
    "category": "reinforcement_learning",
    "formula": "goal_logits.argmax(dim=-1) | dist.sample()",
    "explanation": "Sample goal for worker.",
    "python_code": "def get_goal(self, state: torch.Tensor, deterministic: bool = False) -> torch.Tensor:\n        \"\"\"Sample goal for worker.\"\"\"\n        goal_logits, _ = self.forward(state)\n\n        if deterministic:\n            return goal_logits.argmax(dim=-1)\n\n        dist = Categorical(logits=goal_logits)\n        return dist.sample()",
    "source_file": "core\\rl\\hierarchical_rl.py",
    "academic_reference": null,
    "class_name": "ManagerNetwork"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "(mean, log_std), self.value_head(features)",
    "explanation": "Forward pass conditioned on goal.\n\nArgs:\n    state: Current state\n    goal: Goal from manager (one-hot or embedding)\n\nReturns:\n    action_dist_params: Action distribution parameters\n    value: State-goal value estimate",
    "python_code": "def forward(\n        self,\n        state: torch.Tensor,\n        goal: torch.Tensor,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward pass conditioned on goal.\n\n        Args:\n            state: Current state\n            goal: Goal from manager (one-hot or embedding)\n\n        Returns:\n            action_dist_params: Action distribution parameters\n            value: State-goal value estimate\n        \"\"\"\n        # Concatenate state and goal\n        if goal.dim() == 1:\n            goal = F.one_hot(goal, num_classes=self.encoder[0].in_features - state.size(-1)).float()\n\n        x = torch.cat([state, goal], dim=-1)\n        features = self.encoder(x)\n\n        if self.continuous:\n            mean = self.mean_head(features)\n            log_std = torch.clamp(self.log_std_head(features), -5, 2)\n            return (mean, log_std), self.value_head(features)\n        else:\n            return self.action_head(features), self.value_head(features)",
    "source_file": "core\\rl\\hierarchical_rl.py",
    "academic_reference": null,
    "class_name": "WorkerNetwork"
  },
  {
    "name": "get_action",
    "category": "reinforcement_learning",
    "formula": "torch.tanh(mean) | torch.tanh(action) | dist_params.argmax(dim=-1)",
    "explanation": "Get action conditioned on goal.",
    "python_code": "def get_action(\n        self,\n        state: torch.Tensor,\n        goal: torch.Tensor,\n        deterministic: bool = False,\n    ) -> torch.Tensor:\n        \"\"\"Get action conditioned on goal.\"\"\"\n        dist_params, _ = self.forward(state, goal)\n\n        if self.continuous:\n            mean, log_std = dist_params\n            if deterministic:\n                return torch.tanh(mean)\n            std = log_std.exp()\n            dist = Normal(mean, std)\n            action = dist.sample()\n            return torch.tanh(action)\n        else:\n            if deterministic:\n                return dist_params.argmax(dim=-1)\n            dist = Categorical(logits=dist_params)\n            return dist.sample()",
    "source_file": "core\\rl\\hierarchical_rl.py",
    "academic_reference": null,
    "class_name": "WorkerNetwork"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "Forward pass.\n\nReturns:\n    q_options: Q-values for each option\n    policies: Action probabilities for each option\n    terminations: Termination probabilities for each option",
    "python_code": "def forward(self, state: torch.Tensor) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Forward pass.\n\n        Returns:\n            q_options: Q-values for each option\n            policies: Action probabilities for each option\n            terminations: Termination probabilities for each option\n        \"\"\"\n        features = self.encoder(state)\n\n        q_options = self.q_options(features)\n\n        policies = [F.softmax(policy(features), dim=-1) for policy in self.intra_option_policies]\n        policies = torch.stack(policies, dim=1)  # (batch, n_options, action_dim)\n\n        terminations = [torch.sigmoid(term(features)) for term in self.terminations]\n        terminations = torch.cat(terminations, dim=-1)  # (batch, n_options)\n\n        return {\n            'q_options': q_options,\n            'policies': policies,\n            'terminations': terminations,\n            'features': features,\n        }",
    "source_file": "core\\rl\\hierarchical_rl.py",
    "academic_reference": null,
    "class_name": "OptionCritic"
  },
  {
    "name": "get_option",
    "category": "reinforcement_learning",
    "formula": "torch.randint(0, self.n_options, (state.size(0),), device=state.device) | q_options.argmax(dim=-1)",
    "explanation": "Select option using epsilon-greedy over Q-values.",
    "python_code": "def get_option(self, state: torch.Tensor, epsilon: float = 0.1) -> torch.Tensor:\n        \"\"\"Select option using epsilon-greedy over Q-values.\"\"\"\n        output = self.forward(state)\n        q_options = output['q_options']\n\n        if np.random.random() < epsilon:\n            return torch.randint(0, self.n_options, (state.size(0),), device=state.device)\n        return q_options.argmax(dim=-1)",
    "source_file": "core\\rl\\hierarchical_rl.py",
    "academic_reference": null,
    "class_name": "OptionCritic"
  },
  {
    "name": "get_action",
    "category": "reinforcement_learning",
    "formula": "option_policy.argmax(dim=-1) | dist.sample()",
    "explanation": "Get action for given option.",
    "python_code": "def get_action(\n        self,\n        state: torch.Tensor,\n        option: torch.Tensor,\n        deterministic: bool = False,\n    ) -> torch.Tensor:\n        \"\"\"Get action for given option.\"\"\"\n        output = self.forward(state)\n        policies = output['policies']\n\n        # Select policy for current option\n        batch_idx = torch.arange(state.size(0), device=state.device)\n        option_policy = policies[batch_idx, option]  # (batch, action_dim)\n\n        if deterministic:\n            return option_policy.argmax(dim=-1)\n\n        dist = Categorical(probs=option_policy)\n        return dist.sample()",
    "source_file": "core\\rl\\hierarchical_rl.py",
    "academic_reference": null,
    "class_name": "OptionCritic"
  },
  {
    "name": "should_terminate",
    "category": "reinforcement_learning",
    "formula": "torch.bernoulli(term_prob).bool()",
    "explanation": "Check if option should terminate.",
    "python_code": "def should_terminate(self, state: torch.Tensor, option: torch.Tensor) -> torch.Tensor:\n        \"\"\"Check if option should terminate.\"\"\"\n        output = self.forward(state)\n        terminations = output['terminations']\n\n        batch_idx = torch.arange(state.size(0), device=state.device)\n        term_prob = terminations[batch_idx, option]\n\n        return torch.bernoulli(term_prob).bool()",
    "source_file": "core\\rl\\hierarchical_rl.py",
    "academic_reference": null,
    "class_name": "OptionCritic"
  },
  {
    "name": "select_action",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Select hierarchical action.\n\nReturns:\n    goal: Current goal/option\n    action: Low-level action",
    "python_code": "def select_action(\n        self,\n        state: np.ndarray,\n        deterministic: bool = False,\n    ) -> Tuple[int, int]:\n        \"\"\"\n        Select hierarchical action.\n\n        Returns:\n            goal: Current goal/option\n            action: Low-level action\n        \"\"\"\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n\n            # Check if manager should act\n            if self.current_goal is None or self.steps_since_goal >= self.config.manager_horizon:\n                self.current_goal = self.manager.get_goal(state_tensor, deterministic)\n                self.steps_since_goal = 0\n\n            # Worker acts\n            goal_onehot = F.one_hot(\n                self.current_goal,\n                num_classes=self.config.manager_action_dim\n            ).float()\n\n            action = self.worker.get_action(state_tensor, goal_onehot, deterministic)\n\n            self.steps_since_goal += 1\n\n            return self.current_goal.item(), action.item()",
    "source_file": "core\\rl\\hierarchical_rl.py",
    "academic_reference": null,
    "class_name": "HierarchicalTradingAgent"
  },
  {
    "name": "reset",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Reset for new episode.",
    "python_code": "def reset(self):\n        \"\"\"Reset for new episode.\"\"\"\n        self.current_goal = None\n        self.steps_since_goal = 0",
    "source_file": "core\\rl\\hierarchical_rl.py",
    "academic_reference": null,
    "class_name": "HierarchicalTradingAgent"
  },
  {
    "name": "update_epsilon",
    "category": "statistical",
    "formula": "",
    "explanation": "Decay exploration rate.",
    "python_code": "def update_epsilon(self):\n        \"\"\"Decay exploration rate.\"\"\"\n        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)",
    "source_file": "core\\rl\\hierarchical_rl.py",
    "academic_reference": null,
    "class_name": "OptionCriticTrader"
  },
  {
    "name": "create_hierarchical_agent",
    "category": "reinforcement_learning",
    "formula": "HierarchicalTradingAgent(state_dim, config) | OptionCriticTrader(state_dim, config=config)",
    "explanation": "Factory function to create hierarchical RL agent.\n\nArgs:\n    state_dim: State dimension\n    agent_type: 'feudal' (Manager-Worker) or 'option_critic'\n    config: Hierarchical RL configuration\n\nReturns:\n    Hierarchical RL agent",
    "python_code": "def create_hierarchical_agent(\n    state_dim: int,\n    agent_type: str = 'feudal',\n    config: Optional[HierarchicalConfig] = None,\n) -> Any:\n    \"\"\"\n    Factory function to create hierarchical RL agent.\n\n    Args:\n        state_dim: State dimension\n        agent_type: 'feudal' (Manager-Worker) or 'option_critic'\n        config: Hierarchical RL configuration\n\n    Returns:\n        Hierarchical RL agent\n    \"\"\"\n    if agent_type == 'feudal':\n        return HierarchicalTradingAgent(state_dim, config)\n    elif agent_type == 'option_critic':\n        return OptionCriticTrader(state_dim, config=config)\n    else:\n        raise ValueError(f\"Unknown agent type: {agent_type}\")",
    "source_file": "core\\rl\\hierarchical_rl.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "create_option_critic_trader",
    "category": "reinforcement_learning",
    "formula": "OptionCriticTrader(state_dim, config=config)",
    "explanation": "Factory function to create Option-Critic trader.\n\nArgs:\n    state_dim: State dimension\n    config: Hierarchical RL configuration\n\nReturns:\n    OptionCriticTrader instance",
    "python_code": "def create_option_critic_trader(\n    state_dim: int,\n    config: Optional[HierarchicalConfig] = None,\n) -> OptionCriticTrader:\n    \"\"\"\n    Factory function to create Option-Critic trader.\n\n    Args:\n        state_dim: State dimension\n        config: Hierarchical RL configuration\n\n    Returns:\n        OptionCriticTrader instance\n    \"\"\"\n    return OptionCriticTrader(state_dim, config=config)",
    "source_file": "core\\rl\\hierarchical_rl.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(\n        self,\n        state_dim: int,\n        action_dim: int,\n        max_size: int = 100_000,\n    ):\n        self.max_size = max_size\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.ptr = 0\n        self.size = 0\n\n        self.states = np.zeros((max_size, state_dim), dtype=np.float32)\n        self.actions = np.zeros((max_size, action_dim), dtype=np.float32)\n        self.rewards = np.zeros((max_size, 1), dtype=np.float32)\n        self.next_states = np.zeros((max_size, state_dim), dtype=np.float32)\n        self.dones = np.zeros((max_size, 1), dtype=np.float32)",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ExpertDataset"
  },
  {
    "name": "add",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Add single transition.",
    "python_code": "def add(self, state, action, reward, next_state, done):\n        \"\"\"Add single transition.\"\"\"\n        self.states[self.ptr] = state\n        self.actions[self.ptr] = action\n        self.rewards[self.ptr] = reward\n        self.next_states[self.ptr] = next_state\n        self.dones[self.ptr] = done\n\n        self.ptr = (self.ptr + 1) % self.max_size\n        self.size = min(self.size + 1, self.max_size)",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ExpertDataset"
  },
  {
    "name": "add_trajectory",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Add trajectory of (state, action, reward, next_state, done) tuples.",
    "python_code": "def add_trajectory(self, trajectory: List[Tuple]):\n        \"\"\"Add trajectory of (state, action, reward, next_state, done) tuples.\"\"\"\n        for t in trajectory:\n            self.add(*t)",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ExpertDataset"
  },
  {
    "name": "load_from_numpy",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Load from numpy arrays.",
    "python_code": "def load_from_numpy(\n        self,\n        states: np.ndarray,\n        actions: np.ndarray,\n        rewards: Optional[np.ndarray] = None,\n        next_states: Optional[np.ndarray] = None,\n        dones: Optional[np.ndarray] = None,\n    ):\n        \"\"\"Load from numpy arrays.\"\"\"\n        n = min(len(states), self.max_size)\n        self.states[:n] = states[:n]\n        self.actions[:n] = actions[:n]\n\n        if rewards is not None:\n            self.rewards[:n] = rewards[:n].reshape(-1, 1)\n        if next_states is not None:\n            self.next_states[:n] = next_states[:n]\n        else:\n            self.next_states[:n-1] = states[1:n]\n            self.next_states[n-1] = states[n-1]\n        if dones is not None:\n            self.dones[:n] = dones[:n].reshape(-1, 1)\n\n        self.size = n",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ExpertDataset"
  },
  {
    "name": "sample",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "Sample batch of transitions.",
    "python_code": "def sample(self, batch_size: int, device: str = 'cpu') -> Dict[str, torch.Tensor]:\n        \"\"\"Sample batch of transitions.\"\"\"\n        idx = np.random.randint(0, self.size, size=batch_size)\n        return {\n            'states': torch.FloatTensor(self.states[idx]).to(device),\n            'actions': torch.FloatTensor(self.actions[idx]).to(device),\n            'rewards': torch.FloatTensor(self.rewards[idx]).to(device),\n            'next_states': torch.FloatTensor(self.next_states[idx]).to(device),\n            'dones': torch.FloatTensor(self.dones[idx]).to(device),\n        }",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ExpertDataset"
  },
  {
    "name": "sample_states_actions",
    "category": "reinforcement_learning",
    "formula": "(",
    "explanation": "Sample only states and actions (for BC/GAIL).",
    "python_code": "def sample_states_actions(self, batch_size: int, device: str = 'cpu') -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Sample only states and actions (for BC/GAIL).\"\"\"\n        idx = np.random.randint(0, self.size, size=batch_size)\n        return (\n            torch.FloatTensor(self.states[idx]).to(device),\n            torch.FloatTensor(self.actions[idx]).to(device),\n        )",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ExpertDataset"
  },
  {
    "name": "select_action",
    "category": "reinforcement_learning",
    "formula": "action.cpu().numpy()[0]",
    "explanation": "Select action from learned policy.",
    "python_code": "def select_action(self, state: np.ndarray, deterministic: bool = False) -> np.ndarray:\n        \"\"\"Select action from learned policy.\"\"\"\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n            action = self.policy.get_action(state_tensor, deterministic)\n            return action.cpu().numpy()[0]",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BehavioralCloningTrader"
  },
  {
    "name": "train",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "Train policy via supervised learning on expert actions.",
    "python_code": "def train(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n        \"\"\"Train policy via supervised learning on expert actions.\"\"\"\n        states = batch['states']\n        expert_actions = batch['actions']\n\n        # Get policy output\n        mean, log_std = self.policy(states)\n        std = log_std.exp()\n\n        # MSE loss for deterministic BC\n        mse_loss = F.mse_loss(mean, expert_actions)\n\n        # Optional: Log-likelihood loss for stochastic BC\n        dist = Normal(mean, std)\n        nll_loss = -dist.log_prob(expert_actions).sum(-1).mean()\n\n        # Combined loss\n        loss = mse_loss + 0.1 * nll_loss\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        nn.utils.clip_grad_norm_(self.policy.parameters(), self.config.max_grad_norm)\n        self.optimizer.step()\n\n        return {\n            'bc_loss': loss.item(),\n            'mse_loss': mse_loss.item(),\n            'nll_loss': nll_loss.item(),\n        }",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BehavioralCloningTrader"
  },
  {
    "name": "learn_from_expert",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Train policy on expert demonstrations.\n\nArgs:\n    expert_dataset: Dataset of expert (state, action) pairs\n    epochs: Number of training epochs\n    callback: Optional callback for logging",
    "python_code": "def learn_from_expert(\n        self,\n        expert_dataset: ExpertDataset,\n        epochs: Optional[int] = None,\n        callback=None,\n    ) -> 'BehavioralCloningTrader':\n        \"\"\"\n        Train policy on expert demonstrations.\n\n        Args:\n            expert_dataset: Dataset of expert (state, action) pairs\n            epochs: Number of training epochs\n            callback: Optional callback for logging\n        \"\"\"\n        self.expert_dataset = expert_dataset\n        epochs = epochs or self.config.bc_epochs\n        batch_size = self.config.bc_batch_size\n\n        n_batches = max(1, expert_dataset.size // batch_size)\n\n        for epoch in range(epochs):\n            epoch_loss = 0.0\n\n            for _ in range(n_batches):\n                batch = expert_dataset.sample(batch_size, self.device)\n                metrics = self.train(batch)\n                epoch_loss += metrics['bc_loss']\n\n            epoch_loss /= n_batches\n            self.total_steps += n_batches\n\n            if callback:\n                callback({'epoch': epoch, 'loss': epoch_loss})\n\n        return self",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BehavioralCloningTrader"
  },
  {
    "name": "state_dict",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "",
    "python_code": "def state_dict(self) -> Dict[str, Any]:\n        return {\n            'policy': self.policy.state_dict(),\n            'optimizer': self.optimizer.state_dict(),\n        }",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BehavioralCloningTrader"
  },
  {
    "name": "load_state_dict",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def load_state_dict(self, state_dict: Dict[str, Any]):\n        self.policy.load_state_dict(state_dict['policy'])\n        self.optimizer.load_state_dict(state_dict['optimizer'])",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BehavioralCloningTrader"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Output logit (before sigmoid).",
    "python_code": "def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n        \"\"\"Output logit (before sigmoid).\"\"\"\n        x = torch.cat([state, action], dim=-1)\n        return self.net(x)",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "Discriminator"
  },
  {
    "name": "compute_reward",
    "category": "reinforcement_learning",
    "formula": "-F.logsigmoid(-logit)",
    "explanation": "Compute GAIL reward: -log(1 - D(s, a)).",
    "python_code": "def compute_reward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n        \"\"\"Compute GAIL reward: -log(1 - D(s, a)).\"\"\"\n        logit = self.forward(state, action)\n        # Reward from discriminator\n        return -F.logsigmoid(-logit)",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "Discriminator"
  },
  {
    "name": "_compute_gradient_penalty",
    "category": "machine_learning",
    "formula": "gradient_penalty",
    "explanation": "Compute gradient penalty for WGAN-GP style training.",
    "python_code": "def _compute_gradient_penalty(\n        self,\n        expert_states: torch.Tensor,\n        expert_actions: torch.Tensor,\n        policy_states: torch.Tensor,\n        policy_actions: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"Compute gradient penalty for WGAN-GP style training.\"\"\"\n        batch_size = expert_states.shape[0]\n\n        # Interpolate between expert and policy\n        alpha = torch.rand(batch_size, 1, device=self.device)\n        interp_states = alpha * expert_states + (1 - alpha) * policy_states\n        interp_actions = alpha * expert_actions + (1 - alpha) * policy_actions\n\n        interp_states.requires_grad_(True)\n        interp_actions.requires_grad_(True)\n\n        # Get discriminator output\n        d_interp = self.discriminator(interp_states, interp_actions)\n\n        # Compute gradients\n        gradients = torch.autograd.grad(\n            outputs=d_interp,\n            inputs=[interp_states, interp_actions],\n            grad_outputs=torch.ones_like(d_interp),\n            create_graph=True,\n            retain_graph=True,\n        )\n\n        # Gradient penalty\n        gradient_norm = torch.cat([g.view(batch_size, -1) for g in gradients], dim=1).norm(2, dim=1)\n        gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n\n        return gradient_penalty",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GAILTrader"
  },
  {
    "name": "_train_discriminator",
    "category": "reinforcement_learning",
    "formula": "total_loss / self.config.discriminator_epochs",
    "explanation": "Train discriminator to distinguish expert from policy.",
    "python_code": "def _train_discriminator(\n        self,\n        policy_batch: Dict[str, torch.Tensor],\n    ) -> float:\n        \"\"\"Train discriminator to distinguish expert from policy.\"\"\"\n        policy_states = policy_batch['states']\n        policy_actions = policy_batch['actions']\n        batch_size = policy_states.shape[0]\n\n        # Sample expert data\n        expert_states, expert_actions = self.expert_dataset.sample_states_actions(\n            batch_size, self.device\n        )\n\n        total_loss = 0.0\n\n        for _ in range(self.config.discriminator_epochs):\n            # Expert loss (label=1)\n            expert_logits = self.discriminator(expert_states, expert_actions)\n            expert_loss = F.binary_cross_entropy_with_logits(\n                expert_logits, torch.ones_like(expert_logits)\n            )\n\n            # Policy loss (label=0)\n            policy_logits = self.discriminator(policy_states, policy_actions)\n            policy_loss = F.binary_cross_entropy_with_logits(\n                policy_logits, torch.zeros_like(policy_logits)\n            )\n\n            # Gradient penalty\n            gp = self._compute_gradient_penalty(\n                expert_states, expert_actions,\n                policy_states, policy_actions,\n            )\n\n            # Total discriminator loss\n            discriminator_loss = (\n                expert_loss + policy_loss +\n                self.config.gradient_penalty_coef * gp\n            )\n\n            self.discriminator_optimizer.zero_grad()\n            discriminator_loss.backward()\n            self.discriminator_optimizer.step()\n\n            total_loss += discriminator_loss.item()\n\n        return total_loss / self.config.discriminator_epochs",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GAILTrader"
  },
  {
    "name": "learn",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Train GAIL with expert demonstrations.\n\nArgs:\n    total_timesteps: Total environment steps\n    expert_dataset: Dataset of expert demonstrations\n    callback: Optional callback for logging",
    "python_code": "def learn(\n        self,\n        total_timesteps: int,\n        expert_dataset: ExpertDataset,\n        callback=None,\n    ) -> 'GAILTrader':\n        \"\"\"\n        Train GAIL with expert demonstrations.\n\n        Args:\n            total_timesteps: Total environment steps\n            expert_dataset: Dataset of expert demonstrations\n            callback: Optional callback for logging\n        \"\"\"\n        self.expert_dataset = expert_dataset\n\n        state, _ = self.env.reset()\n        episode_reward = 0\n\n        for step in range(total_timesteps):\n            action = self.select_action(state)\n\n            next_state, reward, terminated, truncated, info = self.env.step(action)\n            done = terminated or truncated\n\n            self.buffer.add(state, action, reward, next_state, float(done))\n\n            state = next_state\n            episode_reward += reward\n            self.total_steps += 1\n\n            if done:\n                state, _ = self.env.reset()\n                if callback:\n                    callback({'episode_reward': episode_reward, 'step': step})\n                episode_reward = 0\n\n            # Train on collected data\n            if self.buffer.size >= self.config.batch_size:\n                batch = self.buffer.sample(self.config.batch_size, self.device)\n                self.train(batch)\n\n        return self",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GAILTrader"
  },
  {
    "name": "_get_beta",
    "category": "reinforcement_learning",
    "formula": "max(0, self.config.initial_beta * (1 - iteration / total_iterations))",
    "explanation": "Get mixing coefficient beta for current iteration.",
    "python_code": "def _get_beta(self, iteration: int, total_iterations: int) -> float:\n        \"\"\"Get mixing coefficient beta for current iteration.\"\"\"\n        if self.config.beta_schedule == 'linear':\n            return max(0, self.config.initial_beta * (1 - iteration / total_iterations))\n        elif self.config.beta_schedule == 'exponential':\n            return self.config.initial_beta * (0.5 ** iteration)\n        else:  # constant\n            return self.config.initial_beta",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DAggerTrader"
  },
  {
    "name": "_train_policy_epoch",
    "category": "reinforcement_learning",
    "formula": "0.0 | total_loss / n_batches",
    "explanation": "Train policy on aggregated dataset for one epoch.",
    "python_code": "def _train_policy_epoch(self, batch_size: int = 256) -> float:\n        \"\"\"Train policy on aggregated dataset for one epoch.\"\"\"\n        if self.aggregated_dataset.size < batch_size:\n            return 0.0\n\n        n_batches = max(1, self.aggregated_dataset.size // batch_size)\n        total_loss = 0.0\n\n        for _ in range(n_batches):\n            states, expert_actions = self.aggregated_dataset.sample_states_actions(\n                batch_size, self.device\n            )\n\n            # MSE loss\n            mean, _ = self.policy(states)\n            loss = F.mse_loss(mean, expert_actions)\n\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n            total_loss += loss.item()\n\n        return total_loss / n_batches",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DAggerTrader"
  },
  {
    "name": "_collect_data",
    "category": "reinforcement_learning",
    "formula": "collected",
    "explanation": "Collect data by running mixed policy.\n\nWith probability beta, use expert action.\nOtherwise, use learned policy action.",
    "python_code": "def _collect_data(\n        self,\n        n_samples: int,\n        beta: float,\n    ) -> List[Tuple[np.ndarray, np.ndarray]]:\n        \"\"\"\n        Collect data by running mixed policy.\n\n        With probability beta, use expert action.\n        Otherwise, use learned policy action.\n        \"\"\"\n        collected = []\n        state, _ = self.env.reset()\n\n        for _ in range(n_samples):\n            # Get expert action\n            expert_action = self.expert_policy(state)\n\n            # Get policy action\n            policy_action = self.select_action(state)\n\n            # Mix with beta\n            if np.random.random() < beta:\n                action = expert_action\n            else:\n                action = policy_action\n\n            # Store (state, expert_action) - always use expert label\n            collected.append((state.copy(), expert_action.copy()))\n\n            # Step environment\n            next_state, _, terminated, truncated, _ = self.env.step(action)\n            done = terminated or truncated\n\n            if done:\n                state, _ = self.env.reset()\n            else:\n                state = next_state\n\n        return collected",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DAggerTrader"
  },
  {
    "name": "create_expert_from_historical_trades",
    "category": "microstructure",
    "formula": "to include | dataset",
    "explanation": "Create expert dataset from historical profitable trades.\n\nArgs:\n    df: DataFrame with trading data\n    state_cols: Columns to use as state features\n    action_col: Column with trading action (position)\n    reward_col: Column with returns (for filtering)\n    success_threshold: Minimum return to include\n\nReturns:\n    ExpertDataset with winning trades",
    "python_code": "def create_expert_from_historical_trades(\n    df,\n    state_cols: List[str],\n    action_col: str,\n    reward_col: Optional[str] = None,\n    success_threshold: float = 0.0,\n) -> ExpertDataset:\n    \"\"\"\n    Create expert dataset from historical profitable trades.\n\n    Args:\n        df: DataFrame with trading data\n        state_cols: Columns to use as state features\n        action_col: Column with trading action (position)\n        reward_col: Column with returns (for filtering)\n        success_threshold: Minimum return to include\n\n    Returns:\n        ExpertDataset with winning trades\n    \"\"\"\n    # Filter by success if reward column provided\n    if reward_col is not None and success_threshold > 0:\n        df = df[df[reward_col] > success_threshold]\n\n    states = df[state_cols].values\n    actions = df[action_col].values.reshape(-1, 1) if df[action_col].ndim == 1 else df[action_col].values\n    rewards = df[reward_col].values if reward_col else None\n\n    dataset = ExpertDataset(\n        state_dim=len(state_cols),\n        action_dim=actions.shape[1],\n        max_size=len(df),\n    )\n    dataset.load_from_numpy(states, actions, rewards)\n\n    return dataset",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "create_expert_from_model",
    "category": "reinforcement_learning",
    "formula": "action | expert_policy",
    "explanation": "Create expert policy function from trained model.\n\nArgs:\n    model: Trained RL agent\n    noise: Add Gaussian noise to actions\n\nReturns:\n    Expert policy function",
    "python_code": "def create_expert_from_model(\n    model: BaseRLAgent,\n    noise: float = 0.0,\n) -> Callable[[np.ndarray], np.ndarray]:\n    \"\"\"\n    Create expert policy function from trained model.\n\n    Args:\n        model: Trained RL agent\n        noise: Add Gaussian noise to actions\n\n    Returns:\n        Expert policy function\n    \"\"\"\n    def expert_policy(state: np.ndarray) -> np.ndarray:\n        action = model.select_action(state, deterministic=True)\n        if noise > 0:\n            action = action + np.random.normal(0, noise, size=action.shape)\n            action = np.clip(action, -1, 1)\n        return action\n\n    return expert_policy",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "create_imitation_trader",
    "category": "reinforcement_learning",
    "formula": "agents[agent_type](env, config)",
    "explanation": "Factory function to create imitation learning traders.\n\nArgs:\n    agent_type: One of 'bc', 'gail', 'dagger'\n    env: Gym environment\n    config: Imitation configuration\n    **kwargs: Additional arguments (e.g., expert_policy for DAgger)\n\nReturns:\n    Imitation learning trader instance",
    "python_code": "def create_imitation_trader(\n    agent_type: str,\n    env: gym.Env,\n    config: Optional[ImitationConfig] = None,\n    **kwargs,\n) -> BaseRLAgent:\n    \"\"\"\n    Factory function to create imitation learning traders.\n\n    Args:\n        agent_type: One of 'bc', 'gail', 'dagger'\n        env: Gym environment\n        config: Imitation configuration\n        **kwargs: Additional arguments (e.g., expert_policy for DAgger)\n\n    Returns:\n        Imitation learning trader instance\n    \"\"\"\n    if agent_type.lower() == 'dagger' and 'expert_policy' not in kwargs:\n        raise ValueError(\"DAgger requires 'expert_policy' argument\")\n\n    agents = {\n        'bc': BehavioralCloningTrader,\n        'behavioral_cloning': BehavioralCloningTrader,\n        'gail': GAILTrader,\n        'dagger': lambda e, c: DAggerTrader(e, kwargs.pop('expert_policy'), c),\n    }\n\n    agent_type = agent_type.lower()\n    if agent_type not in agents:\n        raise ValueError(f\"Unknown agent type: {agent_type}. Available: {list(agents.keys())}\")\n\n    return agents[agent_type](env, config)",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "expert_policy",
    "category": "reinforcement_learning",
    "formula": "action",
    "explanation": "",
    "python_code": "def expert_policy(state: np.ndarray) -> np.ndarray:\n        action = model.select_action(state, deterministic=True)\n        if noise > 0:\n            action = action + np.random.normal(0, noise, size=action.shape)\n            action = np.clip(action, -1, 1)\n        return action",
    "source_file": "core\\rl\\imitation.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(\n        self,\n        task_id: int,\n        train_data: Dict[str, torch.Tensor],\n        test_data: Dict[str, torch.Tensor],\n        metadata: Optional[Dict[str, Any]] = None,\n    ):\n        self.task_id = task_id\n        self.train_data = train_data  # Support set\n        self.test_data = test_data  # Query set\n        self.metadata = metadata or {}",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MetaTask"
  },
  {
    "name": "from_buffer",
    "category": "reinforcement_learning",
    "formula": "cls(task_id, train_data, test_data)",
    "explanation": "Create task from replay buffer by sampling.",
    "python_code": "def from_buffer(\n        cls,\n        task_id: int,\n        buffer: ReplayBuffer,\n        train_size: int,\n        test_size: int,\n        device: str = 'cpu',\n    ) -> 'MetaTask':\n        \"\"\"Create task from replay buffer by sampling.\"\"\"\n        # Sample train/test splits\n        all_indices = np.random.permutation(buffer.size)\n        train_indices = all_indices[:train_size]\n        test_indices = all_indices[train_size:train_size + test_size]\n\n        train_data = {\n            'states': torch.FloatTensor(buffer.states[train_indices]).to(device),\n            'actions': torch.FloatTensor(buffer.actions[train_indices]).to(device),\n            'rewards': torch.FloatTensor(buffer.rewards[train_indices]).to(device),\n            'next_states': torch.FloatTensor(buffer.next_states[train_indices]).to(device),\n            'dones': torch.FloatTensor(buffer.dones[train_indices]).to(device),\n        }\n\n        test_data = {\n            'states': torch.FloatTensor(buffer.states[test_indices]).to(device),\n            'actions': torch.FloatTensor(buffer.actions[test_indices]).to(device),\n            'rewards': torch.FloatTensor(buffer.rewards[test_indices]).to(device),\n            'next_states': torch.FloatTensor(buffer.next_states[test_indices]).to(device),\n            'dones': torch.FloatTensor(buffer.dones[test_indices]).to(device),\n        }\n\n        return cls(task_id, train_data, test_data)",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MetaTask"
  },
  {
    "name": "sample_tasks",
    "category": "reinforcement_learning",
    "formula": "tasks",
    "explanation": "Sample n_tasks tasks from distribution.",
    "python_code": "def sample_tasks(self, n_tasks: int) -> List[MetaTask]:\n        \"\"\"Sample n_tasks tasks from distribution.\"\"\"\n        task_indices = np.random.choice(self.n_tasks, size=n_tasks, replace=False)\n        tasks = []\n\n        for i, idx in enumerate(task_indices):\n            if self.buffers[idx].size < self.config.task_batch_size * 2:\n                # Not enough data, skip\n                continue\n\n            task = MetaTask.from_buffer(\n                task_id=idx,\n                buffer=self.buffers[idx],\n                train_size=self.config.task_batch_size,\n                test_size=self.config.task_batch_size,\n                device=self.device,\n            )\n            tasks.append(task)\n\n        return tasks",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TaskDistribution"
  },
  {
    "name": "collect_data",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Collect data for all tasks using given policy.",
    "python_code": "def collect_data(self, policy: Callable, n_samples: int):\n        \"\"\"Collect data for all tasks using given policy.\"\"\"\n        for i, env in enumerate(self.envs):\n            state, _ = env.reset()\n\n            for _ in range(n_samples):\n                action = policy(state)\n                next_state, reward, terminated, truncated, _ = env.step(action)\n                done = terminated or truncated\n\n                self.buffers[i].add(state, action, reward, next_state, float(done))\n\n                if done:\n                    state, _ = env.reset()\n                else:\n                    state = next_state",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TaskDistribution"
  },
  {
    "name": "_extract_regimes",
    "category": "regime",
    "formula": "tasks",
    "explanation": "Extract regime-based tasks from data.",
    "python_code": "def _extract_regimes(\n        self,\n        data: Dict[str, torch.Tensor],\n    ) -> List[MetaTask]:\n        \"\"\"Extract regime-based tasks from data.\"\"\"\n        n_samples = data['states'].shape[0]\n        window_size = self.config.regime_window\n        step_size = int(window_size * (1 - self.config.regime_overlap))\n\n        tasks = []\n        task_id = 0\n\n        for start in range(0, n_samples - window_size, step_size):\n            end = start + window_size\n\n            # Split into train/test\n            mid = start + window_size // 2\n\n            train_data = {k: v[start:mid] for k, v in data.items()}\n            test_data = {k: v[mid:end] for k, v in data.items()}\n\n            task = MetaTask(task_id, train_data, test_data)\n            tasks.append(task)\n            task_id += 1\n\n        return tasks",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RegimeTaskDistribution"
  },
  {
    "name": "sample_tasks",
    "category": "regime",
    "formula": "[self.tasks[i] for i in indices]",
    "explanation": "Sample n_tasks regime tasks.",
    "python_code": "def sample_tasks(self, n_tasks: int) -> List[MetaTask]:\n        \"\"\"Sample n_tasks regime tasks.\"\"\"\n        if len(self.tasks) < n_tasks:\n            return self.tasks\n        indices = np.random.choice(len(self.tasks), size=n_tasks, replace=False)\n        return [self.tasks[i] for i in indices]",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RegimeTaskDistribution"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "mean, self.log_std.expand(mean.shape[0], -1) | mean, log_std.expand(mean.shape[0], -1)",
    "explanation": "Forward pass with optional external parameters.\n\nArgs:\n    state: Input state\n    params: If provided, use these parameters instead of self.parameters()",
    "python_code": "def forward(\n        self,\n        state: torch.Tensor,\n        params: Optional[Dict[str, torch.Tensor]] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward pass with optional external parameters.\n\n        Args:\n            state: Input state\n            params: If provided, use these parameters instead of self.parameters()\n        \"\"\"\n        if params is None:\n            # Standard forward\n            x = state\n            for layer in self.layers:\n                x = F.relu(layer(x))\n            mean = self.mean_head(x)\n            return mean, self.log_std.expand(mean.shape[0], -1)\n        else:\n            # Functional forward with external params\n            x = state\n            for i, layer in enumerate(self.layers):\n                w = params[f'layers.{i}.weight']\n                b = params[f'layers.{i}.bias']\n                x = F.relu(F.linear(x, w, b))\n            w = params['mean_head.weight']\n            b = params['mean_head.bias']\n            mean = F.linear(x, w, b)\n            log_std = params['log_std']\n            return mean, log_std.expand(mean.shape[0], -1)",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MAMLPolicy"
  },
  {
    "name": "sample",
    "category": "reinforcement_learning",
    "formula": "torch.tanh(action), log_prob",
    "explanation": "Sample action from policy.",
    "python_code": "def sample(\n        self,\n        state: torch.Tensor,\n        params: Optional[Dict[str, torch.Tensor]] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Sample action from policy.\"\"\"\n        mean, log_std = self.forward(state, params)\n        std = log_std.exp()\n        dist = Normal(mean, std)\n        action = dist.rsample()\n        log_prob = dist.log_prob(action).sum(-1, keepdim=True)\n        return torch.tanh(action), log_prob",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MAMLPolicy"
  },
  {
    "name": "get_params",
    "category": "reinforcement_learning",
    "formula": "{name: param.clone() for name, param in self.named_parameters()}",
    "explanation": "Get named parameters as dictionary.",
    "python_code": "def get_params(self) -> Dict[str, torch.Tensor]:\n        \"\"\"Get named parameters as dictionary.\"\"\"\n        return {name: param.clone() for name, param in self.named_parameters()}",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MAMLPolicy"
  },
  {
    "name": "_inner_loop",
    "category": "reinforcement_learning",
    "formula": "adapted_params",
    "explanation": "Perform inner loop adaptation on single task.\n\nReturns adapted parameters.",
    "python_code": "def _inner_loop(\n        self,\n        task: MetaTask,\n        params: Dict[str, torch.Tensor],\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Perform inner loop adaptation on single task.\n\n        Returns adapted parameters.\n        \"\"\"\n        adapted_params = {k: v.clone() for k, v in params.items()}\n\n        for step in range(self.config.inner_steps):\n            # Get task data\n            states = task.train_data['states']\n            actions = task.train_data['actions']\n            rewards = task.train_data['rewards']\n\n            # Compute policy loss\n            _, log_probs = self.policy.sample(states, adapted_params)\n\n            # Simple REINFORCE loss for illustration\n            # In practice, use PPO or other policy gradient\n            loss = -(log_probs * rewards).mean()\n\n            # Compute gradients w.r.t. adapted params\n            grads = torch.autograd.grad(\n                loss,\n                adapted_params.values(),\n                create_graph=True,  # For second-order gradients\n            )\n\n            # Update adapted params\n            adapted_params = {\n                k: v - self.config.inner_lr * g\n                for (k, v), g in zip(adapted_params.items(), grads)\n            }\n\n        return adapted_params",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MAMLTrader"
  },
  {
    "name": "_compute_meta_loss",
    "category": "reinforcement_learning",
    "formula": "meta_loss",
    "explanation": "Compute loss on query set with adapted parameters.",
    "python_code": "def _compute_meta_loss(\n        self,\n        task: MetaTask,\n        adapted_params: Dict[str, torch.Tensor],\n    ) -> torch.Tensor:\n        \"\"\"Compute loss on query set with adapted parameters.\"\"\"\n        states = task.test_data['states']\n        actions = task.test_data['actions']\n        rewards = task.test_data['rewards']\n\n        _, log_probs = self.policy.sample(states, adapted_params)\n        meta_loss = -(log_probs * rewards).mean()\n\n        return meta_loss",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MAMLTrader"
  },
  {
    "name": "meta_train_step",
    "category": "machine_learning",
    "formula": "{'meta_loss': total_meta_loss.item()}",
    "explanation": "Perform one meta-training step on batch of tasks.",
    "python_code": "def meta_train_step(self, tasks: List[MetaTask]) -> Dict[str, float]:\n        \"\"\"\n        Perform one meta-training step on batch of tasks.\n        \"\"\"\n        initial_params = self.policy.get_params()\n\n        total_meta_loss = 0.0\n        n_tasks = len(tasks)\n\n        for task in tasks:\n            # Inner loop: adapt to task\n            adapted_params = self._inner_loop(task, initial_params)\n\n            # Outer loop: compute meta-loss\n            meta_loss = self._compute_meta_loss(task, adapted_params)\n            total_meta_loss = total_meta_loss + meta_loss / n_tasks\n\n        # Update meta-parameters\n        self.meta_optimizer.zero_grad()\n        total_meta_loss.backward()\n        nn.utils.clip_grad_norm_(self.policy.parameters(), self.config.max_grad_norm)\n        self.meta_optimizer.step()\n\n        return {'meta_loss': total_meta_loss.item()}",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MAMLTrader"
  },
  {
    "name": "adapt",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Adapt policy to new task/regime.\n\nArgs:\n    data: Dictionary with 'states', 'actions', 'rewards'\n    n_steps: Number of adaptation steps\n\nReturns:\n    Self with adapted parameters",
    "python_code": "def adapt(\n        self,\n        data: Dict[str, torch.Tensor],\n        n_steps: Optional[int] = None,\n    ) -> 'MAMLTrader':\n        \"\"\"\n        Adapt policy to new task/regime.\n\n        Args:\n            data: Dictionary with 'states', 'actions', 'rewards'\n            n_steps: Number of adaptation steps\n\n        Returns:\n            Self with adapted parameters\n        \"\"\"\n        n_steps = n_steps or self.config.inner_steps\n\n        for step in range(n_steps):\n            states = data['states']\n            rewards = data['rewards']\n\n            _, log_probs = self.policy.sample(states)\n            loss = -(log_probs * rewards).mean()\n\n            # Update policy\n            self.meta_optimizer.zero_grad()\n            loss.backward()\n            self.meta_optimizer.step()\n\n        return self",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MAMLTrader"
  },
  {
    "name": "select_action",
    "category": "reinforcement_learning",
    "formula": "action.cpu().numpy()[0]",
    "explanation": "Select action from policy.",
    "python_code": "def select_action(self, state: np.ndarray, deterministic: bool = False) -> np.ndarray:\n        \"\"\"Select action from policy.\"\"\"\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n            if deterministic:\n                mean, _ = self.policy(state_tensor)\n                action = torch.tanh(mean)\n            else:\n                action, _ = self.policy.sample(state_tensor)\n            return action.cpu().numpy()[0]",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MAMLTrader"
  },
  {
    "name": "train",
    "category": "machine_learning",
    "formula": "{'loss': loss.item()}",
    "explanation": "Standard training on batch (for non-meta updates).",
    "python_code": "def train(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n        \"\"\"Standard training on batch (for non-meta updates).\"\"\"\n        states = batch['states']\n        rewards = batch['rewards']\n\n        _, log_probs = self.policy.sample(states)\n        loss = -(log_probs * rewards).mean()\n\n        self.meta_optimizer.zero_grad()\n        loss.backward()\n        self.meta_optimizer.step()\n\n        return {'loss': loss.item()}",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MAMLTrader"
  },
  {
    "name": "meta_learn",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Main meta-learning loop.\n\nArgs:\n    task_distribution: Distribution over tasks\n    iterations: Number of meta-iterations\n    callback: Optional callback for logging",
    "python_code": "def meta_learn(\n        self,\n        task_distribution: TaskDistribution,\n        iterations: Optional[int] = None,\n        callback=None,\n    ) -> 'MAMLTrader':\n        \"\"\"\n        Main meta-learning loop.\n\n        Args:\n            task_distribution: Distribution over tasks\n            iterations: Number of meta-iterations\n            callback: Optional callback for logging\n        \"\"\"\n        iterations = iterations or self.config.meta_iterations\n\n        for i in range(iterations):\n            # Sample tasks\n            tasks = task_distribution.sample_tasks(self.config.meta_batch_size)\n\n            if len(tasks) == 0:\n                continue\n\n            # Meta-training step\n            metrics = self.meta_train_step(tasks)\n\n            self.total_steps += 1\n\n            if callback and i % self.config.eval_freq == 0:\n                callback({**metrics, 'iteration': i})\n\n        return self",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MAMLTrader"
  },
  {
    "name": "state_dict",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "",
    "python_code": "def state_dict(self) -> Dict[str, Any]:\n        return {\n            'policy': self.policy.state_dict(),\n            'value': self.value.state_dict(),\n            'meta_optimizer': self.meta_optimizer.state_dict(),\n        }",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MAMLTrader"
  },
  {
    "name": "load_state_dict",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def load_state_dict(self, state_dict: Dict[str, Any]):\n        self.policy.load_state_dict(state_dict['policy'])\n        self.value.load_state_dict(state_dict['value'])\n        self.meta_optimizer.load_state_dict(state_dict['meta_optimizer'])",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MAMLTrader"
  },
  {
    "name": "_clone_state_dict",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "Clone current model parameters.",
    "python_code": "def _clone_state_dict(self) -> Dict[str, torch.Tensor]:\n        \"\"\"Clone current model parameters.\"\"\"\n        return {\n            'policy': {k: v.clone() for k, v in self.policy.state_dict().items()},\n            'value': {k: v.clone() for k, v in self.value.state_dict().items()},\n        }",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ReptileTrader"
  },
  {
    "name": "_interpolate_params",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Interpolate current params towards task-specific params.",
    "python_code": "def _interpolate_params(\n        self,\n        old_params: Dict[str, Dict[str, torch.Tensor]],\n        epsilon: float,\n    ):\n        \"\"\"Interpolate current params towards task-specific params.\"\"\"\n        for name, param in self.policy.named_parameters():\n            old_p = old_params['policy'][name]\n            param.data.copy_(old_p + epsilon * (param.data - old_p))\n\n        for name, param in self.value.named_parameters():\n            old_p = old_params['value'][name]\n            param.data.copy_(old_p + epsilon * (param.data - old_p))",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ReptileTrader"
  },
  {
    "name": "_train_on_task",
    "category": "machine_learning",
    "formula": "total_loss / n_steps",
    "explanation": "Train on single task for n_steps.",
    "python_code": "def _train_on_task(\n        self,\n        task: MetaTask,\n        n_steps: int,\n    ) -> float:\n        \"\"\"Train on single task for n_steps.\"\"\"\n        total_loss = 0.0\n\n        for step in range(n_steps):\n            states = task.train_data['states']\n            rewards = task.train_data['rewards']\n\n            _, log_probs = self.policy.sample(states)\n            loss = -(log_probs * rewards).mean()\n\n            self.task_optimizer.zero_grad()\n            loss.backward()\n            self.task_optimizer.step()\n\n            total_loss += loss.item()\n\n        return total_loss / n_steps",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ReptileTrader"
  },
  {
    "name": "meta_train_step",
    "category": "machine_learning",
    "formula": "{'task_loss': task_loss}",
    "explanation": "Perform one Reptile meta-training step.",
    "python_code": "def meta_train_step(self, task: MetaTask) -> Dict[str, float]:\n        \"\"\"\n        Perform one Reptile meta-training step.\n        \"\"\"\n        # Save initial parameters\n        initial_params = self._clone_state_dict()\n\n        # Train on task\n        task_loss = self._train_on_task(task, self.config.inner_steps)\n\n        # Interpolate back towards initial (Reptile update)\n        #  = _old +  * (_new - _old)\n        # Equivalent to: _new = _old +  * (_new - _old)\n        # So we first compute _new (done above), then interpolate\n        self._interpolate_params(initial_params, self.config.reptile_outer_lr)\n\n        return {'task_loss': task_loss}",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ReptileTrader"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "mean, log_std, new_hidden",
    "explanation": "Forward pass.\n\nReturns: mean, log_std, new_hidden",
    "python_code": "def forward(\n        self,\n        state: torch.Tensor,\n        prev_action: torch.Tensor,\n        prev_reward: torch.Tensor,\n        hidden: Optional[torch.Tensor] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Forward pass.\n\n        Returns: mean, log_std, new_hidden\n        \"\"\"\n        # Combine inputs\n        x = torch.cat([state, prev_action, prev_reward], dim=-1)\n        if x.dim() == 2:\n            x = x.unsqueeze(1)  # Add sequence dim\n\n        # GRU\n        if hidden is None:\n            hidden = torch.zeros(self.n_layers, x.shape[0], self.hidden_dim, device=x.device)\n\n        output, new_hidden = self.gru(x, hidden)\n        output = output[:, -1, :]  # Last timestep\n\n        # Policy heads\n        mean = self.mean_head(output)\n        log_std = self.log_std.expand(mean.shape[0], -1)\n\n        return mean, log_std, new_hidden",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RL2Policy"
  },
  {
    "name": "sample",
    "category": "reinforcement_learning",
    "formula": "torch.tanh(action), log_prob, new_hidden",
    "explanation": "Sample action from policy.",
    "python_code": "def sample(\n        self,\n        state: torch.Tensor,\n        prev_action: torch.Tensor,\n        prev_reward: torch.Tensor,\n        hidden: Optional[torch.Tensor] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Sample action from policy.\"\"\"\n        mean, log_std, new_hidden = self.forward(\n            state, prev_action, prev_reward, hidden\n        )\n        std = log_std.exp()\n        dist = Normal(mean, std)\n        action = dist.rsample()\n        log_prob = dist.log_prob(action).sum(-1, keepdim=True)\n        return torch.tanh(action), log_prob, new_hidden",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RL2Policy"
  },
  {
    "name": "update_reward",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Update previous reward for next action selection.",
    "python_code": "def update_reward(self, reward: float):\n        \"\"\"Update previous reward for next action selection.\"\"\"\n        self._prev_reward = torch.tensor([[reward]], dtype=torch.float32, device=self.device)",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RL2Trader"
  },
  {
    "name": "reset_hidden",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Reset RNN hidden state (call at episode start).",
    "python_code": "def reset_hidden(self):\n        \"\"\"Reset RNN hidden state (call at episode start).\"\"\"\n        self._hidden = None\n        self._prev_action = torch.zeros(1, self.action_dim, device=self.device)\n        self._prev_reward = torch.zeros(1, 1, device=self.device)",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RL2Trader"
  },
  {
    "name": "train",
    "category": "machine_learning",
    "formula": "{'loss': 0.0}",
    "explanation": "Train RL on batch of sequences.",
    "python_code": "def train(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n        \"\"\"Train RL on batch of sequences.\"\"\"\n        # This requires sequence data, not standard batch\n        # Simplified for illustration\n        return {'loss': 0.0}",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RL2Trader"
  },
  {
    "name": "create_meta_trader",
    "category": "reinforcement_learning",
    "formula": "agents[agent_type](env, config)",
    "explanation": "Factory function to create meta-learning traders.\n\nArgs:\n    agent_type: One of 'maml', 'reptile', 'rl2'\n    env: Gym environment\n    config: Meta-learning configuration\n\nReturns:\n    Meta-learning trader instance",
    "python_code": "def create_meta_trader(\n    agent_type: str,\n    env: gym.Env,\n    config: Optional[MetaLearningConfig] = None,\n) -> BaseRLAgent:\n    \"\"\"\n    Factory function to create meta-learning traders.\n\n    Args:\n        agent_type: One of 'maml', 'reptile', 'rl2'\n        env: Gym environment\n        config: Meta-learning configuration\n\n    Returns:\n        Meta-learning trader instance\n    \"\"\"\n    agents = {\n        'maml': MAMLTrader,\n        'reptile': ReptileTrader,\n        'rl2': RL2Trader,\n    }\n\n    agent_type = agent_type.lower()\n    if agent_type not in agents:\n        raise ValueError(f\"Unknown agent type: {agent_type}. Available: {list(agents.keys())}\")\n\n    return agents[agent_type](env, config)",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "create_regime_tasks",
    "category": "reinforcement_learning",
    "formula": "RegimeTaskDistribution(tensor_data, config)",
    "explanation": "Create task distribution from historical data.\n\nArgs:\n    data: Dictionary with 'states', 'actions', 'rewards', etc.\n    config: Meta-learning configuration\n\nReturns:\n    RegimeTaskDistribution for meta-training",
    "python_code": "def create_regime_tasks(\n    data: Dict[str, np.ndarray],\n    config: MetaLearningConfig,\n) -> RegimeTaskDistribution:\n    \"\"\"\n    Create task distribution from historical data.\n\n    Args:\n        data: Dictionary with 'states', 'actions', 'rewards', etc.\n        config: Meta-learning configuration\n\n    Returns:\n        RegimeTaskDistribution for meta-training\n    \"\"\"\n    # Convert to tensors\n    tensor_data = {\n        k: torch.FloatTensor(v).to(config.device)\n        for k, v in data.items()\n    }\n    return RegimeTaskDistribution(tensor_data, config)",
    "source_file": "core\\rl\\meta_learning.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(\n        self,\n        state_dim: int,\n        action_dim: int,\n        hidden_dims: List[int] = [256, 256],\n        use_lstm: bool = False,\n        lstm_hidden: int = 128,\n    ):\n        super().__init__()\n\n        self.use_lstm = use_lstm\n\n        # Shared backbone\n        layers = []\n        prev_dim = state_dim\n        for hidden_dim in hidden_dims[:-1]:\n            layers.extend([\n                nn.Linear(prev_dim, hidden_dim),\n                nn.ReLU(),\n            ])\n            prev_dim = hidden_dim\n        self.backbone = nn.Sequential(*layers)\n\n        # Optional LSTM layer\n        if use_lstm:\n            self.lstm = nn.LSTM(prev_dim, lstm_hidden, batch_first=True)\n            prev_dim = lstm_hidden\n\n        # Policy head (Gaussian)\n        self.policy_mean = nn.Linear(prev_dim, action_dim)\n        self.policy_log_std = nn.Parameter(torch.zeros(1, action_dim))\n\n        # Value head\n        self.value_head = nn.Linear(prev_dim, 1)\n\n        # Initialize weights (orthogonal initialization)\n        self._init_weights()",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "SharedActorCritic"
  },
  {
    "name": "_init_weights",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Orthogonal initialization (Schulman et al.)",
    "python_code": "def _init_weights(self):\n        \"\"\"Orthogonal initialization (Schulman et al.)\"\"\"\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n                nn.init.zeros_(m.bias)\n\n        # Smaller init for policy output\n        nn.init.orthogonal_(self.policy_mean.weight, gain=0.01)",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "SharedActorCritic"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "mean, log_std, value, hidden",
    "explanation": "Forward pass.\n\nReturns:\n    action_mean, action_log_std, value, new_hidden",
    "python_code": "def forward(\n        self,\n        state: torch.Tensor,\n        hidden: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Optional[Tuple]]:\n        \"\"\"\n        Forward pass.\n\n        Returns:\n            action_mean, action_log_std, value, new_hidden\n        \"\"\"\n        x = self.backbone(state)\n\n        if self.use_lstm:\n            if len(x.shape) == 2:\n                x = x.unsqueeze(1)  # Add sequence dim\n            x, hidden = self.lstm(x, hidden)\n            x = x.squeeze(1)\n\n        mean = self.policy_mean(x)\n        log_std = self.policy_log_std.expand_as(mean)\n        value = self.value_head(x)\n\n        return mean, log_std, value, hidden",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "SharedActorCritic"
  },
  {
    "name": "get_action",
    "category": "reinforcement_learning",
    "formula": "action, log_prob, value, new_hidden",
    "explanation": "Sample action from policy.",
    "python_code": "def get_action(\n        self,\n        state: torch.Tensor,\n        hidden: Optional[Tuple] = None,\n        deterministic: bool = False,\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Optional[Tuple]]:\n        \"\"\"Sample action from policy.\"\"\"\n        mean, log_std, value, new_hidden = self.forward(state, hidden)\n        std = log_std.exp()\n\n        if deterministic:\n            action = mean\n        else:\n            dist = Normal(mean, std)\n            action = dist.sample()\n\n        action = torch.tanh(action)\n        log_prob = Normal(mean, std).log_prob(action).sum(-1, keepdim=True)\n\n        return action, log_prob, value, new_hidden",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "SharedActorCritic"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Estimate joint value for each agent.",
    "python_code": "def forward(\n        self,\n        states: torch.Tensor,  # [batch, n_agents, state_dim]\n        actions: torch.Tensor,  # [batch, n_agents, action_dim]\n    ) -> torch.Tensor:\n        \"\"\"Estimate joint value for each agent.\"\"\"\n        batch_size = states.shape[0]\n\n        # Flatten all observations and actions\n        x = torch.cat([\n            states.view(batch_size, -1),\n            actions.view(batch_size, -1),\n        ], dim=-1)\n\n        return self.net(x)",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CentralizedCritic"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "agent_values.sum(dim=-1, keepdim=True)",
    "explanation": "Mix agent values into team value.\n\nArgs:\n    agent_values: [batch, n_agents] individual agent values\n\nReturns:\n    team_value: [batch, 1] decomposed team value",
    "python_code": "def forward(self, agent_values: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Mix agent values into team value.\n\n        Args:\n            agent_values: [batch, n_agents] individual agent values\n\n        Returns:\n            team_value: [batch, 1] decomposed team value\n        \"\"\"\n        return agent_values.sum(dim=-1, keepdim=True)",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VDNMixer"
  },
  {
    "name": "sync_with_global",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Copy global network parameters to local.",
    "python_code": "def sync_with_global(self):\n        \"\"\"Copy global network parameters to local.\"\"\"\n        self.local_network.load_state_dict(self.global_network.state_dict())",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "A3CWorker"
  },
  {
    "name": "run",
    "category": "reinforcement_learning",
    "formula": "t_start = t | R = 0 if terminal else V(s_t) | R = r_i + gamma * R",
    "explanation": "Run worker until global max steps reached.\n\nAlgorithm from Mnih et al. (2016):\n```\nrepeat:\n    sync_with_global()\n    t_start = t\n    while t - t_start < T_MAX and not terminal:\n        take action, observe reward\n    R = 0 if terminal else V(s_t)\n    for i in [t-1, ..., t_start]:\n        R = r_i + gamma * R\n        accumulate gradients\n    perform async update\n```",
    "python_code": "def run(self) -> Dict[str, float]:\n        \"\"\"\n        Run worker until global max steps reached.\n\n        Algorithm from Mnih et al. (2016):\n        ```\n        repeat:\n            sync_with_global()\n            t_start = t\n            while t - t_start < T_MAX and not terminal:\n                take action, observe reward\n            R = 0 if terminal else V(s_t)\n            for i in [t-1, ..., t_start]:\n                R = r_i + gamma * R\n                accumulate gradients\n            perform async update\n        ```\n        \"\"\"\n        state, _ = self.env.reset()\n        episode_reward = 0\n        episode_count = 0\n\n        while self.global_step_counter.value < self.config.global_max_steps:\n            self.sync_with_global()\n\n            # Collect T_MAX steps\n            states, actions, rewards, values, log_probs, dones = [], [], [], [], [], []\n            hidden = None\n\n            for _ in range(self.config.t_max):\n                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n\n                with torch.no_grad():\n                    action, log_prob, value, hidden = self.local_network.get_action(\n                        state_tensor, hidden\n                    )\n\n                action_np = action.cpu().numpy()[0]\n                next_state, reward, terminated, truncated, _ = self.env.step(action_np)\n                done = terminated or truncated\n\n                states.append(state)\n                actions.append(action_np)\n                rewards.append(reward)\n                values.append(value.item())\n                log_probs.append(log_prob.item())\n                dones.append(done)\n\n                state = next_state\n                episode_reward += reward\n\n                with self.global_step_counter.get_lock():\n                    self.global_step_counter.value += 1\n\n                if done:\n                    state, _ = self.env.reset()\n                    episode_count += 1\n                    episode_reward = 0\n                    hidden = None\n                    break\n\n            # Compute returns and advantages (GAE)\n            returns = self._compute_returns(rewards, values, dones)\n            advantages = [r - v for r, v in zip(returns, values)]\n\n            # Compute gradients on local network\n            loss = self._compute_loss(states, actions, returns, advantages, log_probs)\n\n            # Apply gradients to global network\n            self._async_update(loss)\n\n        return {'episodes': episode_count}",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "A3CWorker"
  },
  {
    "name": "_compute_returns",
    "category": "reinforcement_learning",
    "formula": "returns",
    "explanation": "Compute n-step returns with bootstrap.",
    "python_code": "def _compute_returns(\n        self,\n        rewards: List[float],\n        values: List[float],\n        dones: List[bool],\n    ) -> List[float]:\n        \"\"\"Compute n-step returns with bootstrap.\"\"\"\n        returns = []\n        R = 0 if dones[-1] else values[-1]\n\n        for i in reversed(range(len(rewards))):\n            R = rewards[i] + self.config.gamma * R * (1 - dones[i])\n            returns.insert(0, R)\n\n        return returns",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "A3CWorker"
  },
  {
    "name": "_compute_loss",
    "category": "reinforcement_learning",
    "formula": "loss",
    "explanation": "Compute A3C loss.",
    "python_code": "def _compute_loss(\n        self,\n        states: List[np.ndarray],\n        actions: List[np.ndarray],\n        returns: List[float],\n        advantages: List[float],\n        old_log_probs: List[float],\n    ) -> torch.Tensor:\n        \"\"\"Compute A3C loss.\"\"\"\n        states_t = torch.FloatTensor(np.array(states)).to(self.device)\n        actions_t = torch.FloatTensor(np.array(actions)).to(self.device)\n        returns_t = torch.FloatTensor(returns).unsqueeze(1).to(self.device)\n        advantages_t = torch.FloatTensor(advantages).unsqueeze(1).to(self.device)\n\n        # Normalize advantages\n        advantages_t = (advantages_t - advantages_t.mean()) / (advantages_t.std() + 1e-8)\n\n        # Forward pass\n        mean, log_std, values, _ = self.local_network(states_t)\n        std = log_std.exp()\n        dist = Normal(mean, std)\n\n        # Policy loss (with entropy bonus)\n        log_probs = dist.log_prob(actions_t).sum(-1, keepdim=True)\n        policy_loss = -(log_probs * advantages_t).mean()\n        entropy = dist.entropy().mean()\n\n        # Value loss\n        value_loss = F.mse_loss(values, returns_t)\n\n        # Total loss (Mnih et al. 2016)\n        loss = policy_loss + self.config.value_coef * value_loss - self.config.entropy_coef * entropy\n\n        return loss",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "A3CWorker"
  },
  {
    "name": "_async_update",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Asynchronous gradient update to global network.",
    "python_code": "def _async_update(self, loss: torch.Tensor):\n        \"\"\"Asynchronous gradient update to global network.\"\"\"\n        # Zero gradients\n        self.global_optimizer.zero_grad()\n\n        # Compute gradients on local network\n        loss.backward()\n\n        # Transfer gradients to global network\n        for local_param, global_param in zip(\n            self.local_network.parameters(),\n            self.global_network.parameters()\n        ):\n            if global_param.grad is None:\n                global_param.grad = local_param.grad.clone()\n            else:\n                global_param.grad.copy_(local_param.grad)\n\n        # Apply update (with optional lock)\n        if self.lock and self.config.use_lock:\n            with self.lock:\n                nn.utils.clip_grad_norm_(self.global_network.parameters(), self.config.max_grad_norm)\n                self.global_optimizer.step()\n        else:\n            nn.utils.clip_grad_norm_(self.global_network.parameters(), self.config.max_grad_norm)\n            self.global_optimizer.step()",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "A3CWorker"
  },
  {
    "name": "_create_env_fn",
    "category": "reinforcement_learning",
    "formula": "a simple wrapper | copy.deepcopy(env) | make_env",
    "explanation": "Create function to instantiate environment copies.",
    "python_code": "def _create_env_fn(self) -> Callable[[], gym.Env]:\n        \"\"\"Create function to instantiate environment copies.\"\"\"\n        # In practice, this should create independent env copies\n        # For now, return a simple wrapper\n        env = self.env\n        def make_env():\n            return copy.deepcopy(env)\n        return make_env",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "A3CTrader"
  },
  {
    "name": "learn",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Train A3C with parallel workers.\n\nArgs:\n    total_timesteps: Maximum training steps\n    callback: Optional callback function",
    "python_code": "def learn(\n        self,\n        total_timesteps: int,\n        callback: Optional[Callable] = None,\n    ) -> 'A3CTrader':\n        \"\"\"\n        Train A3C with parallel workers.\n\n        Args:\n            total_timesteps: Maximum training steps\n            callback: Optional callback function\n        \"\"\"\n        self.config.global_max_steps = total_timesteps\n        env_fn = self._create_env_fn()\n\n        # Create workers\n        self.workers = []\n        for i in range(self.config.n_workers):\n            worker = A3CWorker(\n                worker_id=i,\n                global_network=self.global_network,\n                env_fn=env_fn,\n                config=self.config,\n                global_optimizer=self.optimizer,\n                global_step_counter=self.global_step,\n                lock=self.lock,\n            )\n            self.workers.append(worker)\n\n        # Run workers in threads\n        threads = []\n        for worker in self.workers:\n            t = Thread(target=worker.run)\n            t.start()\n            threads.append(t)\n\n        # Wait for completion\n        for t in threads:\n            t.join()\n\n        self.total_steps = self.global_step.value\n        return self",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "A3CTrader"
  },
  {
    "name": "select_action",
    "category": "reinforcement_learning",
    "formula": "action.cpu().numpy()[0]",
    "explanation": "Select action using global network.",
    "python_code": "def select_action(self, state: np.ndarray, deterministic: bool = False) -> np.ndarray:\n        \"\"\"Select action using global network.\"\"\"\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n            action, _, _, _ = self.global_network.get_action(\n                state_tensor, deterministic=deterministic\n            )\n            return action.cpu().numpy()[0]",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "A3CTrader"
  },
  {
    "name": "train",
    "category": "machine_learning",
    "formula": "{}",
    "explanation": "Not used - A3C trains asynchronously.",
    "python_code": "def train(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n        \"\"\"Not used - A3C trains asynchronously.\"\"\"\n        return {}",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "A3CTrader"
  },
  {
    "name": "state_dict",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "",
    "python_code": "def state_dict(self) -> Dict[str, Any]:\n        return {\n            'global_network': self.global_network.state_dict(),\n            'optimizer': self.optimizer.state_dict(),\n        }",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "A3CTrader"
  },
  {
    "name": "load_state_dict",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def load_state_dict(self, state_dict: Dict[str, Any]):\n        self.global_network.load_state_dict(state_dict['global_network'])\n        self.optimizer.load_state_dict(state_dict['optimizer'])",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "A3CTrader"
  },
  {
    "name": "select_action",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Select action from individual policy.",
    "python_code": "def select_action(\n        self,\n        state: torch.Tensor,\n        deterministic: bool = False,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Select action from individual policy.\"\"\"\n        return self.actor.sample(state) if not deterministic else (\n            self.actor.get_action(state, deterministic=True),\n            torch.zeros(1)\n        )",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MAPPOAgent"
  },
  {
    "name": "select_action",
    "category": "reinforcement_learning",
    "formula": "combined_action",
    "explanation": "Select action using ensemble of agents.",
    "python_code": "def select_action(\n        self,\n        state: np.ndarray,\n        deterministic: bool = False,\n    ) -> np.ndarray:\n        \"\"\"Select action using ensemble of agents.\"\"\"\n        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n\n        # Get action from each agent\n        actions = []\n        for agent in self.agents:\n            with torch.no_grad():\n                action, _ = agent.select_action(state_tensor, deterministic)\n                actions.append(action.cpu().numpy()[0])\n\n        # Combine actions (average for now)\n        combined_action = np.mean(actions, axis=0)\n        return combined_action",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MAPPOTrader"
  },
  {
    "name": "select_actions_all_agents",
    "category": "reinforcement_learning",
    "formula": "actions, log_probs",
    "explanation": "Select actions for all agents.",
    "python_code": "def select_actions_all_agents(\n        self,\n        states: torch.Tensor,  # [batch, n_agents, state_dim]\n        deterministic: bool = False,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Select actions for all agents.\"\"\"\n        batch_size = states.shape[0]\n        actions = []\n        log_probs = []\n\n        for i, agent in enumerate(self.agents):\n            state_i = states[:, i, :]\n            action, log_prob = agent.select_action(state_i, deterministic)\n            actions.append(action)\n            log_probs.append(log_prob)\n\n        actions = torch.stack(actions, dim=1)  # [batch, n_agents, action_dim]\n        log_probs = torch.stack(log_probs, dim=1)  # [batch, n_agents, 1]\n\n        return actions, log_probs",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MAPPOTrader"
  },
  {
    "name": "train",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "Train MAPPO with centralized critic.\n\nPPO update for each agent's actor, centralized critic update.",
    "python_code": "def train(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n        \"\"\"\n        Train MAPPO with centralized critic.\n\n        PPO update for each agent's actor, centralized critic update.\n        \"\"\"\n        states = batch['states']  # [batch, n_agents, state_dim]\n        actions = batch['actions']  # [batch, n_agents, action_dim]\n        rewards = batch['rewards']  # [batch, n_agents]\n        next_states = batch['next_states']\n        dones = batch['dones']\n        old_log_probs = batch['log_probs']  # [batch, n_agents]\n\n        # Compute advantages\n        with torch.no_grad():\n            if self.config.use_centralized_critic:\n                values = self.centralized_critic(states, actions)  # [batch, n_agents]\n                next_actions, _ = self.select_actions_all_agents(next_states, deterministic=True)\n                next_values = self.centralized_critic(next_states, next_actions)\n            else:\n                values = torch.stack([\n                    self.critics[i](states[:, i, :])\n                    for i in range(self.n_agents)\n                ], dim=1).squeeze(-1)\n                next_values = torch.stack([\n                    self.critics[i](next_states[:, i, :])\n                    for i in range(self.n_agents)\n                ], dim=1).squeeze(-1)\n\n            # TD targets\n            targets = rewards + self.config.gamma * (1 - dones) * next_values\n            advantages = targets - values\n\n            # Normalize advantages\n            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n        # PPO update for each agent\n        total_actor_loss = 0.0\n        total_entropy = 0.0\n\n        for epoch in range(self.config.ppo_epochs):\n            for i, agent in enumerate(self.agents):\n                state_i = states[:, i, :]\n                action_i = actions[:, i, :]\n                advantage_i = advantages[:, i].unsqueeze(-1)\n                old_log_prob_i = old_log_probs[:, i].unsqueeze(-1)\n\n                # Current policy\n                mean, log_std = agent.actor(state_i)\n                std = log_std.exp()\n                dist = Normal(mean, std)\n                new_log_prob = dist.log_prob(action_i).sum(-1, keepdim=True)\n                entropy = dist.entropy().sum(-1).mean()\n\n                # PPO clipped objective\n                ratio = (new_log_prob - old_log_prob_i).exp()\n                surr1 = ratio * advantage_i\n                surr2 = torch.clamp(\n                    ratio, 1 - self.config.ppo_clip, 1 + self.config.ppo_clip\n                ) * advantage_i\n                actor_loss = -torch.min(surr1, surr2).mean()\n\n                # Update actor\n                agent.optimizer.zero_grad()\n                (actor_loss - self.config.entropy_coef * entropy).backward()\n                nn.utils.clip_grad_norm_(agent.actor.parameters(), self.config.max_grad_norm)\n                agent.optimizer.step()\n\n                total_actor_loss += actor_loss.item()\n                total_e",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MAPPOTrader"
  },
  {
    "name": "state_dict",
    "category": "reinforcement_learning",
    "formula": "state",
    "explanation": "",
    "python_code": "def state_dict(self) -> Dict[str, Any]:\n        state = {\n            'agents': [agent.actor.state_dict() for agent in self.agents],\n            'agent_optimizers': [agent.optimizer.state_dict() for agent in self.agents],\n        }\n        if self.config.use_centralized_critic:\n            state['centralized_critic'] = self.centralized_critic.state_dict()\n            state['critic_optimizer'] = self.critic_optimizer.state_dict()\n        return state",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MAPPOTrader"
  },
  {
    "name": "add",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def add(\n        self,\n        states: np.ndarray,\n        actions: np.ndarray,\n        rewards: np.ndarray,\n        next_states: np.ndarray,\n        dones: np.ndarray,\n        log_probs: np.ndarray,\n    ):\n        self.states[self.ptr] = states\n        self.actions[self.ptr] = actions\n        self.rewards[self.ptr] = rewards\n        self.next_states[self.ptr] = next_states\n        self.dones[self.ptr] = dones\n        self.log_probs[self.ptr] = log_probs\n\n        self.ptr = (self.ptr + 1) % self.max_size\n        self.size = min(self.size + 1, self.max_size)",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MultiAgentRolloutBuffer"
  },
  {
    "name": "sample",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "",
    "python_code": "def sample(self, batch_size: int, device: str = 'cpu') -> Dict[str, torch.Tensor]:\n        idx = np.random.randint(0, self.size, size=batch_size)\n        return {\n            'states': torch.FloatTensor(self.states[idx]).to(device),\n            'actions': torch.FloatTensor(self.actions[idx]).to(device),\n            'rewards': torch.FloatTensor(self.rewards[idx]).to(device),\n            'next_states': torch.FloatTensor(self.next_states[idx]).to(device),\n            'dones': torch.FloatTensor(self.dones[idx]).to(device),\n            'log_probs': torch.FloatTensor(self.log_probs[idx]).to(device),\n        }",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MultiAgentRolloutBuffer"
  },
  {
    "name": "clear",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def clear(self):\n        self.ptr = 0\n        self.size = 0",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MultiAgentRolloutBuffer"
  },
  {
    "name": "get_quotes",
    "category": "reinforcement_learning",
    "formula": "bid_price, ask_price, bid_size, ask_size",
    "explanation": "Generate bid/ask quotes.\n\nReturns: (bid_price, ask_price, bid_size, ask_size)",
    "python_code": "def get_quotes(\n        self,\n        mid_price: float,\n        state: torch.Tensor,\n    ) -> Tuple[float, float, float, float]:\n        \"\"\"\n        Generate bid/ask quotes.\n\n        Returns: (bid_price, ask_price, bid_size, ask_size)\n        \"\"\"\n        with torch.no_grad():\n            action, _ = self.actor.sample(state)\n            action = action.cpu().numpy()[0]\n\n        # Convert action to quotes\n        bid_offset = abs(action[0]) * 0.001  # Max 10 pips\n        ask_offset = abs(action[1]) * 0.001\n        bid_size = max(0.1, abs(action[2]))  # Min 0.1 lot\n        ask_size = max(0.1, abs(action[3]))\n\n        bid_price = mid_price - bid_offset\n        ask_price = mid_price + ask_offset\n\n        return bid_price, ask_price, bid_size, ask_size",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "LiquidityProviderAgent"
  },
  {
    "name": "get_order",
    "category": "reinforcement_learning",
    "formula": "'buy', best_ask, size | 'sell', best_bid, size | 'hold', 0.0, 0.0",
    "explanation": "Generate market order.\n\nReturns: (side 'buy'/'sell'/'hold', price, size)",
    "python_code": "def get_order(\n        self,\n        state: torch.Tensor,\n        best_bid: float,\n        best_ask: float,\n    ) -> Tuple[str, float, float]:\n        \"\"\"\n        Generate market order.\n\n        Returns: (side 'buy'/'sell'/'hold', price, size)\n        \"\"\"\n        with torch.no_grad():\n            action, _ = self.actor.sample(state)\n            action = action.cpu().numpy()[0]\n\n        direction = np.tanh(action[0])  # [-1, 1]\n        size = max(0.1, abs(action[1]))\n\n        if direction > 0.2:\n            return 'buy', best_ask, size\n        elif direction < -0.2:\n            return 'sell', best_bid, size\n        else:\n            return 'hold', 0.0, 0.0",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "LiquidityTakerAgent"
  },
  {
    "name": "_simulate_market",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "Simulate one step of market interaction.\n\n1. LPs post quotes\n2. LTs submit orders\n3. Match orders\n4. Compute rewards",
    "python_code": "def _simulate_market(\n        self,\n        state: torch.Tensor,\n        mid_price: float,\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Simulate one step of market interaction.\n\n        1. LPs post quotes\n        2. LTs submit orders\n        3. Match orders\n        4. Compute rewards\n        \"\"\"\n        # 1. Liquidity providers post quotes\n        self.order_book = {'bids': [], 'asks': []}\n        for lp in self.liquidity_providers:\n            bid_p, ask_p, bid_s, ask_s = lp.get_quotes(mid_price, state)\n            self.order_book['bids'].append({\n                'agent': lp, 'price': bid_p, 'size': bid_s\n            })\n            self.order_book['asks'].append({\n                'agent': lp, 'price': ask_p, 'size': ask_s\n            })\n\n        # Sort order book\n        self.order_book['bids'].sort(key=lambda x: -x['price'])  # Highest first\n        self.order_book['asks'].sort(key=lambda x: x['price'])  # Lowest first\n\n        best_bid = self.order_book['bids'][0]['price'] if self.order_book['bids'] else mid_price - 0.0001\n        best_ask = self.order_book['asks'][0]['price'] if self.order_book['asks'] else mid_price + 0.0001\n\n        # 2. Liquidity takers submit orders\n        trades = []\n        for lt in self.liquidity_takers:\n            side, price, size = lt.get_order(state, best_bid, best_ask)\n            if side != 'hold':\n                trades.append({\n                    'agent': lt, 'side': side, 'price': price, 'size': size\n                })\n\n        # 3. Match orders and compute market impact\n        lp_rewards = {lp.agent_id: 0.0 for lp in self.liquidity_providers}\n        lt_rewards = {lt.agent_id: 0.0 for lt in self.liquidity_takers}\n\n        for trade in trades:\n            lt = trade['agent']\n            side = trade['side']\n            size = trade['size']\n\n            # Market impact (Kyle 1985)\n            impact = self.config.market_impact_lambda * size\n\n            if side == 'buy':\n                # Match against asks\n                for ask in self.order_book['asks']:\n                    if size <= 0:\n                        break\n\n                    fill_size = min(size, ask['size'])\n                    fill_price = ask['price'] + impact\n\n                    # LP earns spread\n                    spread_earned = (fill_price - mid_price) * fill_size\n                    lp_rewards[ask['agent'].agent_id] += spread_earned\n\n                    # Update LP inventory\n                    ask['agent'].inventory -= fill_size\n\n                    # LT pays spread + impact\n                    lt_cost = (fill_price - mid_price) * fill_size\n                    lt_rewards[lt.agent_id] -= lt_cost\n                    lt.position += fill_size\n\n                    size -= fill_size\n\n            elif side == 'sell':\n                # Match against bids\n                for bid in self.order_book['bids']:\n                    if size <= 0:\n                        break\n\n                    fill_size = min(size, bid['size'])\n             ",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JPMorganMARL"
  },
  {
    "name": "select_action",
    "category": "execution",
    "formula": "np.array([avg_signal])",
    "explanation": "Select combined action from all agents.\n\nFor execution, use the average taker signal as trading direction.",
    "python_code": "def select_action(self, state: np.ndarray, deterministic: bool = False) -> np.ndarray:\n        \"\"\"\n        Select combined action from all agents.\n\n        For execution, use the average taker signal as trading direction.\n        \"\"\"\n        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n\n        # Get taker signals\n        signals = []\n        for lt in self.liquidity_takers:\n            with torch.no_grad():\n                action, _ = lt.actor.sample(state_tensor)\n                direction = np.tanh(action.cpu().numpy()[0, 0])\n                signals.append(direction)\n\n        # Average signal\n        avg_signal = np.mean(signals)\n        return np.array([avg_signal])",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JPMorganMARL"
  },
  {
    "name": "train",
    "category": "reinforcement_learning",
    "formula": "empty metrics | {'lp_loss': 0.0, 'lt_loss': 0.0}",
    "explanation": "Train all agents using PPO.",
    "python_code": "def train(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n        \"\"\"\n        Train all agents using PPO.\n        \"\"\"\n        # This would typically be called with specialized batch format\n        # For now, return empty metrics\n        return {'lp_loss': 0.0, 'lt_loss': 0.0}",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JPMorganMARL"
  },
  {
    "name": "reset",
    "category": "reinforcement_learning",
    "formula": "ma_obs, info",
    "explanation": "",
    "python_code": "def reset(self, **kwargs) -> Tuple[np.ndarray, Dict]:\n        obs, info = self.base_env.reset(**kwargs)\n        # Replicate observation for all agents\n        ma_obs = np.tile(obs, (self.n_agents, 1))\n        return ma_obs, info",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MAForexEnv"
  },
  {
    "name": "step",
    "category": "reinforcement_learning",
    "formula": "ma_obs, ma_rewards, terminated, truncated, info",
    "explanation": "",
    "python_code": "def step(self, actions: np.ndarray) -> Tuple[np.ndarray, np.ndarray, bool, bool, Dict]:\n        # Combine agent actions (average)\n        combined_action = np.mean(actions, axis=0)\n\n        obs, reward, terminated, truncated, info = self.base_env.step(combined_action)\n\n        # Replicate observation and reward for all agents\n        ma_obs = np.tile(obs, (self.n_agents, 1))\n        ma_rewards = np.full(self.n_agents, reward)\n\n        return ma_obs, ma_rewards, terminated, truncated, info",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MAForexEnv"
  },
  {
    "name": "create_multi_agent_trader",
    "category": "reinforcement_learning",
    "formula": "agents[agent_type](env, config)",
    "explanation": "Factory function to create multi-agent RL traders.\n\nArgs:\n    agent_type: One of 'a3c', 'mappo', 'jpmorgan'\n    env: Gym environment\n    config: Multi-agent configuration\n\nReturns:\n    Multi-agent RL trader instance",
    "python_code": "def create_multi_agent_trader(\n    agent_type: str,\n    env: gym.Env,\n    config: Optional[MultiAgentConfig] = None,\n) -> BaseRLAgent:\n    \"\"\"\n    Factory function to create multi-agent RL traders.\n\n    Args:\n        agent_type: One of 'a3c', 'mappo', 'jpmorgan'\n        env: Gym environment\n        config: Multi-agent configuration\n\n    Returns:\n        Multi-agent RL trader instance\n    \"\"\"\n    agents = {\n        'a3c': A3CTrader,\n        'mappo': MAPPOTrader,\n        'jpmorgan': JPMorganMARL,\n    }\n\n    agent_type = agent_type.lower()\n    if agent_type not in agents:\n        raise ValueError(f\"Unknown agent type: {agent_type}. Available: {list(agents.keys())}\")\n\n    return agents[agent_type](env, config)",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "make_env",
    "category": "reinforcement_learning",
    "formula": "copy.deepcopy(env)",
    "explanation": "",
    "python_code": "def make_env():\n            return copy.deepcopy(env)",
    "source_file": "core\\rl\\multi_agent.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(\n        self,\n        state_dim: int,\n        action_dim: int,\n        max_size: int = 1_000_000,\n    ):\n        self.max_size = max_size\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.size = 0\n\n        self.states = np.zeros((max_size, state_dim), dtype=np.float32)\n        self.actions = np.zeros((max_size, action_dim), dtype=np.float32)\n        self.rewards = np.zeros((max_size, 1), dtype=np.float32)\n        self.next_states = np.zeros((max_size, state_dim), dtype=np.float32)\n        self.dones = np.zeros((max_size, 1), dtype=np.float32)\n\n        # Optional: priorities for weighted sampling\n        self.priorities = np.ones(max_size, dtype=np.float32)",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OfflineReplayBuffer"
  },
  {
    "name": "load_from_numpy",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Load entire dataset at once.",
    "python_code": "def load_from_numpy(\n        self,\n        states: np.ndarray,\n        actions: np.ndarray,\n        rewards: np.ndarray,\n        next_states: np.ndarray,\n        dones: np.ndarray,\n    ):\n        \"\"\"Load entire dataset at once.\"\"\"\n        n = min(len(states), self.max_size)\n        self.states[:n] = states[:n]\n        self.actions[:n] = actions[:n]\n        self.rewards[:n] = rewards[:n].reshape(-1, 1)\n        self.next_states[:n] = next_states[:n]\n        self.dones[:n] = dones[:n].reshape(-1, 1)\n        self.size = n",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OfflineReplayBuffer"
  },
  {
    "name": "load_from_dataframe",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Load from pandas DataFrame.",
    "python_code": "def load_from_dataframe(self, df, state_cols: List[str], action_col: str, reward_col: str):\n        \"\"\"Load from pandas DataFrame.\"\"\"\n        states = df[state_cols].values\n        actions = df[action_col].values.reshape(-1, self.action_dim)\n        rewards = df[reward_col].values\n\n        # Next states are shifted\n        next_states = np.roll(states, -1, axis=0)\n        next_states[-1] = states[-1]\n\n        # Done flags (assume no termination in continuous trading)\n        dones = np.zeros(len(df))\n        dones[-1] = 1\n\n        self.load_from_numpy(states, actions, rewards, next_states, dones)",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OfflineReplayBuffer"
  },
  {
    "name": "sample",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "Sample batch uniformly.",
    "python_code": "def sample(self, batch_size: int, device: str = 'cpu') -> Dict[str, torch.Tensor]:\n        \"\"\"Sample batch uniformly.\"\"\"\n        idx = np.random.randint(0, self.size, size=batch_size)\n        return {\n            'states': torch.FloatTensor(self.states[idx]).to(device),\n            'actions': torch.FloatTensor(self.actions[idx]).to(device),\n            'rewards': torch.FloatTensor(self.rewards[idx]).to(device),\n            'next_states': torch.FloatTensor(self.next_states[idx]).to(device),\n            'dones': torch.FloatTensor(self.dones[idx]).to(device),\n        }",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OfflineReplayBuffer"
  },
  {
    "name": "sample_weighted",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "Sample batch with reward-weighted priorities.",
    "python_code": "def sample_weighted(self, batch_size: int, device: str = 'cpu') -> Dict[str, torch.Tensor]:\n        \"\"\"Sample batch with reward-weighted priorities.\"\"\"\n        # Compute probabilities from priorities\n        probs = self.priorities[:self.size] / self.priorities[:self.size].sum()\n        idx = np.random.choice(self.size, size=batch_size, p=probs)\n\n        return {\n            'states': torch.FloatTensor(self.states[idx]).to(device),\n            'actions': torch.FloatTensor(self.actions[idx]).to(device),\n            'rewards': torch.FloatTensor(self.rewards[idx]).to(device),\n            'next_states': torch.FloatTensor(self.next_states[idx]).to(device),\n            'dones': torch.FloatTensor(self.dones[idx]).to(device),\n        }",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OfflineReplayBuffer"
  },
  {
    "name": "compute_priorities_from_rewards",
    "category": "reinforcement_learning",
    "formula": "reward = higher priority).",
    "explanation": "Set priorities based on rewards (higher reward = higher priority).",
    "python_code": "def compute_priorities_from_rewards(self, temperature: float = 1.0):\n        \"\"\"Set priorities based on rewards (higher reward = higher priority).\"\"\"\n        rewards = self.rewards[:self.size].flatten()\n        # Softmax-style priorities\n        exp_rewards = np.exp((rewards - rewards.max()) / temperature)\n        self.priorities[:self.size] = exp_rewards / exp_rewards.sum()",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OfflineReplayBuffer"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Forward pass returning both Q-values.",
    "python_code": "def forward(\n        self,\n        state: torch.Tensor,\n        action: torch.Tensor,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Forward pass returning both Q-values.\"\"\"\n        return self.twin_critic(state, action)",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CQLCritic"
  },
  {
    "name": "q1_forward",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Forward pass for Q1 only.",
    "python_code": "def q1_forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n        \"\"\"Forward pass for Q1 only.\"\"\"\n        return self.twin_critic.q1_forward(state, action)",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CQLCritic"
  },
  {
    "name": "_compute_cql_loss",
    "category": "reinforcement_learning",
    "formula": "L_CQL = E[log(sum_a exp(Q(s,a)))] - E_D[Q(s,a)] | cql_q1_loss + cql_q2_loss",
    "explanation": "Compute CQL conservative penalty.\n\nL_CQL = E[log(sum_a exp(Q(s,a)))] - E_D[Q(s,a)]\n\nReference: Kumar et al. (2020), Equation 4",
    "python_code": "def _compute_cql_loss(\n        self,\n        states: torch.Tensor,\n        actions: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"\n        Compute CQL conservative penalty.\n\n        L_CQL = E[log(sum_a exp(Q(s,a)))] - E_D[Q(s,a)]\n\n        Reference: Kumar et al. (2020), Equation 4\n        \"\"\"\n        batch_size = states.shape[0]\n\n        # Sample random actions for logsumexp\n        random_actions = torch.FloatTensor(\n            batch_size, self.config.cql_n_actions, self.action_dim\n        ).uniform_(-1, 1).to(self.device)\n\n        # Sample policy actions\n        policy_actions = []\n        policy_log_probs = []\n        for _ in range(self.config.cql_n_actions):\n            a, log_p = self.actor.sample(states)\n            policy_actions.append(a)\n            policy_log_probs.append(log_p)\n\n        policy_actions = torch.stack(policy_actions, dim=1)\n        policy_log_probs = torch.stack(policy_log_probs, dim=1)\n\n        # Compute Q-values for random actions\n        q1_rand, q2_rand = [], []\n        for i in range(self.config.cql_n_actions):\n            q1, q2 = self.critic(states, random_actions[:, i])\n            q1_rand.append(q1)\n            q2_rand.append(q2)\n        q1_rand = torch.cat(q1_rand, dim=1)  # [batch, n_actions]\n        q2_rand = torch.cat(q2_rand, dim=1)\n\n        # Compute Q-values for policy actions\n        q1_policy, q2_policy = [], []\n        for i in range(self.config.cql_n_actions):\n            q1, q2 = self.critic(states, policy_actions[:, i])\n            q1_policy.append(q1)\n            q2_policy.append(q2)\n        q1_policy = torch.cat(q1_policy, dim=1)\n        q2_policy = torch.cat(q2_policy, dim=1)\n\n        # Compute Q-values for dataset actions\n        q1_data, q2_data = self.critic(states, actions)\n\n        # Importance sampling correction (optional)\n        if self.config.cql_importance_sample:\n            # Subtract log prob for importance sampling\n            q1_policy = q1_policy - policy_log_probs.squeeze(-1)\n            q2_policy = q2_policy - policy_log_probs.squeeze(-1)\n\n        # Logsumexp over sampled actions\n        # log(1/N * sum(exp(Q))) = logsumexp(Q) - log(N)\n        q1_logsumexp = torch.logsumexp(\n            torch.cat([q1_rand, q1_policy], dim=1), dim=1\n        ) - np.log(2 * self.config.cql_n_actions)\n\n        q2_logsumexp = torch.logsumexp(\n            torch.cat([q2_rand, q2_policy], dim=1), dim=1\n        ) - np.log(2 * self.config.cql_n_actions)\n\n        # CQL penalty\n        cql_q1_loss = (q1_logsumexp - q1_data.squeeze()).mean()\n        cql_q2_loss = (q2_logsumexp - q2_data.squeeze()).mean()\n\n        return cql_q1_loss + cql_q2_loss",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CQLTrader"
  },
  {
    "name": "select_action",
    "category": "reinforcement_learning",
    "formula": "action.cpu().numpy()[0]",
    "explanation": "Select action using trained policy.",
    "python_code": "def select_action(self, state: np.ndarray, deterministic: bool = False) -> np.ndarray:\n        \"\"\"Select action using trained policy.\"\"\"\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n            if deterministic:\n                action = self.actor.get_action(state_tensor, deterministic=True)\n            else:\n                action, _ = self.actor.sample(state_tensor)\n            return action.cpu().numpy()[0]",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CQLTrader"
  },
  {
    "name": "train",
    "category": "reinforcement_learning",
    "formula": "CQL = SAC + Conservative Penalty | {",
    "explanation": "Train CQL on offline batch.\n\nCQL = SAC + Conservative Penalty",
    "python_code": "def train(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n        \"\"\"\n        Train CQL on offline batch.\n\n        CQL = SAC + Conservative Penalty\n        \"\"\"\n        states = batch['states']\n        actions = batch['actions']\n        rewards = batch['rewards']\n        next_states = batch['next_states']\n        dones = batch['dones']\n\n        # ==================== Critic Update ====================\n\n        with torch.no_grad():\n            next_actions, next_log_probs = self.actor.sample(next_states)\n            q1_next, q2_next = self.critic_target(next_states, next_actions)\n            q_next = torch.min(q1_next, q2_next) - self.alpha * next_log_probs\n            q_target = rewards + self.config.gamma * (1 - dones) * q_next\n\n        # Standard SAC critic loss\n        q1, q2 = self.critic(states, actions)\n        critic_loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n\n        # CQL conservative penalty (the key addition!)\n        cql_loss = self._compute_cql_loss(states, actions)\n\n        # Total critic loss\n        if isinstance(self.cql_alpha, torch.Tensor):\n            total_critic_loss = critic_loss + self.cql_alpha * cql_loss\n        else:\n            total_critic_loss = critic_loss + self.cql_alpha * cql_loss\n\n        self.critic_optimizer.zero_grad()\n        total_critic_loss.backward()\n        self.critic_optimizer.step()\n\n        # ==================== Actor Update ====================\n\n        new_actions, log_probs = self.actor.sample(states)\n        q1_new, q2_new = self.critic(states, new_actions)\n        q_new = torch.min(q1_new, q2_new)\n        actor_loss = (self.alpha * log_probs - q_new).mean()\n\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n\n        # ==================== Alpha Update ====================\n\n        alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()\n\n        self.alpha_optimizer.zero_grad()\n        alpha_loss.backward()\n        self.alpha_optimizer.step()\n        self.alpha = self.log_alpha.exp()\n\n        # ==================== CQL Alpha Update (Lagrange) ====================\n\n        if self.config.cql_lagrange:\n            cql_alpha_loss = self.log_cql_alpha * (\n                cql_loss.detach() - self.config.cql_target_action_gap\n            )\n            self.cql_alpha_optimizer.zero_grad()\n            cql_alpha_loss.backward()\n            self.cql_alpha_optimizer.step()\n            self.cql_alpha = self.log_cql_alpha.exp().clamp(0, 1000)\n\n        # ==================== Target Update ====================\n\n        for param, target_param in zip(\n            self.critic.parameters(), self.critic_target.parameters()\n        ):\n            target_param.data.copy_(\n                self.config.tau * param.data + (1 - self.config.tau) * target_param.data\n            )\n\n        return {\n            'critic_loss': critic_loss.item(),\n            'cql_loss': cql_loss.item(),\n            'actor_loss': act",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CQLTrader"
  },
  {
    "name": "learn_offline",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Train on offline dataset.\n\nArgs:\n    dataset: Offline replay buffer with historical data\n    iterations: Number of gradient updates\n    callback: Optional callback for logging",
    "python_code": "def learn_offline(\n        self,\n        dataset: OfflineReplayBuffer,\n        iterations: Optional[int] = None,\n        callback=None,\n    ) -> 'CQLTrader':\n        \"\"\"\n        Train on offline dataset.\n\n        Args:\n            dataset: Offline replay buffer with historical data\n            iterations: Number of gradient updates\n            callback: Optional callback for logging\n        \"\"\"\n        self.buffer = dataset\n        iterations = iterations or self.config.offline_iterations\n\n        for i in range(iterations):\n            batch = self.buffer.sample(self.config.batch_size, self.device)\n            metrics = self.train(batch)\n\n            self.total_steps += 1\n\n            if callback and i % self.config.eval_freq == 0:\n                callback({**metrics, 'iteration': i})\n\n        return self",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CQLTrader"
  },
  {
    "name": "state_dict",
    "category": "reinforcement_learning",
    "formula": "state",
    "explanation": "",
    "python_code": "def state_dict(self) -> Dict[str, Any]:\n        state = {\n            'actor': self.actor.state_dict(),\n            'critic': self.critic.state_dict(),\n            'critic_target': self.critic_target.state_dict(),\n            'actor_optimizer': self.actor_optimizer.state_dict(),\n            'critic_optimizer': self.critic_optimizer.state_dict(),\n            'log_alpha': self.log_alpha,\n            'alpha_optimizer': self.alpha_optimizer.state_dict(),\n        }\n        if self.config.cql_lagrange:\n            state['log_cql_alpha'] = self.log_cql_alpha\n            state['cql_alpha_optimizer'] = self.cql_alpha_optimizer.state_dict()\n        return state",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CQLTrader"
  },
  {
    "name": "load_state_dict",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def load_state_dict(self, state_dict: Dict[str, Any]):\n        self.actor.load_state_dict(state_dict['actor'])\n        self.critic.load_state_dict(state_dict['critic'])\n        self.critic_target.load_state_dict(state_dict['critic_target'])\n        self.actor_optimizer.load_state_dict(state_dict['actor_optimizer'])\n        self.critic_optimizer.load_state_dict(state_dict['critic_optimizer'])\n        self.log_alpha = state_dict['log_alpha']\n        self.alpha = self.log_alpha.exp()\n        self.alpha_optimizer.load_state_dict(state_dict['alpha_optimizer'])\n        if self.config.cql_lagrange and 'log_cql_alpha' in state_dict:\n            self.log_cql_alpha = state_dict['log_cql_alpha']\n            self.cql_alpha = self.log_cql_alpha.exp()\n            self.cql_alpha_optimizer.load_state_dict(state_dict['cql_alpha_optimizer'])",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CQLTrader"
  },
  {
    "name": "encode",
    "category": "reinforcement_learning",
    "formula": "mean, logvar",
    "explanation": "Encode (s, a) to latent distribution.",
    "python_code": "def encode(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Encode (s, a) to latent distribution.\"\"\"\n        x = torch.cat([state, action], dim=-1)\n        h = self.encoder(x)\n        mean = self.mean_head(h)\n        logvar = self.logvar_head(h).clamp(-4, 4)\n        return mean, logvar",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VAE"
  },
  {
    "name": "decode",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Decode (s, z) to action.",
    "python_code": "def decode(self, state: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n        \"\"\"Decode (s, z) to action.\"\"\"\n        x = torch.cat([state, z], dim=-1)\n        return self.decoder(x)",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VAE"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "recon, mean, logvar",
    "explanation": "Forward pass with reparameterization.",
    "python_code": "def forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Forward pass with reparameterization.\"\"\"\n        mean, logvar = self.encode(state, action)\n\n        # Reparameterization trick\n        std = (0.5 * logvar).exp()\n        eps = torch.randn_like(std)\n        z = mean + std * eps\n\n        recon = self.decode(state, z)\n        return recon, mean, logvar",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VAE"
  },
  {
    "name": "sample",
    "category": "reinforcement_learning",
    "formula": "actions.view(batch_size, n_samples, self.action_dim)",
    "explanation": "Sample actions from prior.",
    "python_code": "def sample(self, state: torch.Tensor, n_samples: int = 1) -> torch.Tensor:\n        \"\"\"Sample actions from prior.\"\"\"\n        batch_size = state.shape[0]\n        z = torch.randn(batch_size, n_samples, self.latent_dim).to(state.device)\n\n        state_expanded = state.unsqueeze(1).expand(-1, n_samples, -1)\n        z = z.view(-1, self.latent_dim)\n        state_flat = state_expanded.reshape(-1, state.shape[-1])\n\n        actions = self.decode(state_flat, z)\n        return actions.view(batch_size, n_samples, self.action_dim)",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VAE"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "perturbed_action",
    "explanation": "Apply perturbation to action.",
    "python_code": "def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n        \"\"\"Apply perturbation to action.\"\"\"\n        x = torch.cat([state, action], dim=-1)\n        perturbation = self.net(x)\n        perturbed_action = (action + self.phi * perturbation).clamp(-1, 1)\n        return perturbed_action",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "PerturbationNetwork"
  },
  {
    "name": "select_action",
    "category": "reinforcement_learning",
    "formula": "best_action.cpu().numpy()",
    "explanation": "Select action using BCQ procedure.\n\n1. Sample N actions from VAE\n2. Apply perturbation\n3. Select best according to Q",
    "python_code": "def select_action(self, state: np.ndarray, deterministic: bool = False) -> np.ndarray:\n        \"\"\"\n        Select action using BCQ procedure.\n\n        1. Sample N actions from VAE\n        2. Apply perturbation\n        3. Select best according to Q\n        \"\"\"\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n\n            # Sample actions from VAE\n            n_samples = 100 if not deterministic else 10\n            vae_actions = self.vae.sample(state_tensor, n_samples)  # [1, n_samples, action_dim]\n\n            # Apply perturbation to each action\n            state_expanded = state_tensor.unsqueeze(1).expand(-1, n_samples, -1)\n            state_flat = state_expanded.reshape(-1, self.state_dim)\n            actions_flat = vae_actions.reshape(-1, self.action_dim)\n\n            perturbed_actions = self.perturbation(state_flat, actions_flat)\n            perturbed_actions = perturbed_actions.view(1, n_samples, self.action_dim)\n\n            # Select action with highest Q\n            q1, q2 = self.critic(state_flat, perturbed_actions.view(-1, self.action_dim))\n            q = torch.min(q1, q2).view(1, n_samples)\n\n            best_idx = q.argmax(dim=1)\n            best_action = perturbed_actions[0, best_idx].squeeze()\n\n            return best_action.cpu().numpy()",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BCQTrader"
  },
  {
    "name": "train",
    "category": "machine_learning",
    "formula": "{",
    "explanation": "Train BCQ on offline batch.",
    "python_code": "def train(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n        \"\"\"\n        Train BCQ on offline batch.\n        \"\"\"\n        states = batch['states']\n        actions = batch['actions']\n        rewards = batch['rewards']\n        next_states = batch['next_states']\n        dones = batch['dones']\n\n        # ==================== VAE Update ====================\n\n        recon, mean, logvar = self.vae(states, actions)\n        recon_loss = F.mse_loss(recon, actions)\n        kl_loss = -0.5 * (1 + logvar - mean.pow(2) - logvar.exp()).sum(dim=-1).mean()\n        vae_loss = recon_loss + 0.5 * kl_loss\n\n        self.vae_optimizer.zero_grad()\n        vae_loss.backward()\n        self.vae_optimizer.step()\n\n        # ==================== Critic Update ====================\n\n        with torch.no_grad():\n            # Sample next actions from VAE\n            next_vae_actions = self.vae.sample(next_states, 10)\n            next_state_expanded = next_states.unsqueeze(1).expand(-1, 10, -1)\n\n            # Apply perturbation\n            next_perturbed = self.perturbation(\n                next_state_expanded.reshape(-1, self.state_dim),\n                next_vae_actions.reshape(-1, self.action_dim)\n            ).view(-1, 10, self.action_dim)\n\n            # Get Q-values and select best action\n            q1_next, q2_next = self.critic_target(\n                next_state_expanded.reshape(-1, self.state_dim),\n                next_perturbed.reshape(-1, self.action_dim)\n            )\n            q_next = torch.min(q1_next, q2_next).view(-1, 10)\n            best_next_idx = q_next.argmax(dim=1, keepdim=True)\n\n            # Get Q-value for best next action\n            best_q_next = q_next.gather(1, best_next_idx)\n            q_target = rewards + self.config.gamma * (1 - dones) * best_q_next\n\n        # Current Q-values\n        q1, q2 = self.critic(states, actions)\n        critic_loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n\n        # ==================== Perturbation Update ====================\n\n        # Sample actions from VAE and perturb\n        with torch.no_grad():\n            sampled_actions = self.vae.sample(states, 1).squeeze(1)\n\n        perturbed_actions = self.perturbation(states, sampled_actions)\n        perturbation_loss = -self.critic.q1_forward(states, perturbed_actions).mean()\n\n        self.perturbation_optimizer.zero_grad()\n        perturbation_loss.backward()\n        self.perturbation_optimizer.step()\n\n        # ==================== Target Update ====================\n\n        for param, target_param in zip(\n            self.critic.parameters(), self.critic_target.parameters()\n        ):\n            target_param.data.copy_(\n                self.config.tau * param.data + (1 - self.config.tau) * target_param.data\n            )\n\n        return {\n            'vae_loss': vae_loss.item(),\n            'critic_loss': critic_loss.item(),\n     ",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BCQTrader"
  },
  {
    "name": "state_dict",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "",
    "python_code": "def state_dict(self) -> Dict[str, Any]:\n        return {\n            'vae': self.vae.state_dict(),\n            'perturbation': self.perturbation.state_dict(),\n            'critic': self.critic.state_dict(),\n            'critic_target': self.critic_target.state_dict(),\n            'vae_optimizer': self.vae_optimizer.state_dict(),\n            'perturbation_optimizer': self.perturbation_optimizer.state_dict(),\n            'critic_optimizer': self.critic_optimizer.state_dict(),\n        }",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BCQTrader"
  },
  {
    "name": "select_action",
    "category": "reinforcement_learning",
    "formula": "joint_action.cpu().numpy()[0]",
    "explanation": "Select joint action from all agents.",
    "python_code": "def select_action(self, state: np.ndarray, deterministic: bool = False) -> np.ndarray:\n        \"\"\"Select joint action from all agents.\"\"\"\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n\n            # Single-agent state used for each actor\n            per_agent_state = state_tensor[:, :self.state_dim]\n\n            actions = []\n            for actor in self.actors:\n                if deterministic:\n                    action = actor.get_action(per_agent_state, deterministic=True)\n                else:\n                    action, _ = actor.sample(per_agent_state)\n                actions.append(action)\n\n            joint_action = torch.cat(actions, dim=-1)\n            return joint_action.cpu().numpy()[0]",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CFCQLTrader"
  },
  {
    "name": "_compute_counterfactual_advantage",
    "category": "reinforcement_learning",
    "formula": "A_i = Q(s, a) - Q_cf(s, a_{-i}) | advantage",
    "explanation": "Compute counterfactual advantage for agent i.\n\nA_i = Q(s, a) - Q_cf(s, a_{-i})\n\nwhere a_{-i} is the joint action excluding agent i.",
    "python_code": "def _compute_counterfactual_advantage(\n        self,\n        states: torch.Tensor,\n        actions: torch.Tensor,\n        agent_idx: int,\n    ) -> torch.Tensor:\n        \"\"\"\n        Compute counterfactual advantage for agent i.\n\n        A_i = Q(s, a) - Q_cf(s, a_{-i})\n\n        where a_{-i} is the joint action excluding agent i.\n        \"\"\"\n        # Full joint Q-value\n        q1, q2 = self.critic(states, actions)\n        q_joint = torch.min(q1, q2)\n\n        # Counterfactual Q (without agent i's action)\n        actions_without_i = torch.cat([\n            actions[:, :agent_idx * self.action_dim],\n            actions[:, (agent_idx + 1) * self.action_dim:],\n        ], dim=-1)\n\n        q1_cf, q2_cf = self.cf_critics[agent_idx](states, actions_without_i)\n        q_cf = torch.min(q1_cf, q2_cf)\n\n        # Counterfactual advantage\n        advantage = q_joint - q_cf\n\n        return advantage",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CFCQLTrader"
  },
  {
    "name": "create_offline_trader",
    "category": "reinforcement_learning",
    "formula": "agents[agent_type](env, config, **kwargs)",
    "explanation": "Factory function to create offline RL traders.\n\nArgs:\n    agent_type: One of 'cql', 'bcq', 'cfcql'\n    env: Gym environment\n    config: Offline RL configuration\n    **kwargs: Additional arguments\n\nReturns:\n    Offline RL trader instance",
    "python_code": "def create_offline_trader(\n    agent_type: str,\n    env: gym.Env,\n    config: Optional[OfflineRLConfig] = None,\n    **kwargs,\n) -> BaseRLAgent:\n    \"\"\"\n    Factory function to create offline RL traders.\n\n    Args:\n        agent_type: One of 'cql', 'bcq', 'cfcql'\n        env: Gym environment\n        config: Offline RL configuration\n        **kwargs: Additional arguments\n\n    Returns:\n        Offline RL trader instance\n    \"\"\"\n    agents = {\n        'cql': CQLTrader,\n        'bcq': BCQTrader,\n        'cfcql': CFCQLTrader,\n    }\n\n    agent_type = agent_type.lower()\n    if agent_type not in agents:\n        raise ValueError(f\"Unknown agent type: {agent_type}. Available: {list(agents.keys())}\")\n\n    return agents[agent_type](env, config, **kwargs)",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "load_forex_dataset",
    "category": "reinforcement_learning",
    "formula": "buffer",
    "explanation": "Load forex data into offline buffer.\n\nArgs:\n    parquet_path: Path to parquet file\n    state_cols: Column names for state features\n    action_col: Column name for action (position)\n    reward_col: Column name for reward (return)\n    max_size: Maximum buffer size\n\nReturns:\n    OfflineReplayBuffer with loaded data",
    "python_code": "def load_forex_dataset(\n    parquet_path: str,\n    state_cols: List[str],\n    action_col: str,\n    reward_col: str,\n    max_size: int = 1_000_000,\n) -> OfflineReplayBuffer:\n    \"\"\"\n    Load forex data into offline buffer.\n\n    Args:\n        parquet_path: Path to parquet file\n        state_cols: Column names for state features\n        action_col: Column name for action (position)\n        reward_col: Column name for reward (return)\n        max_size: Maximum buffer size\n\n    Returns:\n        OfflineReplayBuffer with loaded data\n    \"\"\"\n    import pandas as pd\n\n    df = pd.read_parquet(parquet_path)\n\n    buffer = OfflineReplayBuffer(\n        state_dim=len(state_cols),\n        action_dim=1,\n        max_size=max_size,\n    )\n\n    buffer.load_from_dataframe(df, state_cols, action_col, reward_col)\n\n    return buffer",
    "source_file": "core\\rl\\offline_rl.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Initialize Portfolio Environment.\n\nArgs:\n    data_dict: Dict mapping asset name -> price array\n    config: Portfolio configuration",
    "python_code": "def __init__(\n        self,\n        data_dict: Dict[str, np.ndarray],\n        config: Optional[PortfolioConfig] = None,\n    ):\n        \"\"\"\n        Initialize Portfolio Environment.\n\n        Args:\n            data_dict: Dict mapping asset name -> price array\n            config: Portfolio configuration\n        \"\"\"\n        super().__init__()\n\n        self.config = config or PortfolioConfig()\n        self.assets = list(data_dict.keys())\n        self.n_assets = len(self.assets)\n\n        # Align all price series\n        min_len = min(len(prices) for prices in data_dict.values())\n        self.prices = {asset: prices[-min_len:] for asset, prices in data_dict.items()}\n        self.n_steps = min_len\n\n        # Compute returns\n        self.returns = {}\n        for asset, prices in self.prices.items():\n            self.returns[asset] = np.diff(np.log(prices), prepend=0)\n\n        # State dimension\n        lookback = self.config.lookback_window\n        self.state_dim = self.n_assets * lookback + self.n_assets + 3  # returns + weights + stats\n\n        # Spaces\n        self.observation_space = spaces.Box(\n            low=-np.inf,\n            high=np.inf,\n            shape=(self.state_dim,),\n            dtype=np.float32\n        )\n\n        self.action_space = spaces.Box(\n            low=self.config.min_weight,\n            high=self.config.max_weight,\n            shape=(self.n_assets,),\n            dtype=np.float32\n        )\n\n        # Episode state\n        self.current_step = 0\n        self.weights = np.zeros(self.n_assets)\n        self.portfolio_value = self.config.initial_capital\n        self.max_portfolio_value = self.config.initial_capital\n        self.returns_history = []",
    "source_file": "core\\rl\\portfolio.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "PortfolioEnv"
  },
  {
    "name": "_get_state",
    "category": "reinforcement_learning",
    "formula": "state.astype(np.float32)",
    "explanation": "Get current state observation.",
    "python_code": "def _get_state(self) -> np.ndarray:\n        \"\"\"Get current state observation.\"\"\"\n        lookback = self.config.lookback_window\n        start_idx = max(0, self.current_step - lookback)\n\n        # Price returns for each asset\n        returns_window = []\n        for asset in self.assets:\n            rets = self.returns[asset][start_idx:self.current_step]\n            # Pad if needed\n            if len(rets) < lookback:\n                rets = np.pad(rets, (lookback - len(rets), 0), mode='constant')\n            returns_window.append(rets)\n        returns_flat = np.concatenate(returns_window)\n\n        # Portfolio statistics\n        if len(self.returns_history) >= 2:\n            port_rets = np.array(self.returns_history[-lookback:])\n            sharpe = np.mean(port_rets) / (np.std(port_rets) + 1e-8) * np.sqrt(252)\n            vol = np.std(port_rets) * np.sqrt(252)\n        else:\n            sharpe = 0.0\n            vol = 0.0\n\n        drawdown = (self.max_portfolio_value - self.portfolio_value) / self.max_portfolio_value\n\n        stats = np.array([sharpe, vol, drawdown])\n\n        # Combine all features\n        state = np.concatenate([returns_flat, self.weights, stats])\n        return state.astype(np.float32)",
    "source_file": "core\\rl\\portfolio.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "PortfolioEnv"
  },
  {
    "name": "_normalize_weights",
    "category": "statistical",
    "formula": "weights",
    "explanation": "Normalize weights to satisfy constraints.\n\nConstraints:\n- Sum of absolute weights  max_leverage\n- Each weight in [min_weight, max_weight]",
    "python_code": "def _normalize_weights(self, raw_weights: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Normalize weights to satisfy constraints.\n\n        Constraints:\n        - Sum of absolute weights  max_leverage\n        - Each weight in [min_weight, max_weight]\n        \"\"\"\n        # Clip individual weights\n        weights = np.clip(raw_weights, self.config.min_weight, self.config.max_weight)\n\n        # Enforce leverage constraint\n        total_leverage = np.sum(np.abs(weights))\n        if total_leverage > self.config.max_leverage:\n            weights = weights / total_leverage * self.config.max_leverage\n\n        return weights",
    "source_file": "core\\rl\\portfolio.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "PortfolioEnv"
  },
  {
    "name": "_compute_transaction_cost",
    "category": "reinforcement_learning",
    "formula": "weight_change * self.config.transaction_cost_pct * self.portfolio_value",
    "explanation": "Compute transaction cost from rebalancing.",
    "python_code": "def _compute_transaction_cost(self, new_weights: np.ndarray) -> float:\n        \"\"\"Compute transaction cost from rebalancing.\"\"\"\n        weight_change = np.sum(np.abs(new_weights - self.weights))\n        return weight_change * self.config.transaction_cost_pct * self.portfolio_value",
    "source_file": "core\\rl\\portfolio.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "PortfolioEnv"
  },
  {
    "name": "step",
    "category": "reinforcement_learning",
    "formula": "if self.current_step < self.n_steps: | = 0.0 | += self.weights[i] * self.returns[asset][self.current_step]",
    "explanation": "Execute portfolio rebalancing step.",
    "python_code": "def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n        \"\"\"Execute portfolio rebalancing step.\"\"\"\n        # Normalize action to valid weights\n        new_weights = self._normalize_weights(action)\n\n        # Only rebalance if drift exceeds threshold\n        weight_drift = np.max(np.abs(new_weights - self.weights))\n        if weight_drift < self.config.rebalance_threshold:\n            new_weights = self.weights.copy()\n\n        # Compute transaction cost\n        transaction_cost = self._compute_transaction_cost(new_weights)\n\n        # Update weights\n        old_weights = self.weights.copy()\n        self.weights = new_weights\n\n        # Move to next step\n        self.current_step += 1\n\n        # Compute portfolio return\n        if self.current_step < self.n_steps:\n            port_return = 0.0\n            for i, asset in enumerate(self.assets):\n                port_return += self.weights[i] * self.returns[asset][self.current_step]\n\n            # Update portfolio value\n            self.portfolio_value *= (1 + port_return)\n            self.portfolio_value -= transaction_cost\n            self.max_portfolio_value = max(self.max_portfolio_value, self.portfolio_value)\n\n            self.returns_history.append(port_return)\n\n        # Check termination\n        terminated = False\n        truncated = False\n\n        if self.current_step >= self.n_steps - 1:\n            truncated = True\n\n        # Check drawdown constraint\n        drawdown = (self.max_portfolio_value - self.portfolio_value) / self.max_portfolio_value\n        if drawdown > self.config.max_drawdown:\n            terminated = True\n\n        # Compute reward (risk-adjusted return)\n        if len(self.returns_history) >= 2:\n            recent_rets = np.array(self.returns_history[-20:])\n            mean_ret = np.mean(recent_rets)\n            std_ret = np.std(recent_rets) + 1e-8\n\n            # Sharpe-like reward\n            reward = mean_ret / std_ret\n\n            # Volatility penalty\n            ann_vol = std_ret * np.sqrt(252)\n            if ann_vol > self.config.target_volatility:\n                vol_penalty = (ann_vol - self.config.target_volatility) * self.config.risk_aversion\n                reward -= vol_penalty\n        else:\n            reward = 0.0\n\n        # Transaction cost penalty\n        reward -= transaction_cost / self.portfolio_value\n\n        state = self._get_state() if not (terminated or truncated) else np.zeros(self.state_dim, dtype=np.float32)\n\n        info = {\n            'portfolio_value': self.portfolio_value,\n            'weights': self.weights.copy(),\n            'drawdown': drawdown,\n            'transaction_cost': transaction_cost,\n        }\n\n        return state, reward, terminated, truncated, info",
    "source_file": "core\\rl\\portfolio.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "PortfolioEnv"
  },
  {
    "name": "reset",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Reset environment.",
    "python_code": "def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, Dict]:\n        \"\"\"Reset environment.\"\"\"\n        super().reset(seed=seed)\n\n        self.current_step = self.config.lookback_window\n        self.weights = np.zeros(self.n_assets)\n        self.portfolio_value = self.config.initial_capital\n        self.max_portfolio_value = self.config.initial_capital\n        self.returns_history = []\n\n        return self._get_state(), {}",
    "source_file": "core\\rl\\portfolio.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "PortfolioEnv"
  },
  {
    "name": "compute_markowitz_weights",
    "category": "technical",
    "formula": "weights",
    "explanation": "Compute Markowitz mean-variance optimal weights.\n\nReference: Markowitz (1952)\n\"Portfolio Selection\"\n\nw* = (1/) * ^(-1) * \n\nwhere  is risk aversion,  is covariance matrix,  is expected returns",
    "python_code": "def compute_markowitz_weights(self, returns: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute Markowitz mean-variance optimal weights.\n\n        Reference: Markowitz (1952)\n        \"Portfolio Selection\"\n\n        w* = (1/) * ^(-1) * \n\n        where  is risk aversion,  is covariance matrix,  is expected returns\n        \"\"\"\n        mean_returns = np.mean(returns, axis=0)\n        cov_matrix = np.cov(returns.T) + 1e-6 * np.eye(returns.shape[1])\n\n        try:\n            inv_cov = np.linalg.inv(cov_matrix)\n            weights = inv_cov @ mean_returns / self.portfolio_config.risk_aversion\n            # Normalize\n            weights = weights / (np.sum(np.abs(weights)) + 1e-8)\n        except np.linalg.LinAlgError:\n            weights = np.ones(returns.shape[1]) / returns.shape[1]\n\n        return weights",
    "source_file": "core\\rl\\portfolio.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "RLPortfolioOptimizer"
  },
  {
    "name": "compute_risk_parity_weights",
    "category": "reinforcement_learning",
    "formula": "weights",
    "explanation": "Compute Risk Parity weights (equal risk contribution).\n\nReference: Maillard et al. (2010)\n\"The Properties of Equally Weighted Risk Contribution Portfolios\"\n\nEach asset contributes equally to portfolio risk.",
    "python_code": "def compute_risk_parity_weights(self, returns: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute Risk Parity weights (equal risk contribution).\n\n        Reference: Maillard et al. (2010)\n        \"The Properties of Equally Weighted Risk Contribution Portfolios\"\n\n        Each asset contributes equally to portfolio risk.\n        \"\"\"\n        cov_matrix = np.cov(returns.T) + 1e-6 * np.eye(returns.shape[1])\n        n_assets = returns.shape[1]\n\n        # Inverse volatility as starting point\n        vols = np.sqrt(np.diag(cov_matrix))\n        weights = 1.0 / (vols + 1e-8)\n        weights = weights / np.sum(weights)\n\n        # Newton-Raphson iteration for risk parity\n        for _ in range(10):\n            port_vol = np.sqrt(weights @ cov_matrix @ weights)\n            marginal_risk = cov_matrix @ weights / port_vol\n            risk_contrib = weights * marginal_risk\n            target_risk = port_vol / n_assets\n\n            # Gradient step\n            gradient = marginal_risk * (risk_contrib - target_risk)\n            weights = weights - 0.1 * gradient\n            weights = np.maximum(weights, 1e-6)\n            weights = weights / np.sum(weights)\n\n        return weights",
    "source_file": "core\\rl\\portfolio.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "RLPortfolioOptimizer"
  },
  {
    "name": "compute_kelly_weights",
    "category": "risk",
    "formula": "weights",
    "explanation": "Compute Kelly Criterion optimal weights.\n\nReference: Kelly (1956)\n\"A New Interpretation of Information Rate\"\n\nFractional Kelly: w* = f * ( / )\n\nwhere f is the Kelly fraction (typically 0.25 for safety)",
    "python_code": "def compute_kelly_weights(self, returns: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute Kelly Criterion optimal weights.\n\n        Reference: Kelly (1956)\n        \"A New Interpretation of Information Rate\"\n\n        Fractional Kelly: w* = f * ( / )\n\n        where f is the Kelly fraction (typically 0.25 for safety)\n        \"\"\"\n        mean_returns = np.mean(returns, axis=0)\n        variances = np.var(returns, axis=0) + 1e-8\n\n        # Full Kelly\n        kelly_weights = mean_returns / variances\n\n        # Fractional Kelly for safety\n        weights = self.portfolio_config.kelly_fraction * kelly_weights\n\n        # Normalize\n        weights = weights / (np.sum(np.abs(weights)) + 1e-8)\n\n        return weights",
    "source_file": "core\\rl\\portfolio.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "RLPortfolioOptimizer"
  },
  {
    "name": "select_action",
    "category": "reinforcement_learning",
    "formula": "action.cpu().numpy()[0]",
    "explanation": "Select portfolio weights.",
    "python_code": "def select_action(self, state: np.ndarray, deterministic: bool = False) -> np.ndarray:\n        \"\"\"Select portfolio weights.\"\"\"\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n\n            if deterministic:\n                action = self.actor.get_action(state_tensor, deterministic=True)\n            else:\n                action, _ = self.actor.sample(state_tensor)\n\n            return action.cpu().numpy()[0]",
    "source_file": "core\\rl\\portfolio.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "RLPortfolioOptimizer"
  },
  {
    "name": "train",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "Train portfolio optimizer (SAC-style).",
    "python_code": "def train(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n        \"\"\"Train portfolio optimizer (SAC-style).\"\"\"\n        states = batch['states']\n        actions = batch['actions']\n        rewards = batch['rewards']\n        next_states = batch['next_states']\n        dones = batch['dones']\n\n        # Update critic\n        with torch.no_grad():\n            next_actions, next_log_probs = self.actor.sample(next_states)\n            q1_next, q2_next = self.critic_target(next_states, next_actions)\n            q_next = torch.min(q1_next, q2_next) - self.alpha * next_log_probs\n            q_target = rewards + self.config.gamma * (1 - dones) * q_next\n\n        q1, q2 = self.critic(states, actions)\n        critic_loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)\n\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n\n        # Update actor\n        new_actions, log_probs = self.actor.sample(states)\n        q1_new, q2_new = self.critic(states, new_actions)\n        q_new = torch.min(q1_new, q2_new)\n        actor_loss = (self.alpha * log_probs - q_new).mean()\n\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n\n        # Update alpha\n        alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()\n        self.alpha_optimizer.zero_grad()\n        alpha_loss.backward()\n        self.alpha_optimizer.step()\n        self.alpha = self.log_alpha.exp()\n\n        # Soft update target\n        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n            target_param.data.copy_(0.005 * param.data + 0.995 * target_param.data)\n\n        return {\n            'critic_loss': critic_loss.item(),\n            'actor_loss': actor_loss.item(),\n            'alpha': self.alpha.item(),\n        }",
    "source_file": "core\\rl\\portfolio.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "RLPortfolioOptimizer"
  },
  {
    "name": "state_dict",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "",
    "python_code": "def state_dict(self) -> Dict[str, Any]:\n        return {\n            'actor': self.actor.state_dict(),\n            'critic': self.critic.state_dict(),\n            'critic_target': self.critic_target.state_dict(),\n            'log_alpha': self.log_alpha,\n        }",
    "source_file": "core\\rl\\portfolio.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "RLPortfolioOptimizer"
  },
  {
    "name": "load_state_dict",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def load_state_dict(self, state_dict: Dict[str, Any]):\n        self.actor.load_state_dict(state_dict['actor'])\n        self.critic.load_state_dict(state_dict['critic'])\n        self.critic_target.load_state_dict(state_dict['critic_target'])\n        self.log_alpha = state_dict['log_alpha']\n        self.alpha = self.log_alpha.exp()",
    "source_file": "core\\rl\\portfolio.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "RLPortfolioOptimizer"
  },
  {
    "name": "optimize_portfolio_rl",
    "category": "machine_learning",
    "formula": "optimizer",
    "explanation": "Train RL-based portfolio optimizer.\n\nArgs:\n    data_dict: Dict mapping asset name -> price array\n    config: Portfolio configuration\n    total_timesteps: Training timesteps\n\nReturns:\n    Trained portfolio optimizer",
    "python_code": "def optimize_portfolio_rl(\n    data_dict: Dict[str, np.ndarray],\n    config: Optional[PortfolioConfig] = None,\n    total_timesteps: int = 100000,\n) -> RLPortfolioOptimizer:\n    \"\"\"\n    Train RL-based portfolio optimizer.\n\n    Args:\n        data_dict: Dict mapping asset name -> price array\n        config: Portfolio configuration\n        total_timesteps: Training timesteps\n\n    Returns:\n        Trained portfolio optimizer\n    \"\"\"\n    config = config or PortfolioConfig()\n    env = PortfolioEnv(data_dict, config)\n    optimizer = RLPortfolioOptimizer(env)\n    optimizer.learn(total_timesteps)\n    return optimizer",
    "source_file": "core\\rl\\portfolio.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "risk",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(\n        self,\n        env: gym.Env,\n        config: Optional[RiskSensitiveConfig] = None,\n    ):\n        self.config = config or RiskSensitiveConfig()\n        super().__init__(env, self.config)\n\n        # Networks\n        self.actor = GaussianActor(\n            self.state_dim,\n            self.action_dim,\n            self.config.hidden_dims,\n            self.config.activation,\n        ).to(self.device)\n\n        # Value network (for expected return)\n        self.critic = Critic(\n            self.state_dim,\n            0,\n            self.config.hidden_dims,\n            self.config.activation,\n        ).to(self.device)\n\n        # CVaR network (for tail risk)\n        self.cvar_critic = Critic(\n            self.state_dim,\n            0,\n            self.config.hidden_dims,\n            self.config.activation,\n        ).to(self.device)\n\n        # VaR threshold (learnable)\n        self.var_threshold = nn.Parameter(torch.tensor(0.0, device=self.device))\n\n        # Optimizers\n        self.actor_optimizer = torch.optim.Adam(\n            self.actor.parameters(),\n            lr=self.config.learning_rate\n        )\n        self.critic_optimizer = torch.optim.Adam(\n            list(self.critic.parameters()) + list(self.cvar_critic.parameters()) + [self.var_threshold],\n            lr=self.config.learning_rate\n        )\n\n        # Rollout buffer\n        self.buffer = RolloutBuffer(\n            self.state_dim,\n            self.action_dim,\n            self.config.buffer_size,\n        )\n\n        # Track returns for CVaR calculation\n        self.episode_returns = []",
    "source_file": "core\\rl\\risk_sensitive.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CVaRPPO"
  },
  {
    "name": "select_action",
    "category": "risk",
    "formula": "action.cpu().numpy()[0]",
    "explanation": "",
    "python_code": "def select_action(self, state: np.ndarray, deterministic: bool = False) -> np.ndarray:\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n            action = self.actor.get_action(state_tensor, deterministic)\n            return action.cpu().numpy()[0]",
    "source_file": "core\\rl\\risk_sensitive.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CVaRPPO"
  },
  {
    "name": "compute_cvar",
    "category": "risk",
    "formula": "var, cvar",
    "explanation": "Compute CVaR (Conditional Value at Risk).\n\nReference: Rockafellar & Uryasev (2000)\n\"Optimization of Conditional Value-at-Risk\"\nJournal of Risk\n\nCVaR_(X) = E[X | X  VaR_(X)]\n\nReturns:\n    var: Value at Risk (-quantile)\n    cvar: Conditional Value at Risk",
    "python_code": "def compute_cvar(self, returns: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Compute CVaR (Conditional Value at Risk).\n\n        Reference: Rockafellar & Uryasev (2000)\n        \"Optimization of Conditional Value-at-Risk\"\n        Journal of Risk\n\n        CVaR_(X) = E[X | X  VaR_(X)]\n\n        Returns:\n            var: Value at Risk (-quantile)\n            cvar: Conditional Value at Risk\n        \"\"\"\n        alpha = self.config.cvar_alpha\n\n        # Sort returns (ascending, so worst first)\n        sorted_returns, _ = torch.sort(returns)\n\n        # VaR is the -quantile\n        var_idx = int(alpha * len(returns))\n        var = sorted_returns[max(0, var_idx - 1)]\n\n        # CVaR is the mean of returns below VaR\n        cvar = sorted_returns[:max(1, var_idx)].mean()\n\n        return var, cvar",
    "source_file": "core\\rl\\risk_sensitive.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CVaRPPO"
  },
  {
    "name": "compute_cvar_loss",
    "category": "risk",
    "formula": "L_CVaR = var_threshold + (1/) * E[max(0, -R - var_threshold)] | cvar_loss",
    "explanation": "Compute CVaR loss using the dual formulation.\n\nReference: Chow et al. (2017)\n\nL_CVaR = var_threshold + (1/) * E[max(0, -R - var_threshold)]",
    "python_code": "def compute_cvar_loss(self, returns: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute CVaR loss using the dual formulation.\n\n        Reference: Chow et al. (2017)\n\n        L_CVaR = var_threshold + (1/) * E[max(0, -R - var_threshold)]\n        \"\"\"\n        alpha = self.config.cvar_alpha\n\n        # CVaR dual formulation\n        excess_loss = F.relu(-returns - self.var_threshold)\n        cvar_loss = self.var_threshold + (1.0 / alpha) * excess_loss.mean()\n\n        return cvar_loss",
    "source_file": "core\\rl\\risk_sensitive.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CVaRPPO"
  },
  {
    "name": "train",
    "category": "risk",
    "formula": "{",
    "explanation": "Train CVaR-PPO agent.",
    "python_code": "def train(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n        \"\"\"Train CVaR-PPO agent.\"\"\"\n        states = batch['states']\n        actions = batch['actions']\n        old_log_probs = batch['log_probs']\n        advantages = batch['advantages']\n        returns = batch['returns']\n\n        # Normalize advantages\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n        total_loss = 0.0\n        total_pg_loss = 0.0\n        total_value_loss = 0.0\n        total_cvar_loss = 0.0\n\n        for _ in range(self.config.ppo_epochs):\n            # Policy loss (PPO objective)\n            mean, log_std = self.actor(states)\n            std = log_std.exp()\n            dist = Normal(mean, std)\n\n            new_log_probs = dist.log_prob(actions).sum(-1, keepdim=True)\n            entropy = dist.entropy().sum(-1).mean()\n\n            ratio = (new_log_probs - old_log_probs).exp()\n            surr1 = ratio * advantages\n            surr2 = torch.clamp(ratio, 1 - self.config.ppo_clip, 1 + self.config.ppo_clip) * advantages\n            pg_loss = -torch.min(surr1, surr2).mean()\n\n            # Value loss\n            values = self.critic(states)\n            value_loss = F.mse_loss(values, returns)\n\n            # CVaR loss (Tamar et al. 2015)\n            cvar_values = self.cvar_critic(states)\n            cvar_loss = self.compute_cvar_loss(returns - cvar_values)\n\n            # Total loss with CVaR regularization\n            loss = (\n                pg_loss\n                + self.config.value_coef * value_loss\n                + self.config.cvar_coef * cvar_loss\n                - self.config.entropy_coef * entropy\n            )\n\n            # Update networks\n            self.actor_optimizer.zero_grad()\n            self.critic_optimizer.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(self.actor.parameters(), self.config.max_grad_norm)\n            nn.utils.clip_grad_norm_(self.critic.parameters(), self.config.max_grad_norm)\n            self.actor_optimizer.step()\n            self.critic_optimizer.step()\n\n            total_loss += loss.item()\n            total_pg_loss += pg_loss.item()\n            total_value_loss += value_loss.item()\n            total_cvar_loss += cvar_loss.item()\n\n        n_epochs = self.config.ppo_epochs\n        return {\n            'loss': total_loss / n_epochs,\n            'pg_loss': total_pg_loss / n_epochs,\n            'value_loss': total_value_loss / n_epochs,\n            'cvar_loss': total_cvar_loss / n_epochs,\n            'var_threshold': self.var_threshold.item(),\n        }",
    "source_file": "core\\rl\\risk_sensitive.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CVaRPPO"
  },
  {
    "name": "state_dict",
    "category": "risk",
    "formula": "{",
    "explanation": "",
    "python_code": "def state_dict(self) -> Dict[str, Any]:\n        return {\n            'actor': self.actor.state_dict(),\n            'critic': self.critic.state_dict(),\n            'cvar_critic': self.cvar_critic.state_dict(),\n            'var_threshold': self.var_threshold,\n            'actor_optimizer': self.actor_optimizer.state_dict(),\n            'critic_optimizer': self.critic_optimizer.state_dict(),\n        }",
    "source_file": "core\\rl\\risk_sensitive.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CVaRPPO"
  },
  {
    "name": "load_state_dict",
    "category": "risk",
    "formula": "",
    "explanation": "",
    "python_code": "def load_state_dict(self, state_dict: Dict[str, Any]):\n        self.actor.load_state_dict(state_dict['actor'])\n        self.critic.load_state_dict(state_dict['critic'])\n        self.cvar_critic.load_state_dict(state_dict['cvar_critic'])\n        self.var_threshold = state_dict['var_threshold']\n        self.actor_optimizer.load_state_dict(state_dict['actor_optimizer'])\n        self.critic_optimizer.load_state_dict(state_dict['critic_optimizer'])",
    "source_file": "core\\rl\\risk_sensitive.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CVaRPPO"
  },
  {
    "name": "update_drawdown",
    "category": "risk",
    "formula": "",
    "explanation": "Update drawdown tracking.",
    "python_code": "def update_drawdown(self, portfolio_value: float):\n        \"\"\"Update drawdown tracking.\"\"\"\n        self.max_portfolio_value = max(self.max_portfolio_value, portfolio_value)\n        self.current_drawdown = (self.max_portfolio_value - portfolio_value) / self.max_portfolio_value",
    "source_file": "core\\rl\\risk_sensitive.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DrawdownConstrainedPPO"
  },
  {
    "name": "compute_drawdown_cost",
    "category": "risk",
    "formula": "Cost = max(0, drawdown - max_drawdown) | F.relu(drawdowns - self.config.max_drawdown)",
    "explanation": "Compute drawdown constraint violation.\n\nCost = max(0, drawdown - max_drawdown)",
    "python_code": "def compute_drawdown_cost(self, drawdowns: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute drawdown constraint violation.\n\n        Cost = max(0, drawdown - max_drawdown)\n        \"\"\"\n        return F.relu(drawdowns - self.config.max_drawdown)",
    "source_file": "core\\rl\\risk_sensitive.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DrawdownConstrainedPPO"
  },
  {
    "name": "add",
    "category": "risk",
    "formula": "",
    "explanation": "",
    "python_code": "def add(self, state, action, reward, log_prob, done, value, drawdown=0.0):\n        super().add(state, action, reward, log_prob, done, value)\n        self.drawdowns[(self.ptr - 1) % self.max_size] = drawdown",
    "source_file": "core\\rl\\risk_sensitive.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DrawdownBuffer"
  },
  {
    "name": "get",
    "category": "risk",
    "formula": "data",
    "explanation": "",
    "python_code": "def get(self, device: str = 'cpu') -> Dict[str, torch.Tensor]:\n        data = super().get(device)\n        data['drawdowns'] = torch.FloatTensor(self.drawdowns[:self.size]).to(device)\n        return data",
    "source_file": "core\\rl\\risk_sensitive.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DrawdownBuffer"
  },
  {
    "name": "select_action",
    "category": "reinforcement_learning",
    "formula": "action_np",
    "explanation": "Select action with safety check.",
    "python_code": "def select_action(self, state: np.ndarray, deterministic: bool = False) -> np.ndarray:\n        \"\"\"Select action with safety check.\"\"\"\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n\n            # Get action from policy\n            action = self.actor.get_action(state_tensor, deterministic)\n            action_np = action.cpu().numpy()[0]\n\n            # Safety check: scale down action if safety critic predicts high risk\n            if not deterministic:\n                safety_value = self.safety_critic(state_tensor).item()\n                if safety_value > 0.5:  # High risk predicted\n                    # Scale down action proportionally to risk\n                    scale = max(0.1, 1.0 - safety_value)\n                    action_np = action_np * scale\n\n            return action_np",
    "source_file": "core\\rl\\risk_sensitive.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TailSafePPO"
  },
  {
    "name": "compute_tail_risk_penalty",
    "category": "risk",
    "formula": "cvar_loss + self.config.tail_risk_coef * extreme_penalty",
    "explanation": "Compute tail risk penalty.\n\nCombines CVaR and extreme event detection.",
    "python_code": "def compute_tail_risk_penalty(self, returns: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute tail risk penalty.\n\n        Combines CVaR and extreme event detection.\n        \"\"\"\n        alpha = self.config.cvar_alpha\n\n        # CVaR component\n        excess_loss = F.relu(-returns - self.var_threshold)\n        cvar_loss = self.var_threshold + (1.0 / alpha) * excess_loss.mean()\n\n        # Extreme event penalty (returns worse than 3 sigma)\n        mean_ret = returns.mean()\n        std_ret = returns.std() + 1e-8\n        z_scores = (returns - mean_ret) / std_ret\n        extreme_penalty = F.relu(-z_scores - 3.0).mean()  # Penalize < -3 sigma\n\n        return cvar_loss + self.config.tail_risk_coef * extreme_penalty",
    "source_file": "core\\rl\\risk_sensitive.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TailSafePPO"
  },
  {
    "name": "create_risk_sensitive_agent",
    "category": "risk",
    "formula": "agents[agent_type](env, config)",
    "explanation": "Factory function to create risk-sensitive RL agents.\n\nArgs:\n    agent_type: One of 'cvar_ppo', 'drawdown_ppo', 'tail_safe'\n    env: Gym environment\n    config: Agent configuration\n\nReturns:\n    Risk-sensitive RL agent",
    "python_code": "def create_risk_sensitive_agent(\n    agent_type: str,\n    env: gym.Env,\n    config: Optional[RiskSensitiveConfig] = None,\n) -> BaseRLAgent:\n    \"\"\"\n    Factory function to create risk-sensitive RL agents.\n\n    Args:\n        agent_type: One of 'cvar_ppo', 'drawdown_ppo', 'tail_safe'\n        env: Gym environment\n        config: Agent configuration\n\n    Returns:\n        Risk-sensitive RL agent\n    \"\"\"\n    agents = {\n        'cvar_ppo': CVaRPPO,\n        'drawdown_ppo': DrawdownConstrainedPPO,\n        'tail_safe': TailSafePPO,\n    }\n\n    agent_type = agent_type.lower()\n    if agent_type not in agents:\n        raise ValueError(f\"Unknown agent type: {agent_type}. Available: {list(agents.keys())}\")\n\n    return agents[agent_type](env, config)",
    "source_file": "core\\rl\\risk_sensitive.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "technical",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(\n        self,\n        state_dim: int,\n        action_dim: int = 3,  # Long, Flat, Short\n        hidden_dims: List[int] = [256, 256],\n        use_attention: bool = True,\n    ):\n        super().__init__()\n\n        self.use_attention = use_attention\n\n        # Temporal feature encoder (for tick sequences)\n        self.temporal_encoder = nn.LSTM(\n            state_dim,\n            hidden_dims[0],\n            num_layers=2,\n            batch_first=True,\n            bidirectional=False,\n        )\n\n        if use_attention:\n            self.attention = nn.MultiheadAttention(\n                hidden_dims[0],\n                num_heads=4,\n                batch_first=True,\n            )\n\n        # Policy network\n        self.policy = nn.Sequential(\n            nn.Linear(hidden_dims[0], hidden_dims[1]),\n            nn.ReLU(),\n            nn.Linear(hidden_dims[1], action_dim),\n        )\n\n        # Value network\n        self.value = nn.Sequential(\n            nn.Linear(hidden_dims[0], hidden_dims[1]),\n            nn.ReLU(),\n            nn.Linear(hidden_dims[1], 1),\n        )\n\n        # Scalping-specific: profit threshold predictor\n        self.profit_predictor = nn.Sequential(\n            nn.Linear(hidden_dims[0], hidden_dims[1] // 2),\n            nn.ReLU(),\n            nn.Linear(hidden_dims[1] // 2, 1),\n            nn.Sigmoid(),  # Probability of profitable trade\n        )",
    "source_file": "core\\rl\\trademaster.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "DeepScalper"
  },
  {
    "name": "forward",
    "category": "technical",
    "formula": "{",
    "explanation": "Forward pass.",
    "python_code": "def forward(\n        self,\n        x: torch.Tensor,  # (batch, seq_len, state_dim) or (batch, state_dim)\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"Forward pass.\"\"\"\n        # Handle both sequence and single-step input\n        if x.dim() == 2:\n            x = x.unsqueeze(1)  # Add sequence dimension\n\n        # Temporal encoding\n        lstm_out, _ = self.temporal_encoder(x)  # (batch, seq, hidden)\n\n        if self.use_attention:\n            attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n            features = attn_out[:, -1, :]  # Take last timestep\n        else:\n            features = lstm_out[:, -1, :]\n\n        # Outputs\n        policy_logits = self.policy(features)\n        value = self.value(features)\n        profit_prob = self.profit_predictor(features)\n\n        return {\n            'policy': policy_logits,\n            'value': value,\n            'profit_prob': profit_prob,\n        }",
    "source_file": "core\\rl\\trademaster.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "DeepScalper"
  },
  {
    "name": "get_action",
    "category": "microstructure",
    "formula": "action, profit_prob",
    "explanation": "Get scalping action with profit filter.\n\nOnly trades if profit probability > threshold.",
    "python_code": "def get_action(\n        self,\n        x: torch.Tensor,\n        deterministic: bool = False,\n        profit_threshold: float = 0.5,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Get scalping action with profit filter.\n\n        Only trades if profit probability > threshold.\n        \"\"\"\n        output = self.forward(x)\n        policy_logits = output['policy']\n        profit_prob = output['profit_prob']\n\n        if deterministic:\n            action = policy_logits.argmax(dim=-1)\n        else:\n            dist = Categorical(logits=policy_logits)\n            action = dist.sample()\n\n        # Apply profit filter: go flat if profit probability is low\n        flat_action = torch.ones_like(action)  # 1 = Flat\n        action = torch.where(profit_prob.squeeze(-1) > profit_threshold, action, flat_action)\n\n        return action, profit_prob",
    "source_file": "core\\rl\\trademaster.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "DeepScalper"
  },
  {
    "name": "get_weights",
    "category": "technical",
    "formula": "weights",
    "explanation": "Get portfolio weights.",
    "python_code": "def get_weights(\n        self,\n        asset_features: torch.Tensor,\n        risk_features: torch.Tensor,\n        deterministic: bool = True,\n    ) -> torch.Tensor:\n        \"\"\"Get portfolio weights.\"\"\"\n        output = self.forward(asset_features, risk_features)\n        weights = output['weights']\n\n        if not deterministic:\n            # Add noise for exploration\n            noise = torch.randn_like(weights) * 0.1\n            weights = F.softmax(weights + noise, dim=-1)\n\n        return weights",
    "source_file": "core\\rl\\trademaster.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "DeepTrader"
  },
  {
    "name": "forward",
    "category": "technical",
    "formula": "weights",
    "explanation": "Forward pass.\n\nArgs:\n    price_tensor: Price features for each asset\n    previous_weights: Previous portfolio weights (including cash)\n\nReturns:\n    New portfolio weights (including cash)",
    "python_code": "def forward(\n        self,\n        price_tensor: torch.Tensor,  # (batch, n_assets, n_features, window)\n        previous_weights: torch.Tensor,  # (batch, n_assets + 1)\n    ) -> torch.Tensor:\n        \"\"\"\n        Forward pass.\n\n        Args:\n            price_tensor: Price features for each asset\n            previous_weights: Previous portfolio weights (including cash)\n\n        Returns:\n            New portfolio weights (including cash)\n        \"\"\"\n        batch_size = price_tensor.size(0)\n\n        # Process each asset through CNN\n        asset_features = []\n        for i in range(self.n_assets):\n            asset_input = price_tensor[:, i, :, :]  # (batch, features, window)\n            asset_feat = self.cnn(asset_input).squeeze(-1)  # (batch, hidden)\n            asset_features.append(asset_feat)\n\n        asset_features = torch.stack(asset_features, dim=1)  # (batch, n_assets, hidden)\n\n        # Encode previous weights\n        pvm_feat = self.pvm_encoder(previous_weights)  # (batch, hidden)\n        pvm_feat = pvm_feat.unsqueeze(1).expand(-1, self.n_assets, -1)\n\n        # Combine CNN features with PVM\n        combined = torch.cat([asset_features, pvm_feat], dim=-1)  # (batch, n_assets, 2*hidden)\n\n        # Get score for each asset\n        scores = self.policy(combined).squeeze(-1)  # (batch, n_assets)\n\n        # Add cash with bias\n        cash_score = self.cash_bias.expand(batch_size, 1)\n        all_scores = torch.cat([cash_score, scores], dim=-1)  # (batch, n_assets + 1)\n\n        # Softmax to get weights\n        weights = F.softmax(all_scores, dim=-1)\n\n        return weights",
    "source_file": "core\\rl\\trademaster.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "EIIE"
  },
  {
    "name": "imitation_loss",
    "category": "technical",
    "formula": "F.cross_entropy(output['imitation_policy'], expert_action)",
    "explanation": "Behavioral cloning loss.",
    "python_code": "def imitation_loss(\n        self,\n        state: torch.Tensor,\n        expert_action: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"Behavioral cloning loss.\"\"\"\n        output = self.forward(state, use_imitation=True)\n        return F.cross_entropy(output['imitation_policy'], expert_action)",
    "source_file": "core\\rl\\trademaster.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "InvestorImitator"
  },
  {
    "name": "select_action",
    "category": "technical",
    "formula": "action.cpu().numpy()[0] | weights.cpu().numpy()[0] | weights.cpu().numpy()[0]",
    "explanation": "Select action using the chosen algorithm.",
    "python_code": "def select_action(\n        self,\n        state: np.ndarray,\n        deterministic: bool = False,\n        **kwargs,\n    ) -> np.ndarray:\n        \"\"\"Select action using the chosen algorithm.\"\"\"\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n\n            if self.algorithm == 'deepscalper':\n                action, _ = self.model.get_action(state_tensor, deterministic)\n                return action.cpu().numpy()[0]\n\n            elif self.algorithm == 'deeptrader':\n                risk_features = kwargs.get('risk_features')\n                if risk_features is None:\n                    risk_features = np.zeros(self.model.n_assets * 3)\n                risk_tensor = torch.FloatTensor(risk_features).unsqueeze(0).to(self.device)\n\n                # Reshape state to (batch, n_assets, feature_dim)\n                n_assets = self.model.n_assets\n                feature_dim = state.shape[0] // n_assets\n                asset_features = state_tensor.view(1, n_assets, feature_dim)\n\n                weights = self.model.get_weights(asset_features, risk_tensor, deterministic)\n                return weights.cpu().numpy()[0]\n\n            elif self.algorithm == 'eiie':\n                # Requires price tensor and previous weights\n                price_tensor = kwargs.get('price_tensor')\n                prev_weights = kwargs.get('prev_weights')\n\n                if price_tensor is None or prev_weights is None:\n                    raise ValueError(\"EIIE requires price_tensor and prev_weights\")\n\n                price_tensor = torch.FloatTensor(price_tensor).unsqueeze(0).to(self.device)\n                prev_weights = torch.FloatTensor(prev_weights).unsqueeze(0).to(self.device)\n\n                weights = self.model(price_tensor, prev_weights)\n                return weights.cpu().numpy()[0]\n\n            else:  # sarl, investor_imitator\n                output = self.model(state_tensor)\n                policy_logits = output['policy']\n\n                if deterministic:\n                    action = policy_logits.argmax(dim=-1)\n                else:\n                    dist = Categorical(logits=policy_logits)\n                    action = dist.sample()\n\n                return action.cpu().numpy()[0]",
    "source_file": "core\\rl\\trademaster.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "TradeMasterAgent"
  },
  {
    "name": "create_trademaster_agent",
    "category": "technical",
    "formula": "TradeMasterAgent(algorithm, state_dim, action_dim, config, **kwargs)",
    "explanation": "Factory function to create TradeMaster agent.\n\nArgs:\n    algorithm: Algorithm name\n    state_dim: State dimension\n    action_dim: Action dimension\n    config: Configuration\n    **kwargs: Algorithm-specific parameters\n\nReturns:\n    TradeMasterAgent instance",
    "python_code": "def create_trademaster_agent(\n    algorithm: str,\n    state_dim: int,\n    action_dim: int = 3,\n    config: Optional[TradeMasterConfig] = None,\n    **kwargs,\n) -> TradeMasterAgent:\n    \"\"\"\n    Factory function to create TradeMaster agent.\n\n    Args:\n        algorithm: Algorithm name\n        state_dim: State dimension\n        action_dim: Action dimension\n        config: Configuration\n        **kwargs: Algorithm-specific parameters\n\n    Returns:\n        TradeMasterAgent instance\n    \"\"\"\n    return TradeMasterAgent(algorithm, state_dim, action_dim, config, **kwargs)",
    "source_file": "core\\rl\\trademaster.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Initialize trainer.\n\nArgs:\n    env: Training environment\n    agent_type: Type of RL agent\n    agent_config: Agent configuration\n    training_config: Training configuration",
    "python_code": "def __init__(\n        self,\n        env: gym.Env,\n        agent_type: str = 'ppo',\n        agent_config: Optional[AgentConfig] = None,\n        training_config: Optional[TrainingConfig] = None,\n    ):\n        \"\"\"\n        Initialize trainer.\n\n        Args:\n            env: Training environment\n            agent_type: Type of RL agent\n            agent_config: Agent configuration\n            training_config: Training configuration\n        \"\"\"\n        self.env = env\n        self.agent_type = agent_type\n        self.agent_config = agent_config or AgentConfig()\n        self.config = training_config or TrainingConfig()\n\n        # Create save directory\n        self.save_path = Path(self.config.save_path)\n        self.save_path.mkdir(parents=True, exist_ok=True)\n\n        # Training state\n        self.best_reward = float('-inf')\n        self.patience_counter = 0\n        self.training_history = []",
    "source_file": "core\\rl\\trainer.py",
    "academic_reference": "arXiv:1806.08295",
    "class_name": "RLTrainer"
  },
  {
    "name": "evaluate",
    "category": "machine_learning",
    "formula": "metrics",
    "explanation": "Evaluate agent performance.\n\nReference: Henderson et al. (2018)\n\"Deep Reinforcement Learning that Matters\"",
    "python_code": "def evaluate(\n        self,\n        agent: BaseRLAgent,\n        n_episodes: Optional[int] = None,\n    ) -> Dict[str, float]:\n        \"\"\"\n        Evaluate agent performance.\n\n        Reference: Henderson et al. (2018)\n        \"Deep Reinforcement Learning that Matters\"\n        \"\"\"\n        n_episodes = n_episodes or self.config.eval_episodes\n\n        episode_rewards = []\n        episode_lengths = []\n        final_values = []\n        max_drawdowns = []\n\n        for _ in range(n_episodes):\n            state, _ = self.env.reset()\n            episode_reward = 0.0\n            episode_length = 0\n            done = False\n\n            while not done:\n                action = agent.select_action(state, deterministic=True)\n                state, reward, terminated, truncated, info = self.env.step(action)\n                episode_reward += reward\n                episode_length += 1\n                done = terminated or truncated\n\n            episode_rewards.append(episode_reward)\n            episode_lengths.append(episode_length)\n\n            if 'portfolio_value' in info:\n                final_values.append(info['portfolio_value'])\n            if 'drawdown' in info:\n                max_drawdowns.append(info['drawdown'])\n\n        metrics = {\n            'mean_reward': np.mean(episode_rewards),\n            'std_reward': np.std(episode_rewards),\n            'mean_length': np.mean(episode_lengths),\n            'min_reward': np.min(episode_rewards),\n            'max_reward': np.max(episode_rewards),\n        }\n\n        if final_values:\n            metrics['mean_final_value'] = np.mean(final_values)\n            metrics['total_return'] = (np.mean(final_values) - 100000) / 100000  # Assuming initial = 100k\n\n        if max_drawdowns:\n            metrics['max_drawdown'] = np.max(max_drawdowns)\n\n        # Sharpe ratio approximation\n        if len(episode_rewards) > 1:\n            metrics['sharpe_ratio'] = (np.mean(episode_rewards) / (np.std(episode_rewards) + 1e-8)) * np.sqrt(252)\n\n        return metrics",
    "source_file": "core\\rl\\trainer.py",
    "academic_reference": "arXiv:1806.08295",
    "class_name": "RLTrainer"
  },
  {
    "name": "train_single_seed",
    "category": "machine_learning",
    "formula": "agent, history",
    "explanation": "Train agent with single seed.\n\nArgs:\n    seed: Random seed\n    callback: Optional callback function\n\nReturns:\n    Trained agent and training history",
    "python_code": "def train_single_seed(\n        self,\n        seed: int,\n        callback: Optional[Callable] = None,\n    ) -> Tuple[BaseRLAgent, Dict[str, Any]]:\n        \"\"\"\n        Train agent with single seed.\n\n        Args:\n            seed: Random seed\n            callback: Optional callback function\n\n        Returns:\n            Trained agent and training history\n        \"\"\"\n        # Set seeds\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed(seed)\n\n        # Create agent\n        agent = create_agent(self.agent_type, self.env, self.agent_config)\n\n        # Training loop\n        state, _ = self.env.reset(seed=seed)\n        episode_reward = 0.0\n        episode_length = 0\n        episode_count = 0\n\n        history = {\n            'episode_rewards': [],\n            'eval_rewards': [],\n            'timestamps': [],\n        }\n\n        for step in range(self.config.total_timesteps):\n            # Select action\n            action = agent.select_action(state)\n\n            # Environment step\n            next_state, reward, terminated, truncated, info = self.env.step(action)\n            done = terminated or truncated\n\n            # Store experience\n            agent.buffer.add(state, action, reward, next_state, float(done))\n\n            state = next_state\n            episode_reward += reward\n            episode_length += 1\n\n            # Episode done\n            if done:\n                history['episode_rewards'].append(episode_reward)\n                episode_count += 1\n\n                if callback:\n                    callback({\n                        'episode': episode_count,\n                        'reward': episode_reward,\n                        'length': episode_length,\n                        'step': step,\n                    })\n\n                state, _ = self.env.reset()\n                episode_reward = 0.0\n                episode_length = 0\n\n            # Training step\n            if agent.buffer.size >= agent.config.batch_size:\n                batch = agent.buffer.sample(agent.config.batch_size, agent.device)\n                agent.train(batch)\n\n            # Evaluation\n            if step > 0 and step % self.config.eval_frequency == 0:\n                eval_metrics = self.evaluate(agent)\n                history['eval_rewards'].append(eval_metrics['mean_reward'])\n                history['timestamps'].append(step)\n\n                # Early stopping check\n                if self.config.early_stopping:\n                    if eval_metrics['mean_reward'] > self.best_reward + self.config.min_improvement:\n                        self.best_reward = eval_metrics['mean_reward']\n                        self.patience_counter = 0\n                        # Save best model\n                        self._save_checkpoint(agent, seed, step, eval_metrics)\n                    else:\n                        self.patience_counter += 1\n\n                    if self.patience_counter >= self.config.pati",
    "source_file": "core\\rl\\trainer.py",
    "academic_reference": "arXiv:1806.08295",
    "class_name": "RLTrainer"
  },
  {
    "name": "train",
    "category": "machine_learning",
    "formula": "results",
    "explanation": "Train agent with multiple seeds.\n\nReference: Colas et al. (2019)\n\"How Many Random Seeds? Statistical Power Analysis in Deep RL\"",
    "python_code": "def train(\n        self,\n        callback: Optional[Callable] = None,\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Train agent with multiple seeds.\n\n        Reference: Colas et al. (2019)\n        \"How Many Random Seeds? Statistical Power Analysis in Deep RL\"\n        \"\"\"\n        all_histories = []\n        all_agents = []\n\n        for seed in range(self.config.n_seeds):\n            print(f\"\\n=== Training seed {seed + 1}/{self.config.n_seeds} ===\")\n\n            agent, history = self.train_single_seed(seed, callback)\n            all_histories.append(history)\n            all_agents.append(agent)\n\n        # Aggregate results\n        results = self._aggregate_results(all_histories)\n\n        # Save final results\n        self._save_results(results)\n\n        return results",
    "source_file": "core\\rl\\trainer.py",
    "academic_reference": "arXiv:1806.08295",
    "class_name": "RLTrainer"
  },
  {
    "name": "_aggregate_results",
    "category": "machine_learning",
    "formula": "results",
    "explanation": "Aggregate results across seeds.",
    "python_code": "def _aggregate_results(self, histories: List[Dict]) -> Dict[str, Any]:\n        \"\"\"Aggregate results across seeds.\"\"\"\n        # Get final episode rewards from each seed\n        final_rewards = [h['episode_rewards'][-100:] for h in histories if len(h['episode_rewards']) >= 100]\n\n        if final_rewards:\n            all_rewards = np.concatenate(final_rewards)\n            results = {\n                'mean_reward': np.mean(all_rewards),\n                'std_reward': np.std(all_rewards),\n                'median_reward': np.median(all_rewards),\n                'min_reward': np.min(all_rewards),\n                'max_reward': np.max(all_rewards),\n                'n_seeds': len(histories),\n                'histories': histories,\n            }\n        else:\n            results = {\n                'mean_reward': 0.0,\n                'std_reward': 0.0,\n                'n_seeds': len(histories),\n                'histories': histories,\n            }\n\n        return results",
    "source_file": "core\\rl\\trainer.py",
    "academic_reference": "arXiv:1806.08295",
    "class_name": "RLTrainer"
  },
  {
    "name": "_save_checkpoint",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Save training checkpoint.",
    "python_code": "def _save_checkpoint(\n        self,\n        agent: BaseRLAgent,\n        seed: int,\n        step: int,\n        metrics: Optional[Dict] = None,\n    ):\n        \"\"\"Save training checkpoint.\"\"\"\n        checkpoint_path = self.save_path / f'checkpoint_seed{seed}_step{step}.pt'\n        torch.save({\n            'agent_state': agent.state_dict(),\n            'step': step,\n            'seed': seed,\n            'metrics': metrics,\n        }, checkpoint_path)",
    "source_file": "core\\rl\\trainer.py",
    "academic_reference": "arXiv:1806.08295",
    "class_name": "RLTrainer"
  },
  {
    "name": "_save_results",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Save training results.",
    "python_code": "def _save_results(self, results: Dict[str, Any]):\n        \"\"\"Save training results.\"\"\"\n        # Remove non-serializable items\n        save_results = {k: v for k, v in results.items() if k != 'histories'}\n\n        results_path = self.save_path / 'training_results.json'\n        with open(results_path, 'w') as f:\n            json.dump(save_results, f, indent=2)",
    "source_file": "core\\rl\\trainer.py",
    "academic_reference": "arXiv:1806.08295",
    "class_name": "RLTrainer"
  },
  {
    "name": "should_advance",
    "category": "machine_learning",
    "formula": "metrics.get('mean_reward', 0) > threshold",
    "explanation": "Check if should advance to next curriculum stage.",
    "python_code": "def should_advance(self, metrics: Dict[str, float]) -> bool:\n        \"\"\"Check if should advance to next curriculum stage.\"\"\"\n        threshold = 0.6  # Advance when mean reward > threshold\n        return metrics.get('mean_reward', 0) > threshold",
    "source_file": "core\\rl\\trainer.py",
    "academic_reference": "arXiv:1806.08295",
    "class_name": "CurriculumTrainer"
  },
  {
    "name": "train_curriculum",
    "category": "machine_learning",
    "formula": "{'final_agent': agent}",
    "explanation": "Train with curriculum learning.\n\nProgress through increasingly difficult environments.",
    "python_code": "def train_curriculum(self, callback: Optional[Callable] = None) -> Dict[str, Any]:\n        \"\"\"\n        Train with curriculum learning.\n\n        Progress through increasingly difficult environments.\n        \"\"\"\n        agent = create_agent(self.agent_type, self.env, self.agent_config)\n\n        for stage, env in enumerate(self.envs):\n            print(f\"\\n=== Curriculum Stage {stage + 1}/{len(self.envs)} ===\")\n\n            self.env = env\n            self.current_stage = stage\n\n            # Train on current environment\n            state, _ = env.reset()\n            stage_steps = self.config.total_timesteps // len(self.envs)\n\n            for step in range(stage_steps):\n                action = agent.select_action(state)\n                next_state, reward, terminated, truncated, info = env.step(action)\n                done = terminated or truncated\n\n                agent.buffer.add(state, action, reward, next_state, float(done))\n\n                state = next_state\n                if done:\n                    state, _ = env.reset()\n\n                if agent.buffer.size >= agent.config.batch_size:\n                    batch = agent.buffer.sample(agent.config.batch_size, agent.device)\n                    agent.train(batch)\n\n                # Check for stage advancement\n                if step > 0 and step % self.config.eval_frequency == 0:\n                    metrics = self.evaluate(agent)\n                    if self.should_advance(metrics):\n                        print(f\"Advancing to stage {stage + 2}\")\n                        break\n\n        return {'final_agent': agent}",
    "source_file": "core\\rl\\trainer.py",
    "academic_reference": "arXiv:1806.08295",
    "class_name": "CurriculumTrainer"
  },
  {
    "name": "train_rl_agent",
    "category": "reinforcement_learning",
    "formula": "agent, results",
    "explanation": "Convenience function to train RL agent.\n\nArgs:\n    env: Training environment\n    agent_type: Type of agent ('ppo', 'sac', 'a2c', 'td3', 'dqn', 'ddpg')\n    agent_config: Agent configuration\n    training_config: Training configuration\n    callback: Optional callback\n\nReturns:\n    Trained agent and training results",
    "python_code": "def train_rl_agent(\n    env: gym.Env,\n    agent_type: str = 'ppo',\n    agent_config: Optional[AgentConfig] = None,\n    training_config: Optional[TrainingConfig] = None,\n    callback: Optional[Callable] = None,\n) -> Tuple[BaseRLAgent, Dict[str, Any]]:\n    \"\"\"\n    Convenience function to train RL agent.\n\n    Args:\n        env: Training environment\n        agent_type: Type of agent ('ppo', 'sac', 'a2c', 'td3', 'dqn', 'ddpg')\n        agent_config: Agent configuration\n        training_config: Training configuration\n        callback: Optional callback\n\n    Returns:\n        Trained agent and training results\n    \"\"\"\n    trainer = RLTrainer(env, agent_type, agent_config, training_config)\n    results = trainer.train(callback)\n\n    # Get best agent\n    agent = create_agent(agent_type, env, agent_config)\n\n    return agent, results",
    "source_file": "core\\rl\\trainer.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": null
  },
  {
    "name": "evaluate_agent",
    "category": "machine_learning",
    "formula": "metrics",
    "explanation": "Comprehensive agent evaluation.\n\nReturns detailed performance metrics.",
    "python_code": "def evaluate_agent(\n    agent: BaseRLAgent,\n    env: gym.Env,\n    n_episodes: int = 100,\n) -> Dict[str, float]:\n    \"\"\"\n    Comprehensive agent evaluation.\n\n    Returns detailed performance metrics.\n    \"\"\"\n    episode_rewards = []\n    episode_lengths = []\n    portfolio_values = []\n    drawdowns = []\n    win_rates = []\n\n    for _ in range(n_episodes):\n        state, _ = env.reset()\n        episode_reward = 0.0\n        episode_length = 0\n        done = False\n\n        while not done:\n            action = agent.select_action(state, deterministic=True)\n            state, reward, terminated, truncated, info = env.step(action)\n            episode_reward += reward\n            episode_length += 1\n            done = terminated or truncated\n\n        episode_rewards.append(episode_reward)\n        episode_lengths.append(episode_length)\n\n        if 'portfolio_value' in info:\n            portfolio_values.append(info['portfolio_value'])\n        if 'drawdown' in info:\n            drawdowns.append(info['drawdown'])\n        if 'win_rate' in info:\n            win_rates.append(info['win_rate'])\n\n    # Compute metrics\n    metrics = {\n        'mean_episode_reward': np.mean(episode_rewards),\n        'std_episode_reward': np.std(episode_rewards),\n        'median_episode_reward': np.median(episode_rewards),\n        'mean_episode_length': np.mean(episode_lengths),\n        'total_episodes': n_episodes,\n    }\n\n    if portfolio_values:\n        initial = 100000  # Assume initial capital\n        final_values = np.array(portfolio_values)\n        returns = (final_values - initial) / initial\n\n        metrics['mean_return'] = np.mean(returns)\n        metrics['std_return'] = np.std(returns)\n        metrics['sharpe_ratio'] = np.mean(returns) / (np.std(returns) + 1e-8) * np.sqrt(252)\n        metrics['sortino_ratio'] = np.mean(returns) / (np.std(returns[returns < 0]) + 1e-8) * np.sqrt(252)\n\n    if drawdowns:\n        metrics['max_drawdown'] = np.max(drawdowns)\n        metrics['mean_drawdown'] = np.mean(drawdowns)\n\n    if win_rates:\n        metrics['mean_win_rate'] = np.mean(win_rates)\n\n    return metrics",
    "source_file": "core\\rl\\trainer.py",
    "academic_reference": "arXiv:1806.08295",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(\n        self,\n        stoch_dim: int = 32,\n        stoch_discrete: int = 32,\n        deter_dim: int = 512,\n        hidden_dim: int = 512,\n        action_dim: int = 1,\n        embed_dim: int = 1024,\n    ):\n        super().__init__()\n\n        self.stoch_dim = stoch_dim\n        self.stoch_discrete = stoch_discrete\n        self.deter_dim = deter_dim\n        self.stoch_size = stoch_dim * stoch_discrete  # Flattened stochastic state\n\n        # GRU for deterministic dynamics\n        self.gru_input = nn.Linear(self.stoch_size + action_dim, hidden_dim)\n        self.gru = nn.GRUCell(hidden_dim, deter_dim)\n\n        # Prior network: h_t -> z_t\n        self.prior_net = nn.Sequential(\n            nn.Linear(deter_dim, hidden_dim),\n            nn.ELU(),\n            nn.Linear(hidden_dim, stoch_dim * stoch_discrete),\n        )\n\n        # Posterior network: (h_t, embed_t) -> z_t\n        self.posterior_net = nn.Sequential(\n            nn.Linear(deter_dim + embed_dim, hidden_dim),\n            nn.ELU(),\n            nn.Linear(hidden_dim, stoch_dim * stoch_discrete),\n        )",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RSSM"
  },
  {
    "name": "initial_state",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "Get initial RSSM state.",
    "python_code": "def initial_state(self, batch_size: int, device: torch.device) -> Dict[str, torch.Tensor]:\n        \"\"\"Get initial RSSM state.\"\"\"\n        return {\n            'deter': torch.zeros(batch_size, self.deter_dim, device=device),\n            'stoch': torch.zeros(batch_size, self.stoch_size, device=device),\n            'logits': torch.zeros(batch_size, self.stoch_dim, self.stoch_discrete, device=device),\n        }",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RSSM"
  },
  {
    "name": "get_stoch",
    "category": "technical",
    "formula": "stoch.view(*batch_shape, self.stoch_size)",
    "explanation": "Sample stochastic state from logits using straight-through Gumbel-Softmax.",
    "python_code": "def get_stoch(self, logits: torch.Tensor) -> torch.Tensor:\n        \"\"\"Sample stochastic state from logits using straight-through Gumbel-Softmax.\"\"\"\n        batch_shape = logits.shape[:-1]\n        logits = logits.view(*batch_shape, self.stoch_dim, self.stoch_discrete)\n\n        # Gumbel-Softmax (straight-through for gradients)\n        dist = OneHotCategorical(logits=logits)\n        stoch = dist.sample() + dist.probs - dist.probs.detach()\n\n        return stoch.view(*batch_shape, self.stoch_size)",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RSSM"
  },
  {
    "name": "img_step",
    "category": "deep_learning",
    "formula": "h_t = GRU(h_{t-1}, z_{t-1}, a_{t-1}) | {",
    "explanation": "Imagination step (no observation, use prior).\n\nh_t = GRU(h_{t-1}, z_{t-1}, a_{t-1})\nz_t ~ Prior(h_t)",
    "python_code": "def img_step(\n        self,\n        prev_state: Dict[str, torch.Tensor],\n        action: torch.Tensor,\n    ) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Imagination step (no observation, use prior).\n\n        h_t = GRU(h_{t-1}, z_{t-1}, a_{t-1})\n        z_t ~ Prior(h_t)\n        \"\"\"\n        prev_stoch = prev_state['stoch']\n        prev_deter = prev_state['deter']\n\n        # GRU update\n        gru_input = self.gru_input(torch.cat([prev_stoch, action], dim=-1))\n        gru_input = F.elu(gru_input)\n        deter = self.gru(gru_input, prev_deter)\n\n        # Prior\n        prior_logits = self.prior_net(deter)\n        prior_logits = prior_logits.view(-1, self.stoch_dim, self.stoch_discrete)\n        stoch = self.get_stoch(prior_logits)\n\n        return {\n            'deter': deter,\n            'stoch': stoch,\n            'logits': prior_logits,\n        }",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RSSM"
  },
  {
    "name": "obs_step",
    "category": "reinforcement_learning",
    "formula": "post, prior",
    "explanation": "Observation step (use posterior).\n\nReturns both prior and posterior states for KL loss.",
    "python_code": "def obs_step(\n        self,\n        prev_state: Dict[str, torch.Tensor],\n        action: torch.Tensor,\n        embed: torch.Tensor,\n    ) -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]:\n        \"\"\"\n        Observation step (use posterior).\n\n        Returns both prior and posterior states for KL loss.\n        \"\"\"\n        prev_stoch = prev_state['stoch']\n        prev_deter = prev_state['deter']\n\n        # GRU update\n        gru_input = self.gru_input(torch.cat([prev_stoch, action], dim=-1))\n        gru_input = F.elu(gru_input)\n        deter = self.gru(gru_input, prev_deter)\n\n        # Prior\n        prior_logits = self.prior_net(deter)\n        prior_logits = prior_logits.view(-1, self.stoch_dim, self.stoch_discrete)\n\n        # Posterior\n        post_logits = self.posterior_net(torch.cat([deter, embed], dim=-1))\n        post_logits = post_logits.view(-1, self.stoch_dim, self.stoch_discrete)\n        stoch = self.get_stoch(post_logits)\n\n        prior = {\n            'deter': deter,\n            'stoch': self.get_stoch(prior_logits),\n            'logits': prior_logits,\n        }\n        post = {\n            'deter': deter,\n            'stoch': stoch,\n            'logits': post_logits,\n        }\n\n        return post, prior",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RSSM"
  },
  {
    "name": "get_feature",
    "category": "reinforcement_learning",
    "formula": "torch.cat([state['deter'], state['stoch']], dim=-1)",
    "explanation": "Get feature vector from RSSM state (for actor/critic).",
    "python_code": "def get_feature(self, state: Dict[str, torch.Tensor]) -> torch.Tensor:\n        \"\"\"Get feature vector from RSSM state (for actor/critic).\"\"\"\n        return torch.cat([state['deter'], state['stoch']], dim=-1)",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "RSSM"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def forward(self, obs: torch.Tensor) -> torch.Tensor:\n        return self.net(obs)",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "Encoder"
  },
  {
    "name": "forward",
    "category": "machine_learning",
    "formula": "torch.sigmoid(self.net(feature))",
    "explanation": "",
    "python_code": "def forward(self, feature: torch.Tensor) -> torch.Tensor:\n        return torch.sigmoid(self.net(feature))",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ContinuePredictor"
  },
  {
    "name": "forward",
    "category": "machine_learning",
    "formula": "F.softplus(self.net(feature))",
    "explanation": "",
    "python_code": "def forward(self, feature: torch.Tensor) -> torch.Tensor:\n        return F.softplus(self.net(feature))",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CostPredictor"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "mean, std",
    "explanation": "Forward pass returning mean and std.",
    "python_code": "def forward(self, feature: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Forward pass returning mean and std.\"\"\"\n        h = self.net(feature)\n        mean = self.mean_head(h)\n        std = self.std_head(h)\n        std = self.min_std + (self.max_std - self.min_std) * torch.sigmoid(std)\n        return mean, std",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DreamerActor"
  },
  {
    "name": "sample",
    "category": "reinforcement_learning",
    "formula": "action, log_prob",
    "explanation": "Sample action with log probability.",
    "python_code": "def sample(self, feature: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Sample action with log probability.\"\"\"\n        mean, std = self.forward(feature)\n        dist = Normal(mean, std)\n        action = dist.rsample()\n        action = torch.tanh(action)\n\n        # Log prob with tanh correction\n        log_prob = dist.log_prob(torch.atanh(action.clamp(-0.999, 0.999)))\n        log_prob -= torch.log(1 - action.pow(2) + 1e-6)\n        log_prob = log_prob.sum(-1, keepdim=True)\n\n        return action, log_prob",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DreamerActor"
  },
  {
    "name": "_normalize_reward",
    "category": "technical",
    "formula": "(reward - self.reward_ema_mean) / (np.sqrt(self.reward_ema_var) + 1e-8)",
    "explanation": "Normalize reward using EMA statistics (DreamerV3).",
    "python_code": "def _normalize_reward(self, reward: torch.Tensor) -> torch.Tensor:\n        \"\"\"Normalize reward using EMA statistics (DreamerV3).\"\"\"\n        # Update EMA statistics\n        batch_mean = reward.mean().item()\n        batch_var = reward.var().item() + 1e-8\n\n        self.reward_ema_mean = (\n            self.config.reward_ema * self.reward_ema_mean +\n            (1 - self.config.reward_ema) * batch_mean\n        )\n        self.reward_ema_var = (\n            self.config.reward_ema * self.reward_ema_var +\n            (1 - self.config.reward_ema) * batch_var\n        )\n\n        # Normalize\n        return (reward - self.reward_ema_mean) / (np.sqrt(self.reward_ema_var) + 1e-8)",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DreamerTrader"
  },
  {
    "name": "_imagine_ahead",
    "category": "reinforcement_learning",
    "formula": "states[1:], actions, rewards, continues",
    "explanation": "Imagine trajectory from initial state.\n\nReturns:\n    states: List of imagined RSSM states\n    actions: Imagined actions [horizon, batch, action_dim]\n    rewards: Predicted rewards [horizon, batch, 1]\n    continues: Predicted continuation [horizon, batch, 1]",
    "python_code": "def _imagine_ahead(\n        self,\n        initial_state: Dict[str, torch.Tensor],\n        horizon: int,\n    ) -> Tuple[List[Dict[str, torch.Tensor]], torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Imagine trajectory from initial state.\n\n        Returns:\n            states: List of imagined RSSM states\n            actions: Imagined actions [horizon, batch, action_dim]\n            rewards: Predicted rewards [horizon, batch, 1]\n            continues: Predicted continuation [horizon, batch, 1]\n        \"\"\"\n        states = [initial_state]\n        actions = []\n        rewards = []\n        continues = []\n\n        state = initial_state\n\n        for t in range(horizon):\n            feature = self.rssm.get_feature(state)\n\n            # Sample action from actor\n            action, _ = self.actor.sample(feature)\n            actions.append(action)\n\n            # Imagine next state\n            state = self.rssm.img_step(state, action)\n            states.append(state)\n\n            # Predict reward and continuation\n            next_feature = self.rssm.get_feature(state)\n            reward = self.reward_predictor(next_feature)\n            cont = self.continue_predictor(next_feature)\n\n            rewards.append(reward)\n            continues.append(cont)\n\n        actions = torch.stack(actions, dim=0)  # [H, B, A]\n        rewards = torch.stack(rewards, dim=0)  # [H, B, 1]\n        continues = torch.stack(continues, dim=0)  # [H, B, 1]\n\n        return states[1:], actions, rewards, continues",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DreamerTrader"
  },
  {
    "name": "_compute_lambda_returns",
    "category": "reinforcement_learning",
    "formula": "returns",
    "explanation": "Compute -returns for actor-critic training.",
    "python_code": "def _compute_lambda_returns(\n        self,\n        rewards: torch.Tensor,  # [H, B, 1]\n        values: torch.Tensor,  # [H, B, 1]\n        continues: torch.Tensor,  # [H, B, 1]\n        bootstrap: torch.Tensor,  # [B, 1]\n        lambda_: float = 0.95,\n    ) -> torch.Tensor:\n        \"\"\"Compute -returns for actor-critic training.\"\"\"\n        horizon = rewards.shape[0]\n\n        returns = torch.zeros_like(rewards)\n        last_value = bootstrap\n\n        for t in reversed(range(horizon)):\n            returns[t] = rewards[t] + self.config.gamma * continues[t] * (\n                (1 - lambda_) * values[t] + lambda_ * last_value\n            )\n            last_value = returns[t]\n\n        return returns",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DreamerTrader"
  },
  {
    "name": "select_action",
    "category": "reinforcement_learning",
    "formula": "action.cpu().numpy()[0]",
    "explanation": "Select action using actor in latent space.",
    "python_code": "def select_action(self, obs: np.ndarray, deterministic: bool = False) -> np.ndarray:\n        \"\"\"Select action using actor in latent space.\"\"\"\n        with torch.no_grad():\n            obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n\n            # Encode observation\n            embed = self.encoder(obs_tensor)\n\n            # Update RSSM state\n            if self._current_state is None:\n                self._current_state = self.rssm.initial_state(1, self.device)\n                prev_action = torch.zeros(1, self.action_dim, device=self.device)\n            else:\n                prev_action = self._prev_action\n\n            state, _ = self.rssm.obs_step(self._current_state, prev_action, embed)\n            self._current_state = state\n\n            # Get action from actor\n            feature = self.rssm.get_feature(state)\n            action, _ = self.actor.sample(feature)\n\n            if deterministic:\n                mean, _ = self.actor(feature)\n                action = torch.tanh(mean)\n\n            self._prev_action = action\n            return action.cpu().numpy()[0]",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DreamerTrader"
  },
  {
    "name": "train",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Train Dreamer on batch.\n\n1. Train world model on real data\n2. Imagine trajectories\n3. Train actor-critic on imagined data",
    "python_code": "def train(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n        \"\"\"\n        Train Dreamer on batch.\n\n        1. Train world model on real data\n        2. Imagine trajectories\n        3. Train actor-critic on imagined data\n        \"\"\"\n        obs = batch['states']  # [B, obs_dim]\n        actions = batch['actions']  # [B, action_dim]\n        rewards = batch['rewards']  # [B, 1]\n        next_obs = batch['next_states']  # [B, obs_dim]\n        dones = batch['dones']  # [B, 1]\n\n        batch_size = obs.shape[0]\n\n        # ==================== World Model Training ====================\n\n        # Encode observations\n        embed = self.encoder(obs)\n        next_embed = self.encoder(next_obs)\n\n        # Get RSSM states\n        init_state = self.rssm.initial_state(batch_size, self.device)\n        post_state, prior_state = self.rssm.obs_step(init_state, actions, next_embed)\n\n        # Features for prediction\n        feature = self.rssm.get_feature(post_state)\n\n        # Reconstruction loss\n        recon_obs = self.decoder(feature)\n        recon_loss = F.mse_loss(recon_obs, next_obs)\n\n        # Reward prediction loss\n        pred_reward = self.reward_predictor(feature)\n        reward_loss = F.mse_loss(pred_reward, rewards)\n\n        # Continue prediction loss\n        pred_continue = self.continue_predictor(feature)\n        continue_loss = F.binary_cross_entropy(pred_continue, 1 - dones)\n\n        # KL divergence loss (balanced)\n        post_logits = post_state['logits']  # [B, stoch_dim, stoch_discrete]\n        prior_logits = prior_state['logits']\n\n        # Compute KL in both directions\n        post_dist = OneHotCategorical(logits=post_logits)\n        prior_dist = OneHotCategorical(logits=prior_logits)\n\n        kl_post_prior = kl_divergence(post_dist, prior_dist).sum(-1).mean()\n        kl_prior_post = kl_divergence(prior_dist, post_dist).sum(-1).mean()\n\n        # Balanced KL (DreamerV3)\n        kl_loss = (\n            self.config.kl_balance * torch.clamp(kl_post_prior, min=self.config.free_nats) +\n            (1 - self.config.kl_balance) * torch.clamp(kl_prior_post, min=self.config.free_nats)\n        )\n\n        # Total model loss\n        model_loss = recon_loss + reward_loss + continue_loss + kl_loss\n\n        self.model_optimizer.zero_grad()\n        model_loss.backward()\n        nn.utils.clip_grad_norm_(\n            list(self.encoder.parameters()) + list(self.rssm.parameters()) +\n            list(self.decoder.parameters()) + list(self.reward_predictor.parameters()),\n            self.config.max_grad_norm\n        )\n        self.model_optimizer.step()\n\n        # ==================== Imagination Training ====================\n\n        # Start imagination from posterior states\n        with torch.no_grad():\n            init_state = {k: v.detach() for k, v in post_state.items()}\n\n        # Imagine ahead\n        img_states, img_actions, img_rewards, img_continues = self._imagine_ahead(\n            init_state, self.config.imagination_horizon\n        )\n\n  ",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DreamerTrader"
  },
  {
    "name": "learn",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Main training loop.",
    "python_code": "def learn(\n        self,\n        total_timesteps: int,\n        callback=None,\n    ) -> 'DreamerTrader':\n        \"\"\"Main training loop.\"\"\"\n        obs, _ = self.env.reset()\n        episode_reward = 0\n        self._current_state = None\n\n        for step in range(total_timesteps):\n            action = self.select_action(obs)\n\n            next_obs, reward, terminated, truncated, info = self.env.step(action)\n            done = terminated or truncated\n\n            self.buffer.add(obs, action, reward, next_obs, float(done))\n\n            obs = next_obs\n            episode_reward += reward\n            self.total_steps += 1\n\n            if done:\n                obs, _ = self.env.reset()\n                self._current_state = None\n                if callback:\n                    callback({'episode_reward': episode_reward, 'step': step})\n                episode_reward = 0\n\n            # Train\n            if self.buffer.size >= self.config.batch_size:\n                batch = self.buffer.sample(self.config.batch_size, self.device)\n                self.train(batch)\n\n        return self",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DreamerTrader"
  },
  {
    "name": "reset_state",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Reset internal RSSM state (call at episode start).",
    "python_code": "def reset_state(self):\n        \"\"\"Reset internal RSSM state (call at episode start).\"\"\"\n        self._current_state = None",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DreamerTrader"
  },
  {
    "name": "state_dict",
    "category": "reinforcement_learning",
    "formula": "{",
    "explanation": "",
    "python_code": "def state_dict(self) -> Dict[str, Any]:\n        return {\n            'encoder': self.encoder.state_dict(),\n            'rssm': self.rssm.state_dict(),\n            'decoder': self.decoder.state_dict(),\n            'reward_predictor': self.reward_predictor.state_dict(),\n            'continue_predictor': self.continue_predictor.state_dict(),\n            'actor': self.actor.state_dict(),\n            'critic': self.critic.state_dict(),\n            'critic_target': self.critic_target.state_dict(),\n        }",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DreamerTrader"
  },
  {
    "name": "load_state_dict",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def load_state_dict(self, state_dict: Dict[str, Any]):\n        self.encoder.load_state_dict(state_dict['encoder'])\n        self.rssm.load_state_dict(state_dict['rssm'])\n        self.decoder.load_state_dict(state_dict['decoder'])\n        self.reward_predictor.load_state_dict(state_dict['reward_predictor'])\n        self.continue_predictor.load_state_dict(state_dict['continue_predictor'])\n        self.actor.load_state_dict(state_dict['actor'])\n        self.critic.load_state_dict(state_dict['critic'])\n        self.critic_target.load_state_dict(state_dict['critic_target'])",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DreamerTrader"
  },
  {
    "name": "lagrange",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def lagrange(self) -> torch.Tensor:\n        return self.log_lagrange.exp()",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "SafeDreamerTrader"
  },
  {
    "name": "_imagine_ahead_safe",
    "category": "machine_learning",
    "formula": "states[1:], actions, rewards, costs, continues",
    "explanation": "Imagine ahead with cost prediction.",
    "python_code": "def _imagine_ahead_safe(\n        self,\n        initial_state: Dict[str, torch.Tensor],\n        horizon: int,\n    ) -> Tuple[List[Dict], torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Imagine ahead with cost prediction.\"\"\"\n        states = [initial_state]\n        actions = []\n        rewards = []\n        costs = []\n        continues = []\n\n        state = initial_state\n\n        for t in range(horizon):\n            feature = self.rssm.get_feature(state)\n\n            # Sample action\n            action, _ = self.actor.sample(feature)\n            actions.append(action)\n\n            # Imagine next state\n            state = self.rssm.img_step(state, action)\n            states.append(state)\n\n            # Predictions\n            next_feature = self.rssm.get_feature(state)\n            reward = self.reward_predictor(next_feature)\n            cost = self.cost_predictor(next_feature)\n            cont = self.continue_predictor(next_feature)\n\n            rewards.append(reward)\n            costs.append(cost)\n            continues.append(cont)\n\n        actions = torch.stack(actions, dim=0)\n        rewards = torch.stack(rewards, dim=0)\n        costs = torch.stack(costs, dim=0)\n        continues = torch.stack(continues, dim=0)\n\n        return states[1:], actions, rewards, costs, continues",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "SafeDreamerTrader"
  },
  {
    "name": "create_world_model_trader",
    "category": "reinforcement_learning",
    "formula": "agents[agent_type](env, config)",
    "explanation": "Factory function to create world model RL traders.\n\nArgs:\n    agent_type: One of 'dreamer', 'safe_dreamer'\n    env: Gym environment\n    config: World model configuration\n\nReturns:\n    World model RL trader instance",
    "python_code": "def create_world_model_trader(\n    agent_type: str,\n    env: gym.Env,\n    config: Optional[WorldModelConfig] = None,\n) -> BaseRLAgent:\n    \"\"\"\n    Factory function to create world model RL traders.\n\n    Args:\n        agent_type: One of 'dreamer', 'safe_dreamer'\n        env: Gym environment\n        config: World model configuration\n\n    Returns:\n        World model RL trader instance\n    \"\"\"\n    agents = {\n        'dreamer': DreamerTrader,\n        'dreamerv3': DreamerTrader,\n        'safe_dreamer': SafeDreamerTrader,\n        'safedreamer': SafeDreamerTrader,\n    }\n\n    agent_type = agent_type.lower()\n    if agent_type not in agents:\n        raise ValueError(f\"Unknown agent type: {agent_type}. Available: {list(agents.keys())}\")\n\n    return agents[agent_type](env, config)",
    "source_file": "core\\rl\\world_model.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "is_jpy_pair",
    "category": "quantitative",
    "formula": "'JPY' in self.symbol",
    "explanation": "Check if this is a JPY pair (different pip calculation).",
    "python_code": "def is_jpy_pair(self) -> bool:\n        \"\"\"Check if this is a JPY pair (different pip calculation).\"\"\"\n        return 'JPY' in self.symbol",
    "source_file": "core\\symbol\\registry.py",
    "academic_reference": null,
    "class_name": "TradingPair"
  },
  {
    "name": "__init__",
    "category": "quantitative",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self):\n        if SymbolRegistry._instance is not None:\n            raise RuntimeError(\"Use SymbolRegistry.get() instead\")\n\n        self._pairs: Dict[str, TradingPair] = {}\n        self._config_path: Optional[Path] = None\n        self._load_symbols()",
    "source_file": "core\\symbol\\registry.py",
    "academic_reference": null,
    "class_name": "SymbolRegistry"
  },
  {
    "name": "get",
    "category": "quantitative",
    "formula": "cls._instance",
    "explanation": "Get the singleton registry instance.",
    "python_code": "def get(cls) -> 'SymbolRegistry':\n        \"\"\"Get the singleton registry instance.\"\"\"\n        if cls._instance is None:\n            with cls._lock:\n                if cls._instance is None:\n                    cls._instance = cls()\n        return cls._instance",
    "source_file": "core\\symbol\\registry.py",
    "academic_reference": null,
    "class_name": "SymbolRegistry"
  },
  {
    "name": "reset",
    "category": "quantitative",
    "formula": "",
    "explanation": "Reset the singleton (for testing).",
    "python_code": "def reset(cls):\n        \"\"\"Reset the singleton (for testing).\"\"\"\n        with cls._lock:\n            cls._instance = None",
    "source_file": "core\\symbol\\registry.py",
    "academic_reference": null,
    "class_name": "SymbolRegistry"
  },
  {
    "name": "_load_symbols",
    "category": "quantitative",
    "formula": "",
    "explanation": "Load symbols from config.",
    "python_code": "def _load_symbols(self):\n        \"\"\"Load symbols from config.\"\"\"\n        from config.symbols import (\n            SYMBOL_TIERS, get_symbol_config, get_pip_value, parse_symbol\n        )\n\n        for tier, symbols in SYMBOL_TIERS.items():\n            for symbol in symbols:\n                try:\n                    base, quote = parse_symbol(symbol)\n                    self._pairs[symbol] = TradingPair(\n                        symbol=symbol,\n                        tier=tier,\n                        base_currency=base,\n                        quote_currency=quote,\n                        pip_value=get_pip_value(symbol),\n                        config=get_symbol_config(symbol),\n                        enabled=True\n                    )\n                except Exception as e:\n                    logger.warning(f\"Failed to load symbol {symbol}: {e}\")\n\n        logger.info(f\"Loaded {len(self._pairs)} symbols across {len(SYMBOL_TIERS)} tiers\")",
    "source_file": "core\\symbol\\registry.py",
    "academic_reference": null,
    "class_name": "SymbolRegistry"
  },
  {
    "name": "get_pair",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get a trading pair by symbol.",
    "python_code": "def get_pair(self, symbol: str) -> Optional[TradingPair]:\n        \"\"\"Get a trading pair by symbol.\"\"\"\n        return self._pairs.get(symbol)",
    "source_file": "core\\symbol\\registry.py",
    "academic_reference": null,
    "class_name": "SymbolRegistry"
  },
  {
    "name": "get_all",
    "category": "quantitative",
    "formula": "list(self._pairs.values())",
    "explanation": "Get all trading pairs.",
    "python_code": "def get_all(self) -> List[TradingPair]:\n        \"\"\"Get all trading pairs.\"\"\"\n        return list(self._pairs.values())",
    "source_file": "core\\symbol\\registry.py",
    "academic_reference": null,
    "class_name": "SymbolRegistry"
  },
  {
    "name": "get_enabled",
    "category": "filtering",
    "formula": "pairs",
    "explanation": "Get enabled trading pairs, optionally filtered by tier.\n\nArgs:\n    tier: Optional tier filter ('majors', 'crosses', etc.)\n\nReturns:\n    List of enabled TradingPair objects",
    "python_code": "def get_enabled(self, tier: str = None) -> List[TradingPair]:\n        \"\"\"\n        Get enabled trading pairs, optionally filtered by tier.\n\n        Args:\n            tier: Optional tier filter ('majors', 'crosses', etc.)\n\n        Returns:\n            List of enabled TradingPair objects\n        \"\"\"\n        pairs = [p for p in self._pairs.values() if p.enabled]\n        if tier:\n            pairs = [p for p in pairs if p.tier == tier]\n        return pairs",
    "source_file": "core\\symbol\\registry.py",
    "academic_reference": null,
    "class_name": "SymbolRegistry"
  },
  {
    "name": "get_symbols",
    "category": "quantitative",
    "formula": "[p.symbol for p in pairs]",
    "explanation": "Get symbol names as strings.",
    "python_code": "def get_symbols(self, tier: str = None, enabled_only: bool = True) -> List[str]:\n        \"\"\"Get symbol names as strings.\"\"\"\n        if enabled_only:\n            pairs = self.get_enabled(tier)\n        else:\n            pairs = self.get_all()\n            if tier:\n                pairs = [p for p in pairs if p.tier == tier]\n        return [p.symbol for p in pairs]",
    "source_file": "core\\symbol\\registry.py",
    "academic_reference": null,
    "class_name": "SymbolRegistry"
  },
  {
    "name": "get_config",
    "category": "quantitative",
    "formula": "pair.config.copy() | DEFAULT_SYMBOL_CONFIG.copy()",
    "explanation": "Get configuration for a symbol.",
    "python_code": "def get_config(self, symbol: str) -> Dict[str, Any]:\n        \"\"\"Get configuration for a symbol.\"\"\"\n        pair = self._pairs.get(symbol)\n        if pair:\n            return pair.config.copy()\n        from config.symbols import DEFAULT_SYMBOL_CONFIG\n        return DEFAULT_SYMBOL_CONFIG.copy()",
    "source_file": "core\\symbol\\registry.py",
    "academic_reference": null,
    "class_name": "SymbolRegistry"
  },
  {
    "name": "enable",
    "category": "quantitative",
    "formula": "True | False",
    "explanation": "Enable a symbol for trading.",
    "python_code": "def enable(self, symbol: str) -> bool:\n        \"\"\"Enable a symbol for trading.\"\"\"\n        if symbol in self._pairs:\n            self._pairs[symbol].enabled = True\n            logger.info(f\"Enabled {symbol}\")\n            return True\n        return False",
    "source_file": "core\\symbol\\registry.py",
    "academic_reference": null,
    "class_name": "SymbolRegistry"
  },
  {
    "name": "disable",
    "category": "quantitative",
    "formula": "True | False",
    "explanation": "Disable a symbol from trading.",
    "python_code": "def disable(self, symbol: str) -> bool:\n        \"\"\"Disable a symbol from trading.\"\"\"\n        if symbol in self._pairs:\n            self._pairs[symbol].enabled = False\n            logger.info(f\"Disabled {symbol}\")\n            return True\n        return False",
    "source_file": "core\\symbol\\registry.py",
    "academic_reference": null,
    "class_name": "SymbolRegistry"
  },
  {
    "name": "enable_tier",
    "category": "quantitative",
    "formula": "",
    "explanation": "Enable all symbols in a tier.",
    "python_code": "def enable_tier(self, tier: str):\n        \"\"\"Enable all symbols in a tier.\"\"\"\n        for pair in self._pairs.values():\n            if pair.tier == tier:\n                pair.enabled = True\n        logger.info(f\"Enabled tier: {tier}\")",
    "source_file": "core\\symbol\\registry.py",
    "academic_reference": null,
    "class_name": "SymbolRegistry"
  },
  {
    "name": "disable_tier",
    "category": "quantitative",
    "formula": "",
    "explanation": "Disable all symbols in a tier.",
    "python_code": "def disable_tier(self, tier: str):\n        \"\"\"Disable all symbols in a tier.\"\"\"\n        for pair in self._pairs.values():\n            if pair.tier == tier:\n                pair.enabled = False\n        logger.info(f\"Disabled tier: {tier}\")",
    "source_file": "core\\symbol\\registry.py",
    "academic_reference": null,
    "class_name": "SymbolRegistry"
  },
  {
    "name": "set_config",
    "category": "quantitative",
    "formula": "True | False",
    "explanation": "Update a configuration value for a symbol.",
    "python_code": "def set_config(self, symbol: str, key: str, value: Any) -> bool:\n        \"\"\"Update a configuration value for a symbol.\"\"\"\n        if symbol in self._pairs:\n            self._pairs[symbol].config[key] = value\n            return True\n        return False",
    "source_file": "core\\symbol\\registry.py",
    "academic_reference": null,
    "class_name": "SymbolRegistry"
  },
  {
    "name": "save_state",
    "category": "quantitative",
    "formula": "",
    "explanation": "Save enabled/disabled state to file.",
    "python_code": "def save_state(self, path: Path = None):\n        \"\"\"Save enabled/disabled state to file.\"\"\"\n        path = path or Path(\"config/symbol_state.json\")\n        state = {\n            symbol: {'enabled': pair.enabled, 'config': pair.config}\n            for symbol, pair in self._pairs.items()\n        }\n        with open(path, 'w') as f:\n            json.dump(state, f, indent=2)\n        logger.info(f\"Saved symbol state to {path}\")",
    "source_file": "core\\symbol\\registry.py",
    "academic_reference": null,
    "class_name": "SymbolRegistry"
  },
  {
    "name": "load_state",
    "category": "quantitative",
    "formula": "with open(path) as f:",
    "explanation": "Load enabled/disabled state from file.",
    "python_code": "def load_state(self, path: Path = None):\n        \"\"\"Load enabled/disabled state from file.\"\"\"\n        path = path or Path(\"config/symbol_state.json\")\n        if not path.exists():\n            return\n\n        with open(path) as f:\n            state = json.load(f)\n\n        for symbol, data in state.items():\n            if symbol in self._pairs:\n                self._pairs[symbol].enabled = data.get('enabled', True)\n                self._pairs[symbol].config.update(data.get('config', {}))\n\n        logger.info(f\"Loaded symbol state from {path}\")",
    "source_file": "core\\symbol\\registry.py",
    "academic_reference": null,
    "class_name": "SymbolRegistry"
  },
  {
    "name": "summary",
    "category": "quantitative",
    "formula": "{",
    "explanation": "Get summary statistics.",
    "python_code": "def summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary statistics.\"\"\"\n        tiers = {}\n        for pair in self._pairs.values():\n            if pair.tier not in tiers:\n                tiers[pair.tier] = {'total': 0, 'enabled': 0}\n            tiers[pair.tier]['total'] += 1\n            if pair.enabled:\n                tiers[pair.tier]['enabled'] += 1\n\n        return {\n            'total_symbols': len(self._pairs),\n            'enabled_symbols': len([p for p in self._pairs.values() if p.enabled]),\n            'tiers': tiers\n        }",
    "source_file": "core\\symbol\\registry.py",
    "academic_reference": null,
    "class_name": "SymbolRegistry"
  },
  {
    "name": "__init__",
    "category": "quantitative",
    "formula": "None = use tier)",
    "explanation": "Initialize trading bot.\n\nArgs:\n    capital: Starting capital\n    symbols: Specific symbols to trade (None = use tier)\n    tier: Symbol tier (majors, crosses, exotics, all)\n    mode: Trading mode (paper, live)\n    ib_host: IB Gateway host\n    ib_port: IB Gateway port",
    "python_code": "def __init__(\n        self,\n        capital: float = 100000.0,\n        symbols: List[str] = None,\n        tier: str = None,\n        mode: str = \"paper\",\n        ib_host: str = \"localhost\",\n        ib_port: int = 4001,\n    ):\n        \"\"\"\n        Initialize trading bot.\n\n        Args:\n            capital: Starting capital\n            symbols: Specific symbols to trade (None = use tier)\n            tier: Symbol tier (majors, crosses, exotics, all)\n            mode: Trading mode (paper, live)\n            ib_host: IB Gateway host\n            ib_port: IB Gateway port\n        \"\"\"\n        self.capital = capital\n        self.mode = mode\n\n        # Get symbols\n        self.registry = SymbolRegistry.get()\n        if symbols:\n            self.symbols = symbols\n        elif tier:\n            self.symbols = [\n                p.symbol for p in self.registry.get_enabled(tier=tier)\n            ]\n        else:\n            self.symbols = [p.symbol for p in self.registry.get_enabled()]\n\n        # Initialize modules\n        self.model_loader = ModelLoader()\n        self.signal_gen = SignalGenerator(self.model_loader)\n        self.risk_manager = PortfolioRiskManager(capital)\n        self.position_manager = PositionManager()\n        self.executor = OrderExecutor(\n            host=ib_host,\n            port=ib_port,\n            connect=(mode != \"backtest\")\n        )\n\n        # State\n        self._running = False\n        self._lock = threading.RLock()\n        self._tick_count = 0\n        self._signal_count = 0\n        self._trade_count = 0\n\n        # Feature engine (lazy loaded)\n        self._feature_engine = None\n\n        logger.info(\n            f\"TradingBot initialized: {len(self.symbols)} symbols, \"\n            f\"mode={mode}, capital=${capital:,.0f}\"\n        )",
    "source_file": "core\\trading\\bot.py",
    "academic_reference": null,
    "class_name": "TradingBot"
  },
  {
    "name": "_get_feature_engine",
    "category": "quantitative",
    "formula": "",
    "explanation": "Lazy load feature engine.",
    "python_code": "def _get_feature_engine(self):\n        \"\"\"Lazy load feature engine.\"\"\"\n        if self._feature_engine is None:\n            from core.features.engine import HFTFeatureEngine\n            self._feature_engine = HFTFeatureEngine()\n        return self._feature_engine",
    "source_file": "core\\trading\\bot.py",
    "academic_reference": null,
    "class_name": "TradingBot"
  },
  {
    "name": "process_tick",
    "category": "quantitative",
    "formula": "# 3. Get ML signal | # 5. Check confidence threshold | # 6. Calculate position size",
    "explanation": "Process a market tick.\n\nArgs:\n    symbol: Trading symbol\n    bid: Bid price\n    ask: Ask price\n    volume: Tick volume\n    timestamp: Tick timestamp",
    "python_code": "def process_tick(\n        self,\n        symbol: str,\n        bid: float,\n        ask: float,\n        volume: float = 0.0,\n        timestamp: datetime = None\n    ):\n        \"\"\"\n        Process a market tick.\n\n        Args:\n            symbol: Trading symbol\n            bid: Bid price\n            ask: Ask price\n            volume: Tick volume\n            timestamp: Tick timestamp\n        \"\"\"\n        if not self._running:\n            return\n\n        self._tick_count += 1\n        timestamp = timestamp or datetime.now()\n        mid_price = (bid + ask) / 2\n        spread_pips = (ask - bid) * 10000\n\n        # 1. Update position P&L\n        self.position_manager.update_prices({symbol: mid_price})\n\n        # 2. Generate features\n        engine = self._get_feature_engine()\n        features = engine.process_tick(\n            symbol=symbol,\n            bid=bid,\n            ask=ask,\n            volume=volume,\n            timestamp=timestamp\n        )\n\n        if not features:\n            return\n\n        # 3. Get ML signal\n        signal = self.signal_gen.predict(symbol, features)\n        if signal is None or not signal.is_valid:\n            return\n\n        self._signal_count += 1\n\n        # 4. Risk check\n        can_trade, reason = self.risk_manager.can_trade(symbol, spread_pips)\n        if not can_trade:\n            logger.debug(f\"Risk blocked {symbol}: {reason}\")\n            return\n\n        # 5. Check confidence threshold\n        config = self.registry.get_config(symbol)\n        min_conf = config.get('min_confidence', 0.15)\n        if signal.confidence < min_conf:\n            return\n\n        # 6. Calculate position size\n        size = self.risk_manager.calculate_position_size(\n            symbol=symbol,\n            signal_strength=signal.confidence,\n            account_balance=self.capital\n        )\n\n        if size < 1000:  # Minimum size check\n            return\n\n        # 7. Check existing position\n        position = self.position_manager.get_position(symbol)\n        if position.is_open:\n            # Already have a position - check if signal agrees\n            if position.direction == signal.direction:\n                return  # Same direction, don't add\n            # Opposite direction - close existing\n            self._close_position(symbol, mid_price)\n\n        # 8. Execute trade\n        self._execute_trade(symbol, signal, size, mid_price)",
    "source_file": "core\\trading\\bot.py",
    "academic_reference": null,
    "class_name": "TradingBot"
  },
  {
    "name": "_execute_trade",
    "category": "quantitative",
    "formula": "",
    "explanation": "Execute a trade.",
    "python_code": "def _execute_trade(\n        self,\n        symbol: str,\n        signal: Signal,\n        size: float,\n        price: float\n    ):\n        \"\"\"Execute a trade.\"\"\"\n        self._trade_count += 1\n\n        def on_fill(order: Order):\n            if order.status.value == \"filled\":\n                self.position_manager.update_position(\n                    symbol,\n                    order.direction * order.filled_qty,\n                    order.avg_fill_price\n                )\n                self.risk_manager.record_trade(symbol, order.avg_fill_price)\n\n        self.executor.submit(\n            symbol=symbol,\n            direction=signal.direction,\n            quantity=size,\n            order_type=\"MKT\",\n            callback=on_fill\n        )\n\n        logger.info(\n            f\"Trade {symbol}: {'BUY' if signal.direction > 0 else 'SELL'} \"\n            f\"{size:,.0f} @ {price:.5f} (conf={signal.confidence:.2%})\"\n        )",
    "source_file": "core\\trading\\bot.py",
    "academic_reference": null,
    "class_name": "TradingBot"
  },
  {
    "name": "_close_position",
    "category": "quantitative",
    "formula": "",
    "explanation": "Close an existing position.",
    "python_code": "def _close_position(self, symbol: str, price: float):\n        \"\"\"Close an existing position.\"\"\"\n        position = self.position_manager.get_position(symbol)\n        if not position.is_open:\n            return\n\n        self.executor.submit(\n            symbol=symbol,\n            direction=-position.direction,\n            quantity=abs(position.quantity),\n            order_type=\"MKT\"\n        )\n\n        logger.info(f\"Closing {symbol} position: {position.quantity:,.0f}\")",
    "source_file": "core\\trading\\bot.py",
    "academic_reference": null,
    "class_name": "TradingBot"
  },
  {
    "name": "stop",
    "category": "quantitative",
    "formula": "",
    "explanation": "Stop the trading bot.",
    "python_code": "def stop(self):\n        \"\"\"Stop the trading bot.\"\"\"\n        self._running = False\n        self.executor.disconnect()\n        logger.info(\"Trading bot stopped\")",
    "source_file": "core\\trading\\bot.py",
    "academic_reference": null,
    "class_name": "TradingBot"
  },
  {
    "name": "status",
    "category": "quantitative",
    "formula": "{",
    "explanation": "Get bot status.",
    "python_code": "def status(self) -> Dict[str, Any]:\n        \"\"\"Get bot status.\"\"\"\n        return {\n            'running': self._running,\n            'mode': self.mode,\n            'symbols': self.symbols,\n            'ticks_processed': self._tick_count,\n            'signals_generated': self._signal_count,\n            'trades_executed': self._trade_count,\n            'positions': self.position_manager.summary(),\n            'risk': self.risk_manager.summary(),\n            'executor': self.executor.stats(),\n            'models': self.model_loader.stats(),\n        }",
    "source_file": "core\\trading\\bot.py",
    "academic_reference": null,
    "class_name": "TradingBot"
  },
  {
    "name": "on_fill",
    "category": "quantitative",
    "formula": "",
    "explanation": "",
    "python_code": "def on_fill(order: Order):\n            if order.status.value == \"filled\":\n                self.position_manager.update_position(\n                    symbol,\n                    order.direction * order.filled_qty,\n                    order.avg_fill_price\n                )\n                self.risk_manager.record_trade(symbol, order.avg_fill_price)",
    "source_file": "core\\trading\\bot.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "is_complete",
    "category": "quantitative",
    "formula": "",
    "explanation": "Check if order is in terminal state.",
    "python_code": "def is_complete(self) -> bool:\n        \"\"\"Check if order is in terminal state.\"\"\"\n        return self.status in (\n            OrderStatus.FILLED,\n            OrderStatus.CANCELLED,\n            OrderStatus.REJECTED,\n            OrderStatus.EXPIRED,\n            OrderStatus.ERROR\n        )",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerOrder"
  },
  {
    "name": "is_active",
    "category": "quantitative",
    "formula": "",
    "explanation": "Check if order is active.",
    "python_code": "def is_active(self) -> bool:\n        \"\"\"Check if order is active.\"\"\"\n        return self.status in (\n            OrderStatus.PENDING,\n            OrderStatus.SUBMITTED,\n            OrderStatus.ACCEPTED,\n            OrderStatus.PARTIAL\n        )",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerOrder"
  },
  {
    "name": "is_valid",
    "category": "quantitative",
    "formula": "",
    "explanation": "Check if quote is valid.",
    "python_code": "def is_valid(self) -> bool:\n        \"\"\"Check if quote is valid.\"\"\"\n        return self.bid > 0 and self.ask > 0 and self.ask >= self.bid",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerQuote"
  },
  {
    "name": "__init__",
    "category": "quantitative",
    "formula": "",
    "explanation": "Initialize broker.\n\nArgs:\n    config: Broker configuration",
    "python_code": "def __init__(self, config: BrokerConfig):\n        \"\"\"\n        Initialize broker.\n\n        Args:\n            config: Broker configuration\n        \"\"\"\n        self.config = config\n        self._connected = False\n        self._lock = threading.RLock()\n        self._orders: Dict[str, BrokerOrder] = {}\n        self._positions: Dict[str, BrokerPosition] = {}\n        self._callbacks: Dict[str, List[Callable]] = {}\n        self._order_counter = 0",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "connect",
    "category": "quantitative",
    "formula": "",
    "explanation": "Connect to broker.\n\nReturns:\n    True if connected successfully",
    "python_code": "def connect(self) -> bool:\n        \"\"\"\n        Connect to broker.\n\n        Returns:\n            True if connected successfully\n        \"\"\"\n        pass",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "disconnect",
    "category": "quantitative",
    "formula": "",
    "explanation": "Disconnect from broker.",
    "python_code": "def disconnect(self) -> None:\n        \"\"\"Disconnect from broker.\"\"\"\n        pass",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "is_connected",
    "category": "quantitative",
    "formula": "",
    "explanation": "Check if connected to broker.",
    "python_code": "def is_connected(self) -> bool:\n        \"\"\"Check if connected to broker.\"\"\"\n        pass",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "ping",
    "category": "quantitative",
    "formula": "",
    "explanation": "Ping broker to check connection health.\n\nReturns:\n    True if connection is healthy",
    "python_code": "def ping(self) -> bool:\n        \"\"\"\n        Ping broker to check connection health.\n\n        Returns:\n            True if connection is healthy\n        \"\"\"\n        pass",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "get_account",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get account information.\n\nReturns:\n    Account information",
    "python_code": "def get_account(self) -> BrokerAccount:\n        \"\"\"\n        Get account information.\n\n        Returns:\n            Account information\n        \"\"\"\n        pass",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "get_balance",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get account balance.\n\nReturns:\n    Account balance",
    "python_code": "def get_balance(self) -> float:\n        \"\"\"\n        Get account balance.\n\n        Returns:\n            Account balance\n        \"\"\"\n        pass",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "submit_order",
    "category": "quantitative",
    "formula": "",
    "explanation": "Submit an order.\n\nArgs:\n    symbol: Trading symbol\n    side: Buy or sell\n    quantity: Order quantity\n    order_type: Order type\n    limit_price: Limit price for limit orders\n    stop_price: Stop price for stop orders\n    **kwargs: Additional broker-specific parameters\n\nReturns:\n    Order object",
    "python_code": "def submit_order(\n        self,\n        symbol: str,\n        side: OrderSide,\n        quantity: float,\n        order_type: OrderType = OrderType.MARKET,\n        limit_price: Optional[float] = None,\n        stop_price: Optional[float] = None,\n        **kwargs\n    ) -> BrokerOrder:\n        \"\"\"\n        Submit an order.\n\n        Args:\n            symbol: Trading symbol\n            side: Buy or sell\n            quantity: Order quantity\n            order_type: Order type\n            limit_price: Limit price for limit orders\n            stop_price: Stop price for stop orders\n            **kwargs: Additional broker-specific parameters\n\n        Returns:\n            Order object\n        \"\"\"\n        pass",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "cancel_order",
    "category": "quantitative",
    "formula": "",
    "explanation": "Cancel an order.\n\nArgs:\n    order_id: Order ID to cancel\n\nReturns:\n    True if cancellation was successful",
    "python_code": "def cancel_order(self, order_id: str) -> bool:\n        \"\"\"\n        Cancel an order.\n\n        Args:\n            order_id: Order ID to cancel\n\n        Returns:\n            True if cancellation was successful\n        \"\"\"\n        pass",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "get_order",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get order by ID.\n\nArgs:\n    order_id: Order ID\n\nReturns:\n    Order object or None",
    "python_code": "def get_order(self, order_id: str) -> Optional[BrokerOrder]:\n        \"\"\"\n        Get order by ID.\n\n        Args:\n            order_id: Order ID\n\n        Returns:\n            Order object or None\n        \"\"\"\n        pass",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "get_open_orders",
    "category": "filtering",
    "formula": "",
    "explanation": "Get all open orders.\n\nArgs:\n    symbol: Optional symbol filter\n\nReturns:\n    List of open orders",
    "python_code": "def get_open_orders(self, symbol: Optional[str] = None) -> List[BrokerOrder]:\n        \"\"\"\n        Get all open orders.\n\n        Args:\n            symbol: Optional symbol filter\n\n        Returns:\n            List of open orders\n        \"\"\"\n        pass",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "get_positions",
    "category": "filtering",
    "formula": "",
    "explanation": "Get all positions.\n\nArgs:\n    symbol: Optional symbol filter\n\nReturns:\n    List of positions",
    "python_code": "def get_positions(self, symbol: Optional[str] = None) -> List[BrokerPosition]:\n        \"\"\"\n        Get all positions.\n\n        Args:\n            symbol: Optional symbol filter\n\n        Returns:\n            List of positions\n        \"\"\"\n        pass",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "close_position",
    "category": "quantitative",
    "formula": "None = close all)",
    "explanation": "Close a position.\n\nArgs:\n    symbol: Symbol to close\n    quantity: Quantity to close (None = close all)\n\nReturns:\n    Order object for the closing trade",
    "python_code": "def close_position(self, symbol: str, quantity: Optional[float] = None) -> BrokerOrder:\n        \"\"\"\n        Close a position.\n\n        Args:\n            symbol: Symbol to close\n            quantity: Quantity to close (None = close all)\n\n        Returns:\n            Order object for the closing trade\n        \"\"\"\n        pass",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "get_quote",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get current quote for symbol.\n\nArgs:\n    symbol: Trading symbol\n\nReturns:\n    Quote object",
    "python_code": "def get_quote(self, symbol: str) -> BrokerQuote:\n        \"\"\"\n        Get current quote for symbol.\n\n        Args:\n            symbol: Trading symbol\n\n        Returns:\n            Quote object\n        \"\"\"\n        pass",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "get_spread",
    "category": "microstructure",
    "formula": "",
    "explanation": "Get current spread in pips.\n\nArgs:\n    symbol: Trading symbol\n\nReturns:\n    Spread in pips",
    "python_code": "def get_spread(self, symbol: str) -> float:\n        \"\"\"\n        Get current spread in pips.\n\n        Args:\n            symbol: Trading symbol\n\n        Returns:\n            Spread in pips\n        \"\"\"\n        pass",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "get_symbols",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get list of tradeable symbols.\n\nReturns:\n    List of symbol names",
    "python_code": "def get_symbols(self) -> List[str]:\n        \"\"\"\n        Get list of tradeable symbols.\n\n        Returns:\n            List of symbol names\n        \"\"\"\n        pass",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "get_pip_value",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get pip value for symbol.\n\nArgs:\n    symbol: Trading symbol\n\nReturns:\n    Pip value",
    "python_code": "def get_pip_value(self, symbol: str) -> float:\n        \"\"\"\n        Get pip value for symbol.\n\n        Args:\n            symbol: Trading symbol\n\n        Returns:\n            Pip value\n        \"\"\"\n        pass",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "get_min_quantity",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get minimum order quantity for symbol.\n\nArgs:\n    symbol: Trading symbol\n\nReturns:\n    Minimum quantity",
    "python_code": "def get_min_quantity(self, symbol: str) -> float:\n        \"\"\"\n        Get minimum order quantity for symbol.\n\n        Args:\n            symbol: Trading symbol\n\n        Returns:\n            Minimum quantity\n        \"\"\"\n        pass",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "_generate_order_id",
    "category": "quantitative",
    "formula": "f\"{self.config.broker_type.value}_{self._order_counter:08d}\"",
    "explanation": "Generate unique internal order ID.",
    "python_code": "def _generate_order_id(self) -> str:\n        \"\"\"Generate unique internal order ID.\"\"\"\n        with self._lock:\n            self._order_counter += 1\n            return f\"{self.config.broker_type.value}_{self._order_counter:08d}\"",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "_update_order",
    "category": "quantitative",
    "formula": "",
    "explanation": "Update order in internal storage.",
    "python_code": "def _update_order(self, order: BrokerOrder) -> None:\n        \"\"\"Update order in internal storage.\"\"\"\n        with self._lock:\n            self._orders[order.internal_id] = order\n            order.updated_at = datetime.now()",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "_notify_callbacks",
    "category": "quantitative",
    "formula": "",
    "explanation": "Notify registered callbacks.",
    "python_code": "def _notify_callbacks(self, event: str, data: Any) -> None:\n        \"\"\"Notify registered callbacks.\"\"\"\n        callbacks = self._callbacks.get(event, [])\n        for callback in callbacks:\n            try:\n                callback(data)\n            except Exception as e:\n                logger.error(f\"Callback error for {event}: {e}\")",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "register_callback",
    "category": "quantitative",
    "formula": "",
    "explanation": "Register callback for events.\n\nEvents:\n    - order_update\n    - position_update\n    - quote_update\n    - connection_status",
    "python_code": "def register_callback(self, event: str, callback: Callable) -> None:\n        \"\"\"\n        Register callback for events.\n\n        Events:\n            - order_update\n            - position_update\n            - quote_update\n            - connection_status\n        \"\"\"\n        if event not in self._callbacks:\n            self._callbacks[event] = []\n        self._callbacks[event].append(callback)",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "get_stats",
    "category": "quantitative",
    "formula": "{",
    "explanation": "Get broker statistics.",
    "python_code": "def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get broker statistics.\"\"\"\n        orders = list(self._orders.values())\n        return {\n            'broker': self.config.broker_type.value,\n            'connected': self.is_connected(),\n            'account_id': self.config.account_id,\n            'paper': self.config.paper,\n            'total_orders': len(orders),\n            'open_orders': sum(1 for o in orders if o.is_active),\n            'filled_orders': sum(1 for o in orders if o.status == OrderStatus.FILLED),\n            'positions': len(self._positions),\n        }",
    "source_file": "core\\trading\\broker_base.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerBase"
  },
  {
    "name": "__init__",
    "category": "quantitative",
    "formula": "",
    "explanation": "Initialize Forex.com broker.",
    "python_code": "def __init__(self, config: BrokerConfig):\n        \"\"\"Initialize Forex.com broker.\"\"\"\n        super().__init__(config)\n        self._session = None\n        self._base_url = self.DEMO_URL if config.paper else self.LIVE_URL\n        self._session_token = None\n        self._trading_account_id = None",
    "source_file": "core\\trading\\broker_forex_com.py",
    "academic_reference": null,
    "class_name": "ForexComBroker"
  },
  {
    "name": "connect",
    "category": "quantitative",
    "formula": "True | True | False",
    "explanation": "Connect to Forex.com API.",
    "python_code": "def connect(self) -> bool:\n        \"\"\"Connect to Forex.com API.\"\"\"\n        if self._connected:\n            return True\n\n        try:\n            import requests\n            self._session = requests.Session()\n            self._session.headers.update({\n                'Content-Type': 'application/json'\n            })\n\n            # Authenticate\n            auth_response = self._session.post(\n                f\"{self._base_url}/session\",\n                json={\n                    'UserName': self.config.extra.get('username', ''),\n                    'Password': self.config.api_key,\n                    'AppKey': self.config.api_secret,\n                    'AppVersion': '1.0',\n                    'AppComments': 'ForexML Trading System'\n                },\n                timeout=self.config.connect_timeout\n            )\n\n            if auth_response.status_code == 200:\n                result = auth_response.json()\n                self._session_token = result.get('Session')\n                self._trading_account_id = result.get('TradingAccounts', [{}])[0].get('TradingAccountId')\n\n                # Add session header for subsequent requests\n                self._session.headers['Session'] = self._session_token\n                self._session.headers['UserName'] = self.config.extra.get('username', '')\n\n                self._connected = True\n                logger.info(f\"Connected to Forex.com ({self._base_url})\")\n                logger.info(f\"Trading Account: {self._trading_account_id}\")\n                return True\n            else:\n                logger.error(f\"Forex.com auth failed: {auth_response.text}\")\n                return False\n\n        except ImportError:\n            logger.error(\"requests not installed: pip install requests\")\n            return False\n\n        except Exception as e:\n            logger.error(f\"Forex.com connection error: {e}\")\n            self._connected = False\n            return False",
    "source_file": "core\\trading\\broker_forex_com.py",
    "academic_reference": null,
    "class_name": "ForexComBroker"
  },
  {
    "name": "disconnect",
    "category": "quantitative",
    "formula": "",
    "explanation": "Disconnect from Forex.com.",
    "python_code": "def disconnect(self) -> None:\n        \"\"\"Disconnect from Forex.com.\"\"\"\n        if self._session and self._session_token:\n            try:\n                self._session.delete(\n                    f\"{self._base_url}/session\",\n                    timeout=5.0\n                )\n            except Exception:\n                pass\n\n        if self._session:\n            self._session.close()\n            self._session = None\n\n        self._session_token = None\n        self._connected = False\n        logger.info(\"Disconnected from Forex.com\")",
    "source_file": "core\\trading\\broker_forex_com.py",
    "academic_reference": null,
    "class_name": "ForexComBroker"
  },
  {
    "name": "is_connected",
    "category": "quantitative",
    "formula": "",
    "explanation": "Check connection status.",
    "python_code": "def is_connected(self) -> bool:\n        \"\"\"Check connection status.\"\"\"\n        return self._connected and self._session_token is not None",
    "source_file": "core\\trading\\broker_forex_com.py",
    "academic_reference": null,
    "class_name": "ForexComBroker"
  },
  {
    "name": "ping",
    "category": "quantitative",
    "formula": "False | response.status_code == 200 | False",
    "explanation": "Ping Forex.com API.",
    "python_code": "def ping(self) -> bool:\n        \"\"\"Ping Forex.com API.\"\"\"\n        if not self.is_connected():\n            return False\n        try:\n            response = self._session.get(\n                f\"{self._base_url}/UserAccount/ClientAndTradingAccount\",\n                timeout=self.config.request_timeout\n            )\n            return response.status_code == 200\n        except Exception:\n            return False",
    "source_file": "core\\trading\\broker_forex_com.py",
    "academic_reference": null,
    "class_name": "ForexComBroker"
  },
  {
    "name": "_get_market_id",
    "category": "quantitative",
    "formula": "market_id",
    "explanation": "Get Forex.com market ID for symbol.",
    "python_code": "def _get_market_id(self, symbol: str) -> int:\n        \"\"\"Get Forex.com market ID for symbol.\"\"\"\n        market_id = self.SYMBOL_TO_MARKET_ID.get(symbol)\n        if not market_id:\n            raise ValueError(f\"Unknown symbol: {symbol}\")\n        return market_id",
    "source_file": "core\\trading\\broker_forex_com.py",
    "academic_reference": null,
    "class_name": "ForexComBroker"
  },
  {
    "name": "get_account",
    "category": "quantitative",
    "formula": "BrokerAccount(",
    "explanation": "Get account info.",
    "python_code": "def get_account(self) -> BrokerAccount:\n        \"\"\"Get account info.\"\"\"\n        if not self.is_connected():\n            raise ConnectionError(\"Not connected to Forex.com\")\n\n        response = self._session.get(\n            f\"{self._base_url}/UserAccount/ClientAndTradingAccount\",\n            timeout=self.config.request_timeout\n        )\n\n        if response.status_code == 200:\n            data = response.json()\n            trading_account = data.get('TradingAccounts', [{}])[0]\n\n            return BrokerAccount(\n                account_id=str(self._trading_account_id),\n                broker_type=BrokerType.FOREX_COM,\n                balance=float(trading_account.get('TradingAccountBalance', 0)),\n                equity=float(trading_account.get('TradingAccountEquity', 0)),\n                margin_used=float(trading_account.get('TradingAccountMarginUsed', 0)),\n                margin_available=float(trading_account.get('TradingAccountMarginAvailable', 0)),\n                currency=trading_account.get('TradingAccountCurrency', 'USD'),\n                is_active=True,\n                updated_at=datetime.now()\n            )\n\n        raise RuntimeError(f\"Failed to get account: {response.text}\")",
    "source_file": "core\\trading\\broker_forex_com.py",
    "academic_reference": null,
    "class_name": "ForexComBroker"
  },
  {
    "name": "get_balance",
    "category": "quantitative",
    "formula": "account.balance",
    "explanation": "Get account balance.",
    "python_code": "def get_balance(self) -> float:\n        \"\"\"Get account balance.\"\"\"\n        account = self.get_account()\n        return account.balance",
    "source_file": "core\\trading\\broker_forex_com.py",
    "academic_reference": null,
    "class_name": "ForexComBroker"
  },
  {
    "name": "submit_order",
    "category": "quantitative",
    "formula": "order | order",
    "explanation": "Submit order to Forex.com.",
    "python_code": "def submit_order(\n        self,\n        symbol: str,\n        side: OrderSide,\n        quantity: float,\n        order_type: OrderType = OrderType.MARKET,\n        limit_price: Optional[float] = None,\n        stop_price: Optional[float] = None,\n        **kwargs\n    ) -> BrokerOrder:\n        \"\"\"Submit order to Forex.com.\"\"\"\n        if not self.is_connected():\n            raise ConnectionError(\"Not connected to Forex.com\")\n\n        order = BrokerOrder(\n            internal_id=self._generate_order_id(),\n            broker_type=BrokerType.FOREX_COM,\n            symbol=symbol,\n            side=side,\n            order_type=order_type,\n            quantity=quantity,\n            limit_price=limit_price,\n            stop_price=stop_price,\n            status=OrderStatus.PENDING,\n            created_at=datetime.now()\n        )\n\n        try:\n            market_id = self._get_market_id(symbol)\n\n            # Direction: true = buy, false = sell\n            direction = side == OrderSide.BUY\n\n            order_request = {\n                'MarketId': market_id,\n                'Direction': 'buy' if direction else 'sell',\n                'Quantity': quantity,\n                'TradingAccountId': self._trading_account_id,\n                'PositionMethodId': 1,  # LongOrShortOnly\n            }\n\n            if order_type == OrderType.MARKET:\n                # Market order endpoint\n                endpoint = f\"{self._base_url}/order/newtradeorder\"\n            elif order_type == OrderType.LIMIT:\n                endpoint = f\"{self._base_url}/order/newstoplimitorder\"\n                order_request['OrderType'] = 'Limit'\n                order_request['TriggerPrice'] = limit_price\n            elif order_type == OrderType.STOP:\n                endpoint = f\"{self._base_url}/order/newstoplimitorder\"\n                order_request['OrderType'] = 'Stop'\n                order_request['TriggerPrice'] = stop_price\n            else:\n                raise ValueError(f\"Unsupported order type: {order_type}\")\n\n            response = self._session.post(\n                endpoint,\n                json=order_request,\n                timeout=self.config.request_timeout\n            )\n\n            result = response.json()\n\n            if response.status_code == 200:\n                order.broker_id = str(result.get('OrderId', ''))\n                order.status = OrderStatus.FILLED if result.get('Status') == 1 else OrderStatus.SUBMITTED\n                order.submitted_at = datetime.now()\n\n                if order.status == OrderStatus.FILLED:\n                    order.filled_qty = quantity\n                    order.avg_fill_price = float(result.get('Price', 0))\n                    order.filled_at = datetime.now()\n            else:\n                order.status = OrderStatus.REJECTED\n                order.error_message = result.get('ErrorMessage', str(result))\n\n            order.updated_at = datetime.now()\n            self._update_order(order)\n\n            logger.info(\n                f\"Forex.com Order ",
    "source_file": "core\\trading\\broker_forex_com.py",
    "academic_reference": null,
    "class_name": "ForexComBroker"
  },
  {
    "name": "cancel_order",
    "category": "quantitative",
    "formula": "False | False | True",
    "explanation": "Cancel a Forex.com order.",
    "python_code": "def cancel_order(self, order_id: str) -> bool:\n        \"\"\"Cancel a Forex.com order.\"\"\"\n        if not self.is_connected():\n            return False\n\n        order = self._orders.get(order_id)\n        if not order or order.is_complete:\n            return False\n\n        try:\n            if order.broker_id:\n                response = self._session.post(\n                    f\"{self._base_url}/order/cancel\",\n                    json={\n                        'OrderId': int(order.broker_id),\n                        'TradingAccountId': self._trading_account_id\n                    },\n                    timeout=self.config.request_timeout\n                )\n\n                if response.status_code == 200:\n                    order.status = OrderStatus.CANCELLED\n                    order.updated_at = datetime.now()\n                    return True\n\n            return False\n\n        except Exception as e:\n            logger.error(f\"Forex.com cancel error: {e}\")\n            return False",
    "source_file": "core\\trading\\broker_forex_com.py",
    "academic_reference": null,
    "class_name": "ForexComBroker"
  },
  {
    "name": "get_order",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get order by ID.",
    "python_code": "def get_order(self, order_id: str) -> Optional[BrokerOrder]:\n        \"\"\"Get order by ID.\"\"\"\n        return self._orders.get(order_id)",
    "source_file": "core\\trading\\broker_forex_com.py",
    "academic_reference": null,
    "class_name": "ForexComBroker"
  },
  {
    "name": "get_open_orders",
    "category": "quantitative",
    "formula": "[] | orders",
    "explanation": "Get open orders.",
    "python_code": "def get_open_orders(self, symbol: Optional[str] = None) -> List[BrokerOrder]:\n        \"\"\"Get open orders.\"\"\"\n        if not self.is_connected():\n            return []\n\n        response = self._session.get(\n            f\"{self._base_url}/order/openpositions?TradingAccountId={self._trading_account_id}\",\n            timeout=self.config.request_timeout\n        )\n\n        orders = []\n        if response.status_code == 200:\n            for o in response.json().get('OpenPositions', []):\n                market_id = o.get('MarketId')\n                sym = self.MARKET_ID_TO_SYMBOL.get(market_id, str(market_id))\n\n                if symbol and sym != symbol:\n                    continue\n\n                broker_order = BrokerOrder(\n                    internal_id=f\"forexcom_{o.get('OrderId')}\",\n                    broker_id=str(o.get('OrderId')),\n                    broker_type=BrokerType.FOREX_COM,\n                    symbol=sym,\n                    side=OrderSide.BUY if o.get('Direction') == 'buy' else OrderSide.SELL,\n                    quantity=float(o.get('Quantity', 0)),\n                    status=OrderStatus.SUBMITTED\n                )\n                orders.append(broker_order)\n\n        return orders",
    "source_file": "core\\trading\\broker_forex_com.py",
    "academic_reference": null,
    "class_name": "ForexComBroker"
  },
  {
    "name": "get_positions",
    "category": "quantitative",
    "formula": "[] | positions",
    "explanation": "Get all positions.",
    "python_code": "def get_positions(self, symbol: Optional[str] = None) -> List[BrokerPosition]:\n        \"\"\"Get all positions.\"\"\"\n        if not self.is_connected():\n            return []\n\n        response = self._session.get(\n            f\"{self._base_url}/order/openpositions?TradingAccountId={self._trading_account_id}\",\n            timeout=self.config.request_timeout\n        )\n\n        positions = []\n        if response.status_code == 200:\n            for p in response.json().get('OpenPositions', []):\n                market_id = p.get('MarketId')\n                sym = self.MARKET_ID_TO_SYMBOL.get(market_id, str(market_id))\n\n                if symbol and sym != symbol:\n                    continue\n\n                direction = p.get('Direction', '').lower()\n                side = PositionSide.LONG if direction == 'buy' else PositionSide.SHORT\n\n                bp = BrokerPosition(\n                    symbol=sym,\n                    broker_type=BrokerType.FOREX_COM,\n                    account_id=str(self._trading_account_id),\n                    side=side,\n                    quantity=float(p.get('Quantity', 0)),\n                    avg_entry_price=float(p.get('OpenPrice', 0)),\n                    unrealized_pnl=float(p.get('UnrealisedPnL', 0)),\n                    updated_at=datetime.now()\n                )\n                positions.append(bp)\n\n        return positions",
    "source_file": "core\\trading\\broker_forex_com.py",
    "academic_reference": null,
    "class_name": "ForexComBroker"
  },
  {
    "name": "close_position",
    "category": "quantitative",
    "formula": "",
    "explanation": "Close a position.",
    "python_code": "def close_position(self, symbol: str, quantity: Optional[float] = None) -> BrokerOrder:\n        \"\"\"Close a position.\"\"\"\n        positions = self.get_positions(symbol)\n        if not positions:\n            raise ValueError(f\"No position found for {symbol}\")\n\n        pos = positions[0]\n        close_qty = quantity or pos.quantity\n        close_side = OrderSide.SELL if pos.side == PositionSide.LONG else OrderSide.BUY\n\n        return self.submit_order(\n            symbol=symbol,\n            side=close_side,\n            quantity=close_qty,\n            order_type=OrderType.MARKET\n        )",
    "source_file": "core\\trading\\broker_forex_com.py",
    "academic_reference": null,
    "class_name": "ForexComBroker"
  },
  {
    "name": "get_quote",
    "category": "quantitative",
    "formula": "BrokerQuote( | BrokerQuote(symbol=symbol, broker_type=BrokerType.FOREX_COM)",
    "explanation": "Get current quote.",
    "python_code": "def get_quote(self, symbol: str) -> BrokerQuote:\n        \"\"\"Get current quote.\"\"\"\n        if not self.is_connected():\n            raise ConnectionError(\"Not connected to Forex.com\")\n\n        market_id = self._get_market_id(symbol)\n\n        response = self._session.get(\n            f\"{self._base_url}/market/{market_id}/tickhistory?PriceTicks=1\",\n            timeout=self.config.request_timeout\n        )\n\n        if response.status_code == 200:\n            ticks = response.json().get('PriceTicks', [])\n            if ticks:\n                tick = ticks[0]\n                bid = float(tick.get('Bid', 0))\n                ask = float(tick.get('Ask', 0))\n\n                pip_value = self.PIP_VALUES.get(symbol, 0.0001)\n                spread = ask - bid\n                spread_pips = spread / pip_value if pip_value > 0 else 0\n\n                return BrokerQuote(\n                    symbol=symbol,\n                    broker_type=BrokerType.FOREX_COM,\n                    bid=bid,\n                    ask=ask,\n                    mid=(bid + ask) / 2,\n                    spread=spread,\n                    spread_pips=spread_pips,\n                    timestamp=datetime.now()\n                )\n\n        return BrokerQuote(symbol=symbol, broker_type=BrokerType.FOREX_COM)",
    "source_file": "core\\trading\\broker_forex_com.py",
    "academic_reference": null,
    "class_name": "ForexComBroker"
  },
  {
    "name": "get_spread",
    "category": "microstructure",
    "formula": "quote.spread_pips",
    "explanation": "Get current spread in pips.",
    "python_code": "def get_spread(self, symbol: str) -> float:\n        \"\"\"Get current spread in pips.\"\"\"\n        quote = self.get_quote(symbol)\n        return quote.spread_pips",
    "source_file": "core\\trading\\broker_forex_com.py",
    "academic_reference": null,
    "class_name": "ForexComBroker"
  },
  {
    "name": "get_symbols",
    "category": "quantitative",
    "formula": "list(self.SYMBOL_TO_MARKET_ID.keys())",
    "explanation": "Get tradeable symbols.",
    "python_code": "def get_symbols(self) -> List[str]:\n        \"\"\"Get tradeable symbols.\"\"\"\n        return list(self.SYMBOL_TO_MARKET_ID.keys())",
    "source_file": "core\\trading\\broker_forex_com.py",
    "academic_reference": null,
    "class_name": "ForexComBroker"
  },
  {
    "name": "get_pip_value",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get pip value for symbol.",
    "python_code": "def get_pip_value(self, symbol: str) -> float:\n        \"\"\"Get pip value for symbol.\"\"\"\n        return self.PIP_VALUES.get(symbol, 0.0001)",
    "source_file": "core\\trading\\broker_forex_com.py",
    "academic_reference": null,
    "class_name": "ForexComBroker"
  },
  {
    "name": "get_min_quantity",
    "category": "quantitative",
    "formula": "1000.0",
    "explanation": "Get minimum order quantity.",
    "python_code": "def get_min_quantity(self, symbol: str) -> float:\n        \"\"\"Get minimum order quantity.\"\"\"\n        return 1000.0",
    "source_file": "core\\trading\\broker_forex_com.py",
    "academic_reference": null,
    "class_name": "ForexComBroker"
  },
  {
    "name": "create_forex_com_broker",
    "category": "reinforcement_learning",
    "formula": "ForexComBroker(config)",
    "explanation": "Factory function to create Forex.com broker.\n\nArgs:\n    username: Forex.com username\n    password: Forex.com password\n    app_key: API app key\n    paper: Whether demo account\n\nReturns:\n    Configured ForexComBroker instance",
    "python_code": "def create_forex_com_broker(\n    username: str,\n    password: str,\n    app_key: str,\n    paper: bool = True\n) -> ForexComBroker:\n    \"\"\"\n    Factory function to create Forex.com broker.\n\n    Args:\n        username: Forex.com username\n        password: Forex.com password\n        app_key: API app key\n        paper: Whether demo account\n\n    Returns:\n        Configured ForexComBroker instance\n    \"\"\"\n    config = BrokerConfig(\n        broker_type=BrokerType.FOREX_COM,\n        name=\"Forex.com\",\n        api_key=password,\n        api_secret=app_key,\n        paper=paper,\n        extra={'username': username},\n        symbols=list(ForexComBroker.SYMBOL_TO_MARKET_ID.keys()),\n        priority=3\n    )\n\n    return ForexComBroker(config)",
    "source_file": "core\\trading\\broker_forex_com.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "quantitative",
    "formula": "",
    "explanation": "Initialize IB broker.\n\nArgs:\n    config: Broker configuration with IB-specific settings",
    "python_code": "def __init__(self, config: BrokerConfig):\n        \"\"\"\n        Initialize IB broker.\n\n        Args:\n            config: Broker configuration with IB-specific settings\n        \"\"\"\n        super().__init__(config)\n        self._ib = None\n        self._contracts: Dict[str, Any] = {}\n        self._worker_thread: Optional[threading.Thread] = None\n        self._running = False",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": "IBBroker"
  },
  {
    "name": "connect",
    "category": "quantitative",
    "formula": "True | True | False",
    "explanation": "Connect to IB Gateway.",
    "python_code": "def connect(self) -> bool:\n        \"\"\"Connect to IB Gateway.\"\"\"\n        if self._connected:\n            return True\n\n        try:\n            from ib_insync import IB\n            self._ib = IB()\n\n            # Connect with timeout\n            self._ib.connect(\n                host=self.config.host or 'localhost',\n                port=self.config.port or 4004,\n                clientId=self.config.extra.get('client_id', 1),\n                readonly=False,\n                timeout=self.config.connect_timeout\n            )\n\n            self._connected = True\n            self._start_event_loop()\n\n            logger.info(f\"Connected to IB Gateway at {self.config.host}:{self.config.port}\")\n            logger.info(f\"Account: {self._ib.managedAccounts()}\")\n\n            return True\n\n        except ImportError:\n            logger.error(\"ib_insync not installed: pip install ib_insync\")\n            return False\n\n        except Exception as e:\n            logger.error(f\"Failed to connect to IB Gateway: {e}\")\n            self._connected = False\n            return False",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": "IBBroker"
  },
  {
    "name": "disconnect",
    "category": "quantitative",
    "formula": "",
    "explanation": "Disconnect from IB Gateway.",
    "python_code": "def disconnect(self) -> None:\n        \"\"\"Disconnect from IB Gateway.\"\"\"\n        self._running = False\n\n        if self._worker_thread and self._worker_thread.is_alive():\n            self._worker_thread.join(timeout=5.0)\n\n        if self._ib and self._connected:\n            try:\n                self._ib.disconnect()\n            except Exception:\n                pass\n            self._connected = False\n            logger.info(\"Disconnected from IB Gateway\")",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": "IBBroker"
  },
  {
    "name": "is_connected",
    "category": "quantitative",
    "formula": "False",
    "explanation": "Check if connected.",
    "python_code": "def is_connected(self) -> bool:\n        \"\"\"Check if connected.\"\"\"\n        if not self._connected or not self._ib:\n            return False\n        return self._ib.isConnected()",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": "IBBroker"
  },
  {
    "name": "ping",
    "category": "quantitative",
    "formula": "False | True | False",
    "explanation": "Ping IB Gateway.",
    "python_code": "def ping(self) -> bool:\n        \"\"\"Ping IB Gateway.\"\"\"\n        if not self.is_connected():\n            return False\n        try:\n            # Request server time as ping\n            self._ib.reqCurrentTime()\n            return True\n        except Exception:\n            return False",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": "IBBroker"
  },
  {
    "name": "_start_event_loop",
    "category": "quantitative",
    "formula": "",
    "explanation": "Start IB event loop in background thread.",
    "python_code": "def _start_event_loop(self):\n        \"\"\"Start IB event loop in background thread.\"\"\"\n        if self._running:\n            return\n\n        self._running = True\n        self._worker_thread = threading.Thread(\n            target=self._run_event_loop,\n            daemon=True,\n            name=\"IB-EventLoop\"\n        )\n        self._worker_thread.start()",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": "IBBroker"
  },
  {
    "name": "_run_event_loop",
    "category": "quantitative",
    "formula": "",
    "explanation": "Run IB event loop.",
    "python_code": "def _run_event_loop(self):\n        \"\"\"Run IB event loop.\"\"\"\n        while self._running and self._connected:\n            try:\n                self._ib.sleep(0.1)\n            except Exception as e:\n                logger.error(f\"IB event loop error: {e}\")\n                break",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": "IBBroker"
  },
  {
    "name": "get_account",
    "category": "quantitative",
    "formula": "BrokerAccount(",
    "explanation": "Get IB account information.",
    "python_code": "def get_account(self) -> BrokerAccount:\n        \"\"\"Get IB account information.\"\"\"\n        if not self.is_connected():\n            raise ConnectionError(\"Not connected to IB Gateway\")\n\n        account_values = self._ib.accountValues()\n\n        # Parse account values\n        balance = 0.0\n        equity = 0.0\n        margin_used = 0.0\n\n        for av in account_values:\n            if av.tag == 'TotalCashBalance' and av.currency == 'USD':\n                balance = float(av.value)\n            elif av.tag == 'NetLiquidation' and av.currency == 'USD':\n                equity = float(av.value)\n            elif av.tag == 'MaintMarginReq' and av.currency == 'USD':\n                margin_used = float(av.value)\n\n        return BrokerAccount(\n            account_id=self.config.account_id or self._ib.managedAccounts()[0],\n            broker_type=BrokerType.IB,\n            balance=balance,\n            equity=equity,\n            margin_used=margin_used,\n            margin_available=equity - margin_used,\n            currency='USD',\n            is_active=True,\n            updated_at=datetime.now()\n        )",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": "IBBroker"
  },
  {
    "name": "get_balance",
    "category": "quantitative",
    "formula": "account.balance",
    "explanation": "Get account balance.",
    "python_code": "def get_balance(self) -> float:\n        \"\"\"Get account balance.\"\"\"\n        account = self.get_account()\n        return account.balance",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": "IBBroker"
  },
  {
    "name": "submit_order",
    "category": "quantitative",
    "formula": "order | order",
    "explanation": "Submit order to IB Gateway.",
    "python_code": "def submit_order(\n        self,\n        symbol: str,\n        side: OrderSide,\n        quantity: float,\n        order_type: OrderType = OrderType.MARKET,\n        limit_price: Optional[float] = None,\n        stop_price: Optional[float] = None,\n        **kwargs\n    ) -> BrokerOrder:\n        \"\"\"Submit order to IB Gateway.\"\"\"\n        if not self.is_connected():\n            raise ConnectionError(\"Not connected to IB Gateway\")\n\n        from ib_insync import Forex, MarketOrder, LimitOrder, StopOrder, StopLimitOrder\n\n        # Create order record\n        order = BrokerOrder(\n            internal_id=self._generate_order_id(),\n            broker_type=BrokerType.IB,\n            symbol=symbol,\n            side=side,\n            order_type=order_type,\n            quantity=quantity,\n            limit_price=limit_price,\n            stop_price=stop_price,\n            status=OrderStatus.PENDING,\n            created_at=datetime.now()\n        )\n\n        try:\n            # Get or create contract\n            contract = self._get_contract(symbol)\n\n            # Create IB order\n            ib_side = \"BUY\" if side == OrderSide.BUY else \"SELL\"\n\n            if order_type == OrderType.MARKET:\n                ib_order = MarketOrder(ib_side, quantity)\n            elif order_type == OrderType.LIMIT:\n                ib_order = LimitOrder(ib_side, quantity, limit_price)\n            elif order_type == OrderType.STOP:\n                ib_order = StopOrder(ib_side, quantity, stop_price)\n            elif order_type == OrderType.STOP_LIMIT:\n                ib_order = StopLimitOrder(ib_side, quantity, limit_price, stop_price)\n            else:\n                raise ValueError(f\"Unsupported order type: {order_type}\")\n\n            # Submit\n            trade = self._ib.placeOrder(contract, ib_order)\n            order.broker_id = str(trade.order.orderId)\n            order.status = OrderStatus.SUBMITTED\n            order.submitted_at = datetime.now()\n\n            # Wait for fill with timeout\n            start = time.time()\n            timeout = kwargs.get('timeout', 30.0)\n            while not trade.isDone() and time.time() - start < timeout:\n                self._ib.sleep(0.1)\n\n            # Update order from trade\n            self._update_order_from_trade(order, trade)\n\n            # Store order\n            self._update_order(order)\n\n            logger.info(\n                f\"IB Order {order.internal_id}: {order.side.value} {order.quantity} {symbol} \"\n                f\"-> {order.status.value}\"\n            )\n\n            return order\n\n        except Exception as e:\n            order.status = OrderStatus.ERROR\n            order.error_message = str(e)\n            order.updated_at = datetime.now()\n            self._update_order(order)\n            logger.error(f\"IB order error: {e}\")\n            return order",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": "IBBroker"
  },
  {
    "name": "_get_contract",
    "category": "quantitative",
    "formula": "contract",
    "explanation": "Get or create IB Forex contract.",
    "python_code": "def _get_contract(self, symbol: str):\n        \"\"\"Get or create IB Forex contract.\"\"\"\n        if symbol in self._contracts:\n            return self._contracts[symbol]\n\n        from ib_insync import Forex\n\n        # Map symbol to IB format\n        ib_symbol = self.SYMBOL_MAP.get(symbol, symbol)\n\n        # Create contract\n        if '.' in ib_symbol:\n            pair = ib_symbol.split('.')\n            contract = Forex(pair=ib_symbol)\n        else:\n            contract = Forex(symbol)\n\n        # Qualify contract\n        self._ib.qualifyContracts(contract)\n        self._contracts[symbol] = contract\n\n        return contract",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": "IBBroker"
  },
  {
    "name": "_update_order_from_trade",
    "category": "quantitative",
    "formula": "",
    "explanation": "Update order from IB trade object.",
    "python_code": "def _update_order_from_trade(self, order: BrokerOrder, trade):\n        \"\"\"Update order from IB trade object.\"\"\"\n        status_map = {\n            'PendingSubmit': OrderStatus.PENDING,\n            'PreSubmitted': OrderStatus.SUBMITTED,\n            'Submitted': OrderStatus.SUBMITTED,\n            'Filled': OrderStatus.FILLED,\n            'Cancelled': OrderStatus.CANCELLED,\n            'ApiCancelled': OrderStatus.CANCELLED,\n            'Inactive': OrderStatus.REJECTED,\n        }\n\n        ib_status = trade.orderStatus.status\n        order.status = status_map.get(ib_status, OrderStatus.PENDING)\n\n        if trade.orderStatus.filled > 0:\n            order.filled_qty = float(trade.orderStatus.filled)\n            order.avg_fill_price = float(trade.orderStatus.avgFillPrice)\n\n            if order.filled_qty >= order.quantity:\n                order.status = OrderStatus.FILLED\n                order.filled_at = datetime.now()\n            elif order.filled_qty > 0:\n                order.status = OrderStatus.PARTIAL\n\n        order.updated_at = datetime.now()",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": "IBBroker"
  },
  {
    "name": "cancel_order",
    "category": "quantitative",
    "formula": "False | False | True",
    "explanation": "Cancel an IB order.",
    "python_code": "def cancel_order(self, order_id: str) -> bool:\n        \"\"\"Cancel an IB order.\"\"\"\n        if not self.is_connected():\n            return False\n\n        order = self._orders.get(order_id)\n        if not order or order.is_complete:\n            return False\n\n        try:\n            if order.broker_id:\n                # Find the trade\n                for trade in self._ib.openTrades():\n                    if str(trade.order.orderId) == order.broker_id:\n                        self._ib.cancelOrder(trade.order)\n                        order.status = OrderStatus.CANCELLED\n                        order.updated_at = datetime.now()\n                        return True\n\n            return False\n\n        except Exception as e:\n            logger.error(f\"Cancel error: {e}\")\n            return False",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": "IBBroker"
  },
  {
    "name": "get_order",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get order by ID.",
    "python_code": "def get_order(self, order_id: str) -> Optional[BrokerOrder]:\n        \"\"\"Get order by ID.\"\"\"\n        return self._orders.get(order_id)",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": "IBBroker"
  },
  {
    "name": "get_open_orders",
    "category": "quantitative",
    "formula": "orders",
    "explanation": "Get open orders.",
    "python_code": "def get_open_orders(self, symbol: Optional[str] = None) -> List[BrokerOrder]:\n        \"\"\"Get open orders.\"\"\"\n        orders = [o for o in self._orders.values() if o.is_active]\n        if symbol:\n            orders = [o for o in orders if o.symbol == symbol]\n        return orders",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": "IBBroker"
  },
  {
    "name": "get_positions",
    "category": "quantitative",
    "formula": "[] | positions",
    "explanation": "Get all positions.",
    "python_code": "def get_positions(self, symbol: Optional[str] = None) -> List[BrokerPosition]:\n        \"\"\"Get all positions.\"\"\"\n        if not self.is_connected():\n            return []\n\n        positions = []\n        for pos in self._ib.positions():\n            # Extract symbol from contract\n            pos_symbol = pos.contract.symbol\n            if hasattr(pos.contract, 'pair'):\n                pos_symbol = pos.contract.pair.replace('.', '')\n\n            if symbol and pos_symbol != symbol:\n                continue\n\n            side = PositionSide.LONG if pos.position > 0 else (\n                PositionSide.SHORT if pos.position < 0 else PositionSide.FLAT\n            )\n\n            bp = BrokerPosition(\n                symbol=pos_symbol,\n                broker_type=BrokerType.IB,\n                account_id=pos.account,\n                side=side,\n                quantity=abs(pos.position),\n                avg_entry_price=pos.avgCost,\n                updated_at=datetime.now()\n            )\n            positions.append(bp)\n\n        return positions",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": "IBBroker"
  },
  {
    "name": "close_position",
    "category": "quantitative",
    "formula": "",
    "explanation": "Close a position.",
    "python_code": "def close_position(self, symbol: str, quantity: Optional[float] = None) -> BrokerOrder:\n        \"\"\"Close a position.\"\"\"\n        positions = self.get_positions(symbol)\n        if not positions:\n            raise ValueError(f\"No position found for {symbol}\")\n\n        pos = positions[0]\n        close_qty = quantity or pos.quantity\n        close_side = OrderSide.SELL if pos.side == PositionSide.LONG else OrderSide.BUY\n\n        return self.submit_order(\n            symbol=symbol,\n            side=close_side,\n            quantity=close_qty,\n            order_type=OrderType.MARKET\n        )",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": "IBBroker"
  },
  {
    "name": "get_quote",
    "category": "quantitative",
    "formula": "BrokerQuote(",
    "explanation": "Get current quote.",
    "python_code": "def get_quote(self, symbol: str) -> BrokerQuote:\n        \"\"\"Get current quote.\"\"\"\n        if not self.is_connected():\n            raise ConnectionError(\"Not connected to IB Gateway\")\n\n        contract = self._get_contract(symbol)\n\n        # Request ticker\n        ticker = self._ib.reqMktData(contract, '', False, False)\n        self._ib.sleep(1.0)  # Wait for data\n\n        bid = ticker.bid if ticker.bid > 0 else 0\n        ask = ticker.ask if ticker.ask > 0 else 0\n\n        pip_value = self.PIP_VALUES.get(symbol, 0.0001)\n        spread = ask - bid if bid > 0 and ask > 0 else 0\n        spread_pips = spread / pip_value if pip_value > 0 else 0\n\n        self._ib.cancelMktData(contract)\n\n        return BrokerQuote(\n            symbol=symbol,\n            broker_type=BrokerType.IB,\n            bid=bid,\n            ask=ask,\n            mid=(bid + ask) / 2 if bid > 0 and ask > 0 else 0,\n            spread=spread,\n            spread_pips=spread_pips,\n            timestamp=datetime.now()\n        )",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": "IBBroker"
  },
  {
    "name": "get_spread",
    "category": "microstructure",
    "formula": "quote.spread_pips",
    "explanation": "Get current spread in pips.",
    "python_code": "def get_spread(self, symbol: str) -> float:\n        \"\"\"Get current spread in pips.\"\"\"\n        quote = self.get_quote(symbol)\n        return quote.spread_pips",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": "IBBroker"
  },
  {
    "name": "get_symbols",
    "category": "quantitative",
    "formula": "list(self.SYMBOL_MAP.keys())",
    "explanation": "Get tradeable forex symbols.",
    "python_code": "def get_symbols(self) -> List[str]:\n        \"\"\"Get tradeable forex symbols.\"\"\"\n        return list(self.SYMBOL_MAP.keys())",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": "IBBroker"
  },
  {
    "name": "get_pip_value",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get pip value for symbol.",
    "python_code": "def get_pip_value(self, symbol: str) -> float:\n        \"\"\"Get pip value for symbol.\"\"\"\n        return self.PIP_VALUES.get(symbol, 0.0001)",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": "IBBroker"
  },
  {
    "name": "get_min_quantity",
    "category": "quantitative",
    "formula": "25000.0",
    "explanation": "Get minimum order quantity (lot size).",
    "python_code": "def get_min_quantity(self, symbol: str) -> float:\n        \"\"\"Get minimum order quantity (lot size).\"\"\"\n        # IB forex minimum is typically 25,000 units for majors\n        return 25000.0",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": "IBBroker"
  },
  {
    "name": "create_ib_broker",
    "category": "reinforcement_learning",
    "formula": "IBBroker(config)",
    "explanation": "Factory function to create IB broker.\n\nArgs:\n    host: IB Gateway host\n    port: IB Gateway port (4004 for paper)\n    account_id: IB account ID\n    client_id: Client ID for connection\n    paper: Whether paper trading\n\nReturns:\n    Configured IBBroker instance",
    "python_code": "def create_ib_broker(\n    host: str = 'localhost',\n    port: int = 4004,\n    account_id: str = 'DUO423364',\n    client_id: int = 1,\n    paper: bool = True\n) -> IBBroker:\n    \"\"\"\n    Factory function to create IB broker.\n\n    Args:\n        host: IB Gateway host\n        port: IB Gateway port (4004 for paper)\n        account_id: IB account ID\n        client_id: Client ID for connection\n        paper: Whether paper trading\n\n    Returns:\n        Configured IBBroker instance\n    \"\"\"\n    config = BrokerConfig(\n        broker_type=BrokerType.IB,\n        name=\"Interactive Brokers\",\n        host=host,\n        port=port,\n        account_id=account_id,\n        paper=paper,\n        extra={'client_id': client_id},\n        symbols=['EURUSD', 'GBPUSD', 'USDJPY', 'USDCHF', 'AUDUSD', 'USDCAD', 'NZDUSD'],\n        priority=1\n    )\n\n    return IBBroker(config)",
    "source_file": "core\\trading\\broker_ib.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "quantitative",
    "formula": "",
    "explanation": "Initialize OANDA broker.",
    "python_code": "def __init__(self, config: BrokerConfig):\n        \"\"\"Initialize OANDA broker.\"\"\"\n        super().__init__(config)\n        self._session = None\n        self._base_url = self.PRACTICE_URL if config.paper else self.LIVE_URL\n        self._stream_url = self.STREAM_PRACTICE_URL if config.paper else self.STREAM_LIVE_URL",
    "source_file": "core\\trading\\broker_oanda.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "OANDABroker"
  },
  {
    "name": "connect",
    "category": "quantitative",
    "formula": "True | True | False",
    "explanation": "Connect to OANDA API.",
    "python_code": "def connect(self) -> bool:\n        \"\"\"Connect to OANDA API.\"\"\"\n        if self._connected:\n            return True\n\n        try:\n            import requests\n            self._session = requests.Session()\n            self._session.headers.update({\n                'Authorization': f'Bearer {self.config.api_key}',\n                'Content-Type': 'application/json',\n                'Accept-Datetime-Format': 'RFC3339'\n            })\n\n            # Test connection by fetching account\n            response = self._request('GET', f'/v3/accounts/{self.config.account_id}')\n\n            if response.status_code == 200:\n                self._connected = True\n                account_data = response.json().get('account', {})\n                logger.info(f\"Connected to OANDA ({self._base_url})\")\n                logger.info(f\"Account: {self.config.account_id}, Balance: {account_data.get('balance')}\")\n                return True\n            else:\n                logger.error(f\"OANDA connection failed: {response.text}\")\n                return False\n\n        except ImportError:\n            logger.error(\"requests not installed: pip install requests\")\n            return False\n\n        except Exception as e:\n            logger.error(f\"OANDA connection error: {e}\")\n            self._connected = False\n            return False",
    "source_file": "core\\trading\\broker_oanda.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "OANDABroker"
  },
  {
    "name": "disconnect",
    "category": "quantitative",
    "formula": "",
    "explanation": "Disconnect from OANDA.",
    "python_code": "def disconnect(self) -> None:\n        \"\"\"Disconnect from OANDA.\"\"\"\n        if self._session:\n            self._session.close()\n            self._session = None\n        self._connected = False\n        logger.info(\"Disconnected from OANDA\")",
    "source_file": "core\\trading\\broker_oanda.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "OANDABroker"
  },
  {
    "name": "is_connected",
    "category": "quantitative",
    "formula": "",
    "explanation": "Check connection status.",
    "python_code": "def is_connected(self) -> bool:\n        \"\"\"Check connection status.\"\"\"\n        return self._connected and self._session is not None",
    "source_file": "core\\trading\\broker_oanda.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "OANDABroker"
  },
  {
    "name": "ping",
    "category": "quantitative",
    "formula": "False | response.status_code == 200 | False",
    "explanation": "Ping OANDA API.",
    "python_code": "def ping(self) -> bool:\n        \"\"\"Ping OANDA API.\"\"\"\n        if not self.is_connected():\n            return False\n        try:\n            response = self._request('GET', f'/v3/accounts/{self.config.account_id}')\n            return response.status_code == 200\n        except Exception:\n            return False",
    "source_file": "core\\trading\\broker_oanda.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "OANDABroker"
  },
  {
    "name": "_request",
    "category": "quantitative",
    "formula": "",
    "explanation": "Make API request.",
    "python_code": "def _request(self, method: str, endpoint: str, data: Optional[Dict] = None):\n        \"\"\"Make API request.\"\"\"\n        url = f\"{self._base_url}{endpoint}\"\n\n        if method == 'GET':\n            return self._session.get(url, timeout=self.config.request_timeout)\n        elif method == 'POST':\n            return self._session.post(url, json=data, timeout=self.config.request_timeout)\n        elif method == 'PUT':\n            return self._session.put(url, json=data, timeout=self.config.request_timeout)\n        elif method == 'DELETE':\n            return self._session.delete(url, timeout=self.config.request_timeout)\n        else:\n            raise ValueError(f\"Unknown method: {method}\")",
    "source_file": "core\\trading\\broker_oanda.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "OANDABroker"
  },
  {
    "name": "_to_oanda_symbol",
    "category": "deep_learning",
    "formula": "",
    "explanation": "Convert to OANDA symbol format.",
    "python_code": "def _to_oanda_symbol(self, symbol: str) -> str:\n        \"\"\"Convert to OANDA symbol format.\"\"\"\n        return self.SYMBOL_MAP.get(symbol, symbol.replace('', '_'))",
    "source_file": "core\\trading\\broker_oanda.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "OANDABroker"
  },
  {
    "name": "_from_oanda_symbol",
    "category": "deep_learning",
    "formula": "",
    "explanation": "Convert from OANDA symbol format.",
    "python_code": "def _from_oanda_symbol(self, oanda_symbol: str) -> str:\n        \"\"\"Convert from OANDA symbol format.\"\"\"\n        return self.REVERSE_SYMBOL_MAP.get(oanda_symbol, oanda_symbol.replace('_', ''))",
    "source_file": "core\\trading\\broker_oanda.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "OANDABroker"
  },
  {
    "name": "get_account",
    "category": "quantitative",
    "formula": "BrokerAccount(",
    "explanation": "Get OANDA account info.",
    "python_code": "def get_account(self) -> BrokerAccount:\n        \"\"\"Get OANDA account info.\"\"\"\n        if not self.is_connected():\n            raise ConnectionError(\"Not connected to OANDA\")\n\n        response = self._request('GET', f'/v3/accounts/{self.config.account_id}')\n        data = response.json().get('account', {})\n\n        return BrokerAccount(\n            account_id=self.config.account_id,\n            broker_type=BrokerType.OANDA,\n            balance=float(data.get('balance', 0)),\n            equity=float(data.get('NAV', 0)),\n            margin_used=float(data.get('marginUsed', 0)),\n            margin_available=float(data.get('marginAvailable', 0)),\n            unrealized_pnl=float(data.get('unrealizedPL', 0)),\n            realized_pnl=float(data.get('pl', 0)),\n            currency=data.get('currency', 'USD'),\n            is_active=True,\n            updated_at=datetime.now()\n        )",
    "source_file": "core\\trading\\broker_oanda.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "OANDABroker"
  },
  {
    "name": "get_balance",
    "category": "quantitative",
    "formula": "account.balance",
    "explanation": "Get account balance.",
    "python_code": "def get_balance(self) -> float:\n        \"\"\"Get account balance.\"\"\"\n        account = self.get_account()\n        return account.balance",
    "source_file": "core\\trading\\broker_oanda.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "OANDABroker"
  },
  {
    "name": "submit_order",
    "category": "quantitative",
    "formula": "order | order",
    "explanation": "Submit order to OANDA.",
    "python_code": "def submit_order(\n        self,\n        symbol: str,\n        side: OrderSide,\n        quantity: float,\n        order_type: OrderType = OrderType.MARKET,\n        limit_price: Optional[float] = None,\n        stop_price: Optional[float] = None,\n        **kwargs\n    ) -> BrokerOrder:\n        \"\"\"Submit order to OANDA.\"\"\"\n        if not self.is_connected():\n            raise ConnectionError(\"Not connected to OANDA\")\n\n        # Create order record\n        order = BrokerOrder(\n            internal_id=self._generate_order_id(),\n            broker_type=BrokerType.OANDA,\n            symbol=symbol,\n            side=side,\n            order_type=order_type,\n            quantity=quantity,\n            limit_price=limit_price,\n            stop_price=stop_price,\n            status=OrderStatus.PENDING,\n            created_at=datetime.now()\n        )\n\n        try:\n            # Convert symbol\n            oanda_symbol = self._to_oanda_symbol(symbol)\n\n            # Calculate units (positive for buy, negative for sell)\n            units = int(quantity) if side == OrderSide.BUY else int(-quantity)\n\n            # Build order request\n            order_request = {\n                'order': {\n                    'instrument': oanda_symbol,\n                    'units': str(units),\n                    'timeInForce': 'FOK',  # Fill or kill for market\n                    'positionFill': 'DEFAULT'\n                }\n            }\n\n            if order_type == OrderType.MARKET:\n                order_request['order']['type'] = 'MARKET'\n            elif order_type == OrderType.LIMIT:\n                order_request['order']['type'] = 'LIMIT'\n                order_request['order']['price'] = str(limit_price)\n                order_request['order']['timeInForce'] = 'GTC'\n            elif order_type == OrderType.STOP:\n                order_request['order']['type'] = 'STOP'\n                order_request['order']['price'] = str(stop_price)\n                order_request['order']['timeInForce'] = 'GTC'\n            elif order_type == OrderType.STOP_LIMIT:\n                order_request['order']['type'] = 'STOP'\n                order_request['order']['price'] = str(stop_price)\n                order_request['order']['priceBound'] = str(limit_price)\n                order_request['order']['timeInForce'] = 'GTC'\n\n            # Submit order\n            response = self._request(\n                'POST',\n                f'/v3/accounts/{self.config.account_id}/orders',\n                order_request\n            )\n\n            result = response.json()\n\n            if response.status_code in (200, 201):\n                # Check for fill\n                if 'orderFillTransaction' in result:\n                    fill = result['orderFillTransaction']\n                    order.broker_id = fill.get('id')\n                    order.status = OrderStatus.FILLED\n                    order.filled_qty = abs(float(fill.get('units', 0)))\n                    order.avg_fill_price = float(fill.get('price', 0))\n                  ",
    "source_file": "core\\trading\\broker_oanda.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "OANDABroker"
  },
  {
    "name": "cancel_order",
    "category": "quantitative",
    "formula": "False | False | True",
    "explanation": "Cancel an OANDA order.",
    "python_code": "def cancel_order(self, order_id: str) -> bool:\n        \"\"\"Cancel an OANDA order.\"\"\"\n        if not self.is_connected():\n            return False\n\n        order = self._orders.get(order_id)\n        if not order or order.is_complete:\n            return False\n\n        try:\n            if order.broker_id:\n                response = self._request(\n                    'PUT',\n                    f'/v3/accounts/{self.config.account_id}/orders/{order.broker_id}/cancel'\n                )\n\n                if response.status_code == 200:\n                    order.status = OrderStatus.CANCELLED\n                    order.updated_at = datetime.now()\n                    return True\n\n            return False\n\n        except Exception as e:\n            logger.error(f\"OANDA cancel error: {e}\")\n            return False",
    "source_file": "core\\trading\\broker_oanda.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "OANDABroker"
  },
  {
    "name": "get_order",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get order by ID.",
    "python_code": "def get_order(self, order_id: str) -> Optional[BrokerOrder]:\n        \"\"\"Get order by ID.\"\"\"\n        return self._orders.get(order_id)",
    "source_file": "core\\trading\\broker_oanda.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "OANDABroker"
  },
  {
    "name": "get_open_orders",
    "category": "quantitative",
    "formula": "[] | orders",
    "explanation": "Get open orders.",
    "python_code": "def get_open_orders(self, symbol: Optional[str] = None) -> List[BrokerOrder]:\n        \"\"\"Get open orders.\"\"\"\n        if not self.is_connected():\n            return []\n\n        response = self._request(\n            'GET',\n            f'/v3/accounts/{self.config.account_id}/pendingOrders'\n        )\n\n        orders = []\n        if response.status_code == 200:\n            for o in response.json().get('orders', []):\n                oanda_symbol = o.get('instrument', '')\n                sym = self._from_oanda_symbol(oanda_symbol)\n\n                if symbol and sym != symbol:\n                    continue\n\n                broker_order = BrokerOrder(\n                    internal_id=f\"oanda_{o['id']}\",\n                    broker_id=o['id'],\n                    broker_type=BrokerType.OANDA,\n                    symbol=sym,\n                    side=OrderSide.BUY if float(o.get('units', 0)) > 0 else OrderSide.SELL,\n                    quantity=abs(float(o.get('units', 0))),\n                    status=OrderStatus.SUBMITTED\n                )\n                orders.append(broker_order)\n\n        return orders",
    "source_file": "core\\trading\\broker_oanda.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "OANDABroker"
  },
  {
    "name": "get_positions",
    "category": "quantitative",
    "formula": "[] | positions",
    "explanation": "Get all positions.",
    "python_code": "def get_positions(self, symbol: Optional[str] = None) -> List[BrokerPosition]:\n        \"\"\"Get all positions.\"\"\"\n        if not self.is_connected():\n            return []\n\n        response = self._request(\n            'GET',\n            f'/v3/accounts/{self.config.account_id}/openPositions'\n        )\n\n        positions = []\n        if response.status_code == 200:\n            for p in response.json().get('positions', []):\n                oanda_symbol = p.get('instrument', '')\n                sym = self._from_oanda_symbol(oanda_symbol)\n\n                if symbol and sym != symbol:\n                    continue\n\n                long_units = float(p.get('long', {}).get('units', 0))\n                short_units = abs(float(p.get('short', {}).get('units', 0)))\n\n                if long_units > 0:\n                    side = PositionSide.LONG\n                    qty = long_units\n                    avg_price = float(p.get('long', {}).get('averagePrice', 0))\n                    unrealized = float(p.get('long', {}).get('unrealizedPL', 0))\n                elif short_units > 0:\n                    side = PositionSide.SHORT\n                    qty = short_units\n                    avg_price = float(p.get('short', {}).get('averagePrice', 0))\n                    unrealized = float(p.get('short', {}).get('unrealizedPL', 0))\n                else:\n                    continue\n\n                bp = BrokerPosition(\n                    symbol=sym,\n                    broker_type=BrokerType.OANDA,\n                    account_id=self.config.account_id,\n                    side=side,\n                    quantity=qty,\n                    avg_entry_price=avg_price,\n                    unrealized_pnl=unrealized,\n                    updated_at=datetime.now()\n                )\n                positions.append(bp)\n\n        return positions",
    "source_file": "core\\trading\\broker_oanda.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "OANDABroker"
  },
  {
    "name": "close_position",
    "category": "quantitative",
    "formula": "order",
    "explanation": "Close a position.",
    "python_code": "def close_position(self, symbol: str, quantity: Optional[float] = None) -> BrokerOrder:\n        \"\"\"Close a position.\"\"\"\n        oanda_symbol = self._to_oanda_symbol(symbol)\n\n        # Determine close amount\n        close_data = {}\n        if quantity:\n            positions = self.get_positions(symbol)\n            if positions:\n                side = positions[0].side\n                units = int(quantity) if side == PositionSide.SHORT else int(-quantity)\n                if side == PositionSide.LONG:\n                    close_data = {'longUnits': str(abs(units))}\n                else:\n                    close_data = {'shortUnits': str(abs(units))}\n        else:\n            close_data = {'longUnits': 'ALL', 'shortUnits': 'ALL'}\n\n        response = self._request(\n            'PUT',\n            f'/v3/accounts/{self.config.account_id}/positions/{oanda_symbol}/close',\n            close_data\n        )\n\n        # Create order record for the close\n        order = BrokerOrder(\n            internal_id=self._generate_order_id(),\n            broker_type=BrokerType.OANDA,\n            symbol=symbol,\n            side=OrderSide.SELL,  # Simplified\n            order_type=OrderType.MARKET,\n            quantity=quantity or 0,\n            status=OrderStatus.FILLED if response.status_code == 200 else OrderStatus.ERROR,\n            created_at=datetime.now()\n        )\n\n        if response.status_code == 200:\n            result = response.json()\n            if 'longOrderFillTransaction' in result:\n                fill = result['longOrderFillTransaction']\n                order.filled_qty = abs(float(fill.get('units', 0)))\n                order.avg_fill_price = float(fill.get('price', 0))\n            elif 'shortOrderFillTransaction' in result:\n                fill = result['shortOrderFillTransaction']\n                order.filled_qty = abs(float(fill.get('units', 0)))\n                order.avg_fill_price = float(fill.get('price', 0))\n\n        self._update_order(order)\n        return order",
    "source_file": "core\\trading\\broker_oanda.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "OANDABroker"
  },
  {
    "name": "get_quote",
    "category": "quantitative",
    "formula": "BrokerQuote( | BrokerQuote(symbol=symbol, broker_type=BrokerType.OANDA)",
    "explanation": "Get current quote.",
    "python_code": "def get_quote(self, symbol: str) -> BrokerQuote:\n        \"\"\"Get current quote.\"\"\"\n        if not self.is_connected():\n            raise ConnectionError(\"Not connected to OANDA\")\n\n        oanda_symbol = self._to_oanda_symbol(symbol)\n\n        response = self._request(\n            'GET',\n            f'/v3/accounts/{self.config.account_id}/pricing?instruments={oanda_symbol}'\n        )\n\n        if response.status_code == 200:\n            prices = response.json().get('prices', [])\n            if prices:\n                p = prices[0]\n                bid = float(p.get('bids', [{}])[0].get('price', 0))\n                ask = float(p.get('asks', [{}])[0].get('price', 0))\n\n                pip_value = self.PIP_VALUES.get(symbol, 0.0001)\n                spread = ask - bid\n                spread_pips = spread / pip_value if pip_value > 0 else 0\n\n                return BrokerQuote(\n                    symbol=symbol,\n                    broker_type=BrokerType.OANDA,\n                    bid=bid,\n                    ask=ask,\n                    mid=(bid + ask) / 2,\n                    spread=spread,\n                    spread_pips=spread_pips,\n                    timestamp=datetime.now()\n                )\n\n        return BrokerQuote(symbol=symbol, broker_type=BrokerType.OANDA)",
    "source_file": "core\\trading\\broker_oanda.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "OANDABroker"
  },
  {
    "name": "get_spread",
    "category": "microstructure",
    "formula": "quote.spread_pips",
    "explanation": "Get current spread in pips.",
    "python_code": "def get_spread(self, symbol: str) -> float:\n        \"\"\"Get current spread in pips.\"\"\"\n        quote = self.get_quote(symbol)\n        return quote.spread_pips",
    "source_file": "core\\trading\\broker_oanda.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "OANDABroker"
  },
  {
    "name": "get_symbols",
    "category": "quantitative",
    "formula": "list(self.SYMBOL_MAP.keys())",
    "explanation": "Get tradeable symbols.",
    "python_code": "def get_symbols(self) -> List[str]:\n        \"\"\"Get tradeable symbols.\"\"\"\n        return list(self.SYMBOL_MAP.keys())",
    "source_file": "core\\trading\\broker_oanda.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "OANDABroker"
  },
  {
    "name": "get_pip_value",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get pip value for symbol.",
    "python_code": "def get_pip_value(self, symbol: str) -> float:\n        \"\"\"Get pip value for symbol.\"\"\"\n        return self.PIP_VALUES.get(symbol, 0.0001)",
    "source_file": "core\\trading\\broker_oanda.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "OANDABroker"
  },
  {
    "name": "get_min_quantity",
    "category": "quantitative",
    "formula": "1.0",
    "explanation": "Get minimum order quantity (OANDA allows 1 unit).",
    "python_code": "def get_min_quantity(self, symbol: str) -> float:\n        \"\"\"Get minimum order quantity (OANDA allows 1 unit).\"\"\"\n        return 1.0",
    "source_file": "core\\trading\\broker_oanda.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "OANDABroker"
  },
  {
    "name": "create_oanda_broker",
    "category": "reinforcement_learning",
    "formula": "OANDABroker(config)",
    "explanation": "Factory function to create OANDA broker.\n\nArgs:\n    api_key: OANDA API key\n    account_id: OANDA account ID\n    paper: Whether practice account\n\nReturns:\n    Configured OANDABroker instance",
    "python_code": "def create_oanda_broker(\n    api_key: str,\n    account_id: str,\n    paper: bool = True\n) -> OANDABroker:\n    \"\"\"\n    Factory function to create OANDA broker.\n\n    Args:\n        api_key: OANDA API key\n        account_id: OANDA account ID\n        paper: Whether practice account\n\n    Returns:\n        Configured OANDABroker instance\n    \"\"\"\n    config = BrokerConfig(\n        broker_type=BrokerType.OANDA,\n        name=\"OANDA\",\n        api_key=api_key,\n        account_id=account_id,\n        paper=paper,\n        symbols=list(OANDABroker.SYMBOL_MAP.keys()),\n        priority=2\n    )\n\n    return OANDABroker(config)",
    "source_file": "core\\trading\\broker_oanda.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "microstructure",
    "formula": "",
    "explanation": "Initialize broker router.\n\nArgs:\n    strategy: Default routing strategy\n    max_retries: Max retry attempts per broker\n    spread_tolerance_pips: Accept spread within tolerance of best",
    "python_code": "def __init__(\n        self,\n        strategy: RoutingStrategy = RoutingStrategy.BEST_SPREAD,\n        max_retries: int = 3,\n        spread_tolerance_pips: float = 0.5\n    ):\n        \"\"\"\n        Initialize broker router.\n\n        Args:\n            strategy: Default routing strategy\n            max_retries: Max retry attempts per broker\n            spread_tolerance_pips: Accept spread within tolerance of best\n        \"\"\"\n        self.strategy = strategy\n        self.max_retries = max_retries\n        self.spread_tolerance = spread_tolerance_pips\n\n        self._brokers: Dict[BrokerType, BrokerBase] = {}\n        self._status: Dict[BrokerType, BrokerStatus] = {}\n        self._lock = threading.RLock()\n\n        # Round robin counter\n        self._rr_counter = 0\n\n        # Order history per broker\n        self._order_history: Dict[BrokerType, List[BrokerOrder]] = defaultdict(list)\n\n        # Spread cache (symbol -> broker -> spread)\n        self._spread_cache: Dict[str, Dict[BrokerType, float]] = defaultdict(dict)\n        self._spread_cache_time: Dict[str, datetime] = {}",
    "source_file": "core\\trading\\broker_router.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerRouter"
  },
  {
    "name": "add_broker",
    "category": "quantitative",
    "formula": "",
    "explanation": "Add a broker to the router.",
    "python_code": "def add_broker(self, broker: BrokerBase) -> None:\n        \"\"\"Add a broker to the router.\"\"\"\n        with self._lock:\n            broker_type = broker.config.broker_type\n            self._brokers[broker_type] = broker\n            self._status[broker_type] = BrokerStatus(\n                broker_type=broker_type,\n                connected=False\n            )\n            logger.info(f\"Added broker: {broker_type.value}\")",
    "source_file": "core\\trading\\broker_router.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerRouter"
  },
  {
    "name": "remove_broker",
    "category": "quantitative",
    "formula": "",
    "explanation": "Remove a broker from the router.",
    "python_code": "def remove_broker(self, broker_type: BrokerType) -> None:\n        \"\"\"Remove a broker from the router.\"\"\"\n        with self._lock:\n            if broker_type in self._brokers:\n                broker = self._brokers[broker_type]\n                broker.disconnect()\n                del self._brokers[broker_type]\n                del self._status[broker_type]\n                logger.info(f\"Removed broker: {broker_type.value}\")",
    "source_file": "core\\trading\\broker_router.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerRouter"
  },
  {
    "name": "get_broker",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get broker by type.",
    "python_code": "def get_broker(self, broker_type: BrokerType) -> Optional[BrokerBase]:\n        \"\"\"Get broker by type.\"\"\"\n        return self._brokers.get(broker_type)",
    "source_file": "core\\trading\\broker_router.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerRouter"
  },
  {
    "name": "get_brokers",
    "category": "quantitative",
    "formula": "list(self._brokers.values())",
    "explanation": "Get all brokers.",
    "python_code": "def get_brokers(self) -> List[BrokerBase]:\n        \"\"\"Get all brokers.\"\"\"\n        return list(self._brokers.values())",
    "source_file": "core\\trading\\broker_router.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerRouter"
  },
  {
    "name": "get_active_brokers",
    "category": "quantitative",
    "formula": "sorted(active, key=lambda b: b.config.priority)",
    "explanation": "Get connected brokers sorted by priority.",
    "python_code": "def get_active_brokers(self) -> List[BrokerBase]:\n        \"\"\"Get connected brokers sorted by priority.\"\"\"\n        active = [\n            b for b in self._brokers.values()\n            if b.is_connected() and b.config.enabled\n        ]\n        return sorted(active, key=lambda b: b.config.priority)",
    "source_file": "core\\trading\\broker_router.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerRouter"
  },
  {
    "name": "connect_all",
    "category": "quantitative",
    "formula": "results",
    "explanation": "Connect to all brokers.",
    "python_code": "def connect_all(self) -> Dict[BrokerType, bool]:\n        \"\"\"Connect to all brokers.\"\"\"\n        results = {}\n        for broker_type, broker in self._brokers.items():\n            try:\n                success = broker.connect()\n                results[broker_type] = success\n                self._status[broker_type].connected = success\n                if success:\n                    logger.info(f\"Connected to {broker_type.value}\")\n                else:\n                    logger.warning(f\"Failed to connect to {broker_type.value}\")\n            except Exception as e:\n                results[broker_type] = False\n                self._status[broker_type].connected = False\n                self._status[broker_type].last_error = str(e)\n                logger.error(f\"Error connecting to {broker_type.value}: {e}\")\n\n        return results",
    "source_file": "core\\trading\\broker_router.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerRouter"
  },
  {
    "name": "disconnect_all",
    "category": "quantitative",
    "formula": "",
    "explanation": "Disconnect from all brokers.",
    "python_code": "def disconnect_all(self) -> None:\n        \"\"\"Disconnect from all brokers.\"\"\"\n        for broker_type, broker in self._brokers.items():\n            try:\n                broker.disconnect()\n                self._status[broker_type].connected = False\n            except Exception as e:\n                logger.error(f\"Error disconnecting from {broker_type.value}: {e}\")",
    "source_file": "core\\trading\\broker_router.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerRouter"
  },
  {
    "name": "health_check",
    "category": "quantitative",
    "formula": "dict(self._status)",
    "explanation": "Check health of all brokers.",
    "python_code": "def health_check(self) -> Dict[BrokerType, BrokerStatus]:\n        \"\"\"Check health of all brokers.\"\"\"\n        for broker_type, broker in self._brokers.items():\n            status = self._status[broker_type]\n            try:\n                start = time.time()\n                success = broker.ping()\n                latency = (time.time() - start) * 1000\n\n                status.connected = success\n                status.last_ping = datetime.now()\n                status.latency_ms = latency\n\n                if not success:\n                    status.error_count += 1\n\n            except Exception as e:\n                status.connected = False\n                status.error_count += 1\n                status.last_error = str(e)\n\n        return dict(self._status)",
    "source_file": "core\\trading\\broker_router.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerRouter"
  },
  {
    "name": "route_order",
    "category": "quantitative",
    "formula": "order | error_order",
    "explanation": "Route order to optimal broker.\n\nArgs:\n    symbol: Trading symbol\n    side: Order side\n    quantity: Order quantity\n    order_type: Order type\n    limit_price: Limit price\n    stop_price: Stop price\n    strategy: Override routing strategy\n    preferred_broker: Force specific broker\n    **kwargs: Additional order parameters\n\nReturns:\n    Executed order",
    "python_code": "def route_order(\n        self,\n        symbol: str,\n        side: OrderSide,\n        quantity: float,\n        order_type: OrderType = OrderType.MARKET,\n        limit_price: Optional[float] = None,\n        stop_price: Optional[float] = None,\n        strategy: Optional[RoutingStrategy] = None,\n        preferred_broker: Optional[BrokerType] = None,\n        **kwargs\n    ) -> BrokerOrder:\n        \"\"\"\n        Route order to optimal broker.\n\n        Args:\n            symbol: Trading symbol\n            side: Order side\n            quantity: Order quantity\n            order_type: Order type\n            limit_price: Limit price\n            stop_price: Stop price\n            strategy: Override routing strategy\n            preferred_broker: Force specific broker\n            **kwargs: Additional order parameters\n\n        Returns:\n            Executed order\n        \"\"\"\n        routing_strategy = strategy or self.strategy\n\n        # Get routing decision\n        decision = self._get_routing_decision(\n            symbol=symbol,\n            side=side,\n            quantity=quantity,\n            strategy=routing_strategy,\n            preferred_broker=preferred_broker\n        )\n\n        if not decision.broker:\n            raise RuntimeError(f\"No available broker for {symbol}\")\n\n        logger.info(\n            f\"Routing {side.value} {quantity} {symbol} to {decision.broker.config.broker_type.value} \"\n            f\"({decision.reason})\"\n        )\n\n        # Execute with retries\n        last_error = None\n        brokers_to_try = [decision.broker] + decision.alternatives\n\n        for broker in brokers_to_try[:self.max_retries]:\n            try:\n                order = broker.submit_order(\n                    symbol=symbol,\n                    side=side,\n                    quantity=quantity,\n                    order_type=order_type,\n                    limit_price=limit_price,\n                    stop_price=stop_price,\n                    **kwargs\n                )\n\n                # Track order\n                self._order_history[broker.config.broker_type].append(order)\n\n                if order.status not in (OrderStatus.ERROR, OrderStatus.REJECTED):\n                    return order\n\n                last_error = order.error_message\n\n            except Exception as e:\n                last_error = str(e)\n                logger.warning(f\"Order failed on {broker.config.broker_type.value}: {e}\")\n                continue\n\n        # All brokers failed\n        error_order = BrokerOrder(\n            internal_id=f\"FAILED_{datetime.now().timestamp()}\",\n            symbol=symbol,\n            side=side,\n            quantity=quantity,\n            status=OrderStatus.ERROR,\n            error_message=f\"All brokers failed: {last_error}\"\n        )\n        return error_order",
    "source_file": "core\\trading\\broker_router.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerRouter"
  },
  {
    "name": "_get_routing_decision",
    "category": "quantitative",
    "formula": "RoutingDecision(broker=None, reason=\"No active brokers\") | RoutingDecision(broker=None, reason=f\"No broker supports {symbol}\") | RoutingDecision(",
    "explanation": "Get routing decision based on strategy.",
    "python_code": "def _get_routing_decision(\n        self,\n        symbol: str,\n        side: OrderSide,\n        quantity: float,\n        strategy: RoutingStrategy,\n        preferred_broker: Optional[BrokerType] = None\n    ) -> RoutingDecision:\n        \"\"\"Get routing decision based on strategy.\"\"\"\n        active_brokers = self.get_active_brokers()\n\n        if not active_brokers:\n            return RoutingDecision(broker=None, reason=\"No active brokers\")\n\n        # Filter brokers that support the symbol\n        eligible = [\n            b for b in active_brokers\n            if symbol in b.config.symbols or not b.config.symbols\n        ]\n\n        if not eligible:\n            return RoutingDecision(broker=None, reason=f\"No broker supports {symbol}\")\n\n        # Handle preferred broker\n        if preferred_broker:\n            broker = self._brokers.get(preferred_broker)\n            if broker and broker.is_connected():\n                alternatives = [b for b in eligible if b != broker]\n                return RoutingDecision(\n                    broker=broker,\n                    reason=\"Preferred broker\",\n                    alternatives=alternatives\n                )\n\n        # Apply strategy\n        if strategy == RoutingStrategy.BEST_SPREAD:\n            return self._route_best_spread(symbol, eligible)\n\n        elif strategy == RoutingStrategy.PRIORITY:\n            return self._route_priority(eligible)\n\n        elif strategy == RoutingStrategy.ROUND_ROBIN:\n            return self._route_round_robin(eligible)\n\n        elif strategy == RoutingStrategy.FAILOVER:\n            return self._route_failover(eligible)\n\n        else:\n            # Default to priority\n            return self._route_priority(eligible)",
    "source_file": "core\\trading\\broker_router.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerRouter"
  },
  {
    "name": "_route_best_spread",
    "category": "microstructure",
    "formula": "RoutingDecision(",
    "explanation": "Route to broker with best spread.",
    "python_code": "def _route_best_spread(\n        self,\n        symbol: str,\n        brokers: List[BrokerBase]\n    ) -> RoutingDecision:\n        \"\"\"Route to broker with best spread.\"\"\"\n        spreads = []\n\n        for broker in brokers:\n            try:\n                spread = self._get_spread_cached(symbol, broker)\n                spreads.append((broker, spread))\n            except Exception:\n                continue\n\n        if not spreads:\n            return self._route_priority(brokers)\n\n        # Sort by spread (lowest first)\n        spreads.sort(key=lambda x: x[1])\n\n        best_broker, best_spread = spreads[0]\n        alternatives = [b for b, s in spreads[1:]]\n\n        return RoutingDecision(\n            broker=best_broker,\n            reason=f\"Best spread ({best_spread:.1f} pips)\",\n            spread=best_spread,\n            alternatives=alternatives\n        )",
    "source_file": "core\\trading\\broker_router.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerRouter"
  },
  {
    "name": "_route_priority",
    "category": "quantitative",
    "formula": "RoutingDecision(",
    "explanation": "Route based on broker priority.",
    "python_code": "def _route_priority(self, brokers: List[BrokerBase]) -> RoutingDecision:\n        \"\"\"Route based on broker priority.\"\"\"\n        # Already sorted by priority\n        return RoutingDecision(\n            broker=brokers[0],\n            reason=f\"Priority {brokers[0].config.priority}\",\n            alternatives=brokers[1:]\n        )",
    "source_file": "core\\trading\\broker_router.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerRouter"
  },
  {
    "name": "_route_round_robin",
    "category": "quantitative",
    "formula": "RoutingDecision(",
    "explanation": "Distribute orders evenly across brokers.",
    "python_code": "def _route_round_robin(self, brokers: List[BrokerBase]) -> RoutingDecision:\n        \"\"\"Distribute orders evenly across brokers.\"\"\"\n        with self._lock:\n            self._rr_counter += 1\n            idx = self._rr_counter % len(brokers)\n\n        broker = brokers[idx]\n        alternatives = brokers[:idx] + brokers[idx+1:]\n\n        return RoutingDecision(\n            broker=broker,\n            reason=\"Round robin\",\n            alternatives=alternatives\n        )",
    "source_file": "core\\trading\\broker_router.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerRouter"
  },
  {
    "name": "_route_failover",
    "category": "quantitative",
    "formula": "RoutingDecision(",
    "explanation": "Primary broker with failover.",
    "python_code": "def _route_failover(self, brokers: List[BrokerBase]) -> RoutingDecision:\n        \"\"\"Primary broker with failover.\"\"\"\n        return RoutingDecision(\n            broker=brokers[0],\n            reason=\"Primary (failover enabled)\",\n            alternatives=brokers[1:]\n        )",
    "source_file": "core\\trading\\broker_router.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerRouter"
  },
  {
    "name": "_get_spread_cached",
    "category": "microstructure",
    "formula": "spread",
    "explanation": "Get spread with caching.",
    "python_code": "def _get_spread_cached(\n        self,\n        symbol: str,\n        broker: BrokerBase,\n        max_age_seconds: float = 5.0\n    ) -> float:\n        \"\"\"Get spread with caching.\"\"\"\n        broker_type = broker.config.broker_type\n        cache_time = self._spread_cache_time.get(symbol)\n\n        if cache_time and (datetime.now() - cache_time).total_seconds() < max_age_seconds:\n            if broker_type in self._spread_cache[symbol]:\n                return self._spread_cache[symbol][broker_type]\n\n        # Fetch fresh spread\n        spread = broker.get_spread(symbol)\n        self._spread_cache[symbol][broker_type] = spread\n        self._spread_cache_time[symbol] = datetime.now()\n\n        return spread",
    "source_file": "core\\trading\\broker_router.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerRouter"
  },
  {
    "name": "get_all_positions",
    "category": "quantitative",
    "formula": "positions",
    "explanation": "Get positions from all brokers.",
    "python_code": "def get_all_positions(self, symbol: Optional[str] = None) -> Dict[BrokerType, List[BrokerPosition]]:\n        \"\"\"Get positions from all brokers.\"\"\"\n        positions = {}\n        for broker_type, broker in self._brokers.items():\n            if broker.is_connected():\n                try:\n                    positions[broker_type] = broker.get_positions(symbol)\n                except Exception as e:\n                    logger.error(f\"Failed to get positions from {broker_type.value}: {e}\")\n                    positions[broker_type] = []\n        return positions",
    "source_file": "core\\trading\\broker_router.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerRouter"
  },
  {
    "name": "get_all_quotes",
    "category": "quantitative",
    "formula": "quotes",
    "explanation": "Get quotes from all brokers for comparison.",
    "python_code": "def get_all_quotes(self, symbol: str) -> Dict[BrokerType, BrokerQuote]:\n        \"\"\"Get quotes from all brokers for comparison.\"\"\"\n        quotes = {}\n        for broker_type, broker in self._brokers.items():\n            if broker.is_connected():\n                try:\n                    quotes[broker_type] = broker.get_quote(symbol)\n                except Exception:\n                    pass\n        return quotes",
    "source_file": "core\\trading\\broker_router.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerRouter"
  },
  {
    "name": "get_best_quote",
    "category": "microstructure",
    "formula": "min(valid_quotes, key=lambda q: q.spread_pips)",
    "explanation": "Get best quote (lowest spread) across all brokers.",
    "python_code": "def get_best_quote(self, symbol: str) -> Optional[BrokerQuote]:\n        \"\"\"Get best quote (lowest spread) across all brokers.\"\"\"\n        quotes = self.get_all_quotes(symbol)\n        if not quotes:\n            return None\n\n        valid_quotes = [q for q in quotes.values() if q.is_valid]\n        if not valid_quotes:\n            return None\n\n        return min(valid_quotes, key=lambda q: q.spread_pips)",
    "source_file": "core\\trading\\broker_router.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerRouter"
  },
  {
    "name": "get_aggregate_balance",
    "category": "quantitative",
    "formula": "{",
    "explanation": "Get aggregate balance across all brokers.",
    "python_code": "def get_aggregate_balance(self) -> Dict[str, float]:\n        \"\"\"Get aggregate balance across all brokers.\"\"\"\n        total_balance = 0.0\n        total_equity = 0.0\n        total_margin_used = 0.0\n\n        by_broker = {}\n\n        for broker_type, broker in self._brokers.items():\n            if broker.is_connected():\n                try:\n                    account = broker.get_account()\n                    total_balance += account.balance\n                    total_equity += account.equity\n                    total_margin_used += account.margin_used\n                    by_broker[broker_type.value] = account.balance\n                except Exception:\n                    pass\n\n        return {\n            'total_balance': total_balance,\n            'total_equity': total_equity,\n            'total_margin_used': total_margin_used,\n            'by_broker': by_broker\n        }",
    "source_file": "core\\trading\\broker_router.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerRouter"
  },
  {
    "name": "get_stats",
    "category": "quantitative",
    "formula": "{",
    "explanation": "Get router statistics.",
    "python_code": "def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get router statistics.\"\"\"\n        return {\n            'brokers': len(self._brokers),\n            'connected': sum(1 for b in self._brokers.values() if b.is_connected()),\n            'strategy': self.strategy.value,\n            'status': {\n                bt.value: {\n                    'connected': s.connected,\n                    'latency_ms': s.latency_ms,\n                    'error_count': s.error_count,\n                    'orders_today': s.orders_today,\n                }\n                for bt, s in self._status.items()\n            },\n            'order_counts': {\n                bt.value: len(orders)\n                for bt, orders in self._order_history.items()\n            }\n        }",
    "source_file": "core\\trading\\broker_router.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "BrokerRouter"
  },
  {
    "name": "create_multi_broker_router",
    "category": "reinforcement_learning",
    "formula": "config = { | router = create_multi_broker_router(config) | router",
    "explanation": "Factory function to create configured multi-broker router.\n\nArgs:\n    brokers_config: Dict of broker configs keyed by broker name\n    strategy: Default routing strategy\n\nReturns:\n    Configured BrokerRouter\n\nExample:\n    config = {\n        'ib': {\n            'host': 'localhost',\n            'port': 4004,\n            'account_id': 'DUO423364',\n            'client_id': 1,\n        },\n        'oanda': {\n            'api_key': 'xxx',\n            'account_id': '101-001-xxx',\n        },\n        'forex_com': {\n            'username': 'xxx',\n            'password': 'xxx',\n            'app_key': 'xxx',\n        },\n        'tastyfx': {\n            'api_key': 'xxx',\n            'username': 'xxx',\n            'password': 'xxx',\n        }\n    }\n    router = create_multi_broker_router(config)",
    "python_code": "def create_multi_broker_router(\n    brokers_config: Dict[str, Dict[str, Any]],\n    strategy: RoutingStrategy = RoutingStrategy.BEST_SPREAD\n) -> BrokerRouter:\n    \"\"\"\n    Factory function to create configured multi-broker router.\n\n    Args:\n        brokers_config: Dict of broker configs keyed by broker name\n        strategy: Default routing strategy\n\n    Returns:\n        Configured BrokerRouter\n\n    Example:\n        config = {\n            'ib': {\n                'host': 'localhost',\n                'port': 4004,\n                'account_id': 'DUO423364',\n                'client_id': 1,\n            },\n            'oanda': {\n                'api_key': 'xxx',\n                'account_id': '101-001-xxx',\n            },\n            'forex_com': {\n                'username': 'xxx',\n                'password': 'xxx',\n                'app_key': 'xxx',\n            },\n            'tastyfx': {\n                'api_key': 'xxx',\n                'username': 'xxx',\n                'password': 'xxx',\n            }\n        }\n        router = create_multi_broker_router(config)\n    \"\"\"\n    router = BrokerRouter(strategy=strategy)\n\n    # Create IB broker\n    if 'ib' in brokers_config:\n        ib_config = brokers_config['ib']\n        broker = create_ib_broker(\n            host=ib_config.get('host', 'localhost'),\n            port=ib_config.get('port', 4004),\n            account_id=ib_config.get('account_id', 'DUO423364'),\n            client_id=ib_config.get('client_id', 1),\n            paper=ib_config.get('paper', True)\n        )\n        router.add_broker(broker)\n\n    # Create OANDA broker\n    if 'oanda' in brokers_config:\n        oanda_config = brokers_config['oanda']\n        broker = create_oanda_broker(\n            api_key=oanda_config['api_key'],\n            account_id=oanda_config['account_id'],\n            paper=oanda_config.get('paper', True)\n        )\n        router.add_broker(broker)\n\n    # Create Forex.com broker\n    if 'forex_com' in brokers_config:\n        fc_config = brokers_config['forex_com']\n        broker = create_forex_com_broker(\n            username=fc_config['username'],\n            password=fc_config['password'],\n            app_key=fc_config['app_key'],\n            paper=fc_config.get('paper', True)\n        )\n        router.add_broker(broker)\n\n    # Create tastyfx broker\n    if 'tastyfx' in brokers_config:\n        tfx_config = brokers_config['tastyfx']\n        broker = create_tastyfx_broker(\n            api_key=tfx_config['api_key'],\n            username=tfx_config['username'],\n            password=tfx_config['password'],\n            paper=tfx_config.get('paper', True)\n        )\n        router.add_broker(broker)\n\n    # Create IG broker\n    if 'ig' in brokers_config:\n        ig_config = brokers_config['ig']\n        broker = create_ig_broker(\n            api_key=ig_config['api_key'],\n            username=ig_config['username'],\n            password=ig_config['password'],\n            paper=ig_config.get('paper', True)\n        )\n        router.add_bro",
    "source_file": "core\\trading\\broker_router.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "quantitative",
    "formula": "",
    "explanation": "Initialize tastyfx broker.",
    "python_code": "def __init__(self, config: BrokerConfig):\n        \"\"\"Initialize tastyfx broker.\"\"\"\n        super().__init__(config)\n        self._session = None\n        self._base_url = self.DEMO_URL if config.paper else self.LIVE_URL\n        self._cst = None  # Client Security Token\n        self._x_security_token = None\n        self._account_id = None",
    "source_file": "core\\trading\\broker_tastyfx.py",
    "academic_reference": null,
    "class_name": "TastyFXBroker"
  },
  {
    "name": "connect",
    "category": "quantitative",
    "formula": "True | True | False",
    "explanation": "Connect to IG API.",
    "python_code": "def connect(self) -> bool:\n        \"\"\"Connect to IG API.\"\"\"\n        if self._connected:\n            return True\n\n        try:\n            import requests\n            self._session = requests.Session()\n\n            # IG requires specific headers\n            self._session.headers.update({\n                'Content-Type': 'application/json; charset=UTF-8',\n                'Accept': 'application/json; charset=UTF-8',\n                'X-IG-API-KEY': self.config.api_key,\n                'Version': '3'\n            })\n\n            # Authenticate\n            auth_response = self._session.post(\n                f\"{self._base_url}/session\",\n                json={\n                    'identifier': self.config.extra.get('username', ''),\n                    'password': self.config.api_secret,\n                },\n                timeout=self.config.connect_timeout\n            )\n\n            if auth_response.status_code == 200:\n                # Extract security tokens from headers\n                self._cst = auth_response.headers.get('CST')\n                self._x_security_token = auth_response.headers.get('X-SECURITY-TOKEN')\n\n                # Add tokens to session headers\n                self._session.headers['CST'] = self._cst\n                self._session.headers['X-SECURITY-TOKEN'] = self._x_security_token\n\n                result = auth_response.json()\n                self._account_id = result.get('currentAccountId')\n\n                self._connected = True\n                logger.info(f\"Connected to tastyfx/IG ({self._base_url})\")\n                logger.info(f\"Account: {self._account_id}\")\n                return True\n            else:\n                logger.error(f\"tastyfx auth failed: {auth_response.text}\")\n                return False\n\n        except ImportError:\n            logger.error(\"requests not installed: pip install requests\")\n            return False\n\n        except Exception as e:\n            logger.error(f\"tastyfx connection error: {e}\")\n            self._connected = False\n            return False",
    "source_file": "core\\trading\\broker_tastyfx.py",
    "academic_reference": null,
    "class_name": "TastyFXBroker"
  },
  {
    "name": "disconnect",
    "category": "quantitative",
    "formula": "",
    "explanation": "Disconnect from IG API.",
    "python_code": "def disconnect(self) -> None:\n        \"\"\"Disconnect from IG API.\"\"\"\n        if self._session and self._cst:\n            try:\n                self._session.delete(\n                    f\"{self._base_url}/session\",\n                    timeout=5.0\n                )\n            except Exception:\n                pass\n\n        if self._session:\n            self._session.close()\n            self._session = None\n\n        self._cst = None\n        self._x_security_token = None\n        self._connected = False\n        logger.info(\"Disconnected from tastyfx/IG\")",
    "source_file": "core\\trading\\broker_tastyfx.py",
    "academic_reference": null,
    "class_name": "TastyFXBroker"
  },
  {
    "name": "is_connected",
    "category": "quantitative",
    "formula": "",
    "explanation": "Check connection status.",
    "python_code": "def is_connected(self) -> bool:\n        \"\"\"Check connection status.\"\"\"\n        return self._connected and self._cst is not None",
    "source_file": "core\\trading\\broker_tastyfx.py",
    "academic_reference": null,
    "class_name": "TastyFXBroker"
  },
  {
    "name": "ping",
    "category": "quantitative",
    "formula": "False | response.status_code == 200 | False",
    "explanation": "Ping IG API.",
    "python_code": "def ping(self) -> bool:\n        \"\"\"Ping IG API.\"\"\"\n        if not self.is_connected():\n            return False\n        try:\n            response = self._session.get(\n                f\"{self._base_url}/accounts\",\n                timeout=self.config.request_timeout\n            )\n            return response.status_code == 200\n        except Exception:\n            return False",
    "source_file": "core\\trading\\broker_tastyfx.py",
    "academic_reference": null,
    "class_name": "TastyFXBroker"
  },
  {
    "name": "_get_epic",
    "category": "quantitative",
    "formula": "epic",
    "explanation": "Get IG EPIC code for symbol.",
    "python_code": "def _get_epic(self, symbol: str) -> str:\n        \"\"\"Get IG EPIC code for symbol.\"\"\"\n        epic = self.SYMBOL_TO_EPIC.get(symbol)\n        if not epic:\n            raise ValueError(f\"Unknown symbol: {symbol}\")\n        return epic",
    "source_file": "core\\trading\\broker_tastyfx.py",
    "academic_reference": null,
    "class_name": "TastyFXBroker"
  },
  {
    "name": "get_account",
    "category": "quantitative",
    "formula": "BrokerAccount(",
    "explanation": "Get account info.",
    "python_code": "def get_account(self) -> BrokerAccount:\n        \"\"\"Get account info.\"\"\"\n        if not self.is_connected():\n            raise ConnectionError(\"Not connected to tastyfx\")\n\n        response = self._session.get(\n            f\"{self._base_url}/accounts\",\n            timeout=self.config.request_timeout\n        )\n\n        if response.status_code == 200:\n            accounts = response.json().get('accounts', [])\n            account = next(\n                (a for a in accounts if a.get('accountId') == self._account_id),\n                accounts[0] if accounts else {}\n            )\n\n            balance_info = account.get('balance', {})\n\n            return BrokerAccount(\n                account_id=self._account_id,\n                broker_type=BrokerType.TASTYFX,\n                balance=float(balance_info.get('balance', 0)),\n                equity=float(balance_info.get('available', 0)),\n                margin_used=float(balance_info.get('deposit', 0)),\n                margin_available=float(balance_info.get('available', 0)),\n                unrealized_pnl=float(balance_info.get('profitLoss', 0)),\n                currency=account.get('currency', 'USD'),\n                is_active=account.get('status') == 'ENABLED',\n                updated_at=datetime.now()\n            )\n\n        raise RuntimeError(f\"Failed to get account: {response.text}\")",
    "source_file": "core\\trading\\broker_tastyfx.py",
    "academic_reference": null,
    "class_name": "TastyFXBroker"
  },
  {
    "name": "get_balance",
    "category": "quantitative",
    "formula": "account.balance",
    "explanation": "Get account balance.",
    "python_code": "def get_balance(self) -> float:\n        \"\"\"Get account balance.\"\"\"\n        account = self.get_account()\n        return account.balance",
    "source_file": "core\\trading\\broker_tastyfx.py",
    "academic_reference": null,
    "class_name": "TastyFXBroker"
  },
  {
    "name": "submit_order",
    "category": "quantitative",
    "formula": "order | order",
    "explanation": "Submit order to IG.",
    "python_code": "def submit_order(\n        self,\n        symbol: str,\n        side: OrderSide,\n        quantity: float,\n        order_type: OrderType = OrderType.MARKET,\n        limit_price: Optional[float] = None,\n        stop_price: Optional[float] = None,\n        **kwargs\n    ) -> BrokerOrder:\n        \"\"\"Submit order to IG.\"\"\"\n        if not self.is_connected():\n            raise ConnectionError(\"Not connected to tastyfx\")\n\n        order = BrokerOrder(\n            internal_id=self._generate_order_id(),\n            broker_type=BrokerType.TASTYFX,\n            symbol=symbol,\n            side=side,\n            order_type=order_type,\n            quantity=quantity,\n            limit_price=limit_price,\n            stop_price=stop_price,\n            status=OrderStatus.PENDING,\n            created_at=datetime.now()\n        )\n\n        try:\n            epic = self._get_epic(symbol)\n\n            # IG order direction\n            direction = \"BUY\" if side == OrderSide.BUY else \"SELL\"\n\n            order_request = {\n                'epic': epic,\n                'expiry': '-',  # No expiry for spot forex\n                'direction': direction,\n                'size': quantity,\n                'orderType': 'MARKET',\n                'timeInForce': 'FILL_OR_KILL',\n                'guaranteedStop': False,\n                'forceOpen': False,\n                'currencyCode': 'USD'\n            }\n\n            if order_type == OrderType.MARKET:\n                # Use positions endpoint for market orders\n                endpoint = f\"{self._base_url}/positions/otc\"\n            elif order_type == OrderType.LIMIT:\n                endpoint = f\"{self._base_url}/workingorders/otc\"\n                order_request['orderType'] = 'LIMIT'\n                order_request['level'] = limit_price\n                order_request['timeInForce'] = 'GOOD_TILL_CANCELLED'\n            elif order_type == OrderType.STOP:\n                endpoint = f\"{self._base_url}/workingorders/otc\"\n                order_request['orderType'] = 'STOP'\n                order_request['level'] = stop_price\n                order_request['timeInForce'] = 'GOOD_TILL_CANCELLED'\n            else:\n                raise ValueError(f\"Unsupported order type: {order_type}\")\n\n            response = self._session.post(\n                endpoint,\n                json=order_request,\n                timeout=self.config.request_timeout\n            )\n\n            result = response.json()\n\n            if response.status_code == 200:\n                deal_reference = result.get('dealReference')\n                order.broker_id = deal_reference\n                order.submitted_at = datetime.now()\n\n                # Check deal status\n                if order_type == OrderType.MARKET:\n                    # Confirm the deal\n                    confirm_response = self._session.get(\n                        f\"{self._base_url}/confirms/{deal_reference}\",\n                        timeout=self.config.request_timeout\n                    )\n\n                    if confirm_",
    "source_file": "core\\trading\\broker_tastyfx.py",
    "academic_reference": null,
    "class_name": "TastyFXBroker"
  },
  {
    "name": "cancel_order",
    "category": "quantitative",
    "formula": "False | False | True",
    "explanation": "Cancel an IG order.",
    "python_code": "def cancel_order(self, order_id: str) -> bool:\n        \"\"\"Cancel an IG order.\"\"\"\n        if not self.is_connected():\n            return False\n\n        order = self._orders.get(order_id)\n        if not order or order.is_complete:\n            return False\n\n        try:\n            if order.broker_id:\n                response = self._session.delete(\n                    f\"{self._base_url}/workingorders/otc/{order.broker_id}\",\n                    timeout=self.config.request_timeout\n                )\n\n                if response.status_code == 200:\n                    order.status = OrderStatus.CANCELLED\n                    order.updated_at = datetime.now()\n                    return True\n\n            return False\n\n        except Exception as e:\n            logger.error(f\"tastyfx cancel error: {e}\")\n            return False",
    "source_file": "core\\trading\\broker_tastyfx.py",
    "academic_reference": null,
    "class_name": "TastyFXBroker"
  },
  {
    "name": "get_order",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get order by ID.",
    "python_code": "def get_order(self, order_id: str) -> Optional[BrokerOrder]:\n        \"\"\"Get order by ID.\"\"\"\n        return self._orders.get(order_id)",
    "source_file": "core\\trading\\broker_tastyfx.py",
    "academic_reference": null,
    "class_name": "TastyFXBroker"
  },
  {
    "name": "get_open_orders",
    "category": "quantitative",
    "formula": "[] | orders",
    "explanation": "Get open orders.",
    "python_code": "def get_open_orders(self, symbol: Optional[str] = None) -> List[BrokerOrder]:\n        \"\"\"Get open orders.\"\"\"\n        if not self.is_connected():\n            return []\n\n        response = self._session.get(\n            f\"{self._base_url}/workingorders\",\n            timeout=self.config.request_timeout\n        )\n\n        orders = []\n        if response.status_code == 200:\n            for wo in response.json().get('workingOrders', []):\n                epic = wo.get('marketData', {}).get('epic', '')\n                sym = self.EPIC_TO_SYMBOL.get(epic, epic)\n\n                if symbol and sym != symbol:\n                    continue\n\n                broker_order = BrokerOrder(\n                    internal_id=f\"ig_{wo.get('workingOrderData', {}).get('dealId')}\",\n                    broker_id=wo.get('workingOrderData', {}).get('dealId'),\n                    broker_type=BrokerType.TASTYFX,\n                    symbol=sym,\n                    side=OrderSide.BUY if wo.get('workingOrderData', {}).get('direction') == 'BUY' else OrderSide.SELL,\n                    quantity=float(wo.get('workingOrderData', {}).get('orderSize', 0)),\n                    status=OrderStatus.SUBMITTED\n                )\n                orders.append(broker_order)\n\n        return orders",
    "source_file": "core\\trading\\broker_tastyfx.py",
    "academic_reference": null,
    "class_name": "TastyFXBroker"
  },
  {
    "name": "get_positions",
    "category": "quantitative",
    "formula": "[] | positions",
    "explanation": "Get all positions.",
    "python_code": "def get_positions(self, symbol: Optional[str] = None) -> List[BrokerPosition]:\n        \"\"\"Get all positions.\"\"\"\n        if not self.is_connected():\n            return []\n\n        response = self._session.get(\n            f\"{self._base_url}/positions\",\n            timeout=self.config.request_timeout\n        )\n\n        positions = []\n        if response.status_code == 200:\n            for p in response.json().get('positions', []):\n                pos_data = p.get('position', {})\n                market_data = p.get('market', {})\n\n                epic = market_data.get('epic', '')\n                sym = self.EPIC_TO_SYMBOL.get(epic, epic)\n\n                if symbol and sym != symbol:\n                    continue\n\n                direction = pos_data.get('direction', '').upper()\n                side = PositionSide.LONG if direction == 'BUY' else PositionSide.SHORT\n\n                bp = BrokerPosition(\n                    symbol=sym,\n                    broker_type=BrokerType.TASTYFX,\n                    account_id=self._account_id,\n                    side=side,\n                    quantity=float(pos_data.get('dealSize', 0)),\n                    avg_entry_price=float(pos_data.get('openLevel', 0)),\n                    unrealized_pnl=float(market_data.get('netChange', 0)),\n                    updated_at=datetime.now()\n                )\n                positions.append(bp)\n\n        return positions",
    "source_file": "core\\trading\\broker_tastyfx.py",
    "academic_reference": null,
    "class_name": "TastyFXBroker"
  },
  {
    "name": "close_position",
    "category": "quantitative",
    "formula": "",
    "explanation": "Close a position.",
    "python_code": "def close_position(self, symbol: str, quantity: Optional[float] = None) -> BrokerOrder:\n        \"\"\"Close a position.\"\"\"\n        positions = self.get_positions(symbol)\n        if not positions:\n            raise ValueError(f\"No position found for {symbol}\")\n\n        pos = positions[0]\n        close_qty = quantity or pos.quantity\n        close_side = OrderSide.SELL if pos.side == PositionSide.LONG else OrderSide.BUY\n\n        return self.submit_order(\n            symbol=symbol,\n            side=close_side,\n            quantity=close_qty,\n            order_type=OrderType.MARKET\n        )",
    "source_file": "core\\trading\\broker_tastyfx.py",
    "academic_reference": null,
    "class_name": "TastyFXBroker"
  },
  {
    "name": "get_quote",
    "category": "quantitative",
    "formula": "BrokerQuote( | BrokerQuote(symbol=symbol, broker_type=BrokerType.TASTYFX)",
    "explanation": "Get current quote.",
    "python_code": "def get_quote(self, symbol: str) -> BrokerQuote:\n        \"\"\"Get current quote.\"\"\"\n        if not self.is_connected():\n            raise ConnectionError(\"Not connected to tastyfx\")\n\n        epic = self._get_epic(symbol)\n\n        response = self._session.get(\n            f\"{self._base_url}/markets/{epic}\",\n            timeout=self.config.request_timeout\n        )\n\n        if response.status_code == 200:\n            market = response.json()\n            snapshot = market.get('snapshot', {})\n\n            bid = float(snapshot.get('bid', 0))\n            offer = float(snapshot.get('offer', 0))\n\n            pip_value = self.PIP_VALUES.get(symbol, 0.0001)\n            spread = offer - bid\n            spread_pips = spread / pip_value if pip_value > 0 else 0\n\n            return BrokerQuote(\n                symbol=symbol,\n                broker_type=BrokerType.TASTYFX,\n                bid=bid,\n                ask=offer,\n                mid=(bid + offer) / 2,\n                spread=spread,\n                spread_pips=spread_pips,\n                timestamp=datetime.now()\n            )\n\n        return BrokerQuote(symbol=symbol, broker_type=BrokerType.TASTYFX)",
    "source_file": "core\\trading\\broker_tastyfx.py",
    "academic_reference": null,
    "class_name": "TastyFXBroker"
  },
  {
    "name": "get_spread",
    "category": "microstructure",
    "formula": "quote.spread_pips",
    "explanation": "Get current spread in pips.",
    "python_code": "def get_spread(self, symbol: str) -> float:\n        \"\"\"Get current spread in pips.\"\"\"\n        quote = self.get_quote(symbol)\n        return quote.spread_pips",
    "source_file": "core\\trading\\broker_tastyfx.py",
    "academic_reference": null,
    "class_name": "TastyFXBroker"
  },
  {
    "name": "get_symbols",
    "category": "quantitative",
    "formula": "list(self.SYMBOL_TO_EPIC.keys())",
    "explanation": "Get tradeable symbols.",
    "python_code": "def get_symbols(self) -> List[str]:\n        \"\"\"Get tradeable symbols.\"\"\"\n        return list(self.SYMBOL_TO_EPIC.keys())",
    "source_file": "core\\trading\\broker_tastyfx.py",
    "academic_reference": null,
    "class_name": "TastyFXBroker"
  },
  {
    "name": "get_pip_value",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get pip value for symbol.",
    "python_code": "def get_pip_value(self, symbol: str) -> float:\n        \"\"\"Get pip value for symbol.\"\"\"\n        return self.PIP_VALUES.get(symbol, 0.0001)",
    "source_file": "core\\trading\\broker_tastyfx.py",
    "academic_reference": null,
    "class_name": "TastyFXBroker"
  },
  {
    "name": "get_min_quantity",
    "category": "quantitative",
    "formula": "0.5",
    "explanation": "Get minimum order quantity.",
    "python_code": "def get_min_quantity(self, symbol: str) -> float:\n        \"\"\"Get minimum order quantity.\"\"\"\n        return 0.5",
    "source_file": "core\\trading\\broker_tastyfx.py",
    "academic_reference": null,
    "class_name": "TastyFXBroker"
  },
  {
    "name": "create_tastyfx_broker",
    "category": "reinforcement_learning",
    "formula": "TastyFXBroker(config)",
    "explanation": "Factory function to create tastyfx broker.\n\nArgs:\n    api_key: IG API key\n    username: tastyfx username\n    password: tastyfx password\n    paper: Whether demo account\n\nReturns:\n    Configured TastyFXBroker instance",
    "python_code": "def create_tastyfx_broker(\n    api_key: str,\n    username: str,\n    password: str,\n    paper: bool = True\n) -> TastyFXBroker:\n    \"\"\"\n    Factory function to create tastyfx broker.\n\n    Args:\n        api_key: IG API key\n        username: tastyfx username\n        password: tastyfx password\n        paper: Whether demo account\n\n    Returns:\n        Configured TastyFXBroker instance\n    \"\"\"\n    config = BrokerConfig(\n        broker_type=BrokerType.TASTYFX,\n        name=\"tastyfx\",\n        api_key=api_key,\n        api_secret=password,\n        paper=paper,\n        extra={'username': username},\n        symbols=list(TastyFXBroker.SYMBOL_TO_EPIC.keys()),\n        priority=4\n    )\n\n    return TastyFXBroker(config)",
    "source_file": "core\\trading\\broker_tastyfx.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "create_ig_broker",
    "category": "reinforcement_learning",
    "formula": "IGBroker(config)",
    "explanation": "Factory function to create IG Markets broker.\n\nArgs:\n    api_key: IG API key\n    username: IG username\n    password: IG password\n    paper: Whether demo account\n\nReturns:\n    Configured IGBroker instance",
    "python_code": "def create_ig_broker(\n    api_key: str,\n    username: str,\n    password: str,\n    paper: bool = True\n) -> IGBroker:\n    \"\"\"\n    Factory function to create IG Markets broker.\n\n    Args:\n        api_key: IG API key\n        username: IG username\n        password: IG password\n        paper: Whether demo account\n\n    Returns:\n        Configured IGBroker instance\n    \"\"\"\n    config = BrokerConfig(\n        broker_type=BrokerType.IG,\n        name=\"IG Markets\",\n        api_key=api_key,\n        api_secret=password,\n        paper=paper,\n        extra={'username': username},\n        symbols=list(TastyFXBroker.SYMBOL_TO_EPIC.keys()),\n        priority=5\n    )\n\n    return IGBroker(config)",
    "source_file": "core\\trading\\broker_tastyfx.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "side",
    "category": "quantitative",
    "formula": "\"BUY\" if self.direction > 0 else \"SELL\"",
    "explanation": "",
    "python_code": "def side(self) -> str:\n        return \"BUY\" if self.direction > 0 else \"SELL\"",
    "source_file": "core\\trading\\executor.py",
    "academic_reference": null,
    "class_name": "Order"
  },
  {
    "name": "is_complete",
    "category": "quantitative",
    "formula": "",
    "explanation": "",
    "python_code": "def is_complete(self) -> bool:\n        return self.status in (\n            OrderStatus.FILLED,\n            OrderStatus.CANCELLED,\n            OrderStatus.REJECTED,\n            OrderStatus.ERROR\n        )",
    "source_file": "core\\trading\\executor.py",
    "academic_reference": null,
    "class_name": "Order"
  },
  {
    "name": "__init__",
    "category": "quantitative",
    "formula": "",
    "explanation": "Initialize order executor.\n\nArgs:\n    host: IB Gateway host\n    port: IB Gateway port\n    client_id: Client ID for IB connection\n    max_orders_per_second: Rate limit\n    connect: Whether to connect immediately",
    "python_code": "def __init__(\n        self,\n        host: str = \"localhost\",\n        port: int = 4001,\n        client_id: int = 1,\n        max_orders_per_second: float = 10.0,\n        connect: bool = True\n    ):\n        \"\"\"\n        Initialize order executor.\n\n        Args:\n            host: IB Gateway host\n            port: IB Gateway port\n            client_id: Client ID for IB connection\n            max_orders_per_second: Rate limit\n            connect: Whether to connect immediately\n        \"\"\"\n        self.host = host\n        self.port = port\n        self.client_id = client_id\n        self.rate_limit = 1.0 / max_orders_per_second\n\n        # Connection state\n        self._ib = None\n        self._connected = False\n        self._lock = threading.RLock()\n\n        # Order management\n        self._orders: Dict[str, Order] = {}\n        self._order_queue: queue.Queue = queue.Queue()\n        self._next_order_id = 1\n        self._callbacks: Dict[str, List[Callable]] = {}\n\n        # Worker thread\n        self._running = False\n        self._worker_thread: Optional[threading.Thread] = None\n\n        if connect:\n            self.connect()",
    "source_file": "core\\trading\\executor.py",
    "academic_reference": null,
    "class_name": "OrderExecutor"
  },
  {
    "name": "connect",
    "category": "quantitative",
    "formula": "True | True | False",
    "explanation": "Connect to IB Gateway.",
    "python_code": "def connect(self) -> bool:\n        \"\"\"Connect to IB Gateway.\"\"\"\n        if self._connected:\n            return True\n\n        try:\n            from ib_insync import IB\n            self._ib = IB()\n            self._ib.connect(\n                self.host,\n                self.port,\n                clientId=self.client_id,\n                readonly=False\n            )\n            self._connected = True\n            self._start_worker()\n            logger.info(f\"Connected to IB Gateway at {self.host}:{self.port}\")\n            return True\n\n        except ImportError:\n            logger.warning(\"ib_insync not installed, running in simulation mode\")\n            self._connected = False\n            self._start_worker()\n            return False\n\n        except Exception as e:\n            logger.error(f\"Failed to connect to IB Gateway: {e}\")\n            self._connected = False\n            self._start_worker()\n            return False",
    "source_file": "core\\trading\\executor.py",
    "academic_reference": null,
    "class_name": "OrderExecutor"
  },
  {
    "name": "disconnect",
    "category": "quantitative",
    "formula": "",
    "explanation": "Disconnect from IB Gateway.",
    "python_code": "def disconnect(self):\n        \"\"\"Disconnect from IB Gateway.\"\"\"\n        self._running = False\n        if self._worker_thread:\n            self._worker_thread.join(timeout=5.0)\n\n        if self._ib and self._connected:\n            try:\n                self._ib.disconnect()\n            except Exception:\n                pass\n            self._connected = False\n            logger.info(\"Disconnected from IB Gateway\")",
    "source_file": "core\\trading\\executor.py",
    "academic_reference": null,
    "class_name": "OrderExecutor"
  },
  {
    "name": "_start_worker",
    "category": "quantitative",
    "formula": "",
    "explanation": "Start order processing worker.",
    "python_code": "def _start_worker(self):\n        \"\"\"Start order processing worker.\"\"\"\n        if self._running:\n            return\n\n        self._running = True\n        self._worker_thread = threading.Thread(\n            target=self._process_orders,\n            daemon=True,\n            name=\"OrderExecutor-Worker\"\n        )\n        self._worker_thread.start()",
    "source_file": "core\\trading\\executor.py",
    "academic_reference": null,
    "class_name": "OrderExecutor"
  },
  {
    "name": "_process_orders",
    "category": "quantitative",
    "formula": "",
    "explanation": "Worker thread to process order queue.",
    "python_code": "def _process_orders(self):\n        \"\"\"Worker thread to process order queue.\"\"\"\n        last_order_time = 0.0\n\n        while self._running:\n            try:\n                # Get order with timeout\n                try:\n                    order = self._order_queue.get(timeout=0.1)\n                except queue.Empty:\n                    continue\n\n                # Rate limiting\n                now = time.time()\n                elapsed = now - last_order_time\n                if elapsed < self.rate_limit:\n                    time.sleep(self.rate_limit - elapsed)\n\n                # Submit order\n                self._submit_order(order)\n                last_order_time = time.time()\n\n            except Exception as e:\n                logger.error(f\"Order processing error: {e}\")",
    "source_file": "core\\trading\\executor.py",
    "academic_reference": null,
    "class_name": "OrderExecutor"
  },
  {
    "name": "submit",
    "category": "quantitative",
    "formula": "order",
    "explanation": "Submit an order.\n\nArgs:\n    symbol: Trading symbol\n    direction: 1 for buy, -1 for sell\n    quantity: Order quantity\n    order_type: MKT, LMT, or STP\n    limit_price: Limit price (for LMT orders)\n    callback: Function called on order update\n\nReturns:\n    Order object",
    "python_code": "def submit(\n        self,\n        symbol: str,\n        direction: int,\n        quantity: float,\n        order_type: str = \"MKT\",\n        limit_price: Optional[float] = None,\n        callback: Optional[Callable[[Order], None]] = None\n    ) -> Order:\n        \"\"\"\n        Submit an order.\n\n        Args:\n            symbol: Trading symbol\n            direction: 1 for buy, -1 for sell\n            quantity: Order quantity\n            order_type: MKT, LMT, or STP\n            limit_price: Limit price (for LMT orders)\n            callback: Function called on order update\n\n        Returns:\n            Order object\n        \"\"\"\n        with self._lock:\n            order_id = f\"ORD-{self._next_order_id:06d}\"\n            self._next_order_id += 1\n\n        order = Order(\n            order_id=order_id,\n            symbol=symbol,\n            direction=direction,\n            quantity=abs(quantity),\n            order_type=order_type,\n            limit_price=limit_price,\n        )\n\n        with self._lock:\n            self._orders[order_id] = order\n            if callback:\n                self._callbacks[order_id] = [callback]\n\n        # Queue for processing\n        self._order_queue.put(order)\n        logger.info(f\"Queued {order.side} {order.quantity} {symbol} ({order_id})\")\n\n        return order",
    "source_file": "core\\trading\\executor.py",
    "academic_reference": null,
    "class_name": "OrderExecutor"
  },
  {
    "name": "_submit_order",
    "category": "quantitative",
    "formula": "",
    "explanation": "Submit order to IB Gateway or simulate.",
    "python_code": "def _submit_order(self, order: Order):\n        \"\"\"Submit order to IB Gateway or simulate.\"\"\"\n        try:\n            order.status = OrderStatus.SUBMITTED\n            order.updated_at = datetime.now()\n            self._notify_callbacks(order)\n\n            if self._connected and self._ib:\n                self._submit_to_ib(order)\n            else:\n                self._simulate_fill(order)\n\n        except Exception as e:\n            order.status = OrderStatus.ERROR\n            order.error_message = str(e)\n            order.updated_at = datetime.now()\n            logger.error(f\"Order {order.order_id} error: {e}\")\n            self._notify_callbacks(order)",
    "source_file": "core\\trading\\executor.py",
    "academic_reference": null,
    "class_name": "OrderExecutor"
  },
  {
    "name": "_submit_to_ib",
    "category": "quantitative",
    "formula": "",
    "explanation": "Submit order to IB Gateway.",
    "python_code": "def _submit_to_ib(self, order: Order):\n        \"\"\"Submit order to IB Gateway.\"\"\"\n        from ib_insync import Forex, MarketOrder, LimitOrder\n\n        # Create contract\n        contract = Forex(order.symbol)\n\n        # Create order\n        if order.order_type == \"MKT\":\n            ib_order = MarketOrder(\n                order.side,\n                order.quantity\n            )\n        elif order.order_type == \"LMT\":\n            ib_order = LimitOrder(\n                order.side,\n                order.quantity,\n                order.limit_price\n            )\n        else:\n            raise ValueError(f\"Unsupported order type: {order.order_type}\")\n\n        # Submit\n        trade = self._ib.placeOrder(contract, ib_order)\n        order.ib_order_id = trade.order.orderId\n\n        # Wait for fill (with timeout)\n        start = time.time()\n        while not trade.isDone() and time.time() - start < 30:\n            self._ib.sleep(0.1)\n\n        # Update order from trade\n        if trade.orderStatus.status == 'Filled':\n            order.status = OrderStatus.FILLED\n            order.filled_qty = float(trade.orderStatus.filled)\n            order.avg_fill_price = float(trade.orderStatus.avgFillPrice)\n        elif trade.orderStatus.status == 'Cancelled':\n            order.status = OrderStatus.CANCELLED\n        else:\n            order.status = OrderStatus.PARTIAL\n            order.filled_qty = float(trade.orderStatus.filled)\n\n        order.updated_at = datetime.now()\n        logger.info(\n            f\"Order {order.order_id} {order.status.value}: \"\n            f\"{order.filled_qty}@{order.avg_fill_price}\"\n        )\n        self._notify_callbacks(order)",
    "source_file": "core\\trading\\executor.py",
    "academic_reference": null,
    "class_name": "OrderExecutor"
  },
  {
    "name": "_simulate_fill",
    "category": "quantitative",
    "formula": "",
    "explanation": "Simulate order fill for testing.",
    "python_code": "def _simulate_fill(self, order: Order):\n        \"\"\"Simulate order fill for testing.\"\"\"\n        # Simulate small delay\n        time.sleep(0.05)\n\n        # Simulate fill\n        order.status = OrderStatus.FILLED\n        order.filled_qty = order.quantity\n        # Simulate price (would come from market data in reality)\n        order.avg_fill_price = order.limit_price or 1.0\n        order.updated_at = datetime.now()\n\n        logger.info(\n            f\"Simulated fill {order.order_id}: \"\n            f\"{order.side} {order.filled_qty} {order.symbol}\"\n        )\n        self._notify_callbacks(order)",
    "source_file": "core\\trading\\executor.py",
    "academic_reference": null,
    "class_name": "OrderExecutor"
  },
  {
    "name": "_notify_callbacks",
    "category": "quantitative",
    "formula": "",
    "explanation": "Notify callbacks of order update.",
    "python_code": "def _notify_callbacks(self, order: Order):\n        \"\"\"Notify callbacks of order update.\"\"\"\n        callbacks = self._callbacks.get(order.order_id, [])\n        for callback in callbacks:\n            try:\n                callback(order)\n            except Exception as e:\n                logger.error(f\"Callback error for {order.order_id}: {e}\")",
    "source_file": "core\\trading\\executor.py",
    "academic_reference": null,
    "class_name": "OrderExecutor"
  },
  {
    "name": "cancel",
    "category": "quantitative",
    "formula": "False | False | True",
    "explanation": "Cancel a pending order.",
    "python_code": "def cancel(self, order_id: str) -> bool:\n        \"\"\"Cancel a pending order.\"\"\"\n        with self._lock:\n            order = self._orders.get(order_id)\n            if not order:\n                return False\n\n            if order.is_complete:\n                return False\n\n            if self._connected and self._ib and order.ib_order_id:\n                try:\n                    self._ib.cancelOrder(order.ib_order_id)\n                except Exception as e:\n                    logger.error(f\"Cancel error: {e}\")\n\n            order.status = OrderStatus.CANCELLED\n            order.updated_at = datetime.now()\n            self._notify_callbacks(order)\n            return True",
    "source_file": "core\\trading\\executor.py",
    "academic_reference": null,
    "class_name": "OrderExecutor"
  },
  {
    "name": "get_order",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get order by ID.",
    "python_code": "def get_order(self, order_id: str) -> Optional[Order]:\n        \"\"\"Get order by ID.\"\"\"\n        return self._orders.get(order_id)",
    "source_file": "core\\trading\\executor.py",
    "academic_reference": null,
    "class_name": "OrderExecutor"
  },
  {
    "name": "get_pending_orders",
    "category": "quantitative",
    "formula": "[",
    "explanation": "Get all pending orders.",
    "python_code": "def get_pending_orders(self) -> List[Order]:\n        \"\"\"Get all pending orders.\"\"\"\n        return [\n            o for o in self._orders.values()\n            if not o.is_complete\n        ]",
    "source_file": "core\\trading\\executor.py",
    "academic_reference": null,
    "class_name": "OrderExecutor"
  },
  {
    "name": "get_orders_by_symbol",
    "category": "quantitative",
    "formula": "[",
    "explanation": "Get all orders for a symbol.",
    "python_code": "def get_orders_by_symbol(self, symbol: str) -> List[Order]:\n        \"\"\"Get all orders for a symbol.\"\"\"\n        return [\n            o for o in self._orders.values()\n            if o.symbol == symbol\n        ]",
    "source_file": "core\\trading\\executor.py",
    "academic_reference": null,
    "class_name": "OrderExecutor"
  },
  {
    "name": "is_connected",
    "category": "quantitative",
    "formula": "",
    "explanation": "Check if connected to IB Gateway.",
    "python_code": "def is_connected(self) -> bool:\n        \"\"\"Check if connected to IB Gateway.\"\"\"\n        return self._connected",
    "source_file": "core\\trading\\executor.py",
    "academic_reference": null,
    "class_name": "OrderExecutor"
  },
  {
    "name": "stats",
    "category": "quantitative",
    "formula": "{",
    "explanation": "Get executor statistics.",
    "python_code": "def stats(self) -> Dict[str, Any]:\n        \"\"\"Get executor statistics.\"\"\"\n        orders = list(self._orders.values())\n        return {\n            'connected': self._connected,\n            'total_orders': len(orders),\n            'pending': sum(1 for o in orders if o.status == OrderStatus.PENDING),\n            'submitted': sum(1 for o in orders if o.status == OrderStatus.SUBMITTED),\n            'filled': sum(1 for o in orders if o.status == OrderStatus.FILLED),\n            'cancelled': sum(1 for o in orders if o.status == OrderStatus.CANCELLED),\n            'errors': sum(1 for o in orders if o.status == OrderStatus.ERROR),\n            'queue_size': self._order_queue.qsize(),\n        }",
    "source_file": "core\\trading\\executor.py",
    "academic_reference": null,
    "class_name": "OrderExecutor"
  },
  {
    "name": "__init__",
    "category": "filtering",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, process_variance: float, measurement_variance: float):\n        self.Q = process_variance      # Process noise covariance\n        self.R = measurement_variance   # Measurement noise covariance\n\n        # State\n        self.beta = 0.0      # Hedge ratio estimate\n        self.P = 1.0",
    "source_file": "core\\trading\\pairs_trading_kalman.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "KalmanFilter1D"
  },
  {
    "name": "update",
    "category": "filtering",
    "formula": "y =  * x + v",
    "explanation": "Kalman filter update step.\n\nObservation model: y =  * x + v\n\nArgs:\n    y: Observed value of asset 1\n    x: Observed value of asset 2\n\nReturns:\n    Updated  estimate",
    "python_code": "def update(self, y: float, x: float) -> float:\n        \"\"\"\n        Kalman filter update step.\n\n        Observation model: y =  * x + v\n\n        Args:\n            y: Observed value of asset 1\n            x: Observed value of asset 2\n\n        Returns:\n            Updated  estimate\n        \"\"\"\n        # Predict step\n        beta_pred = self.beta  # _t|t-1 = _{t-1|t-1} (random walk)\n        P_pred = self.P + self.Q  # P_t|t-1 = P_{t-1|t-1} + Q\n\n        # Update step\n        # Innovation: y_t - _t|t-1 * x_t\n        innovation = y - beta_pred * x\n\n        # Innovation covariance: S = H * P * H^T + R\n        # where H = x (observation matrix)\n        S = x * P_pred * x + self.R\n\n        # Kalman gain: K = P * H^T * S^{-1}\n        K = P_pred * x / S\n\n        # Update state: _t|t = _t|t-1 + K * innovation\n        self.beta = beta_pred + K * innovation\n\n        # Update covariance: P_t|t = (I - K*H) * P_t|t-1\n        self.P = (1 - K * x) * P_pred\n\n        return self.beta",
    "source_file": "core\\trading\\pairs_trading_kalman.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "KalmanFilter1D"
  },
  {
    "name": "update",
    "category": "microstructure",
    "formula": "0, { | signal, info",
    "explanation": "Update filter and generate trading signal.\n\nArgs:\n    price1: Current price of asset 1\n    price2: Current price of asset 2\n\nReturns:\n    signal: 1 (long spread), -1 (short spread), 0 (exit/hold)\n    info: Dict with , spread, z-score, etc.",
    "python_code": "def update(\n        self,\n        price1: float,\n        price2: float\n    ) -> Tuple[int, Dict[str, float]]:\n        \"\"\"\n        Update filter and generate trading signal.\n\n        Args:\n            price1: Current price of asset 1\n            price2: Current price of asset 2\n\n        Returns:\n            signal: 1 (long spread), -1 (short spread), 0 (exit/hold)\n            info: Dict with , spread, z-score, etc.\n        \"\"\"\n        # Update Kalman filter to get current \n        beta = self.kf.update(price1, price2)\n\n        # Calculate spread\n        spread = price1 - beta * price2\n\n        # Add to history\n        self.spread_history.append(spread)\n\n        # Calculate z-score\n        if len(self.spread_history) < 20:  # Need minimum history\n            return 0, {\n                'beta': beta,\n                'spread': spread,\n                'z_score': 0.0,\n                'position': self.position\n            }\n\n        spread_mean = np.mean(self.spread_history)\n        spread_std = np.std(self.spread_history)\n\n        if spread_std < 1e-8:  # Avoid division by zero\n            z_score = 0.0\n        else:\n            z_score = (spread - spread_mean) / spread_std\n\n        # Generate trading signal\n        signal = self._generate_signal(z_score)\n\n        # Info for logging/monitoring\n        info = {\n            'beta': beta,\n            'spread': spread,\n            'spread_mean': spread_mean,\n            'spread_std': spread_std,\n            'z_score': z_score,\n            'position': self.position,\n            'signal': signal,\n            'P': self.kf.P  # Kalman filter uncertainty\n        }\n\n        return signal, info",
    "source_file": "core\\trading\\pairs_trading_kalman.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "KalmanPairsTrader"
  },
  {
    "name": "_generate_signal",
    "category": "microstructure",
    "formula": "0 | 0  # Exit long | 0  # Exit short",
    "explanation": "Generate trading signal based on z-score.\n\nArgs:\n    z_score: Current z-score of spread\n\nReturns:\n    signal: 1 (long), -1 (short), 0 (exit/hold)",
    "python_code": "def _generate_signal(self, z_score: float) -> int:\n        \"\"\"\n        Generate trading signal based on z-score.\n\n        Args:\n            z_score: Current z-score of spread\n\n        Returns:\n            signal: 1 (long), -1 (short), 0 (exit/hold)\n        \"\"\"\n        # Stop loss\n        if abs(z_score) > self.config.stop_loss_threshold:\n            if self.position != 0:\n                logger.warning(f\"Stop loss triggered at z={z_score:.2f}\")\n                self.position = 0\n                return 0\n\n        # Exit conditions\n        if self.position == 1:  # Currently long spread\n            if z_score > -self.config.exit_threshold:\n                self.position = 0\n                self.num_trades += 1\n                return 0  # Exit long\n\n        elif self.position == -1:  # Currently short spread\n            if z_score < self.config.exit_threshold:\n                self.position = 0\n                self.num_trades += 1\n                return 0  # Exit short\n\n        # Entry conditions\n        if self.position == 0:  # Currently flat\n            if z_score < -self.config.entry_threshold:\n                # Spread is below mean  Long spread\n                self.position = 1\n                return 1\n\n            elif z_score > self.config.entry_threshold:\n                # Spread is above mean  Short spread\n                self.position = -1\n                return -1\n\n        # Hold current position\n        return 0",
    "source_file": "core\\trading\\pairs_trading_kalman.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "KalmanPairsTrader"
  },
  {
    "name": "get_position_sizes",
    "category": "risk",
    "formula": "0.0, 0.0 | qty1, qty2",
    "explanation": "Calculate position sizes for current signal.\n\nArgs:\n    capital: Total capital to allocate\n\nReturns:\n    qty1: Quantity of asset 1\n    qty2: Quantity of asset 2",
    "python_code": "def get_position_sizes(self, capital: float) -> Tuple[float, float]:\n        \"\"\"\n        Calculate position sizes for current signal.\n\n        Args:\n            capital: Total capital to allocate\n\n        Returns:\n            qty1: Quantity of asset 1\n            qty2: Quantity of asset 2\n        \"\"\"\n        if self.position == 0:\n            return 0.0, 0.0\n\n        # Allocate capital based on hedge ratio\n        beta = self.kf.beta\n        allocation = capital * self.config.max_position\n\n        if self.position == 1:  # Long spread\n            # Buy asset 1, sell asset 2\n            qty1 = allocation / (1 + beta)\n            qty2 = -beta * qty1\n\n        else:  # Short spread\n            # Sell asset 1, buy asset 2\n            qty1 = -allocation / (1 + beta)\n            qty2 = beta * qty1\n\n        return qty1, qty2",
    "source_file": "core\\trading\\pairs_trading_kalman.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "KalmanPairsTrader"
  },
  {
    "name": "get_stats",
    "category": "filtering",
    "formula": "{",
    "explanation": "Get trading statistics.",
    "python_code": "def get_stats(self) -> Dict[str, float]:\n        \"\"\"Get trading statistics.\"\"\"\n        return {\n            'num_trades': self.num_trades,\n            'current_position': self.position,\n            'beta': self.kf.beta,\n            'beta_uncertainty': self.kf.P\n        }",
    "source_file": "core\\trading\\pairs_trading_kalman.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "KalmanPairsTrader"
  },
  {
    "name": "create_pairs_trader",
    "category": "filtering",
    "formula": "KalmanPairsTrader(symbol1, symbol2, config)",
    "explanation": "Create Kalman pairs trader for common forex pairs.\n\nArgs:\n    pair_name: One of FOREX_PAIRS keys\n    entry_threshold: Z-score to enter\n    exit_threshold: Z-score to exit\n\nReturns:\n    KalmanPairsTrader instance",
    "python_code": "def create_pairs_trader(\n    pair_name: str,\n    entry_threshold: float = 2.0,\n    exit_threshold: float = 0.5\n) -> KalmanPairsTrader:\n    \"\"\"\n    Create Kalman pairs trader for common forex pairs.\n\n    Args:\n        pair_name: One of FOREX_PAIRS keys\n        entry_threshold: Z-score to enter\n        exit_threshold: Z-score to exit\n\n    Returns:\n        KalmanPairsTrader instance\n    \"\"\"\n    if pair_name not in FOREX_PAIRS:\n        raise ValueError(f\"Unknown pair: {pair_name}. Choose from {list(FOREX_PAIRS.keys())}\")\n\n    symbol1, symbol2 = FOREX_PAIRS[pair_name]\n\n    config = KalmanPairsConfig(\n        entry_threshold=entry_threshold,\n        exit_threshold=exit_threshold\n    )\n\n    return KalmanPairsTrader(symbol1, symbol2, config)",
    "source_file": "core\\trading\\pairs_trading_kalman.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": null
  },
  {
    "name": "is_open",
    "category": "quantitative",
    "formula": "abs(self.quantity) > 0.001",
    "explanation": "",
    "python_code": "def is_open(self) -> bool:\n        return abs(self.quantity) > 0.001",
    "source_file": "core\\trading\\position.py",
    "academic_reference": null,
    "class_name": "Position"
  },
  {
    "name": "is_long",
    "category": "quantitative",
    "formula": "",
    "explanation": "",
    "python_code": "def is_long(self) -> bool:\n        return self.quantity > 0.001",
    "source_file": "core\\trading\\position.py",
    "academic_reference": null,
    "class_name": "Position"
  },
  {
    "name": "is_short",
    "category": "quantitative",
    "formula": "",
    "explanation": "",
    "python_code": "def is_short(self) -> bool:\n        return self.quantity < -0.001",
    "source_file": "core\\trading\\position.py",
    "academic_reference": null,
    "class_name": "Position"
  },
  {
    "name": "direction",
    "category": "quantitative",
    "formula": "1 | -1 | 0",
    "explanation": "",
    "python_code": "def direction(self) -> int:\n        if self.is_long:\n            return 1\n        elif self.is_short:\n            return -1\n        return 0",
    "source_file": "core\\trading\\position.py",
    "academic_reference": null,
    "class_name": "Position"
  },
  {
    "name": "update_pnl",
    "category": "quantitative",
    "formula": "price_diff = current_price - self.avg_price",
    "explanation": "Update unrealized P&L based on current price.",
    "python_code": "def update_pnl(self, current_price: float, pip_value: float = 10.0):\n        \"\"\"Update unrealized P&L based on current price.\"\"\"\n        if not self.is_open:\n            self.unrealized_pnl = 0.0\n            return\n\n        price_diff = current_price - self.avg_price\n        # Convert to pips (for non-JPY pairs, 1 pip = 0.0001)\n        pips = price_diff * 10000\n\n        self.unrealized_pnl = pips * pip_value * self.quantity\n        self.last_update = datetime.now()",
    "source_file": "core\\trading\\position.py",
    "academic_reference": null,
    "class_name": "Position"
  },
  {
    "name": "__init__",
    "category": "quantitative",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self):\n        self._positions: Dict[str, Position] = {}\n        self._lock = threading.RLock()\n        self._trade_history: List[Dict] = []",
    "source_file": "core\\trading\\position.py",
    "academic_reference": null,
    "class_name": "PositionManager"
  },
  {
    "name": "get_position",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get or create position for a symbol.",
    "python_code": "def get_position(self, symbol: str) -> Position:\n        \"\"\"Get or create position for a symbol.\"\"\"\n        with self._lock:\n            if symbol not in self._positions:\n                self._positions[symbol] = Position(symbol=symbol)\n            return self._positions[symbol]",
    "source_file": "core\\trading\\position.py",
    "academic_reference": null,
    "class_name": "PositionManager"
  },
  {
    "name": "update_position",
    "category": "execution",
    "formula": "positive = buy, negative = sell)",
    "explanation": "Update position after a trade.\n\nArgs:\n    symbol: Trading symbol\n    quantity: Signed quantity (positive = buy, negative = sell)\n    price: Execution price\n    is_fill: Whether this is a fill (vs theoretical update)",
    "python_code": "def update_position(\n        self,\n        symbol: str,\n        quantity: float,\n        price: float,\n        is_fill: bool = True\n    ):\n        \"\"\"\n        Update position after a trade.\n\n        Args:\n            symbol: Trading symbol\n            quantity: Signed quantity (positive = buy, negative = sell)\n            price: Execution price\n            is_fill: Whether this is a fill (vs theoretical update)\n        \"\"\"\n        with self._lock:\n            pos = self.get_position(symbol)\n\n            if not pos.is_open:\n                # Opening new position\n                pos.quantity = quantity\n                pos.avg_price = price\n                pos.entry_time = datetime.now()\n            else:\n                # Modifying existing position\n                old_qty = pos.quantity\n                new_qty = old_qty + quantity\n\n                if abs(new_qty) < 0.001:\n                    # Closing position\n                    realized = self._calculate_realized_pnl(\n                        pos, abs(quantity), price\n                    )\n                    pos.realized_pnl += realized\n                    pos.quantity = 0.0\n                    pos.avg_price = 0.0\n                    pos.unrealized_pnl = 0.0\n\n                elif (old_qty > 0 and new_qty > 0) or (old_qty < 0 and new_qty < 0):\n                    # Adding to position\n                    total_cost = (pos.avg_price * abs(old_qty) +\n                                 price * abs(quantity))\n                    pos.avg_price = total_cost / abs(new_qty)\n                    pos.quantity = new_qty\n\n                else:\n                    # Reversing position\n                    close_qty = abs(old_qty)\n                    realized = self._calculate_realized_pnl(pos, close_qty, price)\n                    pos.realized_pnl += realized\n\n                    # Open new position with remaining\n                    pos.quantity = new_qty\n                    pos.avg_price = price\n                    pos.entry_time = datetime.now()\n\n            pos.last_update = datetime.now()\n\n            if is_fill:\n                self._record_trade(symbol, quantity, price)",
    "source_file": "core\\trading\\position.py",
    "academic_reference": null,
    "class_name": "PositionManager"
  },
  {
    "name": "_calculate_realized_pnl",
    "category": "quantitative",
    "formula": "pips * 10.0 * close_qty",
    "explanation": "Calculate realized P&L for closing a position.",
    "python_code": "def _calculate_realized_pnl(\n        self,\n        pos: Position,\n        close_qty: float,\n        close_price: float\n    ) -> float:\n        \"\"\"Calculate realized P&L for closing a position.\"\"\"\n        price_diff = close_price - pos.avg_price\n        if pos.is_short:\n            price_diff = -price_diff  # Profit when price goes down for short\n\n        pips = price_diff * 10000  # Convert to pips\n        return pips * 10.0 * close_qty",
    "source_file": "core\\trading\\position.py",
    "academic_reference": null,
    "class_name": "PositionManager"
  },
  {
    "name": "_record_trade",
    "category": "quantitative",
    "formula": "",
    "explanation": "Record trade in history.",
    "python_code": "def _record_trade(self, symbol: str, quantity: float, price: float):\n        \"\"\"Record trade in history.\"\"\"\n        self._trade_history.append({\n            'symbol': symbol,\n            'quantity': quantity,\n            'price': price,\n            'timestamp': datetime.now().isoformat(),\n        })",
    "source_file": "core\\trading\\position.py",
    "academic_reference": null,
    "class_name": "PositionManager"
  },
  {
    "name": "update_prices",
    "category": "quantitative",
    "formula": "",
    "explanation": "Update unrealized P&L for all positions.",
    "python_code": "def update_prices(self, prices: Dict[str, float]):\n        \"\"\"Update unrealized P&L for all positions.\"\"\"\n        with self._lock:\n            for symbol, price in prices.items():\n                if symbol in self._positions:\n                    self._positions[symbol].update_pnl(price)",
    "source_file": "core\\trading\\position.py",
    "academic_reference": null,
    "class_name": "PositionManager"
  },
  {
    "name": "get_open_positions",
    "category": "quantitative",
    "formula": "[p for p in self._positions.values() if p.is_open]",
    "explanation": "Get all open positions.",
    "python_code": "def get_open_positions(self) -> List[Position]:\n        \"\"\"Get all open positions.\"\"\"\n        with self._lock:\n            return [p for p in self._positions.values() if p.is_open]",
    "source_file": "core\\trading\\position.py",
    "academic_reference": null,
    "class_name": "PositionManager"
  },
  {
    "name": "get_total_pnl",
    "category": "quantitative",
    "formula": "{",
    "explanation": "Get total P&L across all positions.",
    "python_code": "def get_total_pnl(self) -> Dict[str, float]:\n        \"\"\"Get total P&L across all positions.\"\"\"\n        with self._lock:\n            unrealized = sum(p.unrealized_pnl for p in self._positions.values())\n            realized = sum(p.realized_pnl for p in self._positions.values())\n            return {\n                'unrealized': unrealized,\n                'realized': realized,\n                'total': unrealized + realized,\n            }",
    "source_file": "core\\trading\\position.py",
    "academic_reference": null,
    "class_name": "PositionManager"
  },
  {
    "name": "close_all",
    "category": "quantitative",
    "formula": "",
    "explanation": "Close all positions at given prices.",
    "python_code": "def close_all(self, prices: Dict[str, float]):\n        \"\"\"Close all positions at given prices.\"\"\"\n        with self._lock:\n            for symbol, pos in self._positions.items():\n                if pos.is_open and symbol in prices:\n                    self.update_position(\n                        symbol,\n                        -pos.quantity,\n                        prices[symbol]\n                    )",
    "source_file": "core\\trading\\position.py",
    "academic_reference": null,
    "class_name": "PositionManager"
  },
  {
    "name": "summary",
    "category": "quantitative",
    "formula": "{",
    "explanation": "Get position summary.",
    "python_code": "def summary(self) -> Dict:\n        \"\"\"Get position summary.\"\"\"\n        with self._lock:\n            open_positions = self.get_open_positions()\n            pnl = self.get_total_pnl()\n\n            return {\n                'open_count': len(open_positions),\n                'positions': {\n                    p.symbol: {\n                        'quantity': p.quantity,\n                        'avg_price': p.avg_price,\n                        'unrealized_pnl': p.unrealized_pnl,\n                        'direction': 'LONG' if p.is_long else 'SHORT',\n                    }\n                    for p in open_positions\n                },\n                'pnl': pnl,\n                'trade_count': len(self._trade_history),\n            }",
    "source_file": "core\\trading\\position.py",
    "academic_reference": null,
    "class_name": "PositionManager"
  },
  {
    "name": "is_valid",
    "category": "quantitative",
    "formula": "",
    "explanation": "",
    "python_code": "def is_valid(self) -> bool:\n        return self.direction != 0 and self.confidence > 0",
    "source_file": "core\\trading\\signal.py",
    "academic_reference": null,
    "class_name": "Signal"
  },
  {
    "name": "__init__",
    "category": "quantitative",
    "formula": "",
    "explanation": "Initialize signal generator.\n\nArgs:\n    model_loader: ModelLoader instance (created if None)",
    "python_code": "def __init__(self, model_loader=None):\n        \"\"\"\n        Initialize signal generator.\n\n        Args:\n            model_loader: ModelLoader instance (created if None)\n        \"\"\"\n        if model_loader is None:\n            from core.models import ModelLoader\n            model_loader = ModelLoader()\n\n        self.model_loader = model_loader\n        self._feature_cache: Dict[str, List[str]] = {}",
    "source_file": "core\\trading\\signal.py",
    "academic_reference": null,
    "class_name": "SignalGenerator"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "Signal(",
    "explanation": "Generate trading signal for a symbol.\n\nArgs:\n    symbol: Trading symbol\n    features: Feature dictionary from feature engine\n\nReturns:\n    Signal or None if prediction fails",
    "python_code": "def predict(\n        self,\n        symbol: str,\n        features: Dict[str, float]\n    ) -> Optional[Signal]:\n        \"\"\"\n        Generate trading signal for a symbol.\n\n        Args:\n            symbol: Trading symbol\n            features: Feature dictionary from feature engine\n\n        Returns:\n            Signal or None if prediction fails\n        \"\"\"\n        # Load model\n        model_data = self.model_loader.load(symbol)\n        if model_data is None:\n            logger.debug(f\"No model for {symbol}\")\n            return None\n\n        models = model_data.get('models', {})\n        feature_names = model_data.get('feature_names', [])\n\n        if not models or not feature_names:\n            logger.warning(f\"Invalid model data for {symbol}\")\n            return None\n\n        # Prepare feature vector\n        X = self._prepare_features(features, feature_names)\n        if X is None:\n            return None\n\n        # Get predictions from each model\n        predictions = {}\n        probabilities = []\n\n        # XGBoost\n        if 'xgboost' in models and models['xgboost'] is not None:\n            try:\n                import xgboost as xgb\n                dmatrix = xgb.DMatrix(X.reshape(1, -1))\n                prob = float(models['xgboost'].predict(dmatrix)[0])\n                predictions['xgboost'] = 1 if prob > 0.5 else -1\n                probabilities.append(prob)\n            except Exception as e:\n                logger.debug(f\"XGBoost prediction error: {e}\")\n\n        # LightGBM\n        if 'lightgbm' in models and models['lightgbm'] is not None:\n            try:\n                prob = float(models['lightgbm'].predict(X.reshape(1, -1))[0])\n                predictions['lightgbm'] = 1 if prob > 0.5 else -1\n                probabilities.append(prob)\n            except Exception as e:\n                logger.debug(f\"LightGBM prediction error: {e}\")\n\n        # CatBoost\n        if 'catboost' in models and models['catboost'] is not None:\n            try:\n                prob = float(models['catboost'].predict_proba(X.reshape(1, -1))[0, 1])\n                predictions['catboost'] = 1 if prob > 0.5 else -1\n                probabilities.append(prob)\n            except Exception as e:\n                logger.debug(f\"CatBoost prediction error: {e}\")\n\n        if not predictions:\n            logger.warning(f\"No valid predictions for {symbol}\")\n            return None\n\n        # Ensemble voting\n        votes = list(predictions.values())\n        avg_prob = np.mean(probabilities) if probabilities else 0.5\n\n        # Majority vote\n        long_votes = sum(1 for v in votes if v == 1)\n        short_votes = sum(1 for v in votes if v == -1)\n\n        if long_votes > short_votes:\n            direction = 1\n        elif short_votes > long_votes:\n            direction = -1\n        else:\n            direction = 0  # Tie = no signal\n\n        # Confidence from probability distance from 0.5\n        confidence = abs(avg_prob - 0.5) * 2  # Scale to 0-1\n\n        return Signal(\n          ",
    "source_file": "core\\trading\\signal.py",
    "academic_reference": null,
    "class_name": "SignalGenerator"
  },
  {
    "name": "_prepare_features",
    "category": "quantitative",
    "formula": "X",
    "explanation": "Prepare feature vector in correct order.",
    "python_code": "def _prepare_features(\n        self,\n        features: Dict[str, float],\n        feature_names: List[str]\n    ) -> Optional[np.ndarray]:\n        \"\"\"Prepare feature vector in correct order.\"\"\"\n        if not feature_names:\n            return None\n\n        X = np.zeros(len(feature_names))\n\n        for i, name in enumerate(feature_names):\n            if name in features:\n                X[i] = features[name]\n            else:\n                X[i] = 0.0  # Missing features default to 0\n\n        # Handle NaN/Inf\n        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n\n        return X",
    "source_file": "core\\trading\\signal.py",
    "academic_reference": null,
    "class_name": "SignalGenerator"
  },
  {
    "name": "get_available_symbols",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get symbols with available models.",
    "python_code": "def get_available_symbols(self) -> List[str]:\n        \"\"\"Get symbols with available models.\"\"\"\n        return self.model_loader.get_available()",
    "source_file": "core\\trading\\signal.py",
    "academic_reference": null,
    "class_name": "SignalGenerator"
  },
  {
    "name": "preload_models",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Preload models for faster prediction.",
    "python_code": "def preload_models(self, symbols: List[str]):\n        \"\"\"Preload models for faster prediction.\"\"\"\n        self.model_loader.preload(symbols)",
    "source_file": "core\\trading\\signal.py",
    "academic_reference": null,
    "class_name": "SignalGenerator"
  },
  {
    "name": "__init__",
    "category": "statistical",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self,\n                 A_0: float = 0.593,      # Initial accuracy at 1-tick\n                 A_base: float = 0.50,     # Asymptotic baseline\n                 lambda_base: float = 0.213):  # Fitted decay rate\n        self.A_0 = A_0\n        self.A_base = A_base\n        self.lambda_base = lambda_base\n\n        # Regime multipliers for decay rate\n        # Low vol = faster decay (mean-reverting)\n        # High vol = slower decay (trending)\n        self.regime_multipliers = {\n            0: 1.5,   # Low volatility - faster decay\n            1: 1.0,   # Normal volatility - baseline\n            2: 0.7,   # High volatility - slower decay\n        }",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "SignalDecayModel"
  },
  {
    "name": "accuracy",
    "category": "statistical",
    "formula": "",
    "explanation": "Calculate expected accuracy at given horizon and regime.\n\nArgs:\n    horizon: Prediction horizon in ticks\n    regime: Market regime (0=low vol, 1=normal, 2=high vol)\n\nReturns:\n    Expected accuracy (0.5 to A_0)",
    "python_code": "def accuracy(self, horizon: int, regime: int = 1) -> float:\n        \"\"\"\n        Calculate expected accuracy at given horizon and regime.\n\n        Args:\n            horizon: Prediction horizon in ticks\n            regime: Market regime (0=low vol, 1=normal, 2=high vol)\n\n        Returns:\n            Expected accuracy (0.5 to A_0)\n        \"\"\"\n        lam = self.lambda_base * self.regime_multipliers.get(regime, 1.0)\n        return self.A_base + (self.A_0 - self.A_base) * np.exp(-lam * horizon)",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "SignalDecayModel"
  },
  {
    "name": "half_life",
    "category": "statistical",
    "formula": "life = ln(2) / lambda | np.log(2) / lam",
    "explanation": "Calculate signal half-life in ticks.\n\nHalf-life = ln(2) / lambda",
    "python_code": "def half_life(self, regime: int = 1) -> float:\n        \"\"\"\n        Calculate signal half-life in ticks.\n\n        Half-life = ln(2) / lambda\n        \"\"\"\n        lam = self.lambda_base * self.regime_multipliers.get(regime, 1.0)\n        return np.log(2) / lam",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "SignalDecayModel"
  },
  {
    "name": "optimal_horizon",
    "category": "statistical",
    "formula": "max(1, h - 1) | 100",
    "explanation": "Find longest horizon where A(h) > min_accuracy.\n\nArgs:\n    regime: Market regime\n    min_accuracy: Minimum required accuracy\n\nReturns:\n    Optimal horizon in ticks (1 minimum)",
    "python_code": "def optimal_horizon(self, regime: int, min_accuracy: float) -> int:\n        \"\"\"\n        Find longest horizon where A(h) > min_accuracy.\n\n        Args:\n            regime: Market regime\n            min_accuracy: Minimum required accuracy\n\n        Returns:\n            Optimal horizon in ticks (1 minimum)\n        \"\"\"\n        for h in range(1, 100):\n            if self.accuracy(h, regime) < min_accuracy:\n                return max(1, h - 1)\n        return 100",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "SignalDecayModel"
  },
  {
    "name": "fit_from_data",
    "category": "statistical",
    "formula": "",
    "explanation": "Fit lambda parameter from observed accuracy at different horizons.\n\nUses least squares fitting on log-transformed decay equation.",
    "python_code": "def fit_from_data(self, accuracy_by_horizon: Dict[int, float]) -> float:\n        \"\"\"\n        Fit lambda parameter from observed accuracy at different horizons.\n\n        Uses least squares fitting on log-transformed decay equation.\n        \"\"\"\n        if len(accuracy_by_horizon) < 2:\n            return self.lambda_base\n\n        horizons = np.array(list(accuracy_by_horizon.keys()))\n        accuracies = np.array(list(accuracy_by_horizon.values()))\n\n        # Transform: ln(A - A_base) = ln(A_0 - A_base) - lambda * h\n        y = np.log(np.maximum(accuracies - self.A_base, 1e-6))\n\n        # Linear regression: y = a - lambda * h\n        n = len(horizons)\n        sum_h = np.sum(horizons)\n        sum_y = np.sum(y)\n        sum_hy = np.sum(horizons * y)\n        sum_h2 = np.sum(horizons ** 2)\n\n        # lambda = -(n * sum_hy - sum_h * sum_y) / (n * sum_h2 - sum_h^2)\n        denom = n * sum_h2 - sum_h ** 2\n        if abs(denom) < 1e-10:\n            return self.lambda_base\n\n        lambda_fitted = -(n * sum_hy - sum_h * sum_y) / denom\n\n        # Clip to reasonable range\n        self.lambda_base = np.clip(lambda_fitted, 0.05, 1.0)\n        return self.lambda_base",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "SignalDecayModel"
  },
  {
    "name": "expected_profit",
    "category": "volatility",
    "formula": "if correct (bps) | accuracy * R_win - (1 - accuracy) * R_loss - costs",
    "explanation": "Calculate expected profit per trade.\n\nArgs:\n    accuracy: Predicted win probability\n    R_win: Expected return if correct (bps)\n    R_loss: Expected loss if incorrect (bps)\n    kyle_lambda: Price impact coefficient\n    order_size: Trade size (units)\n    volatility: Current volatility\n    horizon: Holding period (ticks)\n\nReturns:\n    Expected profit in bps",
    "python_code": "def expected_profit(self,\n                       accuracy: float,\n                       R_win: float,\n                       R_loss: float,\n                       kyle_lambda: float,\n                       order_size: float,\n                       volatility: float,\n                       horizon: int) -> float:\n        \"\"\"\n        Calculate expected profit per trade.\n\n        Args:\n            accuracy: Predicted win probability\n            R_win: Expected return if correct (bps)\n            R_loss: Expected loss if incorrect (bps)\n            kyle_lambda: Price impact coefficient\n            order_size: Trade size (units)\n            volatility: Current volatility\n            horizon: Holding period (ticks)\n\n        Returns:\n            Expected profit in bps\n        \"\"\"\n        # Total transaction costs\n        costs = (self.spread +\n                 kyle_lambda * order_size +\n                 self.slippage_coef * volatility * np.sqrt(horizon))\n\n        # Expected profit = P(win) * win - P(loss) * loss - costs\n        return accuracy * R_win - (1 - accuracy) * R_loss - costs",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "ProfitabilityCalculator"
  },
  {
    "name": "min_accuracy",
    "category": "microstructure",
    "formula": "A_min = (R_loss + costs) / (R_win + R_loss) | (R_loss + costs) / (R_win + R_loss)",
    "explanation": "Formula 4: Calculate minimum accuracy required for profitability.\n\nA_min = (R_loss + costs) / (R_win + R_loss)\n\nONLY TRADE WHEN: A(h) > A_min",
    "python_code": "def min_accuracy(self,\n                    R_win: float,\n                    R_loss: float,\n                    kyle_lambda: float,\n                    order_size: float,\n                    volatility: float,\n                    horizon: int) -> float:\n        \"\"\"\n        Formula 4: Calculate minimum accuracy required for profitability.\n\n        A_min = (R_loss + costs) / (R_win + R_loss)\n\n        ONLY TRADE WHEN: A(h) > A_min\n        \"\"\"\n        costs = (self.spread +\n                 kyle_lambda * order_size +\n                 self.slippage_coef * volatility * np.sqrt(horizon))\n\n        return (R_loss + costs) / (R_win + R_loss)",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "ProfitabilityCalculator"
  },
  {
    "name": "is_profitable",
    "category": "microstructure",
    "formula": "expected profit. | profit > 0, profit",
    "explanation": "Check if trade is profitable and return expected profit.\n\nReturns:\n    (is_profitable, expected_profit)",
    "python_code": "def is_profitable(self, accuracy: float, **kwargs) -> Tuple[bool, float]:\n        \"\"\"\n        Check if trade is profitable and return expected profit.\n\n        Returns:\n            (is_profitable, expected_profit)\n        \"\"\"\n        profit = self.expected_profit(accuracy, **kwargs)\n        return profit > 0, profit",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "ProfitabilityCalculator"
  },
  {
    "name": "break_even_spread",
    "category": "microstructure",
    "formula": "max(0, edge)",
    "explanation": "Calculate maximum spread that allows profitability.\n\nUseful for determining if institutional spreads are required.",
    "python_code": "def break_even_spread(self, accuracy: float, R_win: float, R_loss: float) -> float:\n        \"\"\"\n        Calculate maximum spread that allows profitability.\n\n        Useful for determining if institutional spreads are required.\n        \"\"\"\n        edge = accuracy * R_win - (1 - accuracy) * R_loss\n        return max(0, edge)",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "ProfitabilityCalculator"
  },
  {
    "name": "update",
    "category": "quantitative",
    "formula": "",
    "explanation": "Update all parameter estimates with new data.\n\nArgs:\n    price: Current price\n    volume: Current volume\n    timestamp: Unix timestamp (for Hawkes)",
    "python_code": "def update(self,\n               price: float,\n               volume: float,\n               timestamp: float = None):\n        \"\"\"\n        Update all parameter estimates with new data.\n\n        Args:\n            price: Current price\n            volume: Current volume\n            timestamp: Unix timestamp (for Hawkes)\n        \"\"\"\n        # Store data\n        self._prices.append(price)\n        self._volumes.append(volume)\n\n        if len(self._prices) >= 2:\n            ret = np.log(self._prices[-1] / self._prices[-2])\n            self._returns.append(ret)\n\n        if timestamp is not None:\n            self._trade_times.append(timestamp)\n\n        # Update estimates if enough data\n        if len(self._prices) >= self.window_size:\n            self._update_kyle_lambda()\n            self._update_garch()\n            self._update_regime()\n\n        if timestamp is not None and len(self._trade_times) >= 5:\n            self._update_hawkes(timestamp)",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "AdaptiveParameterManager"
  },
  {
    "name": "_update_kyle_lambda",
    "category": "microstructure",
    "formula": "lambda = Cov(delta_P, Q) / Var(Q) | delta_p = np.diff(prices)",
    "explanation": "Formula 5: Kyle's lambda estimation.\n\nlambda = Cov(delta_P, Q) / Var(Q)",
    "python_code": "def _update_kyle_lambda(self):\n        \"\"\"\n        Formula 5: Kyle's lambda estimation.\n\n        lambda = Cov(delta_P, Q) / Var(Q)\n        \"\"\"\n        prices = np.array(list(self._prices))[-self.window_size:]\n        volumes = np.array(list(self._volumes))[-self.window_size:]\n\n        if len(prices) < 10 or len(volumes) < 10:\n            return\n\n        delta_p = np.diff(prices)\n        q = volumes[1:]\n\n        if len(delta_p) != len(q):\n            min_len = min(len(delta_p), len(q))\n            delta_p = delta_p[:min_len]\n            q = q[:min_len]\n\n        var_q = np.var(q)\n        if var_q > 1e-10:\n            cov_pq = np.cov(delta_p, q)[0, 1]\n            self._kyle_lambda = abs(cov_pq / var_q)\n\n        # Clip to reasonable range\n        self._kyle_lambda = np.clip(self._kyle_lambda, 1e-6, 0.1)",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "AdaptiveParameterManager"
  },
  {
    "name": "_update_garch",
    "category": "alpha_factor",
    "formula": "_t = omega + alpha * epsilon^2_{t-1} + beta * sigma^2_{t-1} | ret = self._returns[-1]",
    "explanation": "Formula 6: GARCH(1,1) volatility forecast.\n\nsigma^2_t = omega + alpha * epsilon^2_{t-1} + beta * sigma^2_{t-1}",
    "python_code": "def _update_garch(self):\n        \"\"\"\n        Formula 6: GARCH(1,1) volatility forecast.\n\n        sigma^2_t = omega + alpha * epsilon^2_{t-1} + beta * sigma^2_{t-1}\n        \"\"\"\n        if len(self._returns) < 2:\n            return\n\n        ret = self._returns[-1]\n        epsilon2 = ret ** 2\n\n        # GARCH update\n        self._sigma2 = (self._omega +\n                        self._alpha_garch * epsilon2 +\n                        self._beta_garch * self._sigma2)\n\n        # Clip to prevent explosion\n        self._sigma2 = np.clip(self._sigma2, 1e-10, 0.01)\n        self._vol_forecast = np.sqrt(self._sigma2)",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "AdaptiveParameterManager"
  },
  {
    "name": "_update_regime",
    "category": "volatility",
    "formula": "# Rolling volatility",
    "explanation": "Formula 7: Simple regime detection based on volatility.\n\nFull HMM implementation is in core/regime_features.py.\nThis is a fast approximation for real-time use.",
    "python_code": "def _update_regime(self):\n        \"\"\"\n        Formula 7: Simple regime detection based on volatility.\n\n        Full HMM implementation is in core/regime_features.py.\n        This is a fast approximation for real-time use.\n        \"\"\"\n        if len(self._returns) < 20:\n            return\n\n        # Rolling volatility\n        recent_vol = np.std(list(self._returns)[-20:])\n\n        # Classify regime\n        if recent_vol < self._vol_percentiles[0]:\n            self._regime = 0  # Low volatility\n        elif recent_vol > self._vol_percentiles[1]:\n            self._regime = 2  # High volatility\n        else:\n            self._regime = 1",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "AdaptiveParameterManager"
  },
  {
    "name": "_update_hawkes",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Formula 8: Hawkes intensity for order clustering.\n\nlambda(t) = mu + sum(alpha * exp(-beta * (t - t_i)))",
    "python_code": "def _update_hawkes(self, current_time: float):\n        \"\"\"\n        Formula 8: Hawkes intensity for order clustering.\n\n        lambda(t) = mu + sum(alpha * exp(-beta * (t - t_i)))\n        \"\"\"\n        intensity = self._hawkes_mu\n\n        for t_i in self._trade_times:\n            if t_i < current_time:\n                intensity += self._hawkes_alpha * np.exp(\n                    -self._hawkes_beta * (current_time - t_i)\n                )\n\n        self._hawkes_intensity = intensity",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "AdaptiveParameterManager"
  },
  {
    "name": "get_regime",
    "category": "regime",
    "formula": "",
    "explanation": "Get current market regime (0, 1, 2).",
    "python_code": "def get_regime(self) -> int:\n        \"\"\"Get current market regime (0, 1, 2).\"\"\"\n        return self._regime",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "AdaptiveParameterManager"
  },
  {
    "name": "get_volatility",
    "category": "volatility",
    "formula": "",
    "explanation": "Get GARCH volatility forecast.",
    "python_code": "def get_volatility(self) -> float:\n        \"\"\"Get GARCH volatility forecast.\"\"\"\n        return self._vol_forecast",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "AdaptiveParameterManager"
  },
  {
    "name": "get_kyle_lambda",
    "category": "microstructure",
    "formula": "",
    "explanation": "Get current Kyle's lambda estimate.",
    "python_code": "def get_kyle_lambda(self) -> float:\n        \"\"\"Get current Kyle's lambda estimate.\"\"\"\n        return self._kyle_lambda",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "AdaptiveParameterManager"
  },
  {
    "name": "get_hawkes_intensity",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get current Hawkes intensity.",
    "python_code": "def get_hawkes_intensity(self) -> float:\n        \"\"\"Get current Hawkes intensity.\"\"\"\n        return self._hawkes_intensity",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "AdaptiveParameterManager"
  },
  {
    "name": "set_vol_percentiles",
    "category": "volatility",
    "formula": "",
    "explanation": "Update volatility percentile thresholds for regime detection.",
    "python_code": "def set_vol_percentiles(self, low: float, high: float):\n        \"\"\"Update volatility percentile thresholds for regime detection.\"\"\"\n        self._vol_percentiles = [low, high]",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "AdaptiveParameterManager"
  },
  {
    "name": "base_kelly",
    "category": "risk",
    "formula": "b=1): f* = 2p - 1 | 2 * win_prob - 1 | (win_prob * win_loss_ratio - q) / win_loss_ratio",
    "explanation": "Formula 9: Base Kelly fraction.\n\nf* = (p * b - q) / b\n\nFor symmetric payoffs (b=1): f* = 2p - 1",
    "python_code": "def base_kelly(self, win_prob: float, win_loss_ratio: float = 1.0) -> float:\n        \"\"\"\n        Formula 9: Base Kelly fraction.\n\n        f* = (p * b - q) / b\n\n        For symmetric payoffs (b=1): f* = 2p - 1\n        \"\"\"\n        if win_loss_ratio == 1.0:\n            return 2 * win_prob - 1\n\n        q = 1 - win_prob\n        return (win_prob * win_loss_ratio - q) / win_loss_ratio",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "RegimeAwareKelly"
  },
  {
    "name": "position_size",
    "category": "risk",
    "formula": "Position = f_frac * regime_mult * confidence_mult * drawdown_factor * capital | 0  # No edge, no trade | max(0, position)",
    "explanation": "Formula 10: Full position size with all adjustments.\n\nPosition = f_frac * regime_mult * confidence_mult * drawdown_factor * capital",
    "python_code": "def position_size(self,\n                     accuracy: float,\n                     regime: int,\n                     confidence: float,\n                     capital: float,\n                     current_drawdown: float,\n                     win_loss_ratio: float = 1.0) -> float:\n        \"\"\"\n        Formula 10: Full position size with all adjustments.\n\n        Position = f_frac * regime_mult * confidence_mult * drawdown_factor * capital\n        \"\"\"\n        # Base Kelly\n        f_star = self.base_kelly(accuracy, win_loss_ratio)\n\n        if f_star <= 0:\n            return 0  # No edge, no trade\n\n        # Fractional Kelly (quarter Kelly for safety)\n        f_frac = self.kelly_frac * f_star\n\n        # Regime adjustment\n        regime_adj = self.regime_multipliers.get(regime, 1.0)\n\n        # Confidence adjustment (linear scaling, cap at 1.0)\n        conf_adj = min(1.0, confidence / 0.5)\n\n        # Drawdown control (reduce size as drawdown increases)\n        dd_factor = max(0, 1 - current_drawdown / self.max_dd)\n\n        # Final position\n        position = f_frac * regime_adj * conf_adj * dd_factor * capital\n\n        return max(0, position)",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "RegimeAwareKelly"
  },
  {
    "name": "max_position",
    "category": "risk",
    "formula": "capital * risk_per_trade",
    "explanation": "Maximum position size as fraction of capital.",
    "python_code": "def max_position(self, capital: float, risk_per_trade: float = 0.02) -> float:\n        \"\"\"Maximum position size as fraction of capital.\"\"\"\n        return capital * risk_per_trade",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "RegimeAwareKelly"
  },
  {
    "name": "update_confidence_history",
    "category": "filtering",
    "formula": "",
    "explanation": "Add confidence to history for percentile calculation.",
    "python_code": "def update_confidence_history(self, confidence: float):\n        \"\"\"Add confidence to history for percentile calculation.\"\"\"\n        self._confidence_history.append(confidence)\n\n        if len(self._confidence_history) >= 100:\n            self._conf_threshold = np.percentile(\n                list(self._confidence_history),\n                self.conf_percentile\n            )",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "UltraSelectiveFilter"
  },
  {
    "name": "should_trade",
    "category": "microstructure",
    "formula": "positive = buy pressure) | False, f\"Confidence {confidence:.3f} below threshold {self._conf_threshold:.3f}\", filters_passed | False, \"Models disagree on direction\", filters_passed",
    "explanation": "Apply all 4 filter layers.\n\nArgs:\n    predictions: Dict of model predictions {'xgboost': {'direction': 1, 'prob': 0.6}, ...}\n    regime: Current market regime (0, 1, 2)\n    ofi: Order flow imbalance (positive = buy pressure)\n    confidence: Ensemble confidence score\n\nReturns:\n    (should_trade, reason, filters_passed)",
    "python_code": "def should_trade(self,\n                    predictions: Dict[str, Dict],\n                    regime: int,\n                    ofi: float,\n                    confidence: float) -> Tuple[bool, str, List[str]]:\n        \"\"\"\n        Apply all 4 filter layers.\n\n        Args:\n            predictions: Dict of model predictions {'xgboost': {'direction': 1, 'prob': 0.6}, ...}\n            regime: Current market regime (0, 1, 2)\n            ofi: Order flow imbalance (positive = buy pressure)\n            confidence: Ensemble confidence score\n\n        Returns:\n            (should_trade, reason, filters_passed)\n        \"\"\"\n        filters_passed = []\n\n        # Layer 1: Confidence threshold (top N%)\n        if confidence < self._conf_threshold:\n            return False, f\"Confidence {confidence:.3f} below threshold {self._conf_threshold:.3f}\", filters_passed\n        filters_passed.append(\"confidence\")\n\n        # Layer 2: Unanimous agreement\n        if self.unanimous and len(predictions) > 1:\n            directions = [p.get('direction', 0) for p in predictions.values()]\n            if len(set(directions)) > 1:\n                return False, \"Models disagree on direction\", filters_passed\n        filters_passed.append(\"unanimous\")\n\n        # Layer 3: Regime filter\n        if regime not in self.favorable_regimes:\n            return False, f\"Unfavorable regime {regime}\", filters_passed\n        filters_passed.append(\"regime\")\n\n        # Layer 4: Order flow confirmation\n        if self.ofi_confirm:\n            # Get consensus direction\n            if predictions:\n                direction = list(predictions.values())[0].get('direction', 0)\n                # OFI should confirm direction\n                if direction != 0:\n                    ofi_direction = 1 if ofi > 0 else -1 if ofi < 0 else 0\n                    if ofi_direction != 0 and ofi_direction != direction:\n                        return False, \"OFI contradicts prediction\", filters_passed\n        filters_passed.append(\"ofi\")\n\n        # All filters passed - HIGH CONFIDENCE TRADE\n        return True, \"All 4 filters passed - TRADE\", filters_passed",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "UltraSelectiveFilter"
  },
  {
    "name": "theoretical_accuracy",
    "category": "filtering",
    "formula": "(p ** n_models) / (p ** n_models + q ** n_models)",
    "explanation": "Calculate theoretical accuracy when all models agree.\n\nP(correct | all_agree) = p^n / (p^n + (1-p)^n)",
    "python_code": "def theoretical_accuracy(self, base_accuracy: float, n_models: int = 3) -> float:\n        \"\"\"\n        Calculate theoretical accuracy when all models agree.\n\n        P(correct | all_agree) = p^n / (p^n + (1-p)^n)\n        \"\"\"\n        p = base_accuracy\n        q = 1 - p\n        return (p ** n_models) / (p ** n_models + q ** n_models)",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "UltraSelectiveFilter"
  },
  {
    "name": "update_state",
    "category": "quantitative",
    "formula": "",
    "explanation": "Update trading state.",
    "python_code": "def update_state(self, capital: float, drawdown: float, daily_trades: int):\n        \"\"\"Update trading state.\"\"\"\n        self._capital = capital\n        self._drawdown = drawdown\n        self._daily_trades = daily_trades",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "AdaptiveHFTEngine"
  },
  {
    "name": "process_tick",
    "category": "quantitative",
    "formula": "",
    "explanation": "Process incoming tick data.",
    "python_code": "def process_tick(self,\n                    price: float,\n                    volume: float,\n                    timestamp: float = None):\n        \"\"\"Process incoming tick data.\"\"\"\n        self.params.update(price, volume, timestamp)",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "AdaptiveHFTEngine"
  },
  {
    "name": "evaluate_trade",
    "category": "microstructure",
    "formula": "TradeDecision( | TradeDecision( | TradeDecision(",
    "explanation": "Master decision function.\n\nArgs:\n    features: Feature array for models\n    model_predictions: Dict of predictions from each model\n        {'xgboost': {'direction': 1, 'prob': 0.62, 'confidence': 0.24}, ...}\n    ofi: Order flow imbalance\n\nReturns:\n    TradeDecision with all details",
    "python_code": "def evaluate_trade(self,\n                      features: np.ndarray,\n                      model_predictions: Dict[str, Dict],\n                      ofi: float = 0.0) -> TradeDecision:\n        \"\"\"\n        Master decision function.\n\n        Args:\n            features: Feature array for models\n            model_predictions: Dict of predictions from each model\n                {'xgboost': {'direction': 1, 'prob': 0.62, 'confidence': 0.24}, ...}\n            ofi: Order flow imbalance\n\n        Returns:\n            TradeDecision with all details\n        \"\"\"\n        # Check daily trade limit\n        if self._daily_trades >= self._max_daily_trades:\n            return TradeDecision(\n                should_trade=False,\n                direction=0,\n                size=0,\n                horizon=0,\n                expected_profit=0,\n                accuracy=0,\n                regime=self.params.get_regime(),\n                confidence=0,\n                reason=\"Daily trade limit reached\",\n                filters_passed=[]\n            )\n\n        # Get current parameters\n        regime = self.params.get_regime()\n        volatility = self.params.get_volatility()\n        kyle_lambda = self.params.get_kyle_lambda()\n\n        # Calculate ensemble confidence\n        if model_predictions:\n            probs = [p.get('prob', 0.5) for p in model_predictions.values()]\n            confidence = np.mean([abs(p - 0.5) * 2 for p in probs])\n            ensemble_prob = np.mean(probs)\n            ensemble_direction = 1 if ensemble_prob > 0.5 else -1\n        else:\n            confidence = 0\n            ensemble_prob = 0.5\n            ensemble_direction = 0\n\n        # Update confidence history\n        self.filter.update_confidence_history(confidence)\n\n        # Apply 4-layer filter\n        should_pass, reason, filters_passed = self.filter.should_trade(\n            model_predictions, regime, ofi, confidence\n        )\n\n        if not should_pass:\n            return TradeDecision(\n                should_trade=False,\n                direction=0,\n                size=0,\n                horizon=0,\n                expected_profit=0,\n                accuracy=self.decay.accuracy(1, regime),\n                regime=regime,\n                confidence=confidence,\n                reason=reason,\n                filters_passed=filters_passed\n            )\n\n        # Calculate boosted accuracy (all models agree)\n        boosted_accuracy = self.filter.theoretical_accuracy(\n            self.base_accuracy,\n            n_models=len(model_predictions)\n        )\n\n        # Find optimal horizon\n        R_win = R_loss = volatility * 10000  # Symmetric assumption in bps\n\n        best_horizon = 1\n        best_profit = -float('inf')\n\n        for horizon in [1, 3, 5, 10]:\n            # Use boosted accuracy (from filter agreement)\n            accuracy = min(boosted_accuracy, self.decay.accuracy(horizon, regime))\n\n            min_acc = self.profit_calc.min_accuracy(\n                R_win, R_loss, kyle_lambda, 10",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "AdaptiveHFTEngine"
  },
  {
    "name": "get_statistics",
    "category": "quantitative",
    "formula": "{",
    "explanation": "Get current engine statistics.",
    "python_code": "def get_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get current engine statistics.\"\"\"\n        return {\n            'regime': self.params.get_regime(),\n            'volatility': self.params.get_volatility(),\n            'kyle_lambda': self.params.get_kyle_lambda(),\n            'hawkes_intensity': self.params.get_hawkes_intensity(),\n            'decay_half_life': self.decay.half_life(self.params.get_regime()),\n            'accuracy_1_tick': self.decay.accuracy(1, self.params.get_regime()),\n            'accuracy_5_tick': self.decay.accuracy(5, self.params.get_regime()),\n            'min_accuracy_required': self.profit_calc.min_accuracy(\n                2.8, 2.8, self.params.get_kyle_lambda(), 1000,\n                self.params.get_volatility(), 5\n            ),\n            'filter_threshold': self.filter._conf_threshold,\n        }",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "AdaptiveHFTEngine"
  },
  {
    "name": "create_adaptive_engine",
    "category": "quantitative",
    "formula": "AdaptiveHFTEngine(spread_bps=spread_bps, base_accuracy=base_accuracy)",
    "explanation": "Create a configured AdaptiveHFTEngine instance.",
    "python_code": "def create_adaptive_engine(spread_bps: float = 0.5,\n                          base_accuracy: float = 0.593) -> AdaptiveHFTEngine:\n    \"\"\"Create a configured AdaptiveHFTEngine instance.\"\"\"\n    return AdaptiveHFTEngine(spread_bps=spread_bps, base_accuracy=base_accuracy)",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": null
  },
  {
    "name": "evaluate_profitability",
    "category": "microstructure",
    "formula": "{",
    "explanation": "Quick profitability check.",
    "python_code": "def evaluate_profitability(accuracy: float,\n                          spread_bps: float = 0.5,\n                          volatility: float = 0.0003) -> Dict[str, float]:\n    \"\"\"Quick profitability check.\"\"\"\n    calc = ProfitabilityCalculator(spread_bps=spread_bps)\n    R_win = R_loss = volatility * 10000\n\n    min_acc = calc.min_accuracy(R_win, R_loss, 0.01, 1000, volatility, 5)\n    profit = calc.expected_profit(accuracy, R_win, R_loss, 0.01, 1000, volatility, 5)\n\n    return {\n        'accuracy': accuracy,\n        'min_accuracy_required': min_acc,\n        'expected_profit_bps': profit,\n        'is_profitable': accuracy > min_acc,\n        'spread_bps': spread_bps,\n    }",
    "source_file": "core\\_experimental\\adaptive_hft_engine.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "volatility",
    "formula": "",
    "explanation": "Initialize EKF.\n\nArgs:\n    state_dim: State dimension [price, velocity, volatility]\n    obs_dim: Observation dimension (typically 1 for mid price)\n    process_noise: Process noise variance Q\n    measurement_noise: Measurement noise variance R\n    mean_reversion_speed: Speed of mean reversion (kappa)",
    "python_code": "def __init__(self,\n                 state_dim: int = 3,\n                 obs_dim: int = 1,\n                 process_noise: float = 1e-5,\n                 measurement_noise: float = 1e-4,\n                 mean_reversion_speed: float = 0.1):\n        \"\"\"\n        Initialize EKF.\n\n        Args:\n            state_dim: State dimension [price, velocity, volatility]\n            obs_dim: Observation dimension (typically 1 for mid price)\n            process_noise: Process noise variance Q\n            measurement_noise: Measurement noise variance R\n            mean_reversion_speed: Speed of mean reversion (kappa)\n        \"\"\"\n        self.state_dim = state_dim\n        self.obs_dim = obs_dim\n        self.kappa = mean_reversion_speed\n\n        # Initial state: [price, velocity, volatility]\n        self.x = np.zeros(state_dim)\n        self.P = np.eye(state_dim) * 0.01\n\n        # Process noise covariance\n        self.Q = np.eye(state_dim) * process_noise\n        self.Q[2, 2] = process_noise * 10  # Volatility evolves slower\n\n        # Measurement noise covariance\n        self.R = np.eye(obs_dim) * measurement_noise\n\n        # History for analysis\n        self.state_history: List[KalmanState] = []\n        self.initialized = False",
    "source_file": "core\\_experimental\\advanced_kalman_filters.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "ExtendedKalmanFilter"
  },
  {
    "name": "state_transition",
    "category": "volatility",
    "formula": "price_t = price_{t-1} + velocity_{t-1} * dt | velocity_t = -kappa * velocity_{t-1}  (mean-reverting velocity) | volatility_t = volatility_{t-1}  (random walk)",
    "explanation": "Nonlinear state transition function f(x).\n\nModel: Ornstein-Uhlenbeck with stochastic volatility\n    price_t = price_{t-1} + velocity_{t-1} * dt\n    velocity_t = -kappa * velocity_{t-1}  (mean-reverting velocity)\n    volatility_t = volatility_{t-1}  (random walk)",
    "python_code": "def state_transition(self, x: np.ndarray, dt: float = 1.0) -> np.ndarray:\n        \"\"\"\n        Nonlinear state transition function f(x).\n\n        Model: Ornstein-Uhlenbeck with stochastic volatility\n            price_t = price_{t-1} + velocity_{t-1} * dt\n            velocity_t = -kappa * velocity_{t-1}  (mean-reverting velocity)\n            volatility_t = volatility_{t-1}  (random walk)\n        \"\"\"\n        x_new = np.zeros_like(x)\n        x_new[0] = x[0] + x[1] * dt  # Price evolves with velocity\n        x_new[1] = x[1] * (1 - self.kappa * dt)  # Velocity mean-reverts to 0\n        x_new[2] = x[2]  # Volatility as random walk (updated by process noise)\n        return x_new",
    "source_file": "core\\_experimental\\advanced_kalman_filters.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "ExtendedKalmanFilter"
  },
  {
    "name": "state_jacobian",
    "category": "filtering",
    "formula": "F = df/dx. | F",
    "explanation": "Jacobian of state transition F = df/dx.",
    "python_code": "def state_jacobian(self, x: np.ndarray, dt: float = 1.0) -> np.ndarray:\n        \"\"\"\n        Jacobian of state transition F = df/dx.\n        \"\"\"\n        F = np.eye(self.state_dim)\n        F[0, 1] = dt  # dprice/dvelocity\n        F[1, 1] = 1 - self.kappa * dt  # dvelocity/dvelocity\n        return F",
    "source_file": "core\\_experimental\\advanced_kalman_filters.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "ExtendedKalmanFilter"
  },
  {
    "name": "observation_function",
    "category": "filtering",
    "formula": "np.array([x[0]])",
    "explanation": "Observation function h(x).\nWe observe price directly.",
    "python_code": "def observation_function(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Observation function h(x).\n        We observe price directly.\n        \"\"\"\n        return np.array([x[0]])",
    "source_file": "core\\_experimental\\advanced_kalman_filters.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "ExtendedKalmanFilter"
  },
  {
    "name": "observation_jacobian",
    "category": "filtering",
    "formula": "H = dh/dx. | H",
    "explanation": "Jacobian of observation H = dh/dx.",
    "python_code": "def observation_jacobian(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Jacobian of observation H = dh/dx.\n        \"\"\"\n        H = np.zeros((self.obs_dim, self.state_dim))\n        H[0, 0] = 1.0  # Observe price directly\n        return H",
    "source_file": "core\\_experimental\\advanced_kalman_filters.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "ExtendedKalmanFilter"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "KalmanState(mean=x_pred, covariance=P_pred)",
    "explanation": "Prediction step: propagate state through nonlinear dynamics.",
    "python_code": "def predict(self, dt: float = 1.0) -> KalmanState:\n        \"\"\"\n        Prediction step: propagate state through nonlinear dynamics.\n        \"\"\"\n        # Predict state\n        x_pred = self.state_transition(self.x, dt)\n\n        # Linearize and predict covariance\n        F = self.state_jacobian(self.x, dt)\n        P_pred = F @ self.P @ F.T + self.Q\n\n        return KalmanState(mean=x_pred, covariance=P_pred)",
    "source_file": "core\\_experimental\\advanced_kalman_filters.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "ExtendedKalmanFilter"
  },
  {
    "name": "update",
    "category": "filtering",
    "formula": "KalmanState(mean=self.x.copy(), covariance=self.P.copy()) | state",
    "explanation": "Update step: incorporate new observation.\n\nArgs:\n    z: Observed price (mid price)\n    dt: Time step\n\nReturns:\n    Updated state estimate",
    "python_code": "def update(self, z: float, dt: float = 1.0) -> KalmanState:\n        \"\"\"\n        Update step: incorporate new observation.\n\n        Args:\n            z: Observed price (mid price)\n            dt: Time step\n\n        Returns:\n            Updated state estimate\n        \"\"\"\n        if not self.initialized:\n            # Initialize state from first observation\n            self.x[0] = z\n            self.x[1] = 0.0\n            self.x[2] = 0.0001  # Initial volatility estimate\n            self.initialized = True\n            return KalmanState(mean=self.x.copy(), covariance=self.P.copy())\n\n        # Prediction step\n        x_pred = self.state_transition(self.x, dt)\n        F = self.state_jacobian(self.x, dt)\n        P_pred = F @ self.P @ F.T + self.Q\n\n        # Update step\n        z_pred = self.observation_function(x_pred)\n        H = self.observation_jacobian(x_pred)\n\n        # Innovation (prediction error)\n        y = np.array([z]) - z_pred\n\n        # Innovation covariance\n        S = H @ P_pred @ H.T + self.R\n        S_scalar = float(S[0, 0])\n\n        # Kalman gain\n        K = P_pred @ H.T @ np.linalg.inv(S)\n\n        # Update state\n        self.x = x_pred + K @ y\n\n        # Update covariance (Joseph form for numerical stability)\n        I_KH = np.eye(self.state_dim) - K @ H\n        self.P = I_KH @ P_pred @ I_KH.T + K @ self.R @ K.T\n\n        # Compute log-likelihood for model comparison\n        log_lik = -0.5 * (np.log(2 * np.pi * S_scalar) + y[0]**2 / S_scalar)\n\n        state = KalmanState(\n            mean=self.x.copy(),\n            covariance=self.P.copy(),\n            innovation=float(y[0]),\n            innovation_var=S_scalar,\n            kalman_gain=float(K[0, 0]),\n            log_likelihood=log_lik\n        )\n\n        self.state_history.append(state)\n        return state",
    "source_file": "core\\_experimental\\advanced_kalman_filters.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "ExtendedKalmanFilter"
  },
  {
    "name": "get_filtered_price",
    "category": "filtering",
    "formula": "float(self.x[0])",
    "explanation": "Get filtered price estimate (true value).",
    "python_code": "def get_filtered_price(self) -> float:\n        \"\"\"Get filtered price estimate (true value).\"\"\"\n        return float(self.x[0])",
    "source_file": "core\\_experimental\\advanced_kalman_filters.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "ExtendedKalmanFilter"
  },
  {
    "name": "get_velocity",
    "category": "technical",
    "formula": "float(self.x[1])",
    "explanation": "Get price velocity (momentum).",
    "python_code": "def get_velocity(self) -> float:\n        \"\"\"Get price velocity (momentum).\"\"\"\n        return float(self.x[1])",
    "source_file": "core\\_experimental\\advanced_kalman_filters.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "ExtendedKalmanFilter"
  },
  {
    "name": "get_volatility",
    "category": "volatility",
    "formula": "float(abs(self.x[2]))",
    "explanation": "Get volatility estimate.",
    "python_code": "def get_volatility(self) -> float:\n        \"\"\"Get volatility estimate.\"\"\"\n        return float(abs(self.x[2]))",
    "source_file": "core\\_experimental\\advanced_kalman_filters.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "ExtendedKalmanFilter"
  },
  {
    "name": "get_prediction_interval",
    "category": "machine_learning",
    "formula": "(self.x[0] - z_score * std, self.x[0] + z_score * std)",
    "explanation": "Get prediction interval for price.",
    "python_code": "def get_prediction_interval(self, confidence: float = 0.95) -> Tuple[float, float]:\n        \"\"\"Get prediction interval for price.\"\"\"\n        from scipy.stats import norm\n        z_score = norm.ppf((1 + confidence) / 2)\n        std = np.sqrt(self.P[0, 0])\n        return (self.x[0] - z_score * std, self.x[0] + z_score * std)",
    "source_file": "core\\_experimental\\advanced_kalman_filters.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "ExtendedKalmanFilter"
  },
  {
    "name": "smooth",
    "category": "filtering",
    "formula": "x_smooth[:, 0]",
    "explanation": "Rauch-Tung-Striebel smoother for offline analysis.\nUses forward-backward pass for optimal estimates.",
    "python_code": "def smooth(self, observations: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Rauch-Tung-Striebel smoother for offline analysis.\n        Uses forward-backward pass for optimal estimates.\n        \"\"\"\n        n = len(observations)\n\n        # Forward pass\n        x_filt = np.zeros((n, self.state_dim))\n        P_filt = np.zeros((n, self.state_dim, self.state_dim))\n        x_pred = np.zeros((n, self.state_dim))\n        P_pred = np.zeros((n, self.state_dim, self.state_dim))\n\n        # Reset filter\n        self.x = np.zeros(self.state_dim)\n        self.x[0] = observations[0]\n        self.P = np.eye(self.state_dim) * 0.01\n        self.initialized = True\n\n        for t in range(n):\n            # Store prediction\n            x_p = self.state_transition(self.x)\n            F = self.state_jacobian(self.x)\n            P_p = F @ self.P @ F.T + self.Q\n            x_pred[t] = x_p\n            P_pred[t] = P_p\n\n            # Update\n            state = self.update(observations[t])\n            x_filt[t] = state.mean\n            P_filt[t] = state.covariance\n\n        # Backward pass (RTS smoother)\n        x_smooth = np.zeros((n, self.state_dim))\n        x_smooth[-1] = x_filt[-1]\n\n        for t in range(n - 2, -1, -1):\n            F = self.state_jacobian(x_filt[t])\n            G = P_filt[t] @ F.T @ np.linalg.inv(P_pred[t + 1])\n            x_smooth[t] = x_filt[t] + G @ (x_smooth[t + 1] - x_pred[t + 1])\n\n        return x_smooth[:, 0]",
    "source_file": "core\\_experimental\\advanced_kalman_filters.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "ExtendedKalmanFilter"
  },
  {
    "name": "generate_sigma_points",
    "category": "filtering",
    "formula": "sigma_pts",
    "explanation": "Generate sigma points around state estimate.",
    "python_code": "def generate_sigma_points(self, x: np.ndarray, P: np.ndarray) -> np.ndarray:\n        \"\"\"Generate sigma points around state estimate.\"\"\"\n        n = len(x)\n        sigma_pts = np.zeros((self.n_sigma, n))\n\n        # Center point\n        sigma_pts[0] = x\n\n        # Square root of scaled covariance\n        try:\n            sqrt_P = cholesky((n + self.lambda_) * P, lower=True)\n        except np.linalg.LinAlgError:\n            # Fallback if not positive definite\n            sqrt_P = np.real(sqrtm((n + self.lambda_) * P))\n\n        # Sigma points\n        for i in range(n):\n            sigma_pts[i + 1] = x + sqrt_P[:, i]\n            sigma_pts[n + i + 1] = x - sqrt_P[:, i]\n\n        return sigma_pts",
    "source_file": "core\\_experimental\\advanced_kalman_filters.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "UnscentedKalmanFilter"
  },
  {
    "name": "state_transition",
    "category": "filtering",
    "formula": "x_new",
    "explanation": "Nonlinear state transition (same as EKF).",
    "python_code": "def state_transition(self, x: np.ndarray, dt: float = 1.0) -> np.ndarray:\n        \"\"\"Nonlinear state transition (same as EKF).\"\"\"\n        x_new = np.zeros_like(x)\n        x_new[0] = x[0] + x[1] * dt\n        x_new[1] = x[1] * (1 - self.mr * dt)\n        x_new[2] = x[2]\n        return x_new",
    "source_file": "core\\_experimental\\advanced_kalman_filters.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "UnscentedKalmanFilter"
  },
  {
    "name": "update",
    "category": "filtering",
    "formula": "state",
    "explanation": "Update with noise adaptation.",
    "python_code": "def update(self, z: float, dt: float = 1.0) -> KalmanState:\n        \"\"\"Update with noise adaptation.\"\"\"\n        # Get base filter update\n        state = self.filter.update(z, dt)\n\n        # Store innovation statistics\n        self.innovations.append(state.innovation)\n        self.innovation_vars.append(state.innovation_var)\n\n        # Keep window size\n        if len(self.innovations) > self.innovation_window:\n            self.innovations = self.innovations[-self.innovation_window:]\n            self.innovation_vars = self.innovation_vars[-self.innovation_window:]\n\n        # Adapt noise levels\n        if len(self.innovations) >= 10:\n            self._adapt_noise()\n\n        return state",
    "source_file": "core\\_experimental\\advanced_kalman_filters.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "AdaptiveKalmanFilter"
  },
  {
    "name": "_adapt_noise",
    "category": "statistical",
    "formula": "",
    "explanation": "Adapt process/measurement noise based on innovation sequence.\n\nIf normalized innovation variance > 1: underestimating uncertainty\nIf normalized innovation variance < 1: overestimating uncertainty",
    "python_code": "def _adapt_noise(self):\n        \"\"\"\n        Adapt process/measurement noise based on innovation sequence.\n\n        If normalized innovation variance > 1: underestimating uncertainty\n        If normalized innovation variance < 1: overestimating uncertainty\n        \"\"\"\n        # Compute normalized innovation squared (NIS)\n        innovations = np.array(self.innovations[-self.innovation_window:])\n        expected_vars = np.array(self.innovation_vars[-self.innovation_window:])\n\n        # Avoid division by zero\n        expected_vars = np.maximum(expected_vars, 1e-10)\n\n        # Normalized innovation squared\n        nis = innovations**2 / expected_vars\n        mean_nis = np.mean(nis)\n\n        # Adapt: NIS should be ~1 for well-tuned filter\n        if mean_nis > 1.5:\n            # Underestimating uncertainty -> increase process noise\n            self.Q_scale *= (1 + self.adaptation_rate)\n            self.filter.Q *= (1 + self.adaptation_rate)\n        elif mean_nis < 0.5:\n            # Overestimating uncertainty -> decrease process noise\n            self.Q_scale *= (1 - self.adaptation_rate)\n            self.filter.Q *= (1 - self.adaptation_rate)\n\n        # Clamp scaling factors\n        self.Q_scale = np.clip(self.Q_scale, 0.1, 10.0)",
    "source_file": "core\\_experimental\\advanced_kalman_filters.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "AdaptiveKalmanFilter"
  },
  {
    "name": "get_filtered_price",
    "category": "filtering",
    "formula": "",
    "explanation": "",
    "python_code": "def get_filtered_price(self) -> float:\n        return self.filter.get_filtered_price()",
    "source_file": "core\\_experimental\\advanced_kalman_filters.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "AdaptiveKalmanFilter"
  },
  {
    "name": "get_velocity",
    "category": "filtering",
    "formula": "",
    "explanation": "",
    "python_code": "def get_velocity(self) -> float:\n        return self.filter.get_velocity()",
    "source_file": "core\\_experimental\\advanced_kalman_filters.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "AdaptiveKalmanFilter"
  },
  {
    "name": "get_regime_indicator",
    "category": "regime",
    "formula": "0.0 | float(np.std(recent) / np.mean(np.abs(recent) + 1e-10))",
    "explanation": "Get regime indicator based on innovation sequence.\nHigh values indicate high uncertainty / regime change.",
    "python_code": "def get_regime_indicator(self) -> float:\n        \"\"\"\n        Get regime indicator based on innovation sequence.\n        High values indicate high uncertainty / regime change.\n        \"\"\"\n        if len(self.innovations) < 5:\n            return 0.0\n\n        recent = np.array(self.innovations[-10:])\n        return float(np.std(recent) / np.mean(np.abs(recent) + 1e-10))",
    "source_file": "core\\_experimental\\advanced_kalman_filters.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "AdaptiveKalmanFilter"
  },
  {
    "name": "update",
    "category": "machine_learning",
    "formula": "{",
    "explanation": "Update all filters and combine estimates.",
    "python_code": "def update(self, z: float, dt: float = 1.0) -> Dict[str, float]:\n        \"\"\"\n        Update all filters and combine estimates.\n        \"\"\"\n        estimates = []\n        velocities = []\n\n        for i, kf in enumerate(self.filters):\n            state = kf.update(z, dt)\n            estimates.append(kf.get_filtered_price())\n            velocities.append(kf.get_velocity())\n            self.log_likelihoods[i] += state.log_likelihood\n\n        # Update weights via softmax of log-likelihoods\n        max_ll = np.max(self.log_likelihoods)\n        exp_ll = np.exp(self.log_likelihoods - max_ll)\n        self.weights = exp_ll / np.sum(exp_ll)\n\n        # Weighted combination\n        weighted_price = np.sum(self.weights * estimates)\n        weighted_velocity = np.sum(self.weights * velocities)\n\n        return {\n            'filtered_price': float(weighted_price),\n            'velocity': float(weighted_velocity),\n            'price_spread': float(np.max(estimates) - np.min(estimates)),\n            'best_filter': int(np.argmax(self.weights)),\n            'filter_agreement': float(1 - np.std(estimates) / (np.mean(estimates) + 1e-10))\n        }",
    "source_file": "core\\_experimental\\advanced_kalman_filters.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "KalmanFilterEnsemble"
  },
  {
    "name": "compute_kalman_features",
    "category": "machine_learning",
    "formula": "pd.DataFrame(features, index=prices.index)",
    "explanation": "Compute Kalman filter features for HFT.\n\nArgs:\n    prices: Price series\n    filter_type: 'ekf', 'ukf', 'adaptive_ukf', or 'ensemble'\n\nReturns:\n    DataFrame with Kalman features",
    "python_code": "def compute_kalman_features(prices: pd.Series,\n                           filter_type: str = 'adaptive_ukf') -> pd.DataFrame:\n    \"\"\"\n    Compute Kalman filter features for HFT.\n\n    Args:\n        prices: Price series\n        filter_type: 'ekf', 'ukf', 'adaptive_ukf', or 'ensemble'\n\n    Returns:\n        DataFrame with Kalman features\n    \"\"\"\n    if filter_type == 'ekf':\n        kf = ExtendedKalmanFilter()\n    elif filter_type == 'ukf':\n        kf = UnscentedKalmanFilter()\n    elif filter_type == 'ensemble':\n        kf = KalmanFilterEnsemble()\n    else:\n        kf = AdaptiveKalmanFilter(base_filter='ukf')\n\n    features = []\n\n    for price in prices:\n        if filter_type == 'ensemble':\n            result = kf.update(float(price))\n            features.append({\n                'kalman_price': result['filtered_price'],\n                'kalman_velocity': result['velocity'],\n                'kalman_spread': result['price_spread'],\n                'kalman_agreement': result['filter_agreement']\n            })\n        else:\n            state = kf.update(float(price))\n\n            if hasattr(kf, 'filter'):\n                filtered = kf.filter\n            else:\n                filtered = kf\n\n            features.append({\n                'kalman_price': filtered.get_filtered_price(),\n                'kalman_velocity': filtered.get_velocity(),\n                'kalman_volatility': filtered.get_volatility(),\n                'kalman_innovation': state.innovation,\n                'kalman_gain': state.kalman_gain\n            })\n\n    return pd.DataFrame(features, index=prices.index)",
    "source_file": "core\\_experimental\\advanced_kalman_filters.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": null
  },
  {
    "name": "create_kalman_filter",
    "category": "reinforcement_learning",
    "formula": "ExtendedKalmanFilter(**kwargs) | UnscentedKalmanFilter(**kwargs) | AdaptiveKalmanFilter(base_filter='ukf', **kwargs)",
    "explanation": "Factory function to create Kalman filter.\n\nArgs:\n    filter_type: 'ekf', 'ukf', 'adaptive_ukf', 'adaptive_ekf', 'ensemble'\n    **kwargs: Additional arguments for specific filter\n\nReturns:\n    Kalman filter instance",
    "python_code": "def create_kalman_filter(filter_type: str = 'adaptive_ukf', **kwargs):\n    \"\"\"\n    Factory function to create Kalman filter.\n\n    Args:\n        filter_type: 'ekf', 'ukf', 'adaptive_ukf', 'adaptive_ekf', 'ensemble'\n        **kwargs: Additional arguments for specific filter\n\n    Returns:\n        Kalman filter instance\n    \"\"\"\n    if filter_type == 'ekf':\n        return ExtendedKalmanFilter(**kwargs)\n    elif filter_type == 'ukf':\n        return UnscentedKalmanFilter(**kwargs)\n    elif filter_type == 'adaptive_ukf':\n        return AdaptiveKalmanFilter(base_filter='ukf', **kwargs)\n    elif filter_type == 'adaptive_ekf':\n        return AdaptiveKalmanFilter(base_filter='ekf', **kwargs)\n    elif filter_type == 'ensemble':\n        return KalmanFilterEnsemble(**kwargs)\n    else:\n        raise ValueError(f\"Unknown filter type: {filter_type}\")",
    "source_file": "core\\_experimental\\advanced_kalman_filters.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": null
  },
  {
    "name": "rank",
    "category": "alpha_factor",
    "formula": "x.rank(pct=True)",
    "explanation": "Cross-sectional rank (percentile).",
    "python_code": "def rank(x: pd.Series) -> pd.Series:\n        \"\"\"Cross-sectional rank (percentile).\"\"\"\n        return x.rank(pct=True)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "delta",
    "category": "alpha_factor",
    "formula": "x.diff(d)",
    "explanation": "Difference from d periods ago.",
    "python_code": "def delta(x: pd.Series, d: int = 1) -> pd.Series:\n        \"\"\"Difference from d periods ago.\"\"\"\n        return x.diff(d)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "delay",
    "category": "alpha_factor",
    "formula": "x.shift(d)",
    "explanation": "Lag by d periods.",
    "python_code": "def delay(x: pd.Series, d: int = 1) -> pd.Series:\n        \"\"\"Lag by d periods.\"\"\"\n        return x.shift(d)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "correlation",
    "category": "alpha_factor",
    "formula": "x.rolling(d, min_periods=d//2).corr(y)",
    "explanation": "Rolling correlation.",
    "python_code": "def correlation(x: pd.Series, y: pd.Series, d: int) -> pd.Series:\n        \"\"\"Rolling correlation.\"\"\"\n        return x.rolling(d, min_periods=d//2).corr(y)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "covariance",
    "category": "alpha_factor",
    "formula": "x.rolling(d, min_periods=d//2).cov(y)",
    "explanation": "Rolling covariance.",
    "python_code": "def covariance(x: pd.Series, y: pd.Series, d: int) -> pd.Series:\n        \"\"\"Rolling covariance.\"\"\"\n        return x.rolling(d, min_periods=d//2).cov(y)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "ts_rank",
    "category": "alpha_factor",
    "formula": "x.rolling(d, min_periods=d//2).apply(",
    "explanation": "Time-series rank over d periods.",
    "python_code": "def ts_rank(x: pd.Series, d: int) -> pd.Series:\n        \"\"\"Time-series rank over d periods.\"\"\"\n        return x.rolling(d, min_periods=d//2).apply(\n            lambda arr: pd.Series(arr).rank().iloc[-1] / len(arr), raw=False\n        )",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "ts_max",
    "category": "alpha_factor",
    "formula": "x.rolling(d, min_periods=1).max()",
    "explanation": "Rolling maximum.",
    "python_code": "def ts_max(x: pd.Series, d: int) -> pd.Series:\n        \"\"\"Rolling maximum.\"\"\"\n        return x.rolling(d, min_periods=1).max()",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "ts_min",
    "category": "alpha_factor",
    "formula": "x.rolling(d, min_periods=1).min()",
    "explanation": "Rolling minimum.",
    "python_code": "def ts_min(x: pd.Series, d: int) -> pd.Series:\n        \"\"\"Rolling minimum.\"\"\"\n        return x.rolling(d, min_periods=1).min()",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "ts_argmax",
    "category": "alpha_factor",
    "formula": "x.rolling(d, min_periods=1).apply(lambda arr: d - np.argmax(arr) - 1, raw=True)",
    "explanation": "Days since maximum (HIGHDAY).",
    "python_code": "def ts_argmax(x: pd.Series, d: int) -> pd.Series:\n        \"\"\"Days since maximum (HIGHDAY).\"\"\"\n        return x.rolling(d, min_periods=1).apply(lambda arr: d - np.argmax(arr) - 1, raw=True)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "ts_argmin",
    "category": "alpha_factor",
    "formula": "x.rolling(d, min_periods=1).apply(lambda arr: d - np.argmin(arr) - 1, raw=True)",
    "explanation": "Days since minimum (LOWDAY).",
    "python_code": "def ts_argmin(x: pd.Series, d: int) -> pd.Series:\n        \"\"\"Days since minimum (LOWDAY).\"\"\"\n        return x.rolling(d, min_periods=1).apply(lambda arr: d - np.argmin(arr) - 1, raw=True)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "ts_sum",
    "category": "alpha_factor",
    "formula": "x.rolling(d, min_periods=1).sum()",
    "explanation": "Rolling sum (SUM).",
    "python_code": "def ts_sum(x: pd.Series, d: int) -> pd.Series:\n        \"\"\"Rolling sum (SUM).\"\"\"\n        return x.rolling(d, min_periods=1).sum()",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "ts_mean",
    "category": "alpha_factor",
    "formula": "x.rolling(d, min_periods=1).mean()",
    "explanation": "Rolling mean (MEAN).",
    "python_code": "def ts_mean(x: pd.Series, d: int) -> pd.Series:\n        \"\"\"Rolling mean (MEAN).\"\"\"\n        return x.rolling(d, min_periods=1).mean()",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "ts_std",
    "category": "alpha_factor",
    "formula": "x.rolling(d, min_periods=2).std()",
    "explanation": "Rolling standard deviation (STD).",
    "python_code": "def ts_std(x: pd.Series, d: int) -> pd.Series:\n        \"\"\"Rolling standard deviation (STD).\"\"\"\n        return x.rolling(d, min_periods=2).std()",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "ts_count",
    "category": "alpha_factor",
    "formula": "cond.astype(float).rolling(d, min_periods=1).sum()",
    "explanation": "Count True values in rolling window.",
    "python_code": "def ts_count(cond: pd.Series, d: int) -> pd.Series:\n        \"\"\"Count True values in rolling window.\"\"\"\n        return cond.astype(float).rolling(d, min_periods=1).sum()",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "decay_linear",
    "category": "alpha_factor",
    "formula": "x.rolling(d, min_periods=d//2).apply(",
    "explanation": "Linear decay weighted average (DECAYLINEAR).",
    "python_code": "def decay_linear(x: pd.Series, d: int) -> pd.Series:\n        \"\"\"Linear decay weighted average (DECAYLINEAR).\"\"\"\n        weights = np.arange(1, d + 1)\n        return x.rolling(d, min_periods=d//2).apply(\n            lambda arr: np.dot(arr[-len(weights):], weights[-len(arr):]) / weights[-len(arr):].sum() if len(arr) > 0 else np.nan,\n            raw=True\n        )",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "sma",
    "category": "alpha_factor",
    "formula": "x.ewm(alpha=alpha, adjust=False).mean()",
    "explanation": "SMA(A, n, m) = (A * m + DELAY(SMA, 1) * (n - m)) / n\nExponential moving average variant.",
    "python_code": "def sma(x: pd.Series, n: int, m: int) -> pd.Series:\n        \"\"\"\n        SMA(A, n, m) = (A * m + DELAY(SMA, 1) * (n - m)) / n\n        Exponential moving average variant.\n        \"\"\"\n        alpha = m / n\n        return x.ewm(alpha=alpha, adjust=False).mean()",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "wma",
    "category": "alpha_factor",
    "formula": "x.rolling(d).apply(lambda arr: np.dot(arr, weights) / weights.sum(), raw=True)",
    "explanation": "Weighted moving average.",
    "python_code": "def wma(x: pd.Series, d: int) -> pd.Series:\n        \"\"\"Weighted moving average.\"\"\"\n        weights = np.arange(1, d + 1)\n        return x.rolling(d).apply(lambda arr: np.dot(arr, weights) / weights.sum(), raw=True)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "sign",
    "category": "alpha_factor",
    "formula": "np.sign(x)",
    "explanation": "Sign function.",
    "python_code": "def sign(x: pd.Series) -> pd.Series:\n        \"\"\"Sign function.\"\"\"\n        return np.sign(x)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "log",
    "category": "alpha_factor",
    "formula": "np.log(x.replace(0, np.nan).clip(lower=1e-10))",
    "explanation": "Natural log.",
    "python_code": "def log(x: pd.Series) -> pd.Series:\n        \"\"\"Natural log.\"\"\"\n        return np.log(x.replace(0, np.nan).clip(lower=1e-10))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "abs_",
    "category": "alpha_factor",
    "formula": "x.abs()",
    "explanation": "Absolute value.",
    "python_code": "def abs_(x: pd.Series) -> pd.Series:\n        \"\"\"Absolute value.\"\"\"\n        return x.abs()",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "regbeta",
    "category": "alpha_factor",
    "formula": "np.nan | cov / var if var != 0 else np.nan | result",
    "explanation": "Rolling regression beta coefficient.",
    "python_code": "def regbeta(y: pd.Series, x: pd.Series, d: int) -> pd.Series:\n        \"\"\"Rolling regression beta coefficient.\"\"\"\n        def calc_beta(y_arr, x_arr):\n            if len(y_arr) < 2:\n                return np.nan\n            x_mean = np.mean(x_arr)\n            y_mean = np.mean(y_arr)\n            cov = np.sum((x_arr - x_mean) * (y_arr - y_mean))\n            var = np.sum((x_arr - x_mean) ** 2)\n            return cov / var if var != 0 else np.nan\n\n        # Create sequence for regression\n        result = pd.Series(index=y.index, dtype=float)\n        seq = np.arange(1, d + 1)\n        for i in range(d - 1, len(y)):\n            y_window = y.iloc[i - d + 1:i + 1].values\n            result.iloc[i] = calc_beta(y_window, seq)\n        return result",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha001",
    "category": "alpha_factor",
    "formula": "-1 * self.correlation(",
    "explanation": "(-1 * CORR(RANK(DELTA(LOG(VOLUME), 1)), RANK((CLOSE - OPEN) / OPEN), 6))",
    "python_code": "def alpha001(self, close: pd.Series, open_: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(-1 * CORR(RANK(DELTA(LOG(VOLUME), 1)), RANK((CLOSE - OPEN) / OPEN), 6))\"\"\"\n        return -1 * self.correlation(\n            self.rank(self.delta(self.log(volume + 1), 1)),\n            self.rank((close - open_) / (open_ + 1e-8)),\n            6\n        )",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha002",
    "category": "alpha_factor",
    "formula": "-1 * self.delta(inner, 1)",
    "explanation": "(-1 * DELTA((((CLOSE - LOW) - (HIGH - CLOSE)) / (HIGH - LOW)), 1))",
    "python_code": "def alpha002(self, close: pd.Series, open_: pd.Series, high: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"(-1 * DELTA((((CLOSE - LOW) - (HIGH - CLOSE)) / (HIGH - LOW)), 1))\"\"\"\n        inner = ((close - low) - (high - close)) / (high - low + 1e-8)\n        return -1 * self.delta(inner, 1)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha003",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SUM(conditional spread adjustments based on CLOSE vs DELAY)",
    "python_code": "def alpha003(self, close: pd.Series) -> pd.Series:\n        \"\"\"SUM(conditional spread adjustments based on CLOSE vs DELAY)\"\"\"\n        cond1 = close == self.delay(close, 1)\n        cond2 = close > self.delay(close, 1)\n\n        inner = np.where(cond1, 0,\n                 np.where(cond2, close - self.ts_min(low, 6), close - self.ts_max(high, 6)))\n        return self.ts_sum(pd.Series(inner, index=close.index), 6)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha004",
    "category": "alpha_factor",
    "formula": "np.where(cond1, -1, np.where(cond2, 1, np.where(cond3, 1, -1)))",
    "explanation": "Complex conditional based on 8-day vs 2-day averages and volume ratios",
    "python_code": "def alpha004(self, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"Complex conditional based on 8-day vs 2-day averages and volume ratios\"\"\"\n        cond1 = (self.ts_sum(close, 8) / 8 + self.ts_std(close, 8)) < (self.ts_sum(close, 2) / 2)\n        cond2 = (self.ts_sum(close, 2) / 2) < (self.ts_sum(close, 8) / 8 - self.ts_std(close, 8))\n        cond3 = volume / self.ts_mean(volume, 20) >= 1\n\n        return np.where(cond1, -1, np.where(cond2, 1, np.where(cond3, 1, -1)))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha005",
    "category": "alpha_factor",
    "formula": "-1 * self.ts_max(",
    "explanation": "(-1 * TSMAX(CORR(TSRANK(VOLUME, 5), TSRANK(HIGH, 5), 5), 3))",
    "python_code": "def alpha005(self, volume: pd.Series, high: pd.Series) -> pd.Series:\n        \"\"\"(-1 * TSMAX(CORR(TSRANK(VOLUME, 5), TSRANK(HIGH, 5), 5), 3))\"\"\"\n        return -1 * self.ts_max(\n            self.correlation(self.ts_rank(volume + 1, 5), self.ts_rank(high, 5), 5),\n            3\n        )",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha006",
    "category": "alpha_factor",
    "formula": "-1 * self.rank(self.sign(self.delta(open_ * 0.85 + high * 0.15, 4)))",
    "explanation": "(-1 * RANK(SIGN(DELTA((OPEN * 0.85 + HIGH * 0.15), 4))))",
    "python_code": "def alpha006(self, open_: pd.Series, high: pd.Series) -> pd.Series:\n        \"\"\"(-1 * RANK(SIGN(DELTA((OPEN * 0.85 + HIGH * 0.15), 4))))\"\"\"\n        return -1 * self.rank(self.sign(self.delta(open_ * 0.85 + high * 0.15, 4)))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha007",
    "category": "alpha_factor",
    "formula": "(",
    "explanation": "(RANK(TSMAX(VWAP - CLOSE, 3)) + RANK(TSMIN(VWAP - CLOSE, 3))) * RANK(DELTA(VOLUME, 3))",
    "python_code": "def alpha007(self, close: pd.Series, vwap: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(RANK(TSMAX(VWAP - CLOSE, 3)) + RANK(TSMIN(VWAP - CLOSE, 3))) * RANK(DELTA(VOLUME, 3))\"\"\"\n        return (\n            self.rank(self.ts_max(vwap - close, 3)) +\n            self.rank(self.ts_min(vwap - close, 3))\n        ) * self.rank(self.delta(volume, 3))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha008",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "RANK(DELTA(((HIGH + LOW) / 2 * 0.2 + VWAP * 0.8), 4) * -1)",
    "python_code": "def alpha008(self, high: pd.Series, low: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"RANK(DELTA(((HIGH + LOW) / 2 * 0.2 + VWAP * 0.8), 4) * -1)\"\"\"\n        return self.rank(self.delta((high + low) / 2 * 0.2 + vwap * 0.8, 4) * -1)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha009",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SMA(((HIGH + LOW) / 2 - DELAY((HIGH + LOW) / 2, 1)) * (HIGH - LOW) / VOLUME, 7, 2)",
    "python_code": "def alpha009(self, high: pd.Series, low: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"SMA(((HIGH + LOW) / 2 - DELAY((HIGH + LOW) / 2, 1)) * (HIGH - LOW) / VOLUME, 7, 2)\"\"\"\n        mid = (high + low) / 2\n        inner = (mid - self.delay(mid, 1)) * (high - low) / (volume + 1e-8)\n        return self.sma(inner, 7, 2)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha010",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "RANK(TSMAX((if RET < 0 then STD(RET, 20) else CLOSE)^2, 5))",
    "python_code": "def alpha010(self, close: pd.Series, returns: pd.Series) -> pd.Series:\n        \"\"\"RANK(TSMAX((if RET < 0 then STD(RET, 20) else CLOSE)^2, 5))\"\"\"\n        cond = returns < 0\n        inner = np.where(cond, self.ts_std(returns, 20), close)\n        return self.rank(self.ts_max(pd.Series(inner, index=close.index) ** 2, 5))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha011",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SUM(((CLOSE - LOW) - (HIGH - CLOSE)) / (HIGH - LOW) * VOLUME, 6)",
    "python_code": "def alpha011(self, close: pd.Series, high: pd.Series, low: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"SUM(((CLOSE - LOW) - (HIGH - CLOSE)) / (HIGH - LOW) * VOLUME, 6)\"\"\"\n        inner = ((close - low) - (high - close)) / (high - low + 1e-8) * volume\n        return self.ts_sum(inner, 6)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha012",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "RANK(OPEN - SUM(VWAP, 10) / 10) * -1 * RANK(ABS(CLOSE - VWAP))",
    "python_code": "def alpha012(self, open_: pd.Series, close: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"RANK(OPEN - SUM(VWAP, 10) / 10) * -1 * RANK(ABS(CLOSE - VWAP))\"\"\"\n        return self.rank(open_ - self.ts_sum(vwap, 10) / 10) * -1 * self.rank(self.abs_(close - vwap))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha013",
    "category": "alpha_factor",
    "formula": "np.sqrt(high * low) - vwap",
    "explanation": "(HIGH * LOW)^0.5 - VWAP",
    "python_code": "def alpha013(self, high: pd.Series, low: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"(HIGH * LOW)^0.5 - VWAP\"\"\"\n        return np.sqrt(high * low) - vwap",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha014",
    "category": "alpha_factor",
    "formula": "close - self.delay(close, 5)",
    "explanation": "CLOSE - DELAY(CLOSE, 5)",
    "python_code": "def alpha014(self, close: pd.Series) -> pd.Series:\n        \"\"\"CLOSE - DELAY(CLOSE, 5)\"\"\"\n        return close - self.delay(close, 5)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha015",
    "category": "alpha_factor",
    "formula": "open_ / (self.delay(close, 1) + 1e-8) - 1",
    "explanation": "OPEN / DELAY(CLOSE, 1) - 1",
    "python_code": "def alpha015(self, open_: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"OPEN / DELAY(CLOSE, 1) - 1\"\"\"\n        return open_ / (self.delay(close, 1) + 1e-8) - 1",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha016",
    "category": "alpha_factor",
    "formula": "-1 * self.ts_max(",
    "explanation": "(-1 * TSMAX(RANK(CORR(RANK(VOLUME), RANK(VWAP), 5)), 5))",
    "python_code": "def alpha016(self, volume: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"(-1 * TSMAX(RANK(CORR(RANK(VOLUME), RANK(VWAP), 5)), 5))\"\"\"\n        return -1 * self.ts_max(\n            self.rank(self.correlation(self.rank(volume + 1), self.rank(vwap), 5)),\n            5\n        )",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha017",
    "category": "alpha_factor",
    "formula": "base ** exp.clip(-10, 10)",
    "explanation": "RANK(VWAP - TSMAX(VWAP, 15))^DELTA(CLOSE, 5)",
    "python_code": "def alpha017(self, close: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"RANK(VWAP - TSMAX(VWAP, 15))^DELTA(CLOSE, 5)\"\"\"\n        base = self.rank(vwap - self.ts_max(vwap, 15))\n        exp = self.delta(close, 5)\n        return base ** exp.clip(-10, 10)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha018",
    "category": "alpha_factor",
    "formula": "close / (self.delay(close, 5) + 1e-8)",
    "explanation": "CLOSE / DELAY(CLOSE, 5)",
    "python_code": "def alpha018(self, close: pd.Series) -> pd.Series:\n        \"\"\"CLOSE / DELAY(CLOSE, 5)\"\"\"\n        return close / (self.delay(close, 5) + 1e-8)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha019",
    "category": "alpha_factor",
    "formula": "CLOSE = DELAY then 0, else ratio2 | np.where(cond1, (close - delay5) / (delay5 + 1e-8),",
    "explanation": "Conditional: if CLOSE < DELAY then ratio1, elif CLOSE = DELAY then 0, else ratio2",
    "python_code": "def alpha019(self, close: pd.Series) -> pd.Series:\n        \"\"\"Conditional: if CLOSE < DELAY then ratio1, elif CLOSE = DELAY then 0, else ratio2\"\"\"\n        delay5 = self.delay(close, 5)\n        cond1 = close < delay5\n        cond2 = close == delay5\n        return np.where(cond1, (close - delay5) / (delay5 + 1e-8),\n                 np.where(cond2, 0, (close - delay5) / (close + 1e-8)))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha020",
    "category": "alpha_factor",
    "formula": "(close - self.delay(close, 6)) / (self.delay(close, 6) + 1e-8) * 100",
    "explanation": "(CLOSE - DELAY(CLOSE, 6)) / DELAY(CLOSE, 6) * 100",
    "python_code": "def alpha020(self, close: pd.Series) -> pd.Series:\n        \"\"\"(CLOSE - DELAY(CLOSE, 6)) / DELAY(CLOSE, 6) * 100\"\"\"\n        return (close - self.delay(close, 6)) / (self.delay(close, 6) + 1e-8) * 100",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha021",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "REGBETA(MEAN(CLOSE, 6), SEQUENCE(6))",
    "python_code": "def alpha021(self, close: pd.Series) -> pd.Series:\n        \"\"\"REGBETA(MEAN(CLOSE, 6), SEQUENCE(6))\"\"\"\n        return self.regbeta(self.ts_mean(close, 6), close, 6)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha022",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SMA(((CLOSE - MEAN(CLOSE, 6)) / MEAN(CLOSE, 6) - DELAY(..., 3)), 12, 1)",
    "python_code": "def alpha022(self, close: pd.Series) -> pd.Series:\n        \"\"\"SMA(((CLOSE - MEAN(CLOSE, 6)) / MEAN(CLOSE, 6) - DELAY(..., 3)), 12, 1)\"\"\"\n        inner = (close - self.ts_mean(close, 6)) / (self.ts_mean(close, 6) + 1e-8)\n        return self.sma(inner - self.delay(inner, 3), 12, 1)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha023",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Ratio of up-volatility to total volatility, scaled to 100",
    "python_code": "def alpha023(self, close: pd.Series) -> pd.Series:\n        \"\"\"Ratio of up-volatility to total volatility, scaled to 100\"\"\"\n        cond = close > self.delay(close, 1)\n        up_std = self.ts_std(np.where(cond, close, 0), 20)\n        total_std = self.ts_std(close, 20)\n        return self.sma(pd.Series(np.where(cond, up_std, total_std), index=close.index), 20, 1) * 100",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha024",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SMA(CLOSE - DELAY(CLOSE, 5), 5, 1)",
    "python_code": "def alpha024(self, close: pd.Series) -> pd.Series:\n        \"\"\"SMA(CLOSE - DELAY(CLOSE, 5), 5, 1)\"\"\"\n        return self.sma(close - self.delay(close, 5), 5, 1)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha025",
    "category": "alpha_factor",
    "formula": "(",
    "explanation": "Complex: (-1 * RANK(DELTA(CLOSE, 7) * (1 - RANK(DECAYLINEAR(...))))) * (1 + RANK(SUM(RET, 250)))",
    "python_code": "def alpha025(self, close: pd.Series, volume: pd.Series, returns: pd.Series) -> pd.Series:\n        \"\"\"Complex: (-1 * RANK(DELTA(CLOSE, 7) * (1 - RANK(DECAYLINEAR(...))))) * (1 + RANK(SUM(RET, 250)))\"\"\"\n        return (\n            -1 * self.rank(self.delta(close, 7) * (1 - self.rank(self.decay_linear(volume / (self.ts_mean(volume, 20) + 1e-8), 9)))) *\n            (1 + self.rank(self.ts_sum(returns, 250)))\n        )",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha026",
    "category": "alpha_factor",
    "formula": "(self.ts_sum(close, 7) / 7 - close) + self.correlation(vwap, self.delay(close, 5), 230)",
    "explanation": "(SUM(CLOSE, 7) / 7 - CLOSE) + CORR(VWAP, DELAY(CLOSE, 5), 230)",
    "python_code": "def alpha026(self, close: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"(SUM(CLOSE, 7) / 7 - CLOSE) + CORR(VWAP, DELAY(CLOSE, 5), 230)\"\"\"\n        return (self.ts_sum(close, 7) / 7 - close) + self.correlation(vwap, self.delay(close, 5), 230)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha027",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "WMA of price change percentages over 3 and 6 days",
    "python_code": "def alpha027(self, close: pd.Series) -> pd.Series:\n        \"\"\"WMA of price change percentages over 3 and 6 days\"\"\"\n        pct3 = (close - self.delay(close, 3)) / (self.delay(close, 3) + 1e-8) * 100\n        pct6 = (close - self.delay(close, 6)) / (self.delay(close, 6) + 1e-8) * 100\n        return self.wma((pct3 + pct6) / 2, 12)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha028",
    "category": "alpha_factor",
    "formula": "j",
    "explanation": "Triple EMA-based Stochastic variant",
    "python_code": "def alpha028(self, close: pd.Series, high: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"Triple EMA-based Stochastic variant\"\"\"\n        llv9 = self.ts_min(low, 9)\n        hhv9 = self.ts_max(high, 9)\n        rsv = (close - llv9) / (hhv9 - llv9 + 1e-8) * 100\n        k = self.sma(rsv, 3, 1)\n        d = self.sma(k, 3, 1)\n        j = 3 * k - 2 * d\n        return j",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha029",
    "category": "alpha_factor",
    "formula": "(close - self.delay(close, 6)) / (self.delay(close, 6) + 1e-8) * volume",
    "explanation": "(CLOSE - DELAY(CLOSE, 6)) / DELAY(CLOSE, 6) * VOLUME",
    "python_code": "def alpha029(self, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(CLOSE - DELAY(CLOSE, 6)) / DELAY(CLOSE, 6) * VOLUME\"\"\"\n        return (close - self.delay(close, 6)) / (self.delay(close, 6) + 1e-8) * volume",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha030",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Regression-based - simplified",
    "python_code": "def alpha030(self, close: pd.Series) -> pd.Series:\n        \"\"\"Regression-based - simplified\"\"\"\n        return self.regbeta(close, close, 60)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha031",
    "category": "alpha_factor",
    "formula": "(close - self.ts_mean(close, 12)) / (self.ts_mean(close, 12) + 1e-8) * 100",
    "explanation": "(CLOSE - MEAN(CLOSE, 12)) / MEAN(CLOSE, 12) * 100",
    "python_code": "def alpha031(self, close: pd.Series) -> pd.Series:\n        \"\"\"(CLOSE - MEAN(CLOSE, 12)) / MEAN(CLOSE, 12) * 100\"\"\"\n        return (close - self.ts_mean(close, 12)) / (self.ts_mean(close, 12) + 1e-8) * 100",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha032",
    "category": "alpha_factor",
    "formula": "-1 * self.ts_sum(",
    "explanation": "(-1 * SUM(RANK(CORR(RANK(HIGH), RANK(VOLUME), 3)), 3))",
    "python_code": "def alpha032(self, volume: pd.Series, high: pd.Series) -> pd.Series:\n        \"\"\"(-1 * SUM(RANK(CORR(RANK(HIGH), RANK(VOLUME), 3)), 3))\"\"\"\n        return -1 * self.ts_sum(\n            self.rank(self.correlation(self.rank(high), self.rank(volume + 1), 3)),\n            3\n        )",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha033",
    "category": "alpha_factor",
    "formula": "(-1 * ts_min5 + delay_min) * self.rank(ret_factor) * self.ts_rank(close, 5)",
    "explanation": "Multi-component formula with returns range ratios",
    "python_code": "def alpha033(self, close: pd.Series, low: pd.Series, returns: pd.Series) -> pd.Series:\n        \"\"\"Multi-component formula with returns range ratios\"\"\"\n        ts_min5 = self.ts_min(low, 5)\n        delay_min = self.delay(ts_min5, 5)\n        ret_factor = (self.ts_sum(returns, 240) - self.ts_sum(returns, 20)) / 220\n        return (-1 * ts_min5 + delay_min) * self.rank(ret_factor) * self.ts_rank(close, 5)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha034",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "MEAN(CLOSE, 12) / CLOSE",
    "python_code": "def alpha034(self, close: pd.Series) -> pd.Series:\n        \"\"\"MEAN(CLOSE, 12) / CLOSE\"\"\"\n        return self.ts_mean(close, 12) / (close + 1e-8)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha035",
    "category": "alpha_factor",
    "formula": "np.minimum(self.rank(inner1), self.rank(inner2)) * -1",
    "explanation": "MIN of decay-linear ranks",
    "python_code": "def alpha035(self, open_: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"MIN of decay-linear ranks\"\"\"\n        inner1 = self.decay_linear(self.delta(open_, 1), 15)\n        inner2 = self.decay_linear(self.correlation(volume / (self.ts_mean(volume, 20) + 1e-8), open_ * 0.65 + open_ * 0.35, 17), 7)\n        return np.minimum(self.rank(inner1), self.rank(inner2)) * -1",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha036",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "RANK(SUM(CORR(RANK(VOLUME), RANK(VWAP), 6), 2))",
    "python_code": "def alpha036(self, volume: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"RANK(SUM(CORR(RANK(VOLUME), RANK(VWAP), 6), 2))\"\"\"\n        return self.rank(self.ts_sum(self.correlation(self.rank(volume + 1), self.rank(vwap), 6), 2))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha037",
    "category": "alpha_factor",
    "formula": "-1 * self.rank(inner - self.delay(inner, 10))",
    "explanation": "(-1 * RANK(SUM(OPEN, 5) * SUM(RET, 5) - DELAY(..., 10)))",
    "python_code": "def alpha037(self, open_: pd.Series, close: pd.Series, returns: pd.Series) -> pd.Series:\n        \"\"\"(-1 * RANK(SUM(OPEN, 5) * SUM(RET, 5) - DELAY(..., 10)))\"\"\"\n        inner = self.ts_sum(open_, 5) * self.ts_sum(returns, 5)\n        return -1 * self.rank(inner - self.delay(inner, 10))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha038",
    "category": "alpha_factor",
    "formula": "np.where(self.ts_mean(high, 20) < high, -1 * self.delta(high, 2), 0)",
    "explanation": "Conditional: if MA(HIGH, 20) < HIGH then -1 * DELTA(HIGH, 2) else 0",
    "python_code": "def alpha038(self, high: pd.Series) -> pd.Series:\n        \"\"\"Conditional: if MA(HIGH, 20) < HIGH then -1 * DELTA(HIGH, 2) else 0\"\"\"\n        return np.where(self.ts_mean(high, 20) < high, -1 * self.delta(high, 2), 0)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha039",
    "category": "alpha_factor",
    "formula": "(self.rank(inner1) - self.rank(inner2)) * -1",
    "explanation": "Difference of decay-linear correlations with negation",
    "python_code": "def alpha039(self, close: pd.Series, open_: pd.Series, volume: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"Difference of decay-linear correlations with negation\"\"\"\n        adv20 = self.ts_mean(volume, 20)\n        inner1 = self.decay_linear(self.delta(close, 2), 8)\n        inner2 = self.decay_linear(self.correlation(vwap * 0.3 + open_ * 0.7, self.ts_sum(self.ts_mean(volume, 180), 37), 14), 12)\n        return (self.rank(inner1) - self.rank(inner2)) * -1",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha040",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SUM(up volume, 26) / SUM(down volume, 26) * 100",
    "python_code": "def alpha040(self, high: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"SUM(up volume, 26) / SUM(down volume, 26) * 100\"\"\"\n        cond = high > self.delay(high, 1)\n        up_vol = np.where(cond, volume, 0)\n        down_vol = np.where(~cond, volume, 0)\n        return self.ts_sum(pd.Series(up_vol, index=volume.index), 26) / (self.ts_sum(pd.Series(down_vol, index=volume.index), 26) + 1e-8) * 100",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha041",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "RANK(TSMAX(DELTA(VWAP, 3), 5)) * -1",
    "python_code": "def alpha041(self, vwap: pd.Series) -> pd.Series:\n        \"\"\"RANK(TSMAX(DELTA(VWAP, 3), 5)) * -1\"\"\"\n        return self.rank(self.ts_max(self.delta(vwap, 3), 5)) * -1",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha042",
    "category": "alpha_factor",
    "formula": "-1 * self.rank(self.ts_std(high, 10)) * self.correlation(high, volume + 1, 10)",
    "explanation": "(-1 * RANK(STD(HIGH, 10))) * CORR(HIGH, VOLUME, 10)",
    "python_code": "def alpha042(self, high: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(-1 * RANK(STD(HIGH, 10))) * CORR(HIGH, VOLUME, 10)\"\"\"\n        return -1 * self.rank(self.ts_std(high, 10)) * self.correlation(high, volume + 1, 10)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha043",
    "category": "alpha_factor",
    "formula": "pd.Series(result, index=close.index)",
    "explanation": "Directional volume accumulation over 6 days",
    "python_code": "def alpha043(self, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"Directional volume accumulation over 6 days\"\"\"\n        cond = close > self.delay(close, 1)\n        result = self.ts_sum(np.where(cond, volume, -volume), 6)\n        return pd.Series(result, index=close.index)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha044",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Sum of decay-linear Tsrank measures",
    "python_code": "def alpha044(self, volume: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"Sum of decay-linear Tsrank measures\"\"\"\n        inner1 = self.decay_linear(self.correlation(self.rank(low), self.rank(self.ts_mean(volume, 10)), 7), 6)\n        inner2 = self.ts_rank(self.decay_linear(self.delta(vwap, 3), 10), 15)\n        return self.rank(inner1) + inner2",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha045",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "RANK(DELTA(...)) * RANK(CORR(VWAP, MEAN(VOLUME, 150), 15))",
    "python_code": "def alpha045(self, close: pd.Series, volume: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"RANK(DELTA(...)) * RANK(CORR(VWAP, MEAN(VOLUME, 150), 15))\"\"\"\n        inner = (close * 0.6 + open_ * 0.4)\n        return self.rank(self.delta(inner, 1)) * self.rank(self.correlation(vwap, self.ts_mean(volume, 150), 15))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha046",
    "category": "alpha_factor",
    "formula": "(",
    "explanation": "(MEAN(CLOSE, 3) + MEAN(CLOSE, 6) + MEAN(CLOSE, 12) + MEAN(CLOSE, 24)) / (4 * CLOSE)",
    "python_code": "def alpha046(self, close: pd.Series) -> pd.Series:\n        \"\"\"(MEAN(CLOSE, 3) + MEAN(CLOSE, 6) + MEAN(CLOSE, 12) + MEAN(CLOSE, 24)) / (4 * CLOSE)\"\"\"\n        return (\n            self.ts_mean(close, 3) + self.ts_mean(close, 6) +\n            self.ts_mean(close, 12) + self.ts_mean(close, 24)\n        ) / (4 * close + 1e-8)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha047",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SMA(stochastic-like formula, 9, 1)",
    "python_code": "def alpha047(self, high: pd.Series, low: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"SMA(stochastic-like formula, 9, 1)\"\"\"\n        llv9 = self.ts_min(low, 9)\n        hhv9 = self.ts_max(high, 9)\n        rsv = (self.ts_max(high, 6) - close) / (self.ts_max(high, 6) - self.ts_min(low, 6) + 1e-8) * 100\n        return self.sma(rsv, 9, 1)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha048",
    "category": "alpha_factor",
    "formula": "inner",
    "explanation": "Complex ranking of momentum and volume patterns",
    "python_code": "def alpha048(self, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"Complex ranking of momentum and volume patterns\"\"\"\n        adv20 = self.ts_mean(volume, 20)\n        inner = -1 * (self.rank((close - self.delay(close, 1))) * volume) / (adv20 + 1e-8)\n        return inner",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha049",
    "category": "alpha_factor",
    "formula": "np.where(dtm_sum > dbm_sum, (dtm_sum - dbm_sum) / (dtm_sum + 1e-8),",
    "explanation": "Ratio measuring uptrend dominance - DTM/DBM calculation",
    "python_code": "def alpha049(self, high: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"Ratio measuring uptrend dominance - DTM/DBM calculation\"\"\"\n        dtm = np.where(open_ <= self.delay(open_, 1), 0,\n                      np.maximum(high - open_, open_ - self.delay(open_, 1)))\n        dbm = np.where(open_ >= self.delay(open_, 1), 0,\n                      np.maximum(open_ - low, open_ - self.delay(open_, 1)))\n        dtm_sum = self.ts_sum(pd.Series(dtm, index=high.index), 12)\n        dbm_sum = self.ts_sum(pd.Series(dbm, index=high.index), 12)\n        return np.where(dtm_sum > dbm_sum, (dtm_sum - dbm_sum) / (dtm_sum + 1e-8),\n                       np.where(dtm_sum == dbm_sum, 0, (dtm_sum - dbm_sum) / (dbm_sum + 1e-8)))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha050",
    "category": "alpha_factor",
    "formula": "(self.ts_sum(pd.Series(dtm, index=high.index), 12) -",
    "explanation": "Net difference between uptrend and downtrend measures",
    "python_code": "def alpha050(self, high: pd.Series, low: pd.Series, open_: pd.Series) -> pd.Series:\n        \"\"\"Net difference between uptrend and downtrend measures\"\"\"\n        dtm = np.where(open_ <= self.delay(open_, 1), 0,\n                      np.maximum(high - open_, open_ - self.delay(open_, 1)))\n        dbm = np.where(open_ >= self.delay(open_, 1), 0,\n                      np.maximum(open_ - low, open_ - self.delay(open_, 1)))\n        return (self.ts_sum(pd.Series(dtm, index=high.index), 12) -\n                self.ts_sum(pd.Series(dbm, index=high.index), 12)) / \\\n               (self.ts_sum(pd.Series(dtm, index=high.index), 12) +\n                self.ts_sum(pd.Series(dbm, index=high.index), 12) + 1e-8)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha051",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Uptrend strength ratio",
    "python_code": "def alpha051(self, high: pd.Series, low: pd.Series, open_: pd.Series) -> pd.Series:\n        \"\"\"Uptrend strength ratio\"\"\"\n        dtm = np.where(open_ <= self.delay(open_, 1), 0,\n                      np.maximum(high - open_, open_ - self.delay(open_, 1)))\n        dbm = np.where(open_ >= self.delay(open_, 1), 0,\n                      np.maximum(open_ - low, open_ - self.delay(open_, 1)))\n        return self.ts_sum(pd.Series(dtm, index=high.index), 12) / \\\n               (self.ts_sum(pd.Series(dtm, index=high.index), 12) +\n                self.ts_sum(pd.Series(dbm, index=high.index), 12) + 1e-8)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha052",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SUM(MAX(HIGH - delayed midpoint, 0), 26) / SUM(MAX(...), 26)",
    "python_code": "def alpha052(self, high: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"SUM(MAX(HIGH - delayed midpoint, 0), 26) / SUM(MAX(...), 26)\"\"\"\n        mid_delay = self.delay((high + low) / 2, 1)\n        inner = np.maximum(0, high - mid_delay) - np.maximum(0, mid_delay - low)\n        return self.ts_sum(pd.Series(inner, index=high.index), 26) / 26",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha053",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "COUNT(CLOSE > DELAY(CLOSE, 1), 12) / 12 * 100",
    "python_code": "def alpha053(self, close: pd.Series) -> pd.Series:\n        \"\"\"COUNT(CLOSE > DELAY(CLOSE, 1), 12) / 12 * 100\"\"\"\n        cond = close > self.delay(close, 1)\n        return self.ts_count(cond, 12) / 12 * 100",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha054",
    "category": "alpha_factor",
    "formula": "-1 * self.rank(",
    "explanation": "-1 * RANK(STD(ABS(CLOSE - OPEN)) + (CLOSE - OPEN) + CORR(CLOSE, OPEN, 10))",
    "python_code": "def alpha054(self, close: pd.Series, open_: pd.Series) -> pd.Series:\n        \"\"\"-1 * RANK(STD(ABS(CLOSE - OPEN)) + (CLOSE - OPEN) + CORR(CLOSE, OPEN, 10))\"\"\"\n        return -1 * self.rank(\n            self.ts_std(self.abs_(close - open_), 10) +\n            (close - open_) +\n            self.correlation(close, open_, 10)\n        )",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha055",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Complex true range and momentum normalization",
    "python_code": "def alpha055(self, close: pd.Series, high: pd.Series, low: pd.Series, open_: pd.Series) -> pd.Series:\n        \"\"\"Complex true range and momentum normalization\"\"\"\n        inner = (close - self.ts_min(low, 16)) / (self.ts_max(high, 16) - self.ts_min(low, 16) + 1e-8)\n        return self.ts_sum(inner * 16, 16) / self.ts_sum(pd.Series(np.ones_like(close), index=close.index) * 16, 16)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha056",
    "category": "alpha_factor",
    "formula": "np.where(cond, 1, 0)",
    "explanation": "Conditional rank comparison",
    "python_code": "def alpha056(self, volume: pd.Series, high: pd.Series, open_: pd.Series) -> pd.Series:\n        \"\"\"Conditional rank comparison\"\"\"\n        cond = self.rank(open_ - self.ts_min(open_, 12)) < self.rank(self.rank(self.correlation(high, self.ts_mean(volume, 30), 12)))\n        return np.where(cond, 1, 0)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha057",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SMA(stochastic formula, 3, 1)",
    "python_code": "def alpha057(self, close: pd.Series, high: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"SMA(stochastic formula, 3, 1)\"\"\"\n        llv9 = self.ts_min(low, 9)\n        hhv9 = self.ts_max(high, 9)\n        rsv = (close - llv9) / (hhv9 - llv9 + 1e-8) * 100\n        return self.sma(rsv, 3, 1)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha058",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "COUNT(CLOSE > DELAY(CLOSE, 1), 20) / 20 * 100",
    "python_code": "def alpha058(self, close: pd.Series) -> pd.Series:\n        \"\"\"COUNT(CLOSE > DELAY(CLOSE, 1), 20) / 20 * 100\"\"\"\n        cond = close > self.delay(close, 1)\n        return self.ts_count(cond, 20) / 20 * 100",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha059",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SUM of conditional spread adjustments, 20",
    "python_code": "def alpha059(self, close: pd.Series, high: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"SUM of conditional spread adjustments, 20\"\"\"\n        delay_close = self.delay(close, 1)\n        cond1 = close == delay_close\n        inner = np.where(cond1, 0,\n                 np.where(close > delay_close, close - np.minimum(low, delay_close),\n                         close - np.maximum(high, delay_close)))\n        return self.ts_sum(pd.Series(inner, index=close.index), 20)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha060",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SUM(normalized volume-weighted spread, 20)",
    "python_code": "def alpha060(self, close: pd.Series, high: pd.Series, low: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"SUM(normalized volume-weighted spread, 20)\"\"\"\n        inner = ((close - low) - (high - close)) / (high - low + 1e-8) * volume\n        return self.ts_sum(inner, 20)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha061",
    "category": "alpha_factor",
    "formula": "np.maximum(self.rank(inner1), inner2) * -1",
    "explanation": "MAX(RANK(DECAYLINEAR(...)), RANK(...)) * -1",
    "python_code": "def alpha061(self, vwap: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"MAX(RANK(DECAYLINEAR(...)), RANK(...)) * -1\"\"\"\n        adv180 = self.ts_mean(volume, 180)\n        inner1 = self.decay_linear(vwap - self.ts_min(vwap, 16), 12)\n        inner2 = self.ts_rank(self.correlation(vwap, adv180, 18), 20)\n        return np.maximum(self.rank(inner1), inner2) * -1",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha062",
    "category": "alpha_factor",
    "formula": "-1 * self.correlation(high, self.rank(volume + 1), 5)",
    "explanation": "(-1 * CORR(HIGH, RANK(VOLUME), 5))",
    "python_code": "def alpha062(self, volume: pd.Series, high: pd.Series) -> pd.Series:\n        \"\"\"(-1 * CORR(HIGH, RANK(VOLUME), 5))\"\"\"\n        return -1 * self.correlation(high, self.rank(volume + 1), 5)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha063",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SMA(MAX(delta, 0), 6, 1) / SMA(ABS(delta), 6, 1) * 100",
    "python_code": "def alpha063(self, close: pd.Series) -> pd.Series:\n        \"\"\"SMA(MAX(delta, 0), 6, 1) / SMA(ABS(delta), 6, 1) * 100\"\"\"\n        delta = close - self.delay(close, 1)\n        return self.sma(np.maximum(delta, 0), 6, 1) / (self.sma(self.abs_(delta), 6, 1) + 1e-8) * 100",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha064",
    "category": "alpha_factor",
    "formula": "np.maximum(self.rank(inner1), self.rank(inner2)) * -1",
    "explanation": "Complex decay-linear correlation ranks",
    "python_code": "def alpha064(self, close: pd.Series, volume: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"Complex decay-linear correlation ranks\"\"\"\n        adv120 = self.ts_mean(volume, 120)\n        inner1 = self.decay_linear(self.correlation(self.rank(vwap), self.rank(volume + 1), 4), 4)\n        inner2 = self.decay_linear(self.ts_max(self.correlation(self.rank(close), self.rank(adv120), 12), 13), 14)\n        return np.maximum(self.rank(inner1), self.rank(inner2)) * -1",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha065",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "MEAN(CLOSE, 6) / CLOSE",
    "python_code": "def alpha065(self, close: pd.Series) -> pd.Series:\n        \"\"\"MEAN(CLOSE, 6) / CLOSE\"\"\"\n        return self.ts_mean(close, 6) / (close + 1e-8)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha066",
    "category": "alpha_factor",
    "formula": "(close - self.ts_mean(close, 6)) / (self.ts_mean(close, 6) + 1e-8) * 100",
    "explanation": "(CLOSE - MEAN(CLOSE, 6)) / MEAN(CLOSE, 6) * 100",
    "python_code": "def alpha066(self, close: pd.Series) -> pd.Series:\n        \"\"\"(CLOSE - MEAN(CLOSE, 6)) / MEAN(CLOSE, 6) * 100\"\"\"\n        return (close - self.ts_mean(close, 6)) / (self.ts_mean(close, 6) + 1e-8) * 100",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha067",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SMA(MAX(delta, 0), 24, 1) / SMA(ABS(delta), 24, 1) * 100",
    "python_code": "def alpha067(self, close: pd.Series) -> pd.Series:\n        \"\"\"SMA(MAX(delta, 0), 24, 1) / SMA(ABS(delta), 24, 1) * 100\"\"\"\n        delta = close - self.delay(close, 1)\n        return self.sma(np.maximum(delta, 0), 24, 1) / (self.sma(self.abs_(delta), 24, 1) + 1e-8) * 100",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha068",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SMA(midpoint change * range / volume, 15, 2)",
    "python_code": "def alpha068(self, high: pd.Series, low: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"SMA(midpoint change * range / volume, 15, 2)\"\"\"\n        mid = (high + low) / 2\n        inner = (mid - self.delay(mid, 1)) * (high - low) / (volume + 1e-8)\n        return self.sma(inner, 15, 2)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha069",
    "category": "alpha_factor",
    "formula": "np.where(dtm_s > dbm_s, (dtm_s - dbm_s) / (dtm_s + 1e-8),",
    "explanation": "Directional trend measurement (DTM vs DBM)",
    "python_code": "def alpha069(self, high: pd.Series, low: pd.Series, open_: pd.Series) -> pd.Series:\n        \"\"\"Directional trend measurement (DTM vs DBM)\"\"\"\n        dtm = np.where(open_ <= self.delay(open_, 1), 0,\n                      np.maximum(high - open_, open_ - self.delay(open_, 1)))\n        dbm = np.where(open_ >= self.delay(open_, 1), 0,\n                      np.maximum(open_ - low, open_ - self.delay(open_, 1)))\n        dtm_s = self.ts_sum(pd.Series(dtm, index=high.index), 20)\n        dbm_s = self.ts_sum(pd.Series(dbm, index=high.index), 20)\n        return np.where(dtm_s > dbm_s, (dtm_s - dbm_s) / (dtm_s + 1e-8),\n                       np.where(dtm_s == dbm_s, 0, (dtm_s - dbm_s) / (dbm_s + 1e-8)))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha070",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "STD(AMOUNT, 6)",
    "python_code": "def alpha070(self, amount: pd.Series) -> pd.Series:\n        \"\"\"STD(AMOUNT, 6)\"\"\"\n        return self.ts_std(amount, 6)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha071",
    "category": "alpha_factor",
    "formula": "(close - self.ts_mean(close, 24)) / (self.ts_mean(close, 24) + 1e-8) * 100",
    "explanation": "(CLOSE - MEAN(CLOSE, 24)) / MEAN(CLOSE, 24) * 100",
    "python_code": "def alpha071(self, close: pd.Series) -> pd.Series:\n        \"\"\"(CLOSE - MEAN(CLOSE, 24)) / MEAN(CLOSE, 24) * 100\"\"\"\n        return (close - self.ts_mean(close, 24)) / (self.ts_mean(close, 24) + 1e-8) * 100",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha072",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SMA(stochastic formula, 15, 1)",
    "python_code": "def alpha072(self, high: pd.Series, low: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"SMA(stochastic formula, 15, 1)\"\"\"\n        llv6 = self.ts_min(low, 6)\n        hhv6 = self.ts_max(high, 6)\n        rsv = (self.ts_max(high, 6) - close) / (hhv6 - llv6 + 1e-8) * 100\n        return self.sma(rsv, 15, 1)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha073",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Nested decay-linear correlation ranks",
    "python_code": "def alpha073(self, close: pd.Series, volume: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"Nested decay-linear correlation ranks\"\"\"\n        inner1 = self.decay_linear(self.delta(vwap, 5), 3)\n        inner2 = self.decay_linear(self.correlation(close, self.ts_mean(volume, 150), 9), 4) / (self.rank(self.decay_linear(self.correlation(self.ts_rank(high, 4), self.ts_rank(self.ts_mean(volume, 19), 19), 7), 3)) + 1e-8)\n        return self.ts_rank(inner1, 17) * -1 * inner2",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha074",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Sum of correlation ranks",
    "python_code": "def alpha074(self, volume: pd.Series, vwap: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"Sum of correlation ranks\"\"\"\n        adv30 = self.ts_mean(volume, 30)\n        return self.rank(self.correlation(close, adv30, 10)) + self.rank(self.correlation(self.ts_rank(close, 10), self.ts_rank(adv30, 10), 7))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha075",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "COUNT(matched direction with index, 50) / COUNT(index down, 50)",
    "python_code": "def alpha075(self, close: pd.Series, open_: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"COUNT(matched direction with index, 50) / COUNT(index down, 50)\"\"\"\n        # Simplified - using close as proxy for index\n        bench_ret = close.pct_change()\n        stock_ret = open_.pct_change()\n        cond_match = (bench_ret < 0) & (stock_ret < bench_ret)\n        cond_down = bench_ret < 0\n        return self.ts_count(cond_match, 50) / (self.ts_count(cond_down, 50) + 1e-8)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha076",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "STD(normalized volume-price, 20) / MEAN(normalized volume-price, 20)",
    "python_code": "def alpha076(self, volume: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"STD(normalized volume-price, 20) / MEAN(normalized volume-price, 20)\"\"\"\n        inner = self.abs_(close / (self.delay(close, 1) + 1e-8) - 1) / (volume + 1e-8)\n        return self.ts_std(inner, 20) / (self.ts_mean(inner, 20) + 1e-8)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha077",
    "category": "alpha_factor",
    "formula": "np.minimum(self.rank(inner1), self.rank(inner2))",
    "explanation": "MIN(RANK(DECAYLINEAR(...)), RANK(...))",
    "python_code": "def alpha077(self, high: pd.Series, low: pd.Series, volume: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"MIN(RANK(DECAYLINEAR(...)), RANK(...))\"\"\"\n        inner1 = self.decay_linear(((high + low) / 2 + high) - (vwap + high), 20)\n        inner2 = self.decay_linear(self.correlation(high + low, self.ts_mean(volume, 40), 3), 6)\n        return np.minimum(self.rank(inner1), self.rank(inner2))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha078",
    "category": "alpha_factor",
    "formula": "(mid - ma_mid) / (norm * 0.015 + 1e-8)",
    "explanation": "((midpoint - MA(midpoint, 12)) / (normalized range))",
    "python_code": "def alpha078(self, high: pd.Series, low: pd.Series, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"((midpoint - MA(midpoint, 12)) / (normalized range))\"\"\"\n        mid = (high + low + close) / 3\n        ma_mid = self.ts_mean(mid, 12)\n        norm = self.ts_sum(mid, 12) / 12\n        return (mid - ma_mid) / (norm * 0.015 + 1e-8)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha079",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SMA(MAX(delta, 0), 12, 1) / SMA(ABS(delta), 12, 1) * 100",
    "python_code": "def alpha079(self, close: pd.Series) -> pd.Series:\n        \"\"\"SMA(MAX(delta, 0), 12, 1) / SMA(ABS(delta), 12, 1) * 100\"\"\"\n        delta = close - self.delay(close, 1)\n        return self.sma(np.maximum(delta, 0), 12, 1) / (self.sma(self.abs_(delta), 12, 1) + 1e-8) * 100",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha080",
    "category": "alpha_factor",
    "formula": "(volume - self.delay(volume, 5)) / (self.delay(volume, 5) + 1e-8) * 100",
    "explanation": "(VOLUME - DELAY(VOLUME, 5)) / DELAY(VOLUME, 5) * 100",
    "python_code": "def alpha080(self, volume: pd.Series) -> pd.Series:\n        \"\"\"(VOLUME - DELAY(VOLUME, 5)) / DELAY(VOLUME, 5) * 100\"\"\"\n        return (volume - self.delay(volume, 5)) / (self.delay(volume, 5) + 1e-8) * 100",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha081",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SMA(VOLUME, 21, 2)",
    "python_code": "def alpha081(self, volume: pd.Series) -> pd.Series:\n        \"\"\"SMA(VOLUME, 21, 2)\"\"\"\n        return self.sma(volume, 21, 2)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha082",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SMA(stochastic formula, 20, 1)",
    "python_code": "def alpha082(self, high: pd.Series, low: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"SMA(stochastic formula, 20, 1)\"\"\"\n        llv6 = self.ts_min(low, 6)\n        hhv6 = self.ts_max(high, 6)\n        rsv = (self.ts_max(high, 6) - close) / (hhv6 - llv6 + 1e-8) * 100\n        return self.sma(rsv, 20, 1)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha083",
    "category": "alpha_factor",
    "formula": "-1 * self.rank(self.covariance(self.rank(high), self.rank(volume + 1), 5))",
    "explanation": "(-1 * RANK(COV(RANK(HIGH), RANK(VOLUME), 5)))",
    "python_code": "def alpha083(self, high: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(-1 * RANK(COV(RANK(HIGH), RANK(VOLUME), 5)))\"\"\"\n        return -1 * self.rank(self.covariance(self.rank(high), self.rank(volume + 1), 5))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha084",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Directional volume accumulation over 20 days",
    "python_code": "def alpha084(self, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"Directional volume accumulation over 20 days\"\"\"\n        cond = close > self.delay(close, 1)\n        return self.ts_sum(np.where(cond, volume, -volume), 20)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha085",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "TSRANK(volume normalized, 20) * TSRANK(-delta, 8)",
    "python_code": "def alpha085(self, volume: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"TSRANK(volume normalized, 20) * TSRANK(-delta, 8)\"\"\"\n        return self.ts_rank(volume / (self.ts_mean(volume, 20) + 1e-8), 20) * self.ts_rank(-1 * self.delta(close, 7), 8)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha086",
    "category": "alpha_factor",
    "formula": "np.where(cond, -1, 1)",
    "explanation": "Conditional momentum adjustment logic",
    "python_code": "def alpha086(self, close: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"Conditional momentum adjustment logic\"\"\"\n        cond = self.delay(close, 20) * 0.25 + self.delay(vwap, 20) * 0.75 < vwap\n        return np.where(cond, -1, 1)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha087",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Complex decay-linear VWAP-based formula",
    "python_code": "def alpha087(self, close: pd.Series, high: pd.Series, vwap: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"Complex decay-linear VWAP-based formula\"\"\"\n        adv81 = self.ts_mean(volume, 81)\n        inner1 = self.decay_linear(close - self.ts_max(close, 14), 13)\n        inner2 = self.decay_linear(self.delta(vwap, 5), 11)\n        inner3 = self.ts_rank(self.decay_linear(self.correlation(high, adv81, 17), 20), 13)\n        return self.rank(inner1) + inner3 - self.rank(inner2)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha088",
    "category": "alpha_factor",
    "formula": "(close - self.delay(close, 20)) / (self.delay(close, 20) + 1e-8) * 100",
    "explanation": "(CLOSE - DELAY(CLOSE, 20)) / DELAY(CLOSE, 20) * 100",
    "python_code": "def alpha088(self, close: pd.Series) -> pd.Series:\n        \"\"\"(CLOSE - DELAY(CLOSE, 20)) / DELAY(CLOSE, 20) * 100\"\"\"\n        return (close - self.delay(close, 20)) / (self.delay(close, 20) + 1e-8) * 100",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha089",
    "category": "alpha_factor",
    "formula": "2 * (dif - dea)",
    "explanation": "MACD-like: 2 * (SMA13 - SMA27 - SMA(SMA13 - SMA27, 10))",
    "python_code": "def alpha089(self, close: pd.Series) -> pd.Series:\n        \"\"\"MACD-like: 2 * (SMA13 - SMA27 - SMA(SMA13 - SMA27, 10))\"\"\"\n        sma13 = self.sma(close, 13, 2)\n        sma27 = self.sma(close, 27, 2)\n        dif = sma13 - sma27\n        dea = self.sma(dif, 10, 2)\n        return 2 * (dif - dea)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha090",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "RANK(CORR(RANK(VWAP), RANK(VOLUME), 5)) * -1",
    "python_code": "def alpha090(self, volume: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"RANK(CORR(RANK(VWAP), RANK(VOLUME), 5)) * -1\"\"\"\n        return self.rank(self.correlation(self.rank(vwap), self.rank(volume + 1), 5)) * -1",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha091",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "RANK((CLOSE - TSMAX(CLOSE, 5))) * RANK(CORR(...)) * -1",
    "python_code": "def alpha091(self, close: pd.Series, volume: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"RANK((CLOSE - TSMAX(CLOSE, 5))) * RANK(CORR(...)) * -1\"\"\"\n        adv40 = self.ts_mean(volume, 40)\n        return self.rank(close - self.ts_max(close, 5)) * self.rank(self.correlation(adv40, low, 5)) * -1",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha092",
    "category": "alpha_factor",
    "formula": "np.maximum(self.rank(inner1), inner2) * -1",
    "explanation": "MAX(RANK(DECAYLINEAR(...)), TSRANK(...)) * -1",
    "python_code": "def alpha092(self, close: pd.Series, volume: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"MAX(RANK(DECAYLINEAR(...)), TSRANK(...)) * -1\"\"\"\n        adv30 = self.ts_mean(volume, 30)\n        inner1 = self.decay_linear(self.delta(close * 0.35 + vwap * 0.65, 2), 3)\n        inner2 = self.ts_rank(self.decay_linear(self.abs_(self.correlation(adv30, close, 13)), 5), 15)\n        return np.maximum(self.rank(inner1), inner2) * -1",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha093",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SUM(conditional open-low spreads, 20)",
    "python_code": "def alpha093(self, open_: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"SUM(conditional open-low spreads, 20)\"\"\"\n        cond = open_ >= self.delay(open_, 1)\n        return self.ts_sum(np.where(cond, 0, np.maximum(open_ - low, open_ - self.delay(open_, 1))), 20)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha094",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Directional volume accumulation over 30 days",
    "python_code": "def alpha094(self, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"Directional volume accumulation over 30 days\"\"\"\n        cond = close > self.delay(close, 1)\n        return self.ts_sum(np.where(cond, volume, -volume), 30)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha095",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "STD(AMOUNT, 20)",
    "python_code": "def alpha095(self, amount: pd.Series) -> pd.Series:\n        \"\"\"STD(AMOUNT, 20)\"\"\"\n        return self.ts_std(amount, 20)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha096",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Double SMA of stochastic formula",
    "python_code": "def alpha096(self, high: pd.Series, low: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"Double SMA of stochastic formula\"\"\"\n        llv9 = self.ts_min(low, 9)\n        hhv9 = self.ts_max(high, 9)\n        rsv = (close - llv9) / (hhv9 - llv9 + 1e-8) * 100\n        k = self.sma(rsv, 3, 1)\n        return self.sma(k, 3, 1)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha097",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "STD(VOLUME, 10)",
    "python_code": "def alpha097(self, volume: pd.Series) -> pd.Series:\n        \"\"\"STD(VOLUME, 10)\"\"\"\n        return self.ts_std(volume, 10)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha098",
    "category": "alpha_factor",
    "formula": "np.where(cond, -1 * self.delta(close, 3), -1 * (close - self.ts_min(close, 200)))",
    "explanation": "Conditional: if MA change < 5% then MIN spread else DELTA(CLOSE, 3)",
    "python_code": "def alpha098(self, close: pd.Series) -> pd.Series:\n        \"\"\"Conditional: if MA change < 5% then MIN spread else DELTA(CLOSE, 3)\"\"\"\n        mean5 = self.ts_mean(close, 5)\n        mean10 = self.ts_mean(close, 10)\n        cond = (self.delta(mean5, 10) / mean10) < 0.05\n        return np.where(cond, -1 * self.delta(close, 3), -1 * (close - self.ts_min(close, 200)))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha099",
    "category": "alpha_factor",
    "formula": "-1 * self.rank(self.covariance(self.rank(close), self.rank(volume + 1), 5))",
    "explanation": "(-1 * RANK(COV(RANK(CLOSE), RANK(VOLUME), 5)))",
    "python_code": "def alpha099(self, volume: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"(-1 * RANK(COV(RANK(CLOSE), RANK(VOLUME), 5)))\"\"\"\n        return -1 * self.rank(self.covariance(self.rank(close), self.rank(volume + 1), 5))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha100",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "STD(VOLUME, 20)",
    "python_code": "def alpha100(self, volume: pd.Series) -> pd.Series:\n        \"\"\"STD(VOLUME, 20)\"\"\"\n        return self.ts_std(volume, 20)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha101",
    "category": "alpha_factor",
    "formula": "np.where(",
    "explanation": "Conditional comparison of correlation ranks",
    "python_code": "def alpha101(self, close: pd.Series, open_: pd.Series, high: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"Conditional comparison of correlation ranks\"\"\"\n        return np.where(\n            (close - open_) / (high - low + 1e-8) * (high - low) > (high - open_),\n            (close - open_) / (high - low + 1e-8) - (close - open_) / (close - low + 1e-8),\n            0\n        )",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha102",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SMA(MAX(vol delta, 0), 6, 1) / SMA(ABS(vol delta), 6, 1) * 100",
    "python_code": "def alpha102(self, volume: pd.Series) -> pd.Series:\n        \"\"\"SMA(MAX(vol delta, 0), 6, 1) / SMA(ABS(vol delta), 6, 1) * 100\"\"\"\n        delta_vol = self.delta(volume, 1)\n        return self.sma(np.maximum(delta_vol, 0), 6, 1) / (self.sma(self.abs_(delta_vol), 6, 1) + 1e-8) * 100",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha103",
    "category": "alpha_factor",
    "formula": "((20 - self.ts_argmin(low, 20)) / 20) * 100",
    "explanation": "((20 - LOWDAY(LOW, 20)) / 20) * 100",
    "python_code": "def alpha103(self, low: pd.Series) -> pd.Series:\n        \"\"\"((20 - LOWDAY(LOW, 20)) / 20) * 100\"\"\"\n        return ((20 - self.ts_argmin(low, 20)) / 20) * 100",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha104",
    "category": "alpha_factor",
    "formula": "-1 * self.delta(self.correlation(high, volume + 1, 5), 5) * self.rank(self.ts_std(close, 20))",
    "explanation": "(-1 * (DELTA(CORR(HIGH, VOLUME, 5), 5) * RANK(STD(CLOSE, 20))))",
    "python_code": "def alpha104(self, high: pd.Series, volume: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"(-1 * (DELTA(CORR(HIGH, VOLUME, 5), 5) * RANK(STD(CLOSE, 20))))\"\"\"\n        return -1 * self.delta(self.correlation(high, volume + 1, 5), 5) * self.rank(self.ts_std(close, 20))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha105",
    "category": "alpha_factor",
    "formula": "-1 * self.correlation(self.rank(open_), self.rank(volume + 1), 10)",
    "explanation": "(-1 * CORR(RANK(OPEN), RANK(VOLUME), 10))",
    "python_code": "def alpha105(self, open_: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(-1 * CORR(RANK(OPEN), RANK(VOLUME), 10))\"\"\"\n        return -1 * self.correlation(self.rank(open_), self.rank(volume + 1), 10)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha106",
    "category": "alpha_factor",
    "formula": "close - self.delay(close, 20)",
    "explanation": "CLOSE - DELAY(CLOSE, 20)",
    "python_code": "def alpha106(self, close: pd.Series) -> pd.Series:\n        \"\"\"CLOSE - DELAY(CLOSE, 20)\"\"\"\n        return close - self.delay(close, 20)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha107",
    "category": "alpha_factor",
    "formula": "-1 * self.rank(open_ - self.delay(high, 1)) * \\",
    "explanation": "(-1 * RANK(OPEN - DELAY(HIGH, 1)) * RANK(...) * RANK(...))",
    "python_code": "def alpha107(self, open_: pd.Series, close: pd.Series, high: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"(-1 * RANK(OPEN - DELAY(HIGH, 1)) * RANK(...) * RANK(...))\"\"\"\n        return -1 * self.rank(open_ - self.delay(high, 1)) * \\\n               self.rank(open_ - self.delay(close, 1)) * \\\n               self.rank(open_ - self.delay(low, 1))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha108",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "(RANK(HIGH - TSMIN(HIGH, 2))^RANK(CORR(...))) * -1",
    "python_code": "def alpha108(self, high: pd.Series, vwap: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(RANK(HIGH - TSMIN(HIGH, 2))^RANK(CORR(...))) * -1\"\"\"\n        adv120 = self.ts_mean(volume, 120)\n        return self.rank(high - self.ts_min(high, 2)) ** self.rank(self.correlation(high, adv120, 15).clip(-1, 1)) * -1",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha109",
    "category": "alpha_factor",
    "formula": "inner / (self.sma(inner, 10, 2) + 1e-8)",
    "explanation": "SMA(HIGH - LOW, 10, 2) / SMA(SMA(...), 10, 2)",
    "python_code": "def alpha109(self, high: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"SMA(HIGH - LOW, 10, 2) / SMA(SMA(...), 10, 2)\"\"\"\n        inner = self.sma(high - low, 10, 2)\n        return inner / (self.sma(inner, 10, 2) + 1e-8)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha110",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SUM(MAX(HIGH - delayed close, 0), 20) / SUM(MAX(...), 20) * 100",
    "python_code": "def alpha110(self, close: pd.Series, high: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"SUM(MAX(HIGH - delayed close, 0), 20) / SUM(MAX(...), 20) * 100\"\"\"\n        delay_close = self.delay(close, 1)\n        up = np.maximum(0, high - delay_close)\n        down = np.maximum(0, delay_close - low)\n        return self.ts_sum(up, 20) / (self.ts_sum(down, 20) + 1e-8) * 100",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha111",
    "category": "alpha_factor",
    "formula": "inner1 - inner2",
    "explanation": "Difference of volume-weighted stochastic SMAs",
    "python_code": "def alpha111(self, high: pd.Series, low: pd.Series, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"Difference of volume-weighted stochastic SMAs\"\"\"\n        mid = (high + low + close) / 3\n        inner1 = self.sma(volume * (mid - self.delay(mid, 1)), 11, 2)\n        inner2 = self.sma(volume * (mid - self.delay(mid, 1)), 4, 2)\n        return inner1 - inner2",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha112",
    "category": "alpha_factor",
    "formula": "(sum_up - sum_down) / (sum_up + sum_down + 1e-8)",
    "explanation": "Up momentum ratio to total momentum",
    "python_code": "def alpha112(self, close: pd.Series) -> pd.Series:\n        \"\"\"Up momentum ratio to total momentum\"\"\"\n        delta = close - self.delay(close, 1)\n        up = np.where(delta > 0, delta, 0)\n        down = np.where(delta < 0, -delta, 0)\n        sum_up = self.ts_sum(pd.Series(up, index=close.index), 12)\n        sum_down = self.ts_sum(pd.Series(down, index=close.index), 12)\n        return (sum_up - sum_down) / (sum_up + sum_down + 1e-8)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha113",
    "category": "alpha_factor",
    "formula": "-1 * self.rank(self.ts_sum(self.delay(close, 5), 20) / 20) * \\",
    "explanation": "(-1 * RANK(delayed average) * CORR(...) * RANK(CORR(...)))",
    "python_code": "def alpha113(self, close: pd.Series, volume: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"(-1 * RANK(delayed average) * CORR(...) * RANK(CORR(...)))\"\"\"\n        adv30 = self.ts_mean(volume, 30)\n        return -1 * self.rank(self.ts_sum(self.delay(close, 5), 20) / 20) * \\\n               self.correlation(close, volume + 1, 2) * \\\n               self.rank(self.correlation(self.ts_sum(close, 5), self.ts_sum(close, 20), 2))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha114",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Complex normalization of ranked ranges and volume",
    "python_code": "def alpha114(self, high: pd.Series, low: pd.Series, close: pd.Series, vwap: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"Complex normalization of ranked ranges and volume\"\"\"\n        adv20 = self.ts_mean(volume, 20)\n        return self.rank(self.delay(high - low, 2)) * \\\n               self.rank(self.rank(volume + 1)) / \\\n               (high - low + 1e-8) / (adv20 + 1e-8) * -1",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha115",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "RANK(correlation)^RANK(correlation)",
    "python_code": "def alpha115(self, high: pd.Series, low: pd.Series, volume: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"RANK(correlation)^RANK(correlation)\"\"\"\n        adv30 = self.ts_mean(volume, 30)\n        corr1 = self.correlation(high * 0.9 + close * 0.1, adv30, 10)\n        corr2 = self.correlation(self.ts_rank(high + low, 4), self.ts_rank(volume + 1, 10), 7)\n        return self.rank(corr1) ** self.rank(corr2).clip(-2, 2)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha116",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "REGBETA(CLOSE, SEQUENCE(20))",
    "python_code": "def alpha116(self, close: pd.Series) -> pd.Series:\n        \"\"\"REGBETA(CLOSE, SEQUENCE(20))\"\"\"\n        return self.regbeta(close, close, 20)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha117",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Product of three Tsrank components",
    "python_code": "def alpha117(self, high: pd.Series, low: pd.Series, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"Product of three Tsrank components\"\"\"\n        return self.ts_rank(volume + 1, 32) * \\\n               (1 - self.ts_rank((close + high - low), 16)) * \\\n               (1 - self.ts_rank(close.pct_change(), 32))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha118",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SUM(HIGH - OPEN, 20) / SUM(OPEN - LOW, 20) * 100",
    "python_code": "def alpha118(self, high: pd.Series, low: pd.Series, open_: pd.Series) -> pd.Series:\n        \"\"\"SUM(HIGH - OPEN, 20) / SUM(OPEN - LOW, 20) * 100\"\"\"\n        return self.ts_sum(high - open_, 20) / (self.ts_sum(open_ - low, 20) + 1e-8) * 100",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha119",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Difference of complex decay-linear measures",
    "python_code": "def alpha119(self, open_: pd.Series, vwap: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"Difference of complex decay-linear measures\"\"\"\n        adv60 = self.ts_mean(volume, 60)\n        inner1 = self.decay_linear(self.correlation(vwap, self.ts_sum(adv60, 9), 6), 4)\n        inner2 = self.decay_linear(self.rank(self.ts_argmin(self.correlation(self.rank(open_), self.rank(adv60), 21), 9)), 7)\n        return self.rank(inner1) - self.rank(inner2)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha120",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "RANK(VWAP - CLOSE) / RANK(VWAP + CLOSE)",
    "python_code": "def alpha120(self, vwap: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"RANK(VWAP - CLOSE) / RANK(VWAP + CLOSE)\"\"\"\n        return self.rank(vwap - close) / (self.rank(vwap + close) + 1e-8)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha121",
    "category": "alpha_factor",
    "formula": "(self.rank(vwap - self.ts_min(vwap, 12)) **",
    "explanation": "(RANK(VWAP - TSMIN(VWAP, 12))^TSRANK(...)) * -1",
    "python_code": "def alpha121(self, vwap: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(RANK(VWAP - TSMIN(VWAP, 12))^TSRANK(...)) * -1\"\"\"\n        adv60 = self.ts_mean(volume, 60)\n        return (self.rank(vwap - self.ts_min(vwap, 12)) **\n                self.ts_rank(self.correlation(self.ts_rank(vwap, 20), self.ts_rank(adv60, 2), 18), 3).clip(-2, 2)) * -1",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha122",
    "category": "alpha_factor",
    "formula": "sma13 / (self.delay(sma13, 1) + 1e-8) - 1",
    "explanation": "Triple SMA log derivative normalized",
    "python_code": "def alpha122(self, close: pd.Series) -> pd.Series:\n        \"\"\"Triple SMA log derivative normalized\"\"\"\n        sma13 = self.sma(self.sma(self.sma(self.log(close), 13, 2), 13, 2), 13, 2)\n        return sma13 / (self.delay(sma13, 1) + 1e-8) - 1",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha123",
    "category": "alpha_factor",
    "formula": "np.where(self.rank(corr1) < self.rank(corr2), -1, 0)",
    "explanation": "Conditional: if correlation rank1 < rank2 then -1 else 0",
    "python_code": "def alpha123(self, high: pd.Series, low: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"Conditional: if correlation rank1 < rank2 then -1 else 0\"\"\"\n        adv20 = self.ts_mean(volume, 20)\n        corr1 = self.correlation(self.ts_sum((high + low) / 2, 20), self.ts_sum(adv20, 20), 9)\n        corr2 = self.correlation(low, volume + 1, 6)\n        return np.where(self.rank(corr1) < self.rank(corr2), -1, 0)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha124",
    "category": "alpha_factor",
    "formula": "(close - vwap) / (self.decay_linear(self.rank(self.ts_max(close, 30)), 2) + 1e-8)",
    "explanation": "(CLOSE - VWAP) / DECAYLINEAR(RANK(TSMAX(CLOSE, 30)), 2)",
    "python_code": "def alpha124(self, close: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"(CLOSE - VWAP) / DECAYLINEAR(RANK(TSMAX(CLOSE, 30)), 2)\"\"\"\n        return (close - vwap) / (self.decay_linear(self.rank(self.ts_max(close, 30)), 2) + 1e-8)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha125",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Ratio of decay-linear correlation to decay-linear delta",
    "python_code": "def alpha125(self, close: pd.Series, vwap: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"Ratio of decay-linear correlation to decay-linear delta\"\"\"\n        adv80 = self.ts_mean(volume, 80)\n        inner1 = self.decay_linear(self.correlation(vwap, adv80, 17), 4)\n        inner2 = self.decay_linear(self.delta(close * 0.5 + vwap * 0.5, 3), 16)\n        return self.rank(inner1) / (self.rank(inner2) + 1e-8)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha126",
    "category": "alpha_factor",
    "formula": "(close + high + low) / 3",
    "explanation": "(CLOSE + HIGH + LOW) / 3",
    "python_code": "def alpha126(self, high: pd.Series, low: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"(CLOSE + HIGH + LOW) / 3\"\"\"\n        return (close + high + low) / 3",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha127",
    "category": "alpha_factor",
    "formula": "np.sqrt(self.ts_mean(inner, 12))",
    "explanation": "sqrt(MEAN((100 * price deviation / TSMAX)^2, 12))",
    "python_code": "def alpha127(self, close: pd.Series) -> pd.Series:\n        \"\"\"sqrt(MEAN((100 * price deviation / TSMAX)^2, 12))\"\"\"\n        max12 = self.ts_max(close, 12)\n        inner = (100 * (close / (max12 + 1e-8) - 1)) ** 2\n        return np.sqrt(self.ts_mean(inner, 12))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha128",
    "category": "alpha_factor",
    "formula": "100 - 100 / (1 + sum_up / (sum_down + 1e-8))",
    "explanation": "RSI-like formula using volume-weighted midpoints",
    "python_code": "def alpha128(self, high: pd.Series, low: pd.Series, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"RSI-like formula using volume-weighted midpoints\"\"\"\n        mid = (high + low + close) / 3\n        delta = mid - self.delay(mid, 1)\n        up = np.where(delta > 0, delta * volume, 0)\n        down = np.where(delta < 0, -delta * volume, 0)\n        sum_up = self.ts_sum(pd.Series(up, index=close.index), 14)\n        sum_down = self.ts_sum(pd.Series(down, index=close.index), 14)\n        return 100 - 100 / (1 + sum_up / (sum_down + 1e-8))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha129",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SUM(ABS(negative deltas), 12)",
    "python_code": "def alpha129(self, close: pd.Series) -> pd.Series:\n        \"\"\"SUM(ABS(negative deltas), 12)\"\"\"\n        delta = close - self.delay(close, 1)\n        return self.ts_sum(np.where(delta < 0, self.abs_(delta), 0), 12)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha130",
    "category": "alpha_factor",
    "formula": "inner1 / (inner2 + 1e-8)",
    "explanation": "Ratio of correlation-based decay-linear ranks",
    "python_code": "def alpha130(self, high: pd.Series, low: pd.Series, volume: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"Ratio of correlation-based decay-linear ranks\"\"\"\n        adv40 = self.ts_mean(volume, 40)\n        inner1 = self.decay_linear(self.rank((high + low) / 2), 5) * self.decay_linear(self.ts_rank(self.correlation(adv40, high, 5), 19), 17)\n        inner2 = self.rank(self.decay_linear(self.delta((close * 0.68 + low * 0.32), 2), 19))\n        return inner1 / (inner2 + 1e-8)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha131",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "RANK(DELTA(VWAP, 1))^TSRANK(CORR(...), 18)",
    "python_code": "def alpha131(self, vwap: pd.Series, volume: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"RANK(DELTA(VWAP, 1))^TSRANK(CORR(...), 18)\"\"\"\n        adv10 = self.ts_mean(volume, 10)\n        return self.rank(self.delta(vwap, 1)) ** self.ts_rank(self.correlation(close, adv10, 5), 18).clip(-2, 2)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha132",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "MEAN(AMOUNT, 20)",
    "python_code": "def alpha132(self, amount: pd.Series) -> pd.Series:\n        \"\"\"MEAN(AMOUNT, 20)\"\"\"\n        return self.ts_mean(amount, 20)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha133",
    "category": "alpha_factor",
    "formula": "((20 - self.ts_argmax(high, 20)) / 20) * 100 - \\",
    "explanation": "((20 - HIGHDAY(HIGH, 20)) / 20) * 100 - ((20 - LOWDAY(LOW, 20)) / 20) * 100",
    "python_code": "def alpha133(self, high: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"((20 - HIGHDAY(HIGH, 20)) / 20) * 100 - ((20 - LOWDAY(LOW, 20)) / 20) * 100\"\"\"\n        return ((20 - self.ts_argmax(high, 20)) / 20) * 100 - \\\n               ((20 - self.ts_argmin(low, 20)) / 20) * 100",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha134",
    "category": "alpha_factor",
    "formula": "(close - self.delay(close, 12)) / (self.delay(close, 12) + 1e-8) * volume",
    "explanation": "(CLOSE - DELAY(CLOSE, 12)) / DELAY(CLOSE, 12) * VOLUME",
    "python_code": "def alpha134(self, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(CLOSE - DELAY(CLOSE, 12)) / DELAY(CLOSE, 12) * VOLUME\"\"\"\n        return (close - self.delay(close, 12)) / (self.delay(close, 12) + 1e-8) * volume",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha135",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SMA(DELAY(price ratio, 1), 20, 1)",
    "python_code": "def alpha135(self, close: pd.Series) -> pd.Series:\n        \"\"\"SMA(DELAY(price ratio, 1), 20, 1)\"\"\"\n        ratio = close / (self.delay(close, 20) + 1e-8)\n        return self.sma(self.delay(ratio, 1), 20, 1)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha136",
    "category": "alpha_factor",
    "formula": "-1 * self.rank(self.delta(returns, 3)) * self.correlation(open_, volume + 1, 10)",
    "explanation": "(-1 * RANK(DELTA(RET, 3))) * CORR(OPEN, VOLUME, 10)",
    "python_code": "def alpha136(self, open_: pd.Series, volume: pd.Series, returns: pd.Series) -> pd.Series:\n        \"\"\"(-1 * RANK(DELTA(RET, 3))) * CORR(OPEN, VOLUME, 10)\"\"\"\n        return -1 * self.rank(self.delta(returns, 3)) * self.correlation(open_, volume + 1, 10)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha137",
    "category": "alpha_factor",
    "formula": "inner1 / (inner2 + 1e-8) * np.maximum(self.abs_(high - self.delay(close, 1)), self.abs_(low - self.delay(close, 1)))",
    "explanation": "Complex true range normalization with volume",
    "python_code": "def alpha137(self, close: pd.Series, high: pd.Series, low: pd.Series, open_: pd.Series) -> pd.Series:\n        \"\"\"Complex true range normalization with volume\"\"\"\n        delta = close - self.delay(close, 1)\n        inner1 = 16 * (close - self.delay(close, 1) + (close - open_) / 2 + self.delay(close, 1) - self.delay(open_, 1))\n        inner2 = np.maximum(self.abs_(high - self.delay(close, 1)), np.maximum(self.abs_(low - self.delay(close, 1)), self.abs_(high - low)))\n        return inner1 / (inner2 + 1e-8) * np.maximum(self.abs_(high - self.delay(close, 1)), self.abs_(low - self.delay(close, 1)))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha138",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Complex nested decay-linear and Tsrank formula",
    "python_code": "def alpha138(self, low: pd.Series, volume: pd.Series, vwap: pd.Series) -> pd.Series:\n        \"\"\"Complex nested decay-linear and Tsrank formula\"\"\"\n        adv15 = self.ts_mean(volume, 15)\n        inner = self.decay_linear(self.correlation(low, adv15, 7), 5)\n        return self.rank(inner) - self.rank(self.ts_rank(self.decay_linear(self.correlation(self.rank(vwap), self.rank(volume + 1), 6), 4), 17))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha139",
    "category": "alpha_factor",
    "formula": "-1 * self.correlation(open_, volume + 1, 10)",
    "explanation": "(-1 * CORR(OPEN, VOLUME, 10))",
    "python_code": "def alpha139(self, open_: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(-1 * CORR(OPEN, VOLUME, 10))\"\"\"\n        return -1 * self.correlation(open_, volume + 1, 10)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha140",
    "category": "alpha_factor",
    "formula": "np.minimum(self.rank(inner1), inner2)",
    "explanation": "MIN(RANK(DECAYLINEAR(...)), TSRANK(...))",
    "python_code": "def alpha140(self, close: pd.Series, high: pd.Series, low: pd.Series, open_: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"MIN(RANK(DECAYLINEAR(...)), TSRANK(...))\"\"\"\n        adv10 = self.ts_mean(volume, 10)\n        inner1 = self.decay_linear(self.rank(open_ + low - 2 * high), 8)\n        inner2 = self.ts_rank(self.decay_linear(self.correlation(self.ts_rank(close, 8), self.ts_rank(adv10, 17), 6), 4), 3)\n        return np.minimum(self.rank(inner1), inner2)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha141",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "RANK(CORR(RANK(HIGH), RANK(MEAN(VOLUME, 15)), 9)) * -1",
    "python_code": "def alpha141(self, high: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"RANK(CORR(RANK(HIGH), RANK(MEAN(VOLUME, 15)), 9)) * -1\"\"\"\n        return self.rank(self.correlation(self.rank(high), self.rank(self.ts_mean(volume, 15)), 9)) * -1",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha142",
    "category": "alpha_factor",
    "formula": "-1 * self.rank(self.ts_rank(close, 10)) * \\",
    "explanation": "(-1 * RANK(TSRANK(CLOSE, 10))) * RANK(second delta) * RANK(...)",
    "python_code": "def alpha142(self, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(-1 * RANK(TSRANK(CLOSE, 10))) * RANK(second delta) * RANK(...)\"\"\"\n        return -1 * self.rank(self.ts_rank(close, 10)) * \\\n               self.rank(self.delta(self.delta(close, 1), 1)) * \\\n               self.rank(self.ts_rank(volume / (self.ts_mean(volume, 20) + 1e-8), 5))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha143",
    "category": "alpha_factor",
    "formula": "0",
    "explanation": "Returns 0 (unimplemented)",
    "python_code": "def alpha143(self) -> float:\n        \"\"\"Returns 0 (unimplemented)\"\"\"\n        return 0",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha144",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SUMIF(normalized volume-price, 20, condition) / COUNT(condition, 20)",
    "python_code": "def alpha144(self, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"SUMIF(normalized volume-price, 20, condition) / COUNT(condition, 20)\"\"\"\n        cond = close < self.delay(close, 1)\n        inner = self.abs_(close / (self.delay(close, 1) + 1e-8) - 1) / (volume + 1e-8)\n        return self.ts_sum(np.where(cond, inner, 0), 20) / (self.ts_count(cond, 20) + 1e-8)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha145",
    "category": "alpha_factor",
    "formula": "(self.ts_mean(volume, 9) - self.ts_mean(volume, 26)) / (self.ts_mean(volume, 12) + 1e-8) * 100",
    "explanation": "(MEAN(VOL, 9) - MEAN(VOL, 26)) / MEAN(VOL, 12) * 100",
    "python_code": "def alpha145(self, volume: pd.Series) -> pd.Series:\n        \"\"\"(MEAN(VOL, 9) - MEAN(VOL, 26)) / MEAN(VOL, 12) * 100\"\"\"\n        return (self.ts_mean(volume, 9) - self.ts_mean(volume, 26)) / (self.ts_mean(volume, 12) + 1e-8) * 100",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha146",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Complex deviation from SMA normalized ratio",
    "python_code": "def alpha146(self, close: pd.Series) -> pd.Series:\n        \"\"\"Complex deviation from SMA normalized ratio\"\"\"\n        mean6 = self.ts_mean(close, 6)\n        inner = (close - mean6) / mean6\n        inner2 = self.delay(inner, 2) + self.delay(inner, 4)\n        return self.ts_mean(inner - inner2, 61)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha147",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "REGBETA(MEAN(CLOSE, 12), SEQUENCE(12))",
    "python_code": "def alpha147(self, close: pd.Series) -> pd.Series:\n        \"\"\"REGBETA(MEAN(CLOSE, 12), SEQUENCE(12))\"\"\"\n        return self.regbeta(self.ts_mean(close, 12), close, 12)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha148",
    "category": "alpha_factor",
    "formula": "np.where(self.rank(corr) < self.rank(open_ - self.ts_min(open_, 14)), -1, 0)",
    "explanation": "Conditional: if correlation rank < open rank then -1 else 0",
    "python_code": "def alpha148(self, open_: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"Conditional: if correlation rank < open rank then -1 else 0\"\"\"\n        adv60 = self.ts_mean(volume, 60)\n        corr = self.correlation(open_, self.ts_sum(adv60, 9), 6)\n        return np.where(self.rank(corr) < self.rank(open_ - self.ts_min(open_, 14)), -1, 0)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha149",
    "category": "alpha_factor",
    "formula": "0",
    "explanation": "Returns 0 (filter-based, incomplete)",
    "python_code": "def alpha149(self) -> float:\n        \"\"\"Returns 0 (filter-based, incomplete)\"\"\"\n        return 0",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha150",
    "category": "alpha_factor",
    "formula": "(close + high + low) / 3 * volume",
    "explanation": "(CLOSE + HIGH + LOW) / 3 * VOLUME",
    "python_code": "def alpha150(self, high: pd.Series, low: pd.Series, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(CLOSE + HIGH + LOW) / 3 * VOLUME\"\"\"\n        return (close + high + low) / 3 * volume",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha151",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SMA(CLOSE - DELAY(CLOSE, 20), 20, 1)",
    "python_code": "def alpha151(self, close: pd.Series) -> pd.Series:\n        \"\"\"SMA(CLOSE - DELAY(CLOSE, 20), 20, 1)\"\"\"\n        return self.sma(close - self.delay(close, 20), 20, 1)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha152",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Complex nested SMA and delayed formulas",
    "python_code": "def alpha152(self, close: pd.Series) -> pd.Series:\n        \"\"\"Complex nested SMA and delayed formulas\"\"\"\n        inner = self.delay(self.sma(self.delay(close / (self.delay(close, 9) + 1e-8), 1), 9, 1), 1)\n        return self.sma(inner, 12, 1)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha153",
    "category": "alpha_factor",
    "formula": "(self.ts_mean(close, 3) + self.ts_mean(close, 6) +",
    "explanation": "(MEAN(CLOSE, 3) + MEAN(CLOSE, 6) + MEAN(CLOSE, 12) + MEAN(CLOSE, 24)) / 4",
    "python_code": "def alpha153(self, close: pd.Series) -> pd.Series:\n        \"\"\"(MEAN(CLOSE, 3) + MEAN(CLOSE, 6) + MEAN(CLOSE, 12) + MEAN(CLOSE, 24)) / 4\"\"\"\n        return (self.ts_mean(close, 3) + self.ts_mean(close, 6) +\n                self.ts_mean(close, 12) + self.ts_mean(close, 24)) / 4",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha154",
    "category": "alpha_factor",
    "formula": "np.where(cond, 1, 0)",
    "explanation": "Conditional: if (VWAP - TSMIN) < CORR then 1 else 0",
    "python_code": "def alpha154(self, vwap: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"Conditional: if (VWAP - TSMIN) < CORR then 1 else 0\"\"\"\n        adv40 = self.ts_mean(volume, 40)\n        cond = (vwap - self.ts_min(vwap, 16)) < self.correlation(vwap, adv40, 18)\n        return np.where(cond, 1, 0)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha155",
    "category": "alpha_factor",
    "formula": "dif - dea",
    "explanation": "MACD-like volume formula",
    "python_code": "def alpha155(self, volume: pd.Series) -> pd.Series:\n        \"\"\"MACD-like volume formula\"\"\"\n        sma13 = self.sma(volume, 13, 2)\n        sma27 = self.sma(volume, 27, 2)\n        dif = sma13 - sma27\n        dea = self.sma(dif, 10, 2)\n        return dif - dea",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha156",
    "category": "alpha_factor",
    "formula": "np.maximum(self.rank(inner1), self.rank(inner2)) * -1",
    "explanation": "MAX(RANK(DECAYLINEAR(...)), RANK(...)) * -1",
    "python_code": "def alpha156(self, vwap: pd.Series, open_: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"MAX(RANK(DECAYLINEAR(...)), RANK(...)) * -1\"\"\"\n        inner1 = self.decay_linear(self.delta(vwap, 5), 3)\n        inner2 = self.decay_linear(self.delta(open_ * 0.15 + vwap * 0.85, 2), 3) - \\\n                 self.ts_rank(self.decay_linear(self.ts_rank(self.correlation(self.ts_sum(close, 48), self.ts_sum(self.ts_mean(volume, 60), 48), 9), 7), 4), 18)\n        return np.maximum(self.rank(inner1), self.rank(inner2)) * -1",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha157",
    "category": "alpha_factor",
    "formula": "np.minimum(self.rank(self.rank(inner)), 5) + self.ts_rank(self.delay(-1 * returns, 6), 5)",
    "explanation": "Complex nested products and log transformations",
    "python_code": "def alpha157(self, close: pd.Series, returns: pd.Series) -> pd.Series:\n        \"\"\"Complex nested products and log transformations\"\"\"\n        inner = self.log(self.ts_sum(returns, 11))\n        return np.minimum(self.rank(self.rank(inner)), 5) + self.ts_rank(self.delay(-1 * returns, 6), 5)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha158",
    "category": "alpha_factor",
    "formula": "((high - sma15) - (low - sma15)) / (close + 1e-8)",
    "explanation": "((HIGH - SMA(CLOSE, 15, 2)) - (LOW - SMA(CLOSE, 15, 2))) / CLOSE",
    "python_code": "def alpha158(self, high: pd.Series, low: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"((HIGH - SMA(CLOSE, 15, 2)) - (LOW - SMA(CLOSE, 15, 2))) / CLOSE\"\"\"\n        sma15 = self.sma(close, 15, 2)\n        return ((high - sma15) - (low - sma15)) / (close + 1e-8)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": "Qlib (Microsoft) Alpha158 Factor Library",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha159",
    "category": "alpha_factor",
    "formula": "(stoch6 + stoch12 * 2 + stoch24 * 3) / 6",
    "explanation": "Weighted multi-period Stochastic formula",
    "python_code": "def alpha159(self, close: pd.Series, high: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"Weighted multi-period Stochastic formula\"\"\"\n        llv6 = self.ts_min(low, 6)\n        hhv6 = self.ts_max(high, 6)\n        llv12 = self.ts_min(low, 12)\n        hhv12 = self.ts_max(high, 12)\n        llv24 = self.ts_min(low, 24)\n        hhv24 = self.ts_max(high, 24)\n\n        stoch6 = (close - llv6) / (hhv6 - llv6 + 1e-8)\n        stoch12 = (close - llv12) / (hhv12 - llv12 + 1e-8)\n        stoch24 = (close - llv24) / (hhv24 - llv24 + 1e-8)\n\n        return (stoch6 + stoch12 * 2 + stoch24 * 3) / 6",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha160",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SMA(conditional STD, 20, 1)",
    "python_code": "def alpha160(self, close: pd.Series) -> pd.Series:\n        \"\"\"SMA(conditional STD, 20, 1)\"\"\"\n        cond = close < self.delay(close, 1)\n        return self.sma(np.where(cond, self.ts_std(close, 20), 0), 20, 1)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha161",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "MEAN(MAX series, 12)",
    "python_code": "def alpha161(self, close: pd.Series, high: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"MEAN(MAX series, 12)\"\"\"\n        tr = np.maximum(high - low, np.maximum(\n            self.abs_(high - self.delay(close, 1)),\n            self.abs_(low - self.delay(close, 1))\n        ))\n        return self.ts_mean(tr, 12)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha162",
    "category": "alpha_factor",
    "formula": "inner / (self.delay((max_delta / (abs_delta + 1e-8)) * 100, 1) + 1e-8)",
    "explanation": "Normalized oscillator ratio formula",
    "python_code": "def alpha162(self, close: pd.Series) -> pd.Series:\n        \"\"\"Normalized oscillator ratio formula\"\"\"\n        delta = close - self.delay(close, 1)\n        max_delta = self.sma(np.maximum(delta, 0), 12, 1)\n        abs_delta = self.sma(self.abs_(delta), 12, 1)\n\n        inner = (max_delta / (abs_delta + 1e-8)) * 100 - \\\n                (self.delay((max_delta / (abs_delta + 1e-8)) * 100, 1))\n        return inner / (self.delay((max_delta / (abs_delta + 1e-8)) * 100, 1) + 1e-8)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha163",
    "category": "alpha_factor",
    "formula": "* volume * VWAP * spread)\"\"\"",
    "explanation": "RANK(return * volume * VWAP * spread)",
    "python_code": "def alpha163(self, close: pd.Series, high: pd.Series, low: pd.Series, vwap: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"RANK(return * volume * VWAP * spread)\"\"\"\n        returns = close.pct_change()\n        return self.rank(-1 * returns * self.ts_mean(volume, 20) * vwap * (high - low))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha164",
    "category": "alpha_factor",
    "formula": "np.where(",
    "explanation": "Complex inverse-price stochastic formula",
    "python_code": "def alpha164(self, close: pd.Series, high: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"Complex inverse-price stochastic formula\"\"\"\n        cond = close > self.delay(close, 1)\n        return np.where(\n            cond,\n            1 / (close - self.ts_min(low, 12) + 1e-8),\n            0\n        ) + self.sma(\n            np.where(cond,\n                    (close - self.ts_min(low, 12)) / (self.ts_max(high, 12) - self.ts_min(low, 12) + 1e-8),\n                    0),\n            13, 2\n        )",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha165",
    "category": "alpha_factor",
    "formula": "(self.ts_max(close, 48) - self.ts_min(close, 48)) / (self.ts_std(close, 48) + 1e-8)",
    "explanation": "(ROWMAX - ROWMIN) / STD scaling",
    "python_code": "def alpha165(self, close: pd.Series) -> pd.Series:\n        \"\"\"(ROWMAX - ROWMIN) / STD scaling\"\"\"\n        return (self.ts_max(close, 48) - self.ts_min(close, 48)) / (self.ts_std(close, 48) + 1e-8)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha166",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Skewness-like calculation",
    "python_code": "def alpha166(self, close: pd.Series) -> pd.Series:\n        \"\"\"Skewness-like calculation\"\"\"\n        returns = close.pct_change()\n        return self.rank(self.ts_sum(returns, 10)) - self.rank(self.ts_mean(returns, 5))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha167",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SUM(positive deltas, 12)",
    "python_code": "def alpha167(self, close: pd.Series) -> pd.Series:\n        \"\"\"SUM(positive deltas, 12)\"\"\"\n        delta = close - self.delay(close, 1)\n        return self.ts_sum(np.where(delta > 0, delta, 0), 12)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha168",
    "category": "alpha_factor",
    "formula": "-1 * volume / (self.ts_mean(volume, 20) + 1e-8)",
    "explanation": "(-1 * VOLUME) / MEAN(VOLUME, 20)",
    "python_code": "def alpha168(self, volume: pd.Series) -> pd.Series:\n        \"\"\"(-1 * VOLUME) / MEAN(VOLUME, 20)\"\"\"\n        return -1 * volume / (self.ts_mean(volume, 20) + 1e-8)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha169",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Complex nested delayed SMA differences",
    "python_code": "def alpha169(self, close: pd.Series) -> pd.Series:\n        \"\"\"Complex nested delayed SMA differences\"\"\"\n        mean6 = self.ts_mean(close, 6)\n        inner = self.sma(mean6 - self.delay(mean6, 1), 9, 1)\n        return self.sma(inner - self.delay(inner, 1), 12, 1)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha170",
    "category": "alpha_factor",
    "formula": "(self.rank(1 / close) * volume / adv20) * \\",
    "explanation": "Multi-component ranked formula",
    "python_code": "def alpha170(self, high: pd.Series, volume: pd.Series, vwap: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"Multi-component ranked formula\"\"\"\n        adv20 = self.ts_mean(volume, 20)\n        return (self.rank(1 / close) * volume / adv20) * \\\n               (high * self.rank(high - close) / (self.ts_sum(high, 5) / 5 + 1e-8)) - \\\n               self.rank(vwap - self.delay(vwap, 5))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha171",
    "category": "alpha_factor",
    "formula": "-1 * (low - close) * (open_ ** 5) / ((close - high + 1e-8) * (close ** 5 + 1e-8))",
    "explanation": "(-1 * (LOW - CLOSE) * (OPEN^5)) / ((CLOSE - HIGH) * (CLOSE^5))",
    "python_code": "def alpha171(self, open_: pd.Series, close: pd.Series, high: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"(-1 * (LOW - CLOSE) * (OPEN^5)) / ((CLOSE - HIGH) * (CLOSE^5))\"\"\"\n        return -1 * (low - close) * (open_ ** 5) / ((close - high + 1e-8) * (close ** 5 + 1e-8))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha172",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "ADX-like directional measurement",
    "python_code": "def alpha172(self, high: pd.Series, low: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"ADX-like directional measurement\"\"\"\n        tr = np.maximum(high - low, np.maximum(\n            self.abs_(high - self.delay(close, 1)),\n            self.abs_(low - self.delay(close, 1))\n        ))\n        hd = high - self.delay(high, 1)\n        ld = self.delay(low, 1) - low\n\n        dmp = np.where((hd > 0) & (hd > ld), hd, 0)\n        dmm = np.where((ld > 0) & (ld > hd), ld, 0)\n\n        atr14 = self.ts_mean(tr, 14)\n        pdi = self.ts_mean(pd.Series(dmp, index=high.index), 14) / (atr14 + 1e-8) * 100\n        mdi = self.ts_mean(pd.Series(dmm, index=high.index), 14) / (atr14 + 1e-8) * 100\n\n        return self.ts_mean(self.abs_(pdi - mdi) / (pdi + mdi + 1e-8) * 100, 6)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha173",
    "category": "alpha_factor",
    "formula": "3 * self.sma(close, 13, 2) - 2 * self.sma(self.sma(close, 13, 2), 13, 2) + \\",
    "explanation": "Triple SMA triple log formula",
    "python_code": "def alpha173(self, close: pd.Series) -> pd.Series:\n        \"\"\"Triple SMA triple log formula\"\"\"\n        return 3 * self.sma(close, 13, 2) - 2 * self.sma(self.sma(close, 13, 2), 13, 2) + \\\n               self.sma(self.sma(self.sma(self.log(close), 13, 2), 13, 2), 13, 2)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha174",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SMA(conditional STD, 20, 1) - variant",
    "python_code": "def alpha174(self, close: pd.Series) -> pd.Series:\n        \"\"\"SMA(conditional STD, 20, 1) - variant\"\"\"\n        cond = close > self.delay(close, 1)\n        return self.sma(np.where(cond, self.ts_std(close, 20), 0), 20, 1)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha175",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "MEAN(MAX series, 6)",
    "python_code": "def alpha175(self, high: pd.Series, low: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"MEAN(MAX series, 6)\"\"\"\n        tr = np.maximum(high - low, np.maximum(\n            self.abs_(high - self.delay(close, 1)),\n            self.abs_(low - self.delay(close, 1))\n        ))\n        return self.ts_mean(tr, 6)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha176",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "CORR(RANK(Stochastic), RANK(VOLUME), 6)",
    "python_code": "def alpha176(self, high: pd.Series, low: pd.Series, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"CORR(RANK(Stochastic), RANK(VOLUME), 6)\"\"\"\n        llv12 = self.ts_min(low, 12)\n        hhv12 = self.ts_max(high, 12)\n        stoch = (close - llv12) / (hhv12 - llv12 + 1e-8)\n        return self.correlation(self.rank(stoch), self.rank(self.ts_mean(volume, 6)), 6)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha177",
    "category": "alpha_factor",
    "formula": "((20 - self.ts_argmax(high, 20)) / 20) * 100",
    "explanation": "((20 - HIGHDAY(HIGH, 20)) / 20) * 100",
    "python_code": "def alpha177(self, high: pd.Series) -> pd.Series:\n        \"\"\"((20 - HIGHDAY(HIGH, 20)) / 20) * 100\"\"\"\n        return ((20 - self.ts_argmax(high, 20)) / 20) * 100",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha178",
    "category": "alpha_factor",
    "formula": "(close - self.delay(close, 1)) / (self.delay(close, 1) + 1e-8) * volume",
    "explanation": "(CLOSE - DELAY(CLOSE, 1)) / DELAY(CLOSE, 1) * VOLUME",
    "python_code": "def alpha178(self, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(CLOSE - DELAY(CLOSE, 1)) / DELAY(CLOSE, 1) * VOLUME\"\"\"\n        return (close - self.delay(close, 1)) / (self.delay(close, 1) + 1e-8) * volume",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha179",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Product of volume and volume-based correlation ranks",
    "python_code": "def alpha179(self, low: pd.Series, vwap: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"Product of volume and volume-based correlation ranks\"\"\"\n        adv50 = self.ts_mean(volume, 50)\n        return self.rank(self.correlation(vwap, volume + 1, 4)) * \\\n               self.rank(self.correlation(self.rank(low), self.rank(adv50), 12))",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha180",
    "category": "alpha_factor",
    "formula": "np.where(cond, -1 * self.ts_rank(self.abs_(self.delta(close, 7)), 60) * self.sign(self.delta(close, 7)), -volume)",
    "explanation": "Conditional: if MA(VOL, 20) < VOL then Tsrank else -VOL",
    "python_code": "def alpha180(self, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"Conditional: if MA(VOL, 20) < VOL then Tsrank else -VOL\"\"\"\n        mean20 = self.ts_mean(volume, 20)\n        cond = mean20 < volume\n        return np.where(cond, -1 * self.ts_rank(self.abs_(self.delta(close, 7)), 60) * self.sign(self.delta(close, 7)), -volume)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha181",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Return deviation correlation versus benchmark",
    "python_code": "def alpha181(self, close: pd.Series, returns: pd.Series) -> pd.Series:\n        \"\"\"Return deviation correlation versus benchmark\"\"\"\n        # Simplified - using close as benchmark proxy\n        bench_ret = returns\n        return self.ts_sum(\n            (returns - self.ts_mean(returns, 20)) -\n            (bench_ret - self.ts_mean(bench_ret, 20)) ** 2,\n            20\n        )",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha182",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "COUNT(matched directions, 20) / 20",
    "python_code": "def alpha182(self, close: pd.Series, open_: pd.Series, returns: pd.Series) -> pd.Series:\n        \"\"\"COUNT(matched directions, 20) / 20\"\"\"\n        # Simplified correlation\n        cond = (close > open_) == (returns > 0)\n        return self.ts_count(cond, 20) / 20",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha183",
    "category": "alpha_factor",
    "formula": "(self.ts_max(close, 24) - self.ts_min(close, 24)) / (self.ts_std(close, 24) + 1e-8)",
    "explanation": "(ROWMAX - ROWMIN) / STD with 24-period window",
    "python_code": "def alpha183(self, close: pd.Series) -> pd.Series:\n        \"\"\"(ROWMAX - ROWMIN) / STD with 24-period window\"\"\"\n        return (self.ts_max(close, 24) - self.ts_min(close, 24)) / (self.ts_std(close, 24) + 1e-8)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha184",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "RANK(CORR(delayed spread, CLOSE, 200)) + RANK(spread)",
    "python_code": "def alpha184(self, close: pd.Series, open_: pd.Series) -> pd.Series:\n        \"\"\"RANK(CORR(delayed spread, CLOSE, 200)) + RANK(spread)\"\"\"\n        spread = open_ - close\n        return self.rank(self.correlation(self.delay(spread, 1), close, 200)) + self.rank(spread)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha185",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "RANK((-1 * ((1 - OPEN / CLOSE)^2)))",
    "python_code": "def alpha185(self, close: pd.Series, open_: pd.Series) -> pd.Series:\n        \"\"\"RANK((-1 * ((1 - OPEN / CLOSE)^2)))\"\"\"\n        return self.rank(-1 * (1 - open_ / (close + 1e-8)) ** 2)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha186",
    "category": "alpha_factor",
    "formula": "(adx + self.delay(adx, 6)) / 2",
    "explanation": "ADX-like measurement (averaged with delay)",
    "python_code": "def alpha186(self, high: pd.Series, low: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"ADX-like measurement (averaged with delay)\"\"\"\n        tr = np.maximum(high - low, np.maximum(\n            self.abs_(high - self.delay(close, 1)),\n            self.abs_(low - self.delay(close, 1))\n        ))\n        hd = high - self.delay(high, 1)\n        ld = self.delay(low, 1) - low\n\n        dmp = np.where((hd > 0) & (hd > ld), hd, 0)\n        dmm = np.where((ld > 0) & (ld > hd), ld, 0)\n\n        atr14 = self.sma(tr, 14, 1)\n        pdi = self.sma(pd.Series(dmp, index=high.index), 14, 1) / (atr14 + 1e-8) * 100\n        mdi = self.sma(pd.Series(dmm, index=high.index), 14, 1) / (atr14 + 1e-8) * 100\n\n        adx = self.sma(self.abs_(pdi - mdi) / (pdi + mdi + 1e-8) * 100, 6, 1)\n        return (adx + self.delay(adx, 6)) / 2",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha187",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "SUM(conditional open spreads, 20)",
    "python_code": "def alpha187(self, open_: pd.Series, high: pd.Series) -> pd.Series:\n        \"\"\"SUM(conditional open spreads, 20)\"\"\"\n        cond = open_ <= self.delay(open_, 1)\n        return self.ts_sum(np.where(cond, 0, np.maximum(high - open_, open_ - self.delay(open_, 1))), 20)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha188",
    "category": "alpha_factor",
    "formula": "(high - low - sma11) / (sma11 + 1e-8) * 100",
    "explanation": "((HIGH - LOW - SMA(HIGH - LOW, 11, 2)) / SMA(HIGH - LOW, 11, 2)) * 100",
    "python_code": "def alpha188(self, high: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"((HIGH - LOW - SMA(HIGH - LOW, 11, 2)) / SMA(HIGH - LOW, 11, 2)) * 100\"\"\"\n        sma11 = self.sma(high - low, 11, 2)\n        return (high - low - sma11) / (sma11 + 1e-8) * 100",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha189",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "MEAN(ABS(CLOSE - MEAN(CLOSE, 6)), 6)",
    "python_code": "def alpha189(self, close: pd.Series) -> pd.Series:\n        \"\"\"MEAN(ABS(CLOSE - MEAN(CLOSE, 6)), 6)\"\"\"\n        return self.ts_mean(self.abs_(close - self.ts_mean(close, 6)), 6)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha190",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Log-ratio calculation",
    "python_code": "def alpha190(self, close: pd.Series, open_: pd.Series, high: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"Log-ratio calculation\"\"\"\n        pct_change = (close - open_) / (open_ + 1e-8)\n        count_pos = self.ts_count(pct_change > 0.05, 20)\n        return self.log(count_pos + 1)",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "alpha191",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "(CORR(MEAN(VOL, 20), LOW, 5) + ((HIGH + LOW) / 2)) - CLOSE",
    "python_code": "def alpha191(self, close: pd.Series, high: pd.Series, low: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"(CORR(MEAN(VOL, 20), LOW, 5) + ((HIGH + LOW) / 2)) - CLOSE\"\"\"\n        return self.correlation(self.ts_mean(volume, 20), low, 5) + (high + low) / 2 - close",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "generate_all_alphas",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Generate all 191 Alpha signals.\n\nArgs:\n    df: DataFrame with OHLCV columns (open, high, low, close, volume)\n\nReturns:\n    DataFrame with alpha191_001 through alpha191_191 columns added",
    "python_code": "def generate_all_alphas(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all 191 Alpha signals.\n\n        Args:\n            df: DataFrame with OHLCV columns (open, high, low, close, volume)\n\n        Returns:\n            DataFrame with alpha191_001 through alpha191_191 columns added\n        \"\"\"\n        result = df.copy()\n\n        # Extract columns (with forex fallbacks)\n        open_ = df.get('open', df.get('mid', df['close']))\n        high = df.get('high', df.get('ask', df['close']))\n        low = df.get('low', df.get('bid', df['close']))\n        close = df['close']\n        volume = df.get('volume', df.get('tick_count', pd.Series(1, index=df.index)))\n        amount = df.get('amount', volume * close)  # Proxy for amount\n\n        # Calculate derived values\n        returns = close.pct_change()\n        vwap = (high + low + close) / 3  # Simplified VWAP\n\n        # Make open_, high, low, volume available for alphas that reference them\n        globals()['open_'] = open_\n        globals()['high'] = high\n        globals()['low'] = low\n        globals()['close'] = close\n        globals()['volume'] = volume\n\n        # Define all alpha functions with their dependencies\n        alpha_funcs = [\n            ('alpha191_001', lambda: self.alpha001(close, open_, volume)),\n            ('alpha191_002', lambda: self.alpha002(close, open_, high, low)),\n            ('alpha191_003', lambda: self.alpha003(close)),\n            ('alpha191_004', lambda: self.alpha004(close, volume)),\n            ('alpha191_005', lambda: self.alpha005(volume, high)),\n            ('alpha191_006', lambda: self.alpha006(open_, high)),\n            ('alpha191_007', lambda: self.alpha007(close, vwap, volume)),\n            ('alpha191_008', lambda: self.alpha008(high, low, vwap)),\n            ('alpha191_009', lambda: self.alpha009(high, low, volume)),\n            ('alpha191_010', lambda: self.alpha010(close, returns)),\n            ('alpha191_011', lambda: self.alpha011(close, high, low, volume)),\n            ('alpha191_012', lambda: self.alpha012(open_, close, vwap)),\n            ('alpha191_013', lambda: self.alpha013(high, low, vwap)),\n            ('alpha191_014', lambda: self.alpha014(close)),\n            ('alpha191_015', lambda: self.alpha015(open_, close)),\n            ('alpha191_016', lambda: self.alpha016(volume, vwap)),\n            ('alpha191_017', lambda: self.alpha017(close, vwap)),\n            ('alpha191_018', lambda: self.alpha018(close)),\n            ('alpha191_019', lambda: self.alpha019(close)),\n            ('alpha191_020', lambda: self.alpha020(close)),\n            ('alpha191_021', lambda: self.alpha021(close)),\n            ('alpha191_022', lambda: self.alpha022(close)),\n            ('alpha191_023', lambda: self.alpha023(close)),\n            ('alpha191_024', lambda: self.alpha024(close)),\n            ('alpha191_025', lambda: self.alpha025(close, volume, returns)),\n            ('alpha191_026', lambda: self.alpha026(close, vwap)),\n            ('alpha191_027', lambda:",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": "Alpha191GuotaiJunan"
  },
  {
    "name": "calc_beta",
    "category": "alpha_factor",
    "formula": "np.nan | cov / var if var != 0 else np.nan",
    "explanation": "",
    "python_code": "def calc_beta(y_arr, x_arr):\n            if len(y_arr) < 2:\n                return np.nan\n            x_mean = np.mean(x_arr)\n            y_mean = np.mean(y_arr)\n            cov = np.sum((x_arr - x_mean) * (y_arr - y_mean))\n            var = np.sum((x_arr - x_mean) ** 2)\n            return cov / var if var != 0 else np.nan",
    "source_file": "core\\_experimental\\alpha191_guotaijunan.py",
    "academic_reference": " (Guotai Junan) 191 Alpha Factors",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "volatility",
    "formula": "",
    "explanation": "Initialize GARCH model.\n\nArgs:\n    p: Order of GARCH term (lagged variance)\n    q: Order of ARCH term (lagged squared returns)\n    dist: Error distribution ('normal' or 'student-t')",
    "python_code": "def __init__(self,\n                 p: int = 1,\n                 q: int = 1,\n                 dist: str = 'normal'):\n        \"\"\"\n        Initialize GARCH model.\n\n        Args:\n            p: Order of GARCH term (lagged variance)\n            q: Order of ARCH term (lagged squared returns)\n            dist: Error distribution ('normal' or 'student-t')\n        \"\"\"\n        self.p = p\n        self.q = q\n        self.dist = dist\n\n        # Parameters: [omega, alpha_1, ..., alpha_q, beta_1, ..., beta_p, (nu for t)]\n        self.omega = None\n        self.alpha = None\n        self.beta = None\n        self.nu = None  # Degrees of freedom for t-distribution\n\n        self.fitted = False\n        self.conditional_vol = None",
    "source_file": "core\\_experimental\\arima_garch_models.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "GARCH"
  },
  {
    "name": "_negative_log_likelihood",
    "category": "volatility",
    "formula": "1e10 | 1e10 | 1e10",
    "explanation": "Compute negative log-likelihood for optimization.",
    "python_code": "def _negative_log_likelihood(self,\n                                  params: np.ndarray,\n                                  returns: np.ndarray) -> float:\n        \"\"\"\n        Compute negative log-likelihood for optimization.\n        \"\"\"\n        n = len(returns)\n\n        # Extract parameters\n        omega = params[0]\n        alpha = params[1:1 + self.q]\n        beta = params[1 + self.q:1 + self.q + self.p]\n\n        if self.dist == 'student-t':\n            nu = params[-1]\n        else:\n            nu = None\n\n        # Constraints check\n        if omega <= 0 or np.any(alpha < 0) or np.any(beta < 0):\n            return 1e10\n        if np.sum(alpha) + np.sum(beta) >= 1:\n            return 1e10\n        if nu is not None and nu <= 2:\n            return 1e10\n\n        # Initialize variance\n        sigma2 = np.zeros(n)\n        sigma2[0] = np.var(returns)\n\n        # Compute conditional variance recursively\n        for t in range(1, n):\n            sigma2[t] = omega\n\n            for i in range(min(t, self.q)):\n                sigma2[t] += alpha[i] * returns[t - 1 - i]**2\n\n            for j in range(min(t, self.p)):\n                sigma2[t] += beta[j] * sigma2[t - 1 - j]\n\n        # Avoid numerical issues\n        sigma2 = np.maximum(sigma2, 1e-10)\n\n        # Log-likelihood\n        if self.dist == 'normal':\n            ll = -0.5 * np.sum(np.log(2 * np.pi * sigma2) + returns**2 / sigma2)\n        else:\n            # Student-t distribution\n            from scipy.special import gammaln\n            ll = np.sum(\n                gammaln((nu + 1) / 2) - gammaln(nu / 2) -\n                0.5 * np.log(np.pi * (nu - 2) * sigma2) -\n                (nu + 1) / 2 * np.log(1 + returns**2 / ((nu - 2) * sigma2))\n            )\n\n        return -ll",
    "source_file": "core\\_experimental\\arima_garch_models.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "GARCH"
  },
  {
    "name": "fit",
    "category": "volatility",
    "formula": "GARCHResult(",
    "explanation": "Fit GARCH model to returns.\n\nArgs:\n    returns: Return series (not prices!)\n\nReturns:\n    GARCHResult with fitted parameters",
    "python_code": "def fit(self, returns: Union[np.ndarray, pd.Series]) -> GARCHResult:\n        \"\"\"\n        Fit GARCH model to returns.\n\n        Args:\n            returns: Return series (not prices!)\n\n        Returns:\n            GARCHResult with fitted parameters\n        \"\"\"\n        if isinstance(returns, pd.Series):\n            returns = returns.values\n\n        returns = returns.astype(np.float64)\n        n = len(returns)\n\n        # Initial parameter guess\n        var = np.var(returns)\n        omega_init = var * 0.1\n        alpha_init = np.ones(self.q) * 0.1 / self.q\n        beta_init = np.ones(self.p) * 0.8 / self.p\n\n        params_init = np.concatenate([[omega_init], alpha_init, beta_init])\n\n        if self.dist == 'student-t':\n            params_init = np.append(params_init, 10.0)  # Initial df\n\n        # Bounds\n        bounds = [(1e-10, var)] + \\\n                 [(1e-10, 0.99)] * self.q + \\\n                 [(1e-10, 0.99)] * self.p\n\n        if self.dist == 'student-t':\n            bounds.append((2.1, 100))\n\n        # Optimize\n        result = minimize(\n            self._negative_log_likelihood,\n            params_init,\n            args=(returns,),\n            method='L-BFGS-B',\n            bounds=bounds,\n            options={'maxiter': 1000}\n        )\n\n        # Extract parameters\n        self.omega = result.x[0]\n        self.alpha = result.x[1:1 + self.q]\n        self.beta = result.x[1 + self.q:1 + self.q + self.p]\n\n        if self.dist == 'student-t':\n            self.nu = result.x[-1]\n\n        # Compute conditional volatility\n        sigma2 = np.zeros(n)\n        sigma2[0] = np.var(returns)\n\n        for t in range(1, n):\n            sigma2[t] = self.omega\n            for i in range(min(t, self.q)):\n                sigma2[t] += self.alpha[i] * returns[t - 1 - i]**2\n            for j in range(min(t, self.p)):\n                sigma2[t] += self.beta[j] * sigma2[t - 1 - j]\n\n        self.conditional_vol = np.sqrt(sigma2)\n        self.fitted = True\n\n        # Compute statistics\n        persistence = np.sum(self.alpha) + np.sum(self.beta)\n        half_life = np.log(0.5) / np.log(persistence) if persistence < 1 else np.inf\n\n        k = len(result.x)\n        ll = -result.fun\n        aic = 2 * k - 2 * ll\n        bic = k * np.log(n) - 2 * ll\n\n        return GARCHResult(\n            omega=self.omega,\n            alpha=float(self.alpha[0]),\n            beta=float(self.beta[0]),\n            conditional_vol=self.conditional_vol,\n            standardized_residuals=returns / self.conditional_vol,\n            log_likelihood=ll,\n            aic=aic,\n            bic=bic,\n            persistence=persistence,\n            half_life=half_life\n        )",
    "source_file": "core\\_experimental\\arima_garch_models.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "GARCH"
  },
  {
    "name": "forecast",
    "category": "volatility",
    "formula": "np.sqrt(forecasts)",
    "explanation": "Forecast volatility h steps ahead.\n\nArgs:\n    horizon: Number of periods ahead\n\nReturns:\n    Array of forecasted volatilities",
    "python_code": "def forecast(self, horizon: int = 1) -> np.ndarray:\n        \"\"\"\n        Forecast volatility h steps ahead.\n\n        Args:\n            horizon: Number of periods ahead\n\n        Returns:\n            Array of forecasted volatilities\n        \"\"\"\n        if not self.fitted:\n            raise ValueError(\"Model not fitted. Call fit() first.\")\n\n        forecasts = np.zeros(horizon)\n\n        # Last conditional variance\n        sigma2_t = self.conditional_vol[-1]**2\n\n        # Unconditional variance\n        persistence = np.sum(self.alpha) + np.sum(self.beta)\n        uncond_var = self.omega / (1 - persistence) if persistence < 1 else sigma2_t\n\n        for h in range(horizon):\n            if h == 0:\n                forecasts[h] = sigma2_t\n            else:\n                # _{t+h} =  + ( + ) * _{t+h-1}\n                # which converges to unconditional variance\n                forecasts[h] = self.omega + persistence * forecasts[h - 1]\n\n        return np.sqrt(forecasts)",
    "source_file": "core\\_experimental\\arima_garch_models.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "GARCH"
  },
  {
    "name": "_negative_log_likelihood",
    "category": "volatility",
    "formula": "-ll",
    "explanation": "Compute negative log-likelihood.",
    "python_code": "def _negative_log_likelihood(self,\n                                  params: np.ndarray,\n                                  returns: np.ndarray) -> float:\n        \"\"\"Compute negative log-likelihood.\"\"\"\n        n = len(returns)\n\n        omega = params[0]\n        alpha = params[1]\n        gamma = params[2]  # Can be negative\n        beta = params[3]\n\n        # Expected absolute value of standard normal\n        E_abs_z = np.sqrt(2 / np.pi)\n\n        # Initialize log-variance\n        log_sigma2 = np.zeros(n)\n        log_sigma2[0] = np.log(np.var(returns))\n\n        for t in range(1, n):\n            z = returns[t - 1] / np.exp(log_sigma2[t - 1] / 2)\n            log_sigma2[t] = (omega +\n                            alpha * (np.abs(z) - E_abs_z) +\n                            gamma * z +\n                            beta * log_sigma2[t - 1])\n\n        sigma2 = np.exp(log_sigma2)\n        sigma2 = np.maximum(sigma2, 1e-10)\n\n        # Log-likelihood (normal)\n        ll = -0.5 * np.sum(np.log(2 * np.pi * sigma2) + returns**2 / sigma2)\n\n        return -ll",
    "source_file": "core\\_experimental\\arima_garch_models.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "EGARCH"
  },
  {
    "name": "forecast",
    "category": "volatility",
    "formula": "forecasts",
    "explanation": "Forecast volatility.",
    "python_code": "def forecast(self, horizon: int = 1) -> np.ndarray:\n        \"\"\"Forecast volatility.\"\"\"\n        if not self.fitted:\n            raise ValueError(\"Model not fitted\")\n\n        forecasts = np.zeros(horizon)\n        log_sigma2 = 2 * np.log(self.conditional_vol[-1])\n\n        for h in range(horizon):\n            # E[log(_{t+h})]  /(1-) for stationary process\n            log_sigma2 = self.omega + self.beta * log_sigma2\n            forecasts[h] = np.exp(log_sigma2 / 2)\n\n        return forecasts",
    "source_file": "core\\_experimental\\arima_garch_models.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "EGARCH"
  },
  {
    "name": "_negative_log_likelihood",
    "category": "volatility",
    "formula": "1e10 | 1e10 | -ll",
    "explanation": "Compute negative log-likelihood.",
    "python_code": "def _negative_log_likelihood(self,\n                                  params: np.ndarray,\n                                  returns: np.ndarray) -> float:\n        \"\"\"Compute negative log-likelihood.\"\"\"\n        n = len(returns)\n\n        omega = params[0]\n        alpha = params[1]\n        gamma = params[2]\n        beta = params[3]\n\n        # Constraints\n        if omega <= 0 or alpha < 0 or beta < 0:\n            return 1e10\n        if alpha + gamma / 2 + beta >= 1:  # Stationarity for GJR\n            return 1e10\n\n        sigma2 = np.zeros(n)\n        sigma2[0] = np.var(returns)\n\n        for t in range(1, n):\n            I = 1.0 if returns[t - 1] < 0 else 0.0\n            sigma2[t] = (omega +\n                        alpha * returns[t - 1]**2 +\n                        gamma * returns[t - 1]**2 * I +\n                        beta * sigma2[t - 1])\n\n        sigma2 = np.maximum(sigma2, 1e-10)\n        ll = -0.5 * np.sum(np.log(2 * np.pi * sigma2) + returns**2 / sigma2)\n\n        return -ll",
    "source_file": "core\\_experimental\\arima_garch_models.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "GJRGARCH"
  },
  {
    "name": "fit",
    "category": "volatility",
    "formula": "{",
    "explanation": "Fit regime-switching GARCH.\n\nTwo-step approach:\n1. Detect regimes using volatility clustering\n2. Fit separate GARCH for each regime",
    "python_code": "def fit(self, returns: Union[np.ndarray, pd.Series]) -> Dict:\n        \"\"\"\n        Fit regime-switching GARCH.\n\n        Two-step approach:\n        1. Detect regimes using volatility clustering\n        2. Fit separate GARCH for each regime\n        \"\"\"\n        if isinstance(returns, pd.Series):\n            returns = returns.values\n\n        returns = returns.astype(np.float64)\n        n = len(returns)\n\n        # Step 1: Simple regime detection via rolling volatility\n        window = 20\n        rolling_vol = pd.Series(returns).rolling(window).std().values\n\n        # Threshold-based regime assignment\n        vol_median = np.nanmedian(rolling_vol)\n        regimes = np.zeros(n, dtype=int)\n        regimes[rolling_vol > vol_median * 1.5] = 1  # High vol regime\n\n        # Smooth regime assignments\n        for i in range(1, n - 1):\n            if np.isnan(rolling_vol[i]):\n                regimes[i] = 0\n\n        # Estimate transition matrix\n        transitions = np.zeros((self.n_regimes, self.n_regimes))\n        for t in range(1, n):\n            transitions[regimes[t - 1], regimes[t]] += 1\n\n        # Normalize\n        for i in range(self.n_regimes):\n            row_sum = transitions[i].sum()\n            if row_sum > 0:\n                transitions[i] /= row_sum\n\n        self.transition_matrix = transitions\n\n        # Step 2: Fit GARCH for each regime\n        results = []\n        for regime in range(self.n_regimes):\n            mask = regimes == regime\n            if np.sum(mask) > 50:  # Need enough data\n                regime_returns = returns[mask]\n                result = self.garch_models[regime].fit(regime_returns)\n                results.append(result)\n            else:\n                results.append(None)\n\n        # Store regime probabilities\n        self.regime_probs = np.zeros((n, self.n_regimes))\n        for t in range(n):\n            self.regime_probs[t, regimes[t]] = 1.0\n\n        self.fitted = True\n\n        return {\n            'transition_matrix': self.transition_matrix,\n            'regime_probs': self.regime_probs,\n            'garch_results': results,\n            'regimes': regimes\n        }",
    "source_file": "core\\_experimental\\arima_garch_models.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "RegimeSwitchingGARCH"
  },
  {
    "name": "get_current_volatility",
    "category": "volatility",
    "formula": "0.0 | vol",
    "explanation": "Get weighted current volatility estimate.",
    "python_code": "def get_current_volatility(self) -> float:\n        \"\"\"Get weighted current volatility estimate.\"\"\"\n        if not self.fitted:\n            return 0.0\n\n        vol = 0.0\n        for name, model in self.models.items():\n            if model.fitted and name in self.weights:\n                vol += self.weights[name] * model.conditional_vol[-1]\n\n        return vol",
    "source_file": "core\\_experimental\\arima_garch_models.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": "VolatilityForecaster"
  },
  {
    "name": "compute_garch_features",
    "category": "volatility",
    "formula": "features",
    "explanation": "Compute GARCH-based features for HFT.\n\nArgs:\n    returns: Return series\n    annualize: Whether to annualize volatility\n    ann_factor: Annualization factor\n\nReturns:\n    DataFrame with GARCH features",
    "python_code": "def compute_garch_features(returns: pd.Series,\n                          annualize: bool = True,\n                          ann_factor: float = np.sqrt(252 * 24 * 60)) -> pd.DataFrame:\n    \"\"\"\n    Compute GARCH-based features for HFT.\n\n    Args:\n        returns: Return series\n        annualize: Whether to annualize volatility\n        ann_factor: Annualization factor\n\n    Returns:\n        DataFrame with GARCH features\n    \"\"\"\n    # Fit ensemble\n    forecaster = VolatilityForecaster()\n    forecaster.fit(returns.values)\n\n    # Get conditional volatilities\n    features = pd.DataFrame(index=returns.index)\n\n    for name, result in forecaster.results.items():\n        if result is not None:\n            vol = result.conditional_vol\n            if annualize:\n                vol = vol * ann_factor\n\n            features[f'garch_{name}_vol'] = vol\n            features[f'garch_{name}_zscore'] = result.standardized_residuals\n\n    # Ensemble volatility\n    if forecaster.fitted:\n        ensemble_vol = np.zeros(len(returns))\n        for name, result in forecaster.results.items():\n            if result is not None and name in forecaster.weights:\n                ensemble_vol += forecaster.weights[name] * result.conditional_vol\n\n        if annualize:\n            ensemble_vol *= ann_factor\n\n        features['garch_ensemble_vol'] = ensemble_vol\n\n        # Volatility regime\n        vol_percentile = pd.Series(ensemble_vol).rank(pct=True)\n        features['garch_vol_regime'] = np.where(\n            vol_percentile < 0.3, 0,  # Low\n            np.where(vol_percentile > 0.7, 2, 1)  # High / Normal\n        )\n\n    return features",
    "source_file": "core\\_experimental\\arima_garch_models.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": null
  },
  {
    "name": "create_garch_model",
    "category": "volatility",
    "formula": "GARCH(**kwargs) | EGARCH(**kwargs) | GJRGARCH(**kwargs)",
    "explanation": "Create GARCH model.\n\nArgs:\n    model_type: 'garch', 'egarch', 'gjr', 'regime_switching', 'ensemble'",
    "python_code": "def create_garch_model(model_type: str = 'garch', **kwargs):\n    \"\"\"\n    Create GARCH model.\n\n    Args:\n        model_type: 'garch', 'egarch', 'gjr', 'regime_switching', 'ensemble'\n    \"\"\"\n    if model_type == 'garch':\n        return GARCH(**kwargs)\n    elif model_type == 'egarch':\n        return EGARCH(**kwargs)\n    elif model_type == 'gjr':\n        return GJRGARCH(**kwargs)\n    elif model_type == 'regime_switching':\n        return RegimeSwitchingGARCH(**kwargs)\n    elif model_type == 'ensemble':\n        return VolatilityForecaster()\n    else:\n        raise ValueError(f\"Unknown model type: {model_type}\")",
    "source_file": "core\\_experimental\\arima_garch_models.py",
    "academic_reference": "Bollerslev (1986) 'GARCH' J. Econometrics",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, num_characteristics: int, embed_dim: int):\n        super().__init__()\n        self.linear = nn.Linear(num_characteristics, embed_dim)\n        self.norm = nn.LayerNorm(embed_dim)",
    "source_file": "core\\_experimental\\attention_factors.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CharacteristicEmbedding"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Args:\n    x: [B, T, C] characteristics\n\nReturns:\n    [B, T, D] embeddings",
    "python_code": "def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: [B, T, C] characteristics\n\n        Returns:\n            [B, T, D] embeddings\n        \"\"\"\n        return self.norm(self.linear(x))",
    "source_file": "core\\_experimental\\attention_factors.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "CharacteristicEmbedding"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "factor_values, attn_weights",
    "explanation": "Args:\n    x: [B, T, D] characteristic embeddings\n\nReturns:\n    (factor_values, attention_weights)\n    factor_values: [B, T, num_factors]\n    attention_weights: [B, num_factors, T]",
    "python_code": "def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Args:\n            x: [B, T, D] characteristic embeddings\n\n        Returns:\n            (factor_values, attention_weights)\n            factor_values: [B, T, num_factors]\n            attention_weights: [B, num_factors, T]\n        \"\"\"\n        B, T, D = x.shape\n\n        # Expand factor queries for batch\n        queries = self.factor_queries.unsqueeze(0).expand(B, -1, -1)  # [B, F, D]\n\n        # Cross-attention: factors attend to characteristics\n        factor_embeds, attn_weights = self.attention(queries, x, x)\n\n        # Project to factor values\n        factor_values = self.factor_proj(factor_embeds).squeeze(-1)  # [B, F]\n\n        return factor_values, attn_weights",
    "source_file": "core\\_experimental\\attention_factors.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "FactorAttention"
  },
  {
    "name": "__init__",
    "category": "reinforcement_learning",
    "formula": "prediction",
    "explanation": "",
    "python_code": "def __init__(self, config: AttentionFactorConfig):\n        super().__init__()\n        self.config = config\n\n        # Characteristic embedding\n        self.char_embed = CharacteristicEmbedding(config.num_characteristics, config.embed_dim)\n\n        # Temporal transformer\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=config.embed_dim,\n            nhead=config.num_heads,\n            dim_feedforward=config.embed_dim * 4,\n            dropout=config.dropout,\n            batch_first=True\n        )\n        self.temporal_encoder = nn.TransformerEncoder(encoder_layer, config.num_layers)\n\n        # Factor attention\n        self.factor_attention = FactorAttention(\n            config.embed_dim, config.num_heads,\n            config.num_factors, config.dropout\n        )\n\n        # Factor to return prediction\n        self.return_predictor = nn.Sequential(\n            nn.Linear(config.num_factors, config.embed_dim),\n            nn.ReLU(),\n            nn.Dropout(config.dropout),\n            nn.Linear(config.embed_dim, 1)\n        )",
    "source_file": "core\\_experimental\\attention_factors.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AttentionFactorModel"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "predicted_return = self.return_predictor(factor_values) | predicted_return, factor_values, attn_weights",
    "explanation": "Args:\n    x: [B, T, C] characteristics\n\nReturns:\n    (predicted_return, factor_values, attention_weights)",
    "python_code": "def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Args:\n            x: [B, T, C] characteristics\n\n        Returns:\n            (predicted_return, factor_values, attention_weights)\n        \"\"\"\n        # Embed\n        h = self.char_embed(x)\n\n        # Temporal encoding\n        h = self.temporal_encoder(h)\n\n        # Get last hidden state for factors\n        h_last = h[:, -1:, :]  # [B, 1, D]\n\n        # Factor extraction\n        factor_values, attn_weights = self.factor_attention(h)\n\n        # Predict return\n        predicted_return = self.return_predictor(factor_values)\n\n        return predicted_return, factor_values, attn_weights",
    "source_file": "core\\_experimental\\attention_factors.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AttentionFactorModel"
  },
  {
    "name": "prepare_characteristics",
    "category": "reinforcement_learning",
    "formula": "np.column_stack(list(chars.values()))",
    "explanation": "Prepare 20 characteristics from price data.",
    "python_code": "def prepare_characteristics(self, df: pd.DataFrame) -> np.ndarray:\n        \"\"\"Prepare 20 characteristics from price data.\"\"\"\n        mid = df['mid'].values if 'mid' in df.columns else (df['bid'] + df['ask']).values / 2\n\n        chars = {}\n\n        # Returns at multiple horizons\n        for lag in [1, 5, 10, 20, 50]:\n            chars[f'ret_{lag}'] = pd.Series(mid).pct_change(lag).fillna(0).values * 10000\n\n        # Volatility at multiple windows\n        for window in [10, 20, 50]:\n            chars[f'vol_{window}'] = pd.Series(mid).pct_change().rolling(window).std().fillna(0.001).values * 10000\n\n        # Momentum (rolling mean of returns)\n        for window in [5, 10, 20, 50]:\n            chars[f'mom_{window}'] = pd.Series(mid).pct_change().rolling(window).mean().fillna(0).values * 10000\n\n        # Z-scores (mean reversion)\n        for window in [20, 50]:\n            ma = pd.Series(mid).rolling(window).mean()\n            std = pd.Series(mid).rolling(window).std()\n            chars[f'zscore_{window}'] = ((mid - ma) / (std + 1e-10)).fillna(0).values\n\n        # RSI\n        for period in [14, 28]:\n            delta = pd.Series(mid).diff()\n            gain = delta.where(delta > 0, 0).rolling(period).mean()\n            loss = (-delta.where(delta < 0, 0)).rolling(period).mean()\n            chars[f'rsi_{period}'] = ((100 - 100 / (1 + gain / (loss + 1e-10))).fillna(50).values - 50) / 50\n\n        # MACD histogram\n        ema12 = pd.Series(mid).ewm(span=12).mean()\n        ema26 = pd.Series(mid).ewm(span=26).mean()\n        macd = ema12 - ema26\n        signal = macd.ewm(span=9).mean()\n        chars['macd_hist'] = (macd - signal).fillna(0).values * 10000\n\n        # Spread ratio\n        if 'bid' in df.columns and 'ask' in df.columns:\n            spread = (df['ask'] - df['bid']) / mid * 10000\n            spread_ma = spread.rolling(20).mean()\n            chars['spread_ratio'] = (spread / (spread_ma + 1e-10)).fillna(1).values - 1\n        else:\n            chars['spread_ratio'] = np.zeros(len(df))\n\n        # Volume ratio\n        if 'volume' in df.columns:\n            vol = df['volume']\n            vol_ma = vol.rolling(20).mean()\n            chars['vol_ratio'] = (vol / (vol_ma + 1e-10)).fillna(1).values - 1\n        else:\n            chars['vol_ratio'] = np.zeros(len(df))\n\n        self.feature_names = list(chars.keys())\n        return np.column_stack(list(chars.values()))",
    "source_file": "core\\_experimental\\attention_factors.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AttentionFactorsForex"
  },
  {
    "name": "fit",
    "category": "reinforcement_learning",
    "formula": "X = self.prepare_characteristics(df) | mid = df['mid'].values if 'mid' in df.columns else (df['bid'] + df['ask']).values / 2",
    "explanation": "Train attention factor model.",
    "python_code": "def fit(self, df: pd.DataFrame, epochs: int = 50, lr: float = 0.001, batch_size: int = 32):\n        \"\"\"Train attention factor model.\"\"\"\n        if not HAS_TORCH:\n            logger.warning(\"PyTorch not available\")\n            return\n\n        X = self.prepare_characteristics(df)\n\n        # Target: future return\n        mid = df['mid'].values if 'mid' in df.columns else (df['bid'] + df['ask']).values / 2\n        returns = pd.Series(mid).pct_change().shift(-1).fillna(0).values * 10000\n\n        # Create sequences\n        X_seq, y_seq = [], []\n        for i in range(self.seq_len, len(X) - 1):\n            X_seq.append(X[i-self.seq_len:i])\n            y_seq.append(returns[i])\n\n        X_tensor = torch.FloatTensor(np.array(X_seq))\n        y_tensor = torch.FloatTensor(np.array(y_seq)).unsqueeze(1)\n\n        # Train\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n        criterion = nn.MSELoss()\n\n        n_batches = len(X_tensor) // batch_size\n\n        self.model.train()\n        for epoch in range(epochs):\n            total_loss = 0\n            for b in range(n_batches):\n                start = b * batch_size\n                end = start + batch_size\n\n                optimizer.zero_grad()\n                pred, factors, attn = self.model(X_tensor[start:end])\n                loss = criterion(pred, y_tensor[start:end])\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n\n            if epoch % 10 == 0:\n                logger.info(f\"Epoch {epoch}, Loss: {total_loss/n_batches:.6f}\")\n\n        self.is_fitted = True\n        logger.info(\"Attention Factors model trained\")",
    "source_file": "core\\_experimental\\attention_factors.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AttentionFactorsForex"
  },
  {
    "name": "predict",
    "category": "reinforcement_learning",
    "formula": "and get factor exposures. | {'predicted_return': 0.0, 'direction_prob': 0.5, 'factors': []} | = pred.item()",
    "explanation": "Predict return and get factor exposures.\n\nReturns:\n    Dict with predicted return, factors, and top characteristics",
    "python_code": "def predict(self, df: pd.DataFrame) -> Dict[str, float]:\n        \"\"\"\n        Predict return and get factor exposures.\n\n        Returns:\n            Dict with predicted return, factors, and top characteristics\n        \"\"\"\n        if not HAS_TORCH or not self.is_fitted:\n            return {'predicted_return': 0.0, 'direction_prob': 0.5, 'factors': []}\n\n        X = self.prepare_characteristics(df)[-self.seq_len:]\n\n        self.model.eval()\n        with torch.no_grad():\n            X_tensor = torch.FloatTensor(X).unsqueeze(0)\n            pred, factors, attn_weights = self.model(X_tensor)\n\n        predicted_return = pred.item()\n        direction_prob = 1 / (1 + np.exp(-predicted_return))\n\n        # Get factor values\n        factor_values = factors[0].numpy().tolist()\n\n        # Get top attention weights (which characteristics matter)\n        attn = attn_weights[0].numpy()  # [num_factors, T]\n        avg_attn = attn.mean(axis=0)  # Average across factors\n        top_indices = np.argsort(avg_attn)[-5:][::-1]\n        top_chars = [(self.feature_names[i], float(avg_attn[i])) for i in top_indices]\n\n        return {\n            'predicted_return': predicted_return,\n            'direction_prob': direction_prob,\n            'factors': factor_values,\n            'top_characteristics': top_chars,\n            'signal': 1 if direction_prob > 0.55 else (-1 if direction_prob < 0.45 else 0)\n        }",
    "source_file": "core\\_experimental\\attention_factors.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AttentionFactorsForex"
  },
  {
    "name": "get_factor_loadings",
    "category": "reinforcement_learning",
    "formula": "{} | {",
    "explanation": "Get factor loadings (for interpretability).",
    "python_code": "def get_factor_loadings(self) -> Dict[str, np.ndarray]:\n        \"\"\"Get factor loadings (for interpretability).\"\"\"\n        if not self.is_fitted:\n            return {}\n\n        # Extract factor query weights\n        queries = self.model.factor_attention.factor_queries.detach().numpy()\n\n        return {\n            f'factor_{i}': queries[i]\n            for i in range(self.num_factors)\n        }",
    "source_file": "core\\_experimental\\attention_factors.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AttentionFactorsForex"
  },
  {
    "name": "calculate",
    "category": "reinforcement_learning",
    "formula": "MicropriceResult(mid, 0.0, mid, 0.5) | MicropriceResult(",
    "explanation": "Calculate microprice from L2 data.\n\nArgs:\n    bid: Best bid price\n    ask: Best ask price\n    bid_size: Volume at best bid\n    ask_size: Volume at best ask\n\nReturns:\n    MicropriceResult with all components",
    "python_code": "def calculate(bid: float, ask: float,\n                  bid_size: float, ask_size: float) -> MicropriceResult:\n        \"\"\"\n        Calculate microprice from L2 data.\n\n        Args:\n            bid: Best bid price\n            ask: Best ask price\n            bid_size: Volume at best bid\n            ask_size: Volume at best ask\n\n        Returns:\n            MicropriceResult with all components\n        \"\"\"\n        mid = (bid + ask) / 2\n        spread = ask - bid\n\n        total_size = bid_size + ask_size\n        if total_size == 0:\n            return MicropriceResult(mid, 0.0, mid, 0.5)\n\n        imbalance = (bid_size - ask_size) / total_size\n        microprice = mid + spread * imbalance / 2\n\n        # Weighted mid (size-weighted)\n        weighted_mid = (bid * ask_size + ask * bid_size) / total_size\n\n        # Pressure ratio (buy pressure)\n        pressure_ratio = bid_size / total_size\n\n        return MicropriceResult(\n            microprice=microprice,\n            imbalance=imbalance,\n            weighted_mid=weighted_mid,\n            pressure_ratio=pressure_ratio\n        )",
    "source_file": "core\\_experimental\\chinese_hft_factors.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "Microprice"
  },
  {
    "name": "multi_level_microprice",
    "category": "statistical",
    "formula": "0.0 | (bids[0][0] + asks[0][0]) / 2 | (weighted_bid / bid_weight_sum + weighted_ask / ask_weight_sum) / 2",
    "explanation": "Multi-level microprice with exponential decay.\n\nArgs:\n    bids: List of (price, size) tuples\n    asks: List of (price, size) tuples\n    decay: Weight decay for deeper levels\n\nReturns:\n    Multi-level microprice",
    "python_code": "def multi_level_microprice(bids: List[Tuple[float, float]],\n                               asks: List[Tuple[float, float]],\n                               decay: float = 0.5) -> float:\n        \"\"\"\n        Multi-level microprice with exponential decay.\n\n        Args:\n            bids: List of (price, size) tuples\n            asks: List of (price, size) tuples\n            decay: Weight decay for deeper levels\n\n        Returns:\n            Multi-level microprice\n        \"\"\"\n        if not bids or not asks:\n            return 0.0\n\n        weighted_bid = 0.0\n        weighted_ask = 0.0\n        bid_weight_sum = 0.0\n        ask_weight_sum = 0.0\n\n        for i, (price, size) in enumerate(bids):\n            weight = (decay ** i) * size\n            weighted_bid += price * weight\n            bid_weight_sum += weight\n\n        for i, (price, size) in enumerate(asks):\n            weight = (decay ** i) * size\n            weighted_ask += price * weight\n            ask_weight_sum += weight\n\n        if bid_weight_sum == 0 or ask_weight_sum == 0:\n            return (bids[0][0] + asks[0][0]) / 2\n\n        return (weighted_bid / bid_weight_sum + weighted_ask / ask_weight_sum) / 2",
    "source_file": "core\\_experimental\\chinese_hft_factors.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "Microprice"
  },
  {
    "name": "__init__",
    "category": "technical",
    "formula": "",
    "explanation": "Args:\n    window: Lookback window\n    threshold_pct: Percentile for \"large\" trades",
    "python_code": "def __init__(self, window: int = 20, threshold_pct: float = 0.7):\n        \"\"\"\n        Args:\n            window: Lookback window\n            threshold_pct: Percentile for \"large\" trades\n        \"\"\"\n        self.window = window\n        self.threshold_pct = threshold_pct",
    "source_file": "core\\_experimental\\chinese_hft_factors.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "SmartMoneyFactor"
  },
  {
    "name": "calculate",
    "category": "technical",
    "formula": "smart_money.fillna(0)",
    "explanation": "Calculate Smart Money Factor.\n\nArgs:\n    df: DataFrame with 'close', 'volume' columns\n\nReturns:\n    Series of smart money factor values",
    "python_code": "def calculate(self, df: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Calculate Smart Money Factor.\n\n        Args:\n            df: DataFrame with 'close', 'volume' columns\n\n        Returns:\n            Series of smart money factor values\n        \"\"\"\n        returns = df['close'].pct_change()\n        volume = df['volume'] if 'volume' in df.columns else pd.Series(1, index=df.index)\n\n        # Weight = |return| * volume (impact-weighted)\n        weights = np.abs(returns) * volume\n\n        # Signed impact\n        signed_impact = np.sign(returns) * weights\n\n        # Rolling smart money factor\n        numerator = signed_impact.rolling(self.window).sum()\n        denominator = weights.rolling(self.window).sum()\n\n        smart_money = numerator / (denominator + 1e-10)\n\n        return smart_money.fillna(0)",
    "source_file": "core\\_experimental\\chinese_hft_factors.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "SmartMoneyFactor"
  },
  {
    "name": "calculate_v2",
    "category": "volatility",
    "formula": "smart_money_v2.fillna(0)",
    "explanation": "Smart Money Factor 2.0 - Enhanced version.\n\nImprovements:\n- Separate large vs small trades\n- Time-weighted decay\n- Volatility normalization",
    "python_code": "def calculate_v2(self, df: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Smart Money Factor 2.0 - Enhanced version.\n\n        Improvements:\n        - Separate large vs small trades\n        - Time-weighted decay\n        - Volatility normalization\n        \"\"\"\n        returns = df['close'].pct_change()\n        volume = df['volume'] if 'volume' in df.columns else pd.Series(1, index=df.index)\n\n        # Identify large trades (above threshold)\n        vol_threshold = volume.rolling(self.window).quantile(self.threshold_pct)\n        is_large = volume > vol_threshold\n\n        # Large trade impact\n        large_impact = np.where(is_large, np.sign(returns) * np.abs(returns) * volume, 0)\n\n        # Small trade impact (contrarian)\n        small_impact = np.where(~is_large, np.sign(returns) * np.abs(returns) * volume, 0)\n\n        # Smart money = large trade direction\n        large_sum = pd.Series(large_impact).rolling(self.window).sum()\n        small_sum = pd.Series(small_impact).rolling(self.window).sum()\n\n        # Normalize by volatility\n        volatility = returns.rolling(self.window).std()\n\n        smart_money_v2 = (large_sum - small_sum) / (volatility * volume.rolling(self.window).sum() + 1e-10)\n\n        return smart_money_v2.fillna(0)",
    "source_file": "core\\_experimental\\chinese_hft_factors.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "SmartMoneyFactor"
  },
  {
    "name": "calculate_vwap_distance",
    "category": "execution",
    "formula": "vwap_zscore.fillna(0)",
    "explanation": "Calculate distance from VWAP (Volume Weighted Average Price).\n\nReturns z-score of price relative to VWAP.",
    "python_code": "def calculate_vwap_distance(self, df: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Calculate distance from VWAP (Volume Weighted Average Price).\n\n        Returns z-score of price relative to VWAP.\n        \"\"\"\n        typical_price = (df['high'] + df['low'] + df['close']) / 3 if 'high' in df.columns else df['close']\n        volume = df['volume'] if 'volume' in df.columns else pd.Series(1, index=df.index)\n\n        vwap = (typical_price * volume).rolling(self.lookback).sum() / volume.rolling(self.lookback).sum()\n\n        # Standard deviation for z-score\n        std = typical_price.rolling(self.lookback).std()\n\n        vwap_zscore = (df['close'] - vwap) / (std + 1e-10)\n\n        return vwap_zscore.fillna(0)",
    "source_file": "core\\_experimental\\chinese_hft_factors.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "ChipDistributionFactor"
  },
  {
    "name": "calculate_volume_profile",
    "category": "microstructure",
    "formula": "{",
    "explanation": "Calculate volume profile metrics.\n\nReturns:\n    Dict with poc (point of control), value_area_high, value_area_low",
    "python_code": "def calculate_volume_profile(self, df: pd.DataFrame) -> Dict[str, float]:\n        \"\"\"\n        Calculate volume profile metrics.\n\n        Returns:\n            Dict with poc (point of control), value_area_high, value_area_low\n        \"\"\"\n        prices = df['close'].values[-self.lookback:]\n        volumes = df['volume'].values[-self.lookback:] if 'volume' in df.columns else np.ones(len(prices))\n\n        # Create price bins\n        price_min, price_max = prices.min(), prices.max()\n        bins = np.linspace(price_min, price_max, self.n_bins + 1)\n\n        # Accumulate volume in each bin\n        volume_profile = np.zeros(self.n_bins)\n        for i, (price, vol) in enumerate(zip(prices, volumes)):\n            bin_idx = min(int((price - price_min) / (price_max - price_min + 1e-10) * self.n_bins), self.n_bins - 1)\n            volume_profile[bin_idx] += vol\n\n        # Point of Control (POC) - price level with most volume\n        poc_idx = np.argmax(volume_profile)\n        poc = (bins[poc_idx] + bins[poc_idx + 1]) / 2\n\n        # Value Area (70% of volume)\n        total_vol = volume_profile.sum()\n        target_vol = total_vol * 0.7\n\n        sorted_indices = np.argsort(volume_profile)[::-1]\n        cumulative = 0\n        value_area_bins = []\n        for idx in sorted_indices:\n            cumulative += volume_profile[idx]\n            value_area_bins.append(idx)\n            if cumulative >= target_vol:\n                break\n\n        value_area_low = bins[min(value_area_bins)]\n        value_area_high = bins[max(value_area_bins) + 1]\n\n        return {\n            'poc': poc,\n            'value_area_high': value_area_high,\n            'value_area_low': value_area_low,\n            'current_in_value_area': value_area_low <= prices[-1] <= value_area_high\n        }",
    "source_file": "core\\_experimental\\chinese_hft_factors.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "ChipDistributionFactor"
  },
  {
    "name": "calculate_chip_concentration",
    "category": "technical",
    "formula": "concentration = potential breakout | concentration = mean reversion likely | 0.5",
    "explanation": "Calculate chip concentration ratio.\n\nHigh concentration = potential breakout\nLow concentration = mean reversion likely",
    "python_code": "def calculate_chip_concentration(self, df: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Calculate chip concentration ratio.\n\n        High concentration = potential breakout\n        Low concentration = mean reversion likely\n        \"\"\"\n        closes = df['close']\n        volumes = df['volume'] if 'volume' in df.columns else pd.Series(1, index=df.index)\n\n        def calc_concentration(window_data):\n            if len(window_data) < 10:\n                return 0.5\n            prices = window_data['close'].values\n            vols = window_data['volume'].values if 'volume' in window_data.columns else np.ones(len(prices))\n\n            # Weighted standard deviation\n            mean_price = np.average(prices, weights=vols)\n            variance = np.average((prices - mean_price) ** 2, weights=vols)\n\n            # Normalize by price range\n            price_range = prices.max() - prices.min()\n            if price_range == 0:\n                return 1.0\n\n            concentration = 1 - np.sqrt(variance) / price_range\n            return max(0, min(1, concentration))\n\n        result = df.rolling(self.lookback).apply(\n            lambda x: calc_concentration(df.loc[x.index]), raw=False\n        )['close']\n\n        return result.fillna(0.5)",
    "source_file": "core\\_experimental\\chinese_hft_factors.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "ChipDistributionFactor"
  },
  {
    "name": "estimate_lambda",
    "category": "microstructure",
    "formula": "Lambda = Cov(ret, signed_volume) / Var(signed_volume) | kyle_lambda.fillna(kyle_lambda.mean())",
    "explanation": "Estimate Kyle's Lambda using regression.\n\nLambda = Cov(ret, signed_volume) / Var(signed_volume)",
    "python_code": "def estimate_lambda(self, df: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Estimate Kyle's Lambda using regression.\n\n        Lambda = Cov(ret, signed_volume) / Var(signed_volume)\n        \"\"\"\n        returns = df['close'].pct_change() * 10000  # bps\n\n        # Signed volume (using tick rule if no trade direction)\n        if 'signed_volume' in df.columns:\n            signed_vol = df['signed_volume']\n        else:\n            # Estimate using price direction\n            volume = df['volume'] if 'volume' in df.columns else pd.Series(1, index=df.index)\n            signed_vol = np.sign(returns) * volume\n\n        # Rolling regression: ret = alpha + lambda * signed_vol\n        cov = returns.rolling(self.window).cov(signed_vol)\n        var = signed_vol.rolling(self.window).var()\n\n        kyle_lambda = cov / (var + 1e-10)\n\n        return kyle_lambda.fillna(kyle_lambda.mean())",
    "source_file": "core\\_experimental\\chinese_hft_factors.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "KyleLambda"
  },
  {
    "name": "estimate_permanent_impact",
    "category": "microstructure",
    "formula": "impact = portion of price change that doesn't revert | permanent_ratio.fillna(0)",
    "explanation": "Estimate permanent price impact (information content).\n\nPermanent impact = portion of price change that doesn't revert",
    "python_code": "def estimate_permanent_impact(self, df: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Estimate permanent price impact (information content).\n\n        Permanent impact = portion of price change that doesn't revert\n        \"\"\"\n        returns = df['close'].pct_change()\n\n        # Future returns (does impact persist?)\n        future_ret_5 = returns.shift(-5).rolling(5).sum()\n        current_ret = returns\n\n        # Correlation = permanent impact ratio\n        permanent_ratio = current_ret.rolling(self.window).corr(future_ret_5)\n\n        return permanent_ratio.fillna(0)",
    "source_file": "core\\_experimental\\chinese_hft_factors.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "KyleLambda"
  },
  {
    "name": "calculate",
    "category": "microstructure",
    "formula": "amihud_zscore.fillna(0)",
    "explanation": "Calculate Amihud illiquidity ratio.",
    "python_code": "def calculate(self, df: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Calculate Amihud illiquidity ratio.\n        \"\"\"\n        returns = df['close'].pct_change().abs()\n\n        if 'volume' in df.columns:\n            dollar_volume = df['volume'] * df['close']\n        else:\n            dollar_volume = df['close']  # Proxy\n\n        illiq = returns / (dollar_volume + 1e-10)\n\n        # Rolling average\n        amihud = illiq.rolling(self.window).mean()\n\n        # Normalize to z-score\n        amihud_zscore = (amihud - amihud.rolling(100).mean()) / (amihud.rolling(100).std() + 1e-10)\n\n        return amihud_zscore.fillna(0)",
    "source_file": "core\\_experimental\\chinese_hft_factors.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "AmihudIlliquidity"
  },
  {
    "name": "calculate_tick",
    "category": "microstructure",
    "formula": "0.0 | ofi",
    "explanation": "Calculate integrated OFI for single tick.\n\nArgs:\n    bids: List of (price, size) at each level\n    asks: List of (price, size) at each level\n\nReturns:\n    Integrated OFI value",
    "python_code": "def calculate_tick(self, bids: List[Tuple[float, float]],\n                       asks: List[Tuple[float, float]]) -> float:\n        \"\"\"\n        Calculate integrated OFI for single tick.\n\n        Args:\n            bids: List of (price, size) at each level\n            asks: List of (price, size) at each level\n\n        Returns:\n            Integrated OFI value\n        \"\"\"\n        if self.prev_bids is None:\n            self.prev_bids = bids\n            self.prev_asks = asks\n            return 0.0\n\n        ofi = 0.0\n\n        for i in range(min(self.n_levels, len(bids), len(self.prev_bids))):\n            weight = self.decay ** i\n\n            curr_bid_price, curr_bid_size = bids[i]\n            prev_bid_price, prev_bid_size = self.prev_bids[i]\n\n            # Bid side contribution\n            if curr_bid_price > prev_bid_price:\n                ofi += weight * curr_bid_size\n            elif curr_bid_price == prev_bid_price:\n                ofi += weight * (curr_bid_size - prev_bid_size)\n            else:\n                ofi -= weight * prev_bid_size\n\n        for i in range(min(self.n_levels, len(asks), len(self.prev_asks))):\n            weight = self.decay ** i\n\n            curr_ask_price, curr_ask_size = asks[i]\n            prev_ask_price, prev_ask_size = self.prev_asks[i]\n\n            # Ask side contribution (negative for sells)\n            if curr_ask_price < prev_ask_price:\n                ofi -= weight * curr_ask_size\n            elif curr_ask_price == prev_ask_price:\n                ofi -= weight * (curr_ask_size - prev_ask_size)\n            else:\n                ofi += weight * prev_ask_size\n\n        self.prev_bids = bids\n        self.prev_asks = asks\n\n        return ofi",
    "source_file": "core\\_experimental\\chinese_hft_factors.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "IntegratedOFI"
  },
  {
    "name": "calculate_series",
    "category": "microstructure",
    "formula": "(np.sign(returns) * volume).fillna(0) | ofi.fillna(0)",
    "explanation": "Calculate integrated OFI from DataFrame with bid/ask data.",
    "python_code": "def calculate_series(self, df: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Calculate integrated OFI from DataFrame with bid/ask data.\n        \"\"\"\n        if 'bid_size' not in df.columns or 'ask_size' not in df.columns:\n            # Fallback to simple volume-based OFI\n            returns = df['close'].pct_change()\n            volume = df['volume'] if 'volume' in df.columns else pd.Series(1, index=df.index)\n            return (np.sign(returns) * volume).fillna(0)\n\n        bid_changes = df['bid'].diff()\n        ask_changes = df['ask'].diff()\n\n        ofi = pd.Series(0.0, index=df.index)\n\n        # Bid side\n        ofi += np.where(bid_changes > 0, df['bid_size'], 0)\n        ofi += np.where(bid_changes == 0, df['bid_size'].diff(), 0)\n        ofi -= np.where(bid_changes < 0, df['bid_size'].shift(1), 0)\n\n        # Ask side (inverted)\n        ofi -= np.where(ask_changes < 0, df['ask_size'], 0)\n        ofi -= np.where(ask_changes == 0, df['ask_size'].diff(), 0)\n        ofi += np.where(ask_changes > 0, df['ask_size'].shift(1), 0)\n\n        return ofi.fillna(0)",
    "source_file": "core\\_experimental\\chinese_hft_factors.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "IntegratedOFI"
  },
  {
    "name": "detect_elephant",
    "category": "reinforcement_learning",
    "formula": "pd.Series(elephants, index=df.index)",
    "explanation": "Detect large orders (elephants).\n\nReturns:\n    Series with 1 (buy elephant), -1 (sell elephant), 0 (none)",
    "python_code": "def detect_elephant(self, df: pd.DataFrame,\n                       window: int = 100) -> pd.Series:\n        \"\"\"\n        Detect large orders (elephants).\n\n        Returns:\n            Series with 1 (buy elephant), -1 (sell elephant), 0 (none)\n        \"\"\"\n        volume = df['volume'] if 'volume' in df.columns else pd.Series(1, index=df.index)\n\n        threshold = volume.rolling(window).quantile(self.size_threshold_pct)\n        is_elephant = volume > threshold\n\n        # Direction from price change\n        returns = df['close'].pct_change()\n        direction = np.sign(returns)\n\n        elephants = np.where(is_elephant, direction, 0)\n\n        return pd.Series(elephants, index=df.index)",
    "source_file": "core\\_experimental\\chinese_hft_factors.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "PennyJumpDetector"
  },
  {
    "name": "calculate_jump_signal",
    "category": "reinforcement_learning",
    "formula": "Signal = Follow elephants with tight stop. | pd.Series(signal, index=df.index)",
    "explanation": "Calculate penny jump trading signal.\n\nSignal = Follow elephants with tight stop.",
    "python_code": "def calculate_jump_signal(self, df: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Calculate penny jump trading signal.\n\n        Signal = Follow elephants with tight stop.\n        \"\"\"\n        elephants = self.detect_elephant(df)\n\n        # Only signal if spread is wide enough\n        if 'bid' in df.columns and 'ask' in df.columns:\n            spread = df['ask'] - df['bid']\n            tick_size = spread.min()\n            wide_spread = spread >= tick_size * self.min_spread_ticks\n\n            signal = np.where(wide_spread, elephants, 0)\n        else:\n            signal = elephants\n\n        return pd.Series(signal, index=df.index)",
    "source_file": "core\\_experimental\\chinese_hft_factors.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "PennyJumpDetector"
  },
  {
    "name": "resample",
    "category": "reinforcement_learning",
    "formula": "df | pd.DataFrame(bars).set_index('timestamp') if bars else df",
    "explanation": "Resample data to volume clock.\n\nReturns:\n    DataFrame with volume-based bars",
    "python_code": "def resample(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Resample data to volume clock.\n\n        Returns:\n            DataFrame with volume-based bars\n        \"\"\"\n        if 'volume' not in df.columns:\n            logger.warning(\"No volume column, using time-based sampling\")\n            return df\n\n        volume = df['volume'].values\n        close = df['close'].values\n\n        bars = []\n        current_volume = 0\n        bar_start_idx = 0\n\n        for i in range(len(df)):\n            current_volume += volume[i]\n\n            if current_volume >= self.bucket_volume:\n                bar = {\n                    'timestamp': df.index[i],\n                    'open': close[bar_start_idx],\n                    'high': close[bar_start_idx:i+1].max(),\n                    'low': close[bar_start_idx:i+1].min(),\n                    'close': close[i],\n                    'volume': current_volume,\n                    'n_ticks': i - bar_start_idx + 1\n                }\n                bars.append(bar)\n\n                current_volume = 0\n                bar_start_idx = i + 1\n\n        return pd.DataFrame(bars).set_index('timestamp') if bars else df",
    "source_file": "core\\_experimental\\chinese_hft_factors.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "VolumeClockSampler"
  },
  {
    "name": "generate_all_factors",
    "category": "reinforcement_learning",
    "formula": "result",
    "explanation": "Generate all Chinese HFT factors.\n\nReturns:\n    DataFrame with factor columns added",
    "python_code": "def generate_all_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all Chinese HFT factors.\n\n        Returns:\n            DataFrame with factor columns added\n        \"\"\"\n        result = df.copy()\n\n        # Smart Money Factors\n        result['smart_money_v1'] = self.smart_money.calculate(df)\n        result['smart_money_v2'] = self.smart_money.calculate_v2(df)\n\n        # Chip Distribution\n        result['vwap_zscore'] = self.chip_dist.calculate_vwap_distance(df)\n\n        # Kyle Lambda (Market Impact)\n        result['kyle_lambda'] = self.kyle_lambda.estimate_lambda(df)\n        result['permanent_impact'] = self.kyle_lambda.estimate_permanent_impact(df)\n\n        # Amihud Illiquidity\n        result['amihud_illiq'] = self.amihud.calculate(df)\n\n        # Integrated OFI\n        result['integrated_ofi'] = self.integrated_ofi.calculate_series(df)\n\n        # Penny Jump\n        result['elephant_detect'] = self.penny_jump.detect_elephant(df)\n        result['penny_jump_signal'] = self.penny_jump.calculate_jump_signal(df)\n\n        # Microprice (if L2 data available)\n        if 'bid' in df.columns and 'ask' in df.columns:\n            bid_size = df['bid_size'] if 'bid_size' in df.columns else pd.Series(1, index=df.index)\n            ask_size = df['ask_size'] if 'ask_size' in df.columns else pd.Series(1, index=df.index)\n\n            microprice_results = [\n                self.microprice.calculate(b, a, bs, as_)\n                for b, a, bs, as_ in zip(df['bid'], df['ask'], bid_size, ask_size)\n            ]\n\n            result['microprice'] = [r.microprice for r in microprice_results]\n            result['microprice_imbalance'] = [r.imbalance for r in microprice_results]\n            result['pressure_ratio'] = [r.pressure_ratio for r in microprice_results]\n\n        logger.info(f\"Generated {len([c for c in result.columns if c not in df.columns])} Chinese HFT factors\")\n\n        return result",
    "source_file": "core\\_experimental\\chinese_hft_factors.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "ChineseHFTFactorEngine"
  },
  {
    "name": "get_factor_names",
    "category": "reinforcement_learning",
    "formula": "[",
    "explanation": "Return list of factor names.",
    "python_code": "def get_factor_names(self) -> List[str]:\n        \"\"\"Return list of factor names.\"\"\"\n        return [\n            'smart_money_v1', 'smart_money_v2',\n            'vwap_zscore',\n            'kyle_lambda', 'permanent_impact',\n            'amihud_illiq',\n            'integrated_ofi',\n            'elephant_detect', 'penny_jump_signal',\n            'microprice', 'microprice_imbalance', 'pressure_ratio'\n        ]",
    "source_file": "core\\_experimental\\chinese_hft_factors.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "ChineseHFTFactorEngine"
  },
  {
    "name": "compute_all_features",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Compute all Chinese HFT features.\nInterface compatible with HFT Feature Engine.",
    "python_code": "def compute_all_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Compute all Chinese HFT features.\n        Interface compatible with HFT Feature Engine.\n        \"\"\"\n        return self.engine.generate_all_factors(df)",
    "source_file": "core\\_experimental\\chinese_hft_factors.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "ChineseHFTFactors"
  },
  {
    "name": "calc_concentration",
    "category": "reinforcement_learning",
    "formula": "0.5 | 1.0 | max(0, min(1, concentration))",
    "explanation": "",
    "python_code": "def calc_concentration(window_data):\n            if len(window_data) < 10:\n                return 0.5\n            prices = window_data['close'].values\n            vols = window_data['volume'].values if 'volume' in window_data.columns else np.ones(len(prices))\n\n            # Weighted standard deviation\n            mean_price = np.average(prices, weights=vols)\n            variance = np.average((prices - mean_price) ** 2, weights=vols)\n\n            # Normalize by price range\n            price_range = prices.max() - prices.min()\n            if price_range == 0:\n                return 1.0\n\n            concentration = 1 - np.sqrt(variance) / price_range\n            return max(0, min(1, concentration))",
    "source_file": "core\\_experimental\\chinese_hft_factors.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "volatility",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, hidden_size: int = 32, n_layers: int = 2,\n                 lookback: int = 50, dropout: float = 0.1):\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.lookback = lookback\n        self.dropout = dropout\n\n        # GARCH parameters (will be updated during fitting)\n        self.omega = 0.00001\n        self.alpha = 0.05\n        self.beta = 0.90\n\n        # LSTM model\n        self.model = None\n        self.is_fitted = False\n\n        if HAS_TORCH:\n            self._build_model()",
    "source_file": "core\\_experimental\\deep_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GARCHLSTMHybrid"
  },
  {
    "name": "_build_model",
    "category": "volatility",
    "formula": "class LSTMVolModel(nn.Module): | out",
    "explanation": "Build LSTM model.",
    "python_code": "def _build_model(self):\n        \"\"\"Build LSTM model.\"\"\"\n        if not HAS_TORCH:\n            return\n\n        class LSTMVolModel(nn.Module):\n            def __init__(self, input_size, hidden_size, n_layers, dropout):\n                super().__init__()\n                self.lstm = nn.LSTM(\n                    input_size=input_size,\n                    hidden_size=hidden_size,\n                    num_layers=n_layers,\n                    dropout=dropout,\n                    batch_first=True\n                )\n                self.fc = nn.Linear(hidden_size, 1)\n\n            def forward(self, x):\n                lstm_out, _ = self.lstm(x)\n                out = self.fc(lstm_out[:, -1, :])\n                return out\n\n        # Input: [returns, garch_vol, residuals, lagged_vols]\n        self.model = LSTMVolModel(4, self.hidden_size, self.n_layers, self.dropout)",
    "source_file": "core\\_experimental\\deep_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GARCHLSTMHybrid"
  },
  {
    "name": "_garch_filter",
    "category": "alpha_factor",
    "formula": "sigma2, residuals",
    "explanation": "Apply GARCH(1,1) filter.\n\nsigma_t^2 = omega + alpha * r_{t-1}^2 + beta * sigma_{t-1}^2\n\nReturns:\n    (conditional_variance, standardized_residuals)",
    "python_code": "def _garch_filter(self, returns: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Apply GARCH(1,1) filter.\n\n        sigma_t^2 = omega + alpha * r_{t-1}^2 + beta * sigma_{t-1}^2\n\n        Returns:\n            (conditional_variance, standardized_residuals)\n        \"\"\"\n        n = len(returns)\n        sigma2 = np.zeros(n)\n        sigma2[0] = np.var(returns[:min(20, n)])  # Initial variance\n\n        for t in range(1, n):\n            sigma2[t] = (self.omega +\n                        self.alpha * returns[t-1]**2 +\n                        self.beta * sigma2[t-1])\n\n        # Standardized residuals\n        residuals = returns / (np.sqrt(sigma2) + 1e-10)\n\n        return sigma2, residuals",
    "source_file": "core\\_experimental\\deep_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GARCHLSTMHybrid"
  },
  {
    "name": "fit",
    "category": "volatility",
    "formula": "# Step 1: Fit GARCH parameters | # Convert to tensors",
    "explanation": "Fit GARCH-LSTM model.\n\nArgs:\n    returns: Return series (in decimal, not percentage)\n    epochs: Training epochs\n    lr: Learning rate",
    "python_code": "def fit(self, returns: np.ndarray, epochs: int = 100, lr: float = 0.001):\n        \"\"\"\n        Fit GARCH-LSTM model.\n\n        Args:\n            returns: Return series (in decimal, not percentage)\n            epochs: Training epochs\n            lr: Learning rate\n        \"\"\"\n        if not HAS_TORCH:\n            logger.warning(\"PyTorch not available, using simple GARCH\")\n            self._fit_garch_only(returns)\n            return\n\n        # Step 1: Fit GARCH parameters\n        self._fit_garch_only(returns)\n\n        # Step 2: Get GARCH outputs\n        sigma2, residuals = self._garch_filter(returns)\n\n        # Step 3: Prepare LSTM training data\n        X, y = self._prepare_lstm_data(returns, sigma2, residuals)\n\n        if len(X) < 100:\n            logger.warning(\"Not enough data for LSTM training\")\n            self.is_fitted = True\n            return\n\n        # Convert to tensors\n        X_tensor = torch.FloatTensor(X)\n        y_tensor = torch.FloatTensor(y)\n\n        # Train\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n        criterion = nn.MSELoss()\n\n        self.model.train()\n        for epoch in range(epochs):\n            optimizer.zero_grad()\n            pred = self.model(X_tensor)\n            loss = criterion(pred, y_tensor)\n            loss.backward()\n            optimizer.step()\n\n            if epoch % 20 == 0:\n                logger.debug(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")\n\n        self.is_fitted = True\n        logger.info(\"GARCH-LSTM model fitted\")",
    "source_file": "core\\_experimental\\deep_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GARCHLSTMHybrid"
  },
  {
    "name": "_fit_garch_only",
    "category": "volatility",
    "formula": "",
    "explanation": "Fit simple GARCH(1,1) using maximum likelihood approximation.",
    "python_code": "def _fit_garch_only(self, returns: np.ndarray):\n        \"\"\"Fit simple GARCH(1,1) using maximum likelihood approximation.\"\"\"\n        # Estimate initial parameters\n        variance = np.var(returns)\n\n        # Grid search for best parameters\n        best_loss = float('inf')\n        best_params = (self.omega, self.alpha, self.beta)\n\n        for alpha in [0.01, 0.05, 0.1, 0.15]:\n            for beta in [0.8, 0.85, 0.9, 0.95]:\n                omega = variance * (1 - alpha - beta)\n                if omega <= 0:\n                    continue\n\n                # Calculate log-likelihood proxy\n                self.omega, self.alpha, self.beta = omega, alpha, beta\n                sigma2, _ = self._garch_filter(returns)\n\n                # Gaussian log-likelihood (simplified)\n                ll = -0.5 * np.sum(np.log(sigma2 + 1e-10) + returns**2 / (sigma2 + 1e-10))\n                loss = -ll\n\n                if loss < best_loss:\n                    best_loss = loss\n                    best_params = (omega, alpha, beta)\n\n        self.omega, self.alpha, self.beta = best_params\n        logger.info(f\"GARCH params: omega={self.omega:.6f}, alpha={self.alpha:.3f}, beta={self.beta:.3f}\")",
    "source_file": "core\\_experimental\\deep_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GARCHLSTMHybrid"
  },
  {
    "name": "_prepare_lstm_data",
    "category": "volatility",
    "formula": "np.array(X), np.array(y).reshape(-1, 1)",
    "explanation": "Prepare data for LSTM training.",
    "python_code": "def _prepare_lstm_data(self, returns: np.ndarray, sigma2: np.ndarray,\n                          residuals: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Prepare data for LSTM training.\"\"\"\n        n = len(returns)\n        X, y = [], []\n\n        for t in range(self.lookback, n - 1):\n            # Features: [returns, garch_vol, residuals, lagged_realized_vol]\n            features = np.column_stack([\n                returns[t-self.lookback:t],\n                np.sqrt(sigma2[t-self.lookback:t]),\n                residuals[t-self.lookback:t],\n                pd.Series(returns).rolling(10).std().values[t-self.lookback:t]\n            ])\n            X.append(features)\n\n            # Target: next period realized vol\n            y.append(np.abs(returns[t+1]))\n\n        return np.array(X), np.array(y).reshape(-1, 1)",
    "source_file": "core\\_experimental\\deep_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GARCHLSTMHybrid"
  },
  {
    "name": "predict",
    "category": "volatility",
    "formula": "series (at least lookback periods) | VolatilityPrediction(vol, vol, vol, vol, 1, 0.5) | VolatilityPrediction(",
    "explanation": "Predict volatility.\n\nArgs:\n    returns: Recent return series (at least lookback periods)\n\nReturns:\n    VolatilityPrediction with forecasts",
    "python_code": "def predict(self, returns: np.ndarray) -> VolatilityPrediction:\n        \"\"\"\n        Predict volatility.\n\n        Args:\n            returns: Recent return series (at least lookback periods)\n\n        Returns:\n            VolatilityPrediction with forecasts\n        \"\"\"\n        if len(returns) < self.lookback:\n            # Not enough data\n            vol = np.std(returns) if len(returns) > 1 else 0.01\n            return VolatilityPrediction(vol, vol, vol, vol, 1, 0.5)\n\n        # GARCH baseline\n        sigma2, residuals = self._garch_filter(returns)\n        current_vol = np.sqrt(sigma2[-1])\n\n        # LSTM adjustment\n        lstm_adj = 0.0\n        if HAS_TORCH and self.model is not None and self.is_fitted:\n            try:\n                features = np.column_stack([\n                    returns[-self.lookback:],\n                    np.sqrt(sigma2[-self.lookback:]),\n                    residuals[-self.lookback:],\n                    pd.Series(returns).rolling(10).std().values[-self.lookback:]\n                ])\n\n                self.model.eval()\n                with torch.no_grad():\n                    X = torch.FloatTensor(features).unsqueeze(0)\n                    lstm_adj = self.model(X).item()\n            except Exception as e:\n                logger.debug(f\"LSTM prediction failed: {e}\")\n\n        # Combined prediction\n        forecast_1 = max(0.0001, current_vol + lstm_adj)\n\n        # Multi-step forecasts using GARCH mean reversion\n        long_run_vol = np.sqrt(self.omega / (1 - self.alpha - self.beta + 1e-10))\n        forecast_5 = current_vol * 0.8 + long_run_vol * 0.2\n        forecast_10 = current_vol * 0.6 + long_run_vol * 0.4\n\n        # Regime classification\n        vol_percentile = (current_vol - long_run_vol) / (long_run_vol + 1e-10)\n        if vol_percentile < -0.3:\n            regime = 0  # Low vol\n        elif vol_percentile > 0.5:\n            regime = 2  # High vol\n        else:\n            regime = 1  # Normal\n\n        # Confidence based on residual normality\n        recent_residuals = residuals[-20:] if len(residuals) >= 20 else residuals\n        normality = 1 - min(1, np.abs(np.mean(recent_residuals)))\n\n        return VolatilityPrediction(\n            current_vol=current_vol,\n            forecast_1=forecast_1,\n            forecast_5=forecast_5,\n            forecast_10=forecast_10,\n            regime=regime,\n            confidence=normality\n        )",
    "source_file": "core\\_experimental\\deep_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GARCHLSTMHybrid"
  },
  {
    "name": "_build_model",
    "category": "volatility",
    "formula": "class GRULSTMModel(nn.Module): | out",
    "explanation": "Build GRU-LSTM hybrid model.",
    "python_code": "def _build_model(self, gru_layers, lstm_layers, dropout):\n        \"\"\"Build GRU-LSTM hybrid model.\"\"\"\n        if not HAS_TORCH:\n            return\n\n        class GRULSTMModel(nn.Module):\n            def __init__(self, input_size, hidden_size, gru_layers, lstm_layers, dropout):\n                super().__init__()\n                self.gru = nn.GRU(\n                    input_size=input_size,\n                    hidden_size=hidden_size,\n                    num_layers=gru_layers,\n                    dropout=dropout if gru_layers > 1 else 0,\n                    batch_first=True\n                )\n                self.lstm = nn.LSTM(\n                    input_size=hidden_size,\n                    hidden_size=hidden_size,\n                    num_layers=lstm_layers,\n                    dropout=dropout if lstm_layers > 1 else 0,\n                    batch_first=True\n                )\n                self.fc = nn.Sequential(\n                    nn.Linear(hidden_size, 32),\n                    nn.ReLU(),\n                    nn.Dropout(dropout),\n                    nn.Linear(32, 1),\n                    nn.Sigmoid()\n                )\n\n            def forward(self, x):\n                gru_out, _ = self.gru(x)\n                lstm_out, _ = self.lstm(gru_out)\n                out = self.fc(lstm_out[:, -1, :])\n                return out\n\n        self.model = GRULSTMModel(self.input_size, self.hidden_size,\n                                  gru_layers, lstm_layers, dropout)",
    "source_file": "core\\_experimental\\deep_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GRULSTMHybrid"
  },
  {
    "name": "prepare_features",
    "category": "volatility",
    "formula": "features.fillna(0).values",
    "explanation": "Prepare features for GRU-LSTM.\n\nStandard features for forex:\n- Returns at multiple lags\n- Volatility\n- Momentum\n- Mean reversion signals",
    "python_code": "def prepare_features(self, df: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Prepare features for GRU-LSTM.\n\n        Standard features for forex:\n        - Returns at multiple lags\n        - Volatility\n        - Momentum\n        - Mean reversion signals\n        \"\"\"\n        mid = df['mid'].values if 'mid' in df.columns else (df['bid'] + df['ask']).values / 2\n\n        features = pd.DataFrame(index=df.index)\n\n        # Returns\n        for lag in [1, 5, 10, 20]:\n            features[f'ret_{lag}'] = pd.Series(mid).pct_change(lag).values\n\n        # Volatility\n        features['vol_10'] = pd.Series(mid).pct_change().rolling(10).std().values\n        features['vol_20'] = pd.Series(mid).pct_change().rolling(20).std().values\n\n        # Momentum\n        features['mom_10'] = pd.Series(mid).pct_change(10).values\n        features['rsi_14'] = self._compute_rsi(mid, 14)\n\n        # Mean reversion\n        features['zscore_20'] = ((mid - pd.Series(mid).rolling(20).mean()) /\n                                 (pd.Series(mid).rolling(20).std() + 1e-10)).values\n\n        # Time features\n        if 'timestamp' in df.columns:\n            ts = pd.to_datetime(df['timestamp'])\n            features['hour_sin'] = np.sin(2 * np.pi * ts.dt.hour / 24)\n\n        self.feature_names = list(features.columns)\n        return features.fillna(0).values",
    "source_file": "core\\_experimental\\deep_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GRULSTMHybrid"
  },
  {
    "name": "_compute_rsi",
    "category": "volatility",
    "formula": "rsi",
    "explanation": "Compute RSI.",
    "python_code": "def _compute_rsi(self, prices: np.ndarray, period: int = 14) -> np.ndarray:\n        \"\"\"Compute RSI.\"\"\"\n        delta = np.diff(prices, prepend=prices[0])\n        gain = np.where(delta > 0, delta, 0)\n        loss = np.where(delta < 0, -delta, 0)\n\n        avg_gain = pd.Series(gain).rolling(period).mean().values\n        avg_loss = pd.Series(loss).rolling(period).mean().values\n\n        rs = avg_gain / (avg_loss + 1e-10)\n        rsi = 100 - 100 / (1 + rs)\n        return rsi",
    "source_file": "core\\_experimental\\deep_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GRULSTMHybrid"
  },
  {
    "name": "fit",
    "category": "volatility",
    "formula": "# Prepare features",
    "explanation": "Fit GRU-LSTM model.\n\nArgs:\n    df: DataFrame with features and target\n    target_col: Target column name\n    epochs: Training epochs\n    lr: Learning rate",
    "python_code": "def fit(self, df: pd.DataFrame, target_col: str = 'target_direction_5',\n           epochs: int = 50, lr: float = 0.001, batch_size: int = 64):\n        \"\"\"\n        Fit GRU-LSTM model.\n\n        Args:\n            df: DataFrame with features and target\n            target_col: Target column name\n            epochs: Training epochs\n            lr: Learning rate\n        \"\"\"\n        if not HAS_TORCH:\n            logger.warning(\"PyTorch not available\")\n            return\n\n        # Prepare features\n        X = self.prepare_features(df)\n        y = df[target_col].values if target_col in df.columns else np.zeros(len(df))\n\n        # Create sequences\n        X_seq, y_seq = [], []\n        for i in range(self.lookback, len(X)):\n            X_seq.append(X[i-self.lookback:i])\n            y_seq.append(y[i])\n\n        X_tensor = torch.FloatTensor(np.array(X_seq))\n        y_tensor = torch.FloatTensor(np.array(y_seq)).reshape(-1, 1)\n\n        # Train\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n        criterion = nn.BCELoss()\n\n        n_batches = len(X_tensor) // batch_size\n\n        self.model.train()\n        for epoch in range(epochs):\n            total_loss = 0\n            for b in range(n_batches):\n                start = b * batch_size\n                end = start + batch_size\n\n                optimizer.zero_grad()\n                pred = self.model(X_tensor[start:end])\n                loss = criterion(pred, y_tensor[start:end])\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n\n            if epoch % 10 == 0:\n                logger.info(f\"Epoch {epoch}, Loss: {total_loss/n_batches:.4f}\")\n\n        self.is_fitted = True\n        logger.info(\"GRU-LSTM model fitted\")",
    "source_file": "core\\_experimental\\deep_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GRULSTMHybrid"
  },
  {
    "name": "predict",
    "category": "volatility",
    "formula": "np.full(len(df), 0.5) | np.full(len(df), 0.5) | predictions",
    "explanation": "Predict direction probabilities.",
    "python_code": "def predict(self, df: pd.DataFrame) -> np.ndarray:\n        \"\"\"Predict direction probabilities.\"\"\"\n        if not HAS_TORCH or not self.is_fitted:\n            return np.full(len(df), 0.5)\n\n        X = self.prepare_features(df)\n\n        if len(X) < self.lookback:\n            return np.full(len(df), 0.5)\n\n        predictions = np.full(len(df), 0.5)\n\n        self.model.eval()\n        with torch.no_grad():\n            for i in range(self.lookback, len(X)):\n                seq = torch.FloatTensor(X[i-self.lookback:i]).unsqueeze(0)\n                pred = self.model(seq).item()\n                predictions[i] = pred\n\n        return predictions",
    "source_file": "core\\_experimental\\deep_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GRULSTMHybrid"
  },
  {
    "name": "fit",
    "category": "volatility",
    "formula": "",
    "explanation": "Fit volatility models.",
    "python_code": "def fit(self, df: pd.DataFrame):\n        \"\"\"Fit volatility models.\"\"\"\n        mid = df['mid'].values if 'mid' in df.columns else (df['bid'] + df['ask']).values / 2\n        returns = np.diff(np.log(mid))\n\n        self.garch_lstm.fit(returns, epochs=50)\n        self.is_fitted = True",
    "source_file": "core\\_experimental\\deep_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DeepVolatilityFeatures"
  },
  {
    "name": "compute_features",
    "category": "volatility",
    "formula": "features",
    "explanation": "Compute deep volatility features.",
    "python_code": "def compute_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Compute deep volatility features.\"\"\"\n        mid = df['mid'].values if 'mid' in df.columns else (df['bid'] + df['ask']).values / 2\n        returns = np.diff(np.log(mid), prepend=0)\n\n        features = pd.DataFrame(index=df.index)\n\n        # Get predictions\n        pred = self.garch_lstm.predict(returns)\n\n        features['garch_lstm_vol'] = pred.current_vol\n        features['garch_lstm_forecast_1'] = pred.forecast_1\n        features['garch_lstm_forecast_5'] = pred.forecast_5\n        features['garch_lstm_regime'] = pred.regime\n        features['garch_lstm_confidence'] = pred.confidence\n\n        # Vol surprise (realized vs forecast)\n        realized_vol = pd.Series(returns).rolling(10).std().values\n        features['vol_surprise'] = realized_vol - pred.forecast_1\n\n        return features",
    "source_file": "core\\_experimental\\deep_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DeepVolatilityFeatures"
  },
  {
    "name": "forward",
    "category": "volatility",
    "formula": "out",
    "explanation": "",
    "python_code": "def forward(self, x):\n                lstm_out, _ = self.lstm(x)\n                out = self.fc(lstm_out[:, -1, :])\n                return out",
    "source_file": "core\\_experimental\\deep_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "LSTMVolModel"
  },
  {
    "name": "__init__",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, capacity: int):\n        self.buffer = deque(maxlen=capacity)",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ReplayBuffer"
  },
  {
    "name": "push",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Add experience to buffer.",
    "python_code": "def push(self, state: np.ndarray, action: int, reward: float,\n             next_state: np.ndarray, done: bool):\n        \"\"\"Add experience to buffer.\"\"\"\n        self.buffer.append((state, action, reward, next_state, done))",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ReplayBuffer"
  },
  {
    "name": "sample",
    "category": "reinforcement_learning",
    "formula": "states, actions, rewards, next_states, dones",
    "explanation": "Sample batch of experiences.",
    "python_code": "def sample(self, batch_size: int) -> Tuple:\n        \"\"\"Sample batch of experiences.\"\"\"\n        batch = random.sample(self.buffer, batch_size)\n\n        states = np.array([e[0] for e in batch])\n        actions = np.array([e[1] for e in batch])\n        rewards = np.array([e[2] for e in batch])\n        next_states = np.array([e[3] for e in batch])\n        dones = np.array([e[4] for e in batch])\n\n        return states, actions, rewards, next_states, dones",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ReplayBuffer"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "q_values",
    "explanation": "",
    "python_code": "def forward(self, x: torch.Tensor) -> torch.Tensor:\n        features = self.feature(x)\n        value = self.value_stream(features)\n        advantage = self.advantage_stream(features)\n\n        # Combine: Q = V + A - mean(A)\n        q_values = value + advantage - advantage.mean(dim=1, keepdim=True)\n        return q_values",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DuelingDQN"
  },
  {
    "name": "reset_parameters",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def reset_parameters(self):\n        mu_range = 1 / np.sqrt(self.in_features)\n        self.weight_mu.data.uniform_(-mu_range, mu_range)\n        self.weight_sigma.data.fill_(self.sigma_init / np.sqrt(self.in_features))\n        self.bias_mu.data.uniform_(-mu_range, mu_range)\n        self.bias_sigma.data.fill_(self.sigma_init / np.sqrt(self.out_features))",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "NoisyLinear"
  },
  {
    "name": "reset_noise",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def reset_noise(self):\n        epsilon_in = self._scale_noise(self.in_features)\n        epsilon_out = self._scale_noise(self.out_features)\n        self.weight_epsilon.copy_(epsilon_out.outer(epsilon_in))\n        self.bias_epsilon.copy_(epsilon_out)",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "NoisyLinear"
  },
  {
    "name": "_scale_noise",
    "category": "reinforcement_learning",
    "formula": "x.sign() * x.abs().sqrt()",
    "explanation": "",
    "python_code": "def _scale_noise(self, size: int) -> torch.Tensor:\n        x = torch.randn(size)\n        return x.sign() * x.abs().sqrt()",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "NoisyLinear"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "F.linear(x, weight, bias)",
    "explanation": "",
    "python_code": "def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.training:\n            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n        else:\n            weight = self.weight_mu\n            bias = self.bias_mu\n        return F.linear(x, weight, bias)",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "NoisyLinear"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "value + advantage - advantage.mean(dim=1, keepdim=True)",
    "explanation": "",
    "python_code": "def forward(self, x: torch.Tensor) -> torch.Tensor:\n        features = self.feature(x)\n        value = self.value_stream(features)\n        advantage = self.advantage_stream(features)\n        return value + advantage - advantage.mean(dim=1, keepdim=True)",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "NoisyDuelingDQN"
  },
  {
    "name": "sample",
    "category": "reinforcement_learning",
    "formula": "states, actions, rewards, next_states, dones, indices, weights",
    "explanation": "Sample batch with priorities.",
    "python_code": "def sample(self, batch_size: int) -> Tuple:\n        \"\"\"Sample batch with priorities.\"\"\"\n        if len(self.buffer) == self.capacity:\n            priorities = self.priorities\n        else:\n            priorities = self.priorities[:len(self.buffer)]\n\n        # Calculate sampling probabilities\n        probs = priorities ** self.alpha\n        probs /= probs.sum()\n\n        # Sample indices\n        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n\n        # Calculate importance sampling weights\n        total = len(self.buffer)\n        weights = (total * probs[indices]) ** (-self.beta)\n        weights /= weights.max()\n\n        # Anneal beta\n        self.beta = min(1.0, self.beta + self.beta_increment)\n\n        # Gather experiences\n        batch = [self.buffer[i] for i in indices]\n        states = np.array([e[0] for e in batch])\n        actions = np.array([e[1] for e in batch])\n        rewards = np.array([e[2] for e in batch])\n        next_states = np.array([e[3] for e in batch])\n        dones = np.array([e[4] for e in batch])\n\n        return states, actions, rewards, next_states, dones, indices, weights",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "PrioritizedReplayBuffer"
  },
  {
    "name": "update_priorities",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Update priorities based on TD errors.",
    "python_code": "def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):\n        \"\"\"Update priorities based on TD errors.\"\"\"\n        for idx, priority in zip(indices, priorities):\n            self.priorities[idx] = priority + 1e-6  # Small constant to avoid zero\n            self.max_priority = max(self.max_priority, priority)",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "PrioritizedReplayBuffer"
  },
  {
    "name": "__init__",
    "category": "reinforcement_learning",
    "formula": "# Networks",
    "explanation": "",
    "python_code": "def __init__(self, config: DQNConfig = None, use_prioritized: bool = True,\n                 use_noisy: bool = True, use_double: bool = True):\n        self.config = config or DQNConfig()\n        self.use_prioritized = use_prioritized\n        self.use_double = use_double\n\n        if not HAS_TORCH:\n            logger.warning(\"PyTorch not available\")\n            return\n\n        # Networks\n        if use_noisy:\n            self.policy_net = NoisyDuelingDQN(\n                self.config.state_dim,\n                self.config.action_dim,\n                self.config.hidden_dim\n            )\n            self.target_net = NoisyDuelingDQN(\n                self.config.state_dim,\n                self.config.action_dim,\n                self.config.hidden_dim\n            )\n        else:\n            self.policy_net = DuelingDQN(\n                self.config.state_dim,\n                self.config.action_dim,\n                self.config.hidden_dim\n            )\n            self.target_net = DuelingDQN(\n                self.config.state_dim,\n                self.config.action_dim,\n                self.config.hidden_dim\n            )\n\n        self.target_net.load_state_dict(self.policy_net.state_dict())\n        self.target_net.eval()\n\n        # Optimizer\n        self.optimizer = optim.Adam(\n            self.policy_net.parameters(),\n            lr=self.config.learning_rate\n        )\n\n        # Replay buffer\n        if use_prioritized:\n            self.buffer = PrioritizedReplayBuffer(self.config.buffer_size)\n        else:\n            self.buffer = ReplayBuffer(self.config.buffer_size)\n\n        # Exploration\n        self.epsilon = self.config.epsilon_start\n        self.steps = 0\n\n        self.is_fitted = False",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DQNPositionSizer"
  },
  {
    "name": "prepare_state",
    "category": "volatility",
    "formula": "(bps) | pd.Series(mid).pct_change(20).iloc[-1] * 10000,  # 20-tick return | state",
    "explanation": "Prepare state vector from market data and account state.\n\nReturns 20-dimensional state:\n- 10 price features\n- 5 volatility/risk features\n- 5 account/position features",
    "python_code": "def prepare_state(self, df: pd.DataFrame, position: float = 0.0,\n                     account_value: float = 10000.0, peak_value: float = 10000.0,\n                     recent_pnl: float = 0.0) -> np.ndarray:\n        \"\"\"\n        Prepare state vector from market data and account state.\n\n        Returns 20-dimensional state:\n        - 10 price features\n        - 5 volatility/risk features\n        - 5 account/position features\n        \"\"\"\n        mid = df['mid'].values if 'mid' in df.columns else (df['bid'] + df['ask']).values / 2\n\n        # Price features (10)\n        price_features = [\n            pd.Series(mid).pct_change().iloc[-1] * 10000,  # Last return (bps)\n            pd.Series(mid).pct_change(5).iloc[-1] * 10000,  # 5-tick return\n            pd.Series(mid).pct_change(20).iloc[-1] * 10000,  # 20-tick return\n            pd.Series(mid).pct_change().rolling(10).mean().iloc[-1] * 10000,  # Momentum\n            pd.Series(mid).pct_change().rolling(20).mean().iloc[-1] * 10000,  # Longer momentum\n            ((mid[-1] - pd.Series(mid).rolling(20).mean().iloc[-1]) /\n             (pd.Series(mid).rolling(20).std().iloc[-1] + 1e-10)),  # Z-score\n            ((mid[-1] - pd.Series(mid).rolling(50).mean().iloc[-1]) /\n             (pd.Series(mid).rolling(50).std().iloc[-1] + 1e-10)),  # Longer z-score\n            pd.Series(mid).pct_change().diff().iloc[-1] * 10000,  # Acceleration\n            mid[-1] / mid[-20] - 1,  # Trend strength\n            (mid[-1] - min(mid[-20:])) / (max(mid[-20:]) - min(mid[-20:]) + 1e-10)  # Range position\n        ]\n\n        # Volatility features (5)\n        vol_features = [\n            pd.Series(mid).pct_change().rolling(10).std().iloc[-1] * 10000,  # Short vol\n            pd.Series(mid).pct_change().rolling(20).std().iloc[-1] * 10000,  # Medium vol\n            pd.Series(mid).pct_change().rolling(50).std().iloc[-1] * 10000,  # Long vol\n            pd.Series(mid).pct_change().rolling(10).std().iloc[-1] /\n            (pd.Series(mid).pct_change().rolling(50).std().iloc[-1] + 1e-10),  # Vol ratio\n            (max(mid[-20:]) - min(mid[-20:])) / mid[-1] * 10000  # Recent range\n        ]\n\n        # Account features (5)\n        drawdown = (peak_value - account_value) / peak_value\n        account_features = [\n            position,  # Current position (-1 to 1)\n            drawdown,  # Current drawdown (0 to 1)\n            recent_pnl / account_value,  # Recent PnL as fraction\n            account_value / peak_value,  # Account health (0 to 1)\n            abs(position)  # Position magnitude\n        ]\n\n        state = np.array(price_features + vol_features + account_features, dtype=np.float32)\n        state = np.nan_to_num(state, nan=0.0, posinf=0.0, neginf=0.0)\n\n        return state",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DQNPositionSizer"
  },
  {
    "name": "select_action",
    "category": "reinforcement_learning",
    "formula": "2  # Default to 50% | random.randrange(self.config.action_dim) | q_values.argmax().item()",
    "explanation": "Select action using epsilon-greedy or noisy exploration.",
    "python_code": "def select_action(self, state: np.ndarray, training: bool = True) -> int:\n        \"\"\"Select action using epsilon-greedy or noisy exploration.\"\"\"\n        if not HAS_TORCH:\n            return 2  # Default to 50%\n\n        if training and not isinstance(self.policy_net, NoisyDuelingDQN):\n            # Epsilon-greedy for non-noisy networks\n            if random.random() < self.epsilon:\n                return random.randrange(self.config.action_dim)\n\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n            q_values = self.policy_net(state_tensor)\n            return q_values.argmax().item()",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DQNPositionSizer"
  },
  {
    "name": "get_position_size",
    "category": "risk",
    "formula": "",
    "explanation": "Get optimal position size for given state.\n\nArgs:\n    state: State vector\n    direction: 1 for long, -1 for short\n\nReturns:\n    Position size as fraction (0.0 to 1.0)",
    "python_code": "def get_position_size(self, state: np.ndarray, direction: int = 1) -> float:\n        \"\"\"\n        Get optimal position size for given state.\n\n        Args:\n            state: State vector\n            direction: 1 for long, -1 for short\n\n        Returns:\n            Position size as fraction (0.0 to 1.0)\n        \"\"\"\n        action = self.select_action(state, training=False)\n        return self.POSITION_SIZES[action] * direction",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DQNPositionSizer"
  },
  {
    "name": "train_step",
    "category": "reinforcement_learning",
    "formula": "loss.item()",
    "explanation": "Perform one training step.",
    "python_code": "def train_step(self) -> Optional[float]:\n        \"\"\"Perform one training step.\"\"\"\n        if not HAS_TORCH:\n            return None\n\n        if len(self.buffer) < self.config.batch_size:\n            return None\n\n        # Sample batch\n        if self.use_prioritized:\n            states, actions, rewards, next_states, dones, indices, weights = \\\n                self.buffer.sample(self.config.batch_size)\n            weights = torch.FloatTensor(weights)\n        else:\n            states, actions, rewards, next_states, dones = \\\n                self.buffer.sample(self.config.batch_size)\n            weights = torch.ones(self.config.batch_size)\n\n        # Convert to tensors\n        states = torch.FloatTensor(states)\n        actions = torch.LongTensor(actions).unsqueeze(1)\n        rewards = torch.FloatTensor(rewards)\n        next_states = torch.FloatTensor(next_states)\n        dones = torch.FloatTensor(dones)\n\n        # Current Q values\n        current_q = self.policy_net(states).gather(1, actions).squeeze()\n\n        # Target Q values\n        with torch.no_grad():\n            if self.use_double:\n                # Double DQN: select action with policy, evaluate with target\n                next_actions = self.policy_net(next_states).argmax(1).unsqueeze(1)\n                next_q = self.target_net(next_states).gather(1, next_actions).squeeze()\n            else:\n                next_q = self.target_net(next_states).max(1)[0]\n\n            target_q = rewards + (1 - dones) * self.config.gamma * next_q\n\n        # Compute loss\n        td_errors = current_q - target_q\n        loss = (weights * td_errors.pow(2)).mean()\n\n        # Update priorities if using prioritized replay\n        if self.use_prioritized:\n            self.buffer.update_priorities(indices, td_errors.abs().detach().numpy())\n\n        # Optimize\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10)\n        self.optimizer.step()\n\n        # Update target network (soft update)\n        if self.steps % self.config.target_update == 0:\n            for target_param, policy_param in zip(\n                self.target_net.parameters(), self.policy_net.parameters()\n            ):\n                target_param.data.copy_(\n                    self.config.tau * policy_param.data +\n                    (1 - self.config.tau) * target_param.data\n                )\n\n        # Update epsilon\n        self.epsilon = max(\n            self.config.epsilon_end,\n            self.epsilon * self.config.epsilon_decay\n        )\n\n        # Reset noise for noisy networks\n        if isinstance(self.policy_net, NoisyDuelingDQN):\n            self.policy_net.reset_noise()\n            self.target_net.reset_noise()\n\n        self.steps += 1\n        return loss.item()",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DQNPositionSizer"
  },
  {
    "name": "train",
    "category": "reinforcement_learning",
    "formula": "{} | history",
    "explanation": "Train DQN on historical data.\n\nSimulates trading and learns optimal position sizes.",
    "python_code": "def train(self, df: pd.DataFrame, epochs: int = 100,\n             transaction_cost: float = 0.0001) -> Dict[str, List[float]]:\n        \"\"\"\n        Train DQN on historical data.\n\n        Simulates trading and learns optimal position sizes.\n        \"\"\"\n        if not HAS_TORCH:\n            logger.warning(\"PyTorch not available\")\n            return {}\n\n        mid = df['mid'].values if 'mid' in df.columns else (df['bid'] + df['ask']).values / 2\n        returns = pd.Series(mid).pct_change().fillna(0).values\n\n        history = {'rewards': [], 'losses': [], 'positions': []}\n\n        for epoch in range(epochs):\n            # Initialize episode\n            account_value = 10000.0\n            peak_value = 10000.0\n            position = 0.0\n            recent_pnl = 0.0\n            episode_reward = 0.0\n\n            for t in range(100, len(df) - 1):\n                # Get state\n                state = self.prepare_state(\n                    df.iloc[t-100:t],\n                    position=position,\n                    account_value=account_value,\n                    peak_value=peak_value,\n                    recent_pnl=recent_pnl\n                )\n\n                # Select action\n                action = self.select_action(state, training=True)\n                new_position = self.POSITION_SIZES[action]\n\n                # Calculate reward\n                ret = returns[t + 1]\n                pnl = new_position * ret * account_value\n                cost = abs(new_position - position) * transaction_cost * account_value\n\n                account_value += pnl - cost\n                peak_value = max(peak_value, account_value)\n                recent_pnl = pnl - cost\n\n                # Risk-adjusted reward\n                drawdown = (peak_value - account_value) / peak_value\n                reward = (pnl - cost) / 100  # Scale reward\n                reward -= drawdown * 10  # Penalty for drawdown\n\n                episode_reward += reward\n\n                # Get next state\n                if t + 1 < len(df) - 1:\n                    next_state = self.prepare_state(\n                        df.iloc[t-99:t+1],\n                        position=new_position,\n                        account_value=account_value,\n                        peak_value=peak_value,\n                        recent_pnl=recent_pnl\n                    )\n                    done = False\n                else:\n                    next_state = state\n                    done = True\n\n                # Store experience\n                self.buffer.push(state, action, reward, next_state, done)\n\n                # Train\n                loss = self.train_step()\n                if loss is not None:\n                    history['losses'].append(loss)\n\n                position = new_position\n                history['positions'].append(position)\n\n            history['rewards'].append(episode_reward)\n\n            if epoch % 10 == 0:\n                avg_reward = np.mean(history['rewards'][-10:]) if history['rewards'] else 0\n    ",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DQNPositionSizer"
  },
  {
    "name": "predict",
    "category": "risk",
    "formula": "{ | {",
    "explanation": "Predict optimal position size.\n\nReturns:\n    Dict with position_size, action, q_values",
    "python_code": "def predict(self, df: pd.DataFrame, position: float = 0.0,\n               account_value: float = 10000.0, peak_value: float = 10000.0,\n               direction: int = 1) -> Dict[str, Any]:\n        \"\"\"\n        Predict optimal position size.\n\n        Returns:\n            Dict with position_size, action, q_values\n        \"\"\"\n        if not HAS_TORCH or not self.is_fitted:\n            return {\n                'position_size': 0.5,\n                'action': 2,\n                'q_values': [0.0] * 5,\n                'confidence': 0.0\n            }\n\n        state = self.prepare_state(df, position, account_value, peak_value)\n\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n            q_values = self.policy_net(state_tensor)[0].numpy()\n\n        action = int(np.argmax(q_values))\n        position_size = self.POSITION_SIZES[action] * direction\n\n        # Confidence based on Q-value gap\n        sorted_q = np.sort(q_values)[::-1]\n        confidence = (sorted_q[0] - sorted_q[1]) / (abs(sorted_q[0]) + 1e-10)\n\n        return {\n            'position_size': position_size,\n            'action': action,\n            'q_values': q_values.tolist(),\n            'confidence': min(1.0, max(0.0, confidence))\n        }",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DQNPositionSizer"
  },
  {
    "name": "__init__",
    "category": "reinforcement_learning",
    "formula": "# Actor network (policy)",
    "explanation": "",
    "python_code": "def __init__(self, state_dim: int = 20, hidden_dim: int = 128,\n                 lr: float = 3e-4, gamma: float = 0.99,\n                 clip_epsilon: float = 0.2, epochs_per_update: int = 10):\n        self.state_dim = state_dim\n        self.hidden_dim = hidden_dim\n        self.gamma = gamma\n        self.clip_epsilon = clip_epsilon\n        self.epochs_per_update = epochs_per_update\n\n        if not HAS_TORCH:\n            return\n\n        # Actor network (policy)\n        self.actor = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, 2)  # Mean and log_std\n        )\n\n        # Critic network (value)\n        self.critic = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n        self.optimizer = optim.Adam(\n            list(self.actor.parameters()) + list(self.critic.parameters()),\n            lr=lr\n        )\n\n        self.is_fitted = False",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "PPOPositionSizer"
  },
  {
    "name": "get_action",
    "category": "reinforcement_learning",
    "formula": "0.5, 0.0 | action.item(), log_std.item()",
    "explanation": "Get action from policy.",
    "python_code": "def get_action(self, state: np.ndarray, training: bool = True) -> Tuple[float, float]:\n        \"\"\"Get action from policy.\"\"\"\n        if not HAS_TORCH:\n            return 0.5, 0.0\n\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        output = self.actor(state_tensor)\n        mean = torch.sigmoid(output[0, 0])  # Position size in [0, 1]\n        log_std = output[0, 1].clamp(-2, 0)  # Bounded log std\n\n        if training:\n            std = torch.exp(log_std)\n            action = torch.normal(mean, std)\n            action = action.clamp(0, 1)\n        else:\n            action = mean\n\n        return action.item(), log_std.item()",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "PPOPositionSizer"
  },
  {
    "name": "get_value",
    "category": "reinforcement_learning",
    "formula": "0.0",
    "explanation": "Get value estimate.",
    "python_code": "def get_value(self, state: np.ndarray) -> float:\n        \"\"\"Get value estimate.\"\"\"\n        if not HAS_TORCH:\n            return 0.0\n\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        return self.critic(state_tensor).item()",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "PPOPositionSizer"
  },
  {
    "name": "update",
    "category": "reinforcement_learning",
    "formula": "0.0 | total_loss / self.epochs_per_update",
    "explanation": "Update policy using PPO objective.",
    "python_code": "def update(self, states: np.ndarray, actions: np.ndarray,\n               old_log_probs: np.ndarray, returns: np.ndarray,\n               advantages: np.ndarray) -> float:\n        \"\"\"Update policy using PPO objective.\"\"\"\n        if not HAS_TORCH:\n            return 0.0\n\n        states = torch.FloatTensor(states)\n        actions = torch.FloatTensor(actions)\n        old_log_probs = torch.FloatTensor(old_log_probs)\n        returns = torch.FloatTensor(returns)\n        advantages = torch.FloatTensor(advantages)\n\n        # Normalize advantages\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n        total_loss = 0\n        for _ in range(self.epochs_per_update):\n            # Get current policy distribution\n            output = self.actor(states)\n            means = torch.sigmoid(output[:, 0])\n            log_stds = output[:, 1].clamp(-2, 0)\n            stds = torch.exp(log_stds)\n\n            # Calculate log probs\n            dist = torch.distributions.Normal(means, stds)\n            log_probs = dist.log_prob(actions)\n\n            # PPO clipped objective\n            ratio = torch.exp(log_probs - old_log_probs)\n            surr1 = ratio * advantages\n            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n            policy_loss = -torch.min(surr1, surr2).mean()\n\n            # Value loss\n            values = self.critic(states).squeeze()\n            value_loss = F.mse_loss(values, returns)\n\n            # Entropy bonus\n            entropy = dist.entropy().mean()\n\n            # Total loss\n            loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n\n            self.optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(\n                list(self.actor.parameters()) + list(self.critic.parameters()), 0.5\n            )\n            self.optimizer.step()\n\n            total_loss += loss.item()\n\n        return total_loss / self.epochs_per_update",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "PPOPositionSizer"
  },
  {
    "name": "create_position_sizer",
    "category": "risk",
    "formula": "DQNPositionSizer( | PPOPositionSizer(",
    "explanation": "Factory function for position sizers.\n\nArgs:\n    method: 'dqn' or 'ppo'\n    **kwargs: Configuration parameters\n\nReturns:\n    Position sizer instance",
    "python_code": "def create_position_sizer(method: str = 'dqn', **kwargs) -> Any:\n    \"\"\"\n    Factory function for position sizers.\n\n    Args:\n        method: 'dqn' or 'ppo'\n        **kwargs: Configuration parameters\n\n    Returns:\n        Position sizer instance\n    \"\"\"\n    if method == 'dqn':\n        config = DQNConfig(**{k: v for k, v in kwargs.items() if hasattr(DQNConfig, k)})\n        return DQNPositionSizer(\n            config,\n            use_prioritized=kwargs.get('use_prioritized', True),\n            use_noisy=kwargs.get('use_noisy', True),\n            use_double=kwargs.get('use_double', True)\n        )\n    elif method == 'ppo':\n        return PPOPositionSizer(\n            state_dim=kwargs.get('state_dim', 20),\n            hidden_dim=kwargs.get('hidden_dim', 128),\n            lr=kwargs.get('lr', 3e-4)\n        )\n    else:\n        raise ValueError(f\"Unknown method: {method}. Use 'dqn' or 'ppo'.\")",
    "source_file": "core\\_experimental\\dqn_position_sizing.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, forward_periods: int = 1):\n        self.forward_periods = forward_periods\n        self.ic_history = []",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "InformationCoefficientEngine"
  },
  {
    "name": "calculate_ic",
    "category": "reinforcement_learning",
    "formula": "0.0 | ic if not np.isnan(ic) else 0.0",
    "explanation": "Calculate Information Coefficient.\n\nArgs:\n    factor: Factor values\n    forward_return: Forward returns\n    method: 'pearson' or 'spearman'\n\nReturns:\n    IC value",
    "python_code": "def calculate_ic(self, factor: pd.Series,\n                     forward_return: pd.Series,\n                     method: str = 'spearman') -> float:\n        \"\"\"\n        Calculate Information Coefficient.\n\n        Args:\n            factor: Factor values\n            forward_return: Forward returns\n            method: 'pearson' or 'spearman'\n\n        Returns:\n            IC value\n        \"\"\"\n        # Align and drop NaN\n        df = pd.DataFrame({'factor': factor, 'return': forward_return}).dropna()\n\n        if len(df) < 10:\n            return 0.0\n\n        if method == 'spearman':\n            ic, _ = stats.spearmanr(df['factor'], df['return'])\n        else:\n            ic, _ = stats.pearsonr(df['factor'], df['return'])\n\n        return ic if not np.isnan(ic) else 0.0",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "InformationCoefficientEngine"
  },
  {
    "name": "calculate_icir",
    "category": "reinforcement_learning",
    "formula": "0.0 | ic_series.mean() / ic_series.std()",
    "explanation": "Calculate IC Information Ratio.",
    "python_code": "def calculate_icir(self, ic_series: pd.Series) -> float:\n        \"\"\"Calculate IC Information Ratio.\"\"\"\n        if len(ic_series) < 5 or ic_series.std() == 0:\n            return 0.0\n        return ic_series.mean() / ic_series.std()",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "InformationCoefficientEngine"
  },
  {
    "name": "rolling_ic",
    "category": "statistical",
    "formula": "pd.Series(ic_values, index=factor.index[window:])",
    "explanation": "Calculate rolling IC.",
    "python_code": "def rolling_ic(self, factor: pd.Series, returns: pd.Series,\n                   window: int = 20) -> pd.Series:\n        \"\"\"Calculate rolling IC.\"\"\"\n        ic_values = []\n\n        for i in range(window, len(factor)):\n            f = factor.iloc[i-window:i]\n            r = returns.iloc[i-window:i]\n            ic = self.calculate_ic(f, r)\n            ic_values.append(ic)\n\n        return pd.Series(ic_values, index=factor.index[window:])",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "InformationCoefficientEngine"
  },
  {
    "name": "evaluate_factor",
    "category": "statistical",
    "formula": "FactorEvaluation(",
    "explanation": "Complete factor evaluation.\n\nReturns IC, ICIR, Rank IC, T-stat, Turnover, Decay",
    "python_code": "def evaluate_factor(self, factor: pd.Series,\n                       returns: pd.Series) -> FactorEvaluation:\n        \"\"\"\n        Complete factor evaluation.\n\n        Returns IC, ICIR, Rank IC, T-stat, Turnover, Decay\n        \"\"\"\n        # Align data\n        forward_ret = returns.shift(-self.forward_periods)\n        df = pd.DataFrame({\n            'factor': factor,\n            'return': forward_ret\n        }).dropna()\n\n        # IC and Rank IC\n        ic = self.calculate_ic(df['factor'], df['return'], 'pearson')\n        rank_ic = self.calculate_ic(df['factor'], df['return'], 'spearman')\n\n        # Rolling IC for ICIR\n        rolling = self.rolling_ic(df['factor'], df['return'])\n        icir = self.calculate_icir(rolling) if len(rolling) > 0 else 0.0\n\n        # T-statistic\n        n = len(df)\n        t_stat = ic * np.sqrt(n - 2) / np.sqrt(1 - ic**2 + 1e-10)\n\n        # Turnover\n        factor_diff = factor.diff().abs()\n        turnover = factor_diff.mean() / (factor.abs().mean() + 1e-10)\n\n        # Decay half-life\n        decay_halflife = self._estimate_decay(factor, returns)\n\n        return FactorEvaluation(\n            ic=ic, icir=icir, rank_ic=rank_ic,\n            t_stat=t_stat, turnover=turnover,\n            decay_halflife=decay_halflife\n        )",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "InformationCoefficientEngine"
  },
  {
    "name": "_estimate_decay",
    "category": "statistical",
    "formula": "float('inf') | i + 1 | max_lag",
    "explanation": "Estimate factor decay half-life.",
    "python_code": "def _estimate_decay(self, factor: pd.Series,\n                        returns: pd.Series,\n                        max_lag: int = 20) -> float:\n        \"\"\"Estimate factor decay half-life.\"\"\"\n        ics = []\n        for lag in range(1, max_lag + 1):\n            forward_ret = returns.shift(-lag)\n            ic = self.calculate_ic(factor, forward_ret)\n            ics.append(abs(ic))\n\n        if len(ics) == 0 or ics[0] == 0:\n            return float('inf')\n\n        # Find where IC drops to half\n        initial_ic = ics[0]\n        for i, ic in enumerate(ics):\n            if ic < initial_ic / 2:\n                return i + 1\n\n        return max_lag",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "InformationCoefficientEngine"
  },
  {
    "name": "fit_transform",
    "category": "reinforcement_learning",
    "formula": "factor_df | result",
    "explanation": "Orthogonalize factors using PCA.\n\nArgs:\n    factor_df: DataFrame with factor columns\n\nReturns:\n    DataFrame with orthogonalized PC factors",
    "python_code": "def fit_transform(self, factor_df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Orthogonalize factors using PCA.\n\n        Args:\n            factor_df: DataFrame with factor columns\n\n        Returns:\n            DataFrame with orthogonalized PC factors\n        \"\"\"\n        if not HAS_SKLEARN:\n            logger.warning(\"sklearn not available\")\n            return factor_df\n\n        self.feature_names = list(factor_df.columns)\n\n        # Standardize\n        self.scaler = StandardScaler()\n        X = self.scaler.fit_transform(factor_df.fillna(0))\n\n        # Determine components\n        if self.n_components is None:\n            # Auto-select based on variance\n            pca_full = PCA()\n            pca_full.fit(X)\n            cumvar = np.cumsum(pca_full.explained_variance_ratio_)\n            self.n_components = np.argmax(cumvar >= self.variance_threshold) + 1\n\n        # Fit PCA\n        self.pca = PCA(n_components=self.n_components)\n        pc_factors = self.pca.fit_transform(X)\n\n        # Create DataFrame\n        pc_columns = [f'PC_{i+1}' for i in range(self.n_components)]\n        result = pd.DataFrame(pc_factors, index=factor_df.index, columns=pc_columns)\n\n        logger.info(f\"PCA reduced {len(self.feature_names)} factors to {self.n_components} PCs\")\n        logger.info(f\"Explained variance: {self.pca.explained_variance_ratio_.sum():.2%}\")\n\n        return result",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "PCAFactorOrthogonalizer"
  },
  {
    "name": "get_factor_loadings",
    "category": "reinforcement_learning",
    "formula": "pd.DataFrame() | loadings",
    "explanation": "Get factor loadings for interpretation.",
    "python_code": "def get_factor_loadings(self) -> pd.DataFrame:\n        \"\"\"Get factor loadings for interpretation.\"\"\"\n        if self.pca is None:\n            return pd.DataFrame()\n\n        loadings = pd.DataFrame(\n            self.pca.components_.T,\n            index=self.feature_names,\n            columns=[f'PC_{i+1}' for i in range(self.n_components)]\n        )\n        return loadings",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "PCAFactorOrthogonalizer"
  },
  {
    "name": "fit",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Fit Elastic Net to find optimal factor weights.\n\nArgs:\n    factor_df: DataFrame with factor columns\n    returns: Target returns (forward)\n\nReturns:\n    self",
    "python_code": "def fit(self, factor_df: pd.DataFrame,\n            returns: pd.Series) -> 'ElasticNetFactorCombiner':\n        \"\"\"\n        Fit Elastic Net to find optimal factor weights.\n\n        Args:\n            factor_df: DataFrame with factor columns\n            returns: Target returns (forward)\n\n        Returns:\n            self\n        \"\"\"\n        if not HAS_SKLEARN:\n            logger.warning(\"sklearn not available\")\n            return self\n\n        # Align data\n        df = factor_df.copy()\n        df['return'] = returns\n        df = df.dropna()\n\n        X = df.drop('return', axis=1)\n        y = df['return']\n\n        self.feature_names = list(X.columns)\n\n        # Standardize\n        self.scaler = StandardScaler()\n        X_scaled = self.scaler.fit_transform(X)\n\n        # Fit Elastic Net\n        self.model = ElasticNet(alpha=self.alpha, l1_ratio=self.l1_ratio)\n        self.model.fit(X_scaled, y)\n\n        # Store coefficients\n        self.coefficients = dict(zip(self.feature_names, self.model.coef_))\n\n        # Log non-zero factors\n        non_zero = sum(1 for c in self.model.coef_ if abs(c) > 1e-6)\n        logger.info(f\"Elastic Net selected {non_zero}/{len(self.feature_names)} factors\")\n\n        return self",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "ElasticNetFactorCombiner"
  },
  {
    "name": "predict",
    "category": "reinforcement_learning",
    "formula": "pd.Series(0, index=factor_df.index) | pd.Series(self.model.predict(X_scaled), index=factor_df.index)",
    "explanation": "Predict combined factor signal.",
    "python_code": "def predict(self, factor_df: pd.DataFrame) -> pd.Series:\n        \"\"\"Predict combined factor signal.\"\"\"\n        if self.model is None or self.scaler is None:\n            return pd.Series(0, index=factor_df.index)\n\n        X = factor_df[self.feature_names].fillna(0)\n        X_scaled = self.scaler.transform(X)\n\n        return pd.Series(self.model.predict(X_scaled), index=factor_df.index)",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "ElasticNetFactorCombiner"
  },
  {
    "name": "get_top_factors",
    "category": "reinforcement_learning",
    "formula": "sorted_factors[:n]",
    "explanation": "Get top N factors by absolute weight.",
    "python_code": "def get_top_factors(self, n: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"Get top N factors by absolute weight.\"\"\"\n        sorted_factors = sorted(\n            self.coefficients.items(),\n            key=lambda x: abs(x[1]),\n            reverse=True\n        )\n        return sorted_factors[:n]",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "ElasticNetFactorCombiner"
  },
  {
    "name": "calculate_skewness",
    "category": "statistical",
    "formula": "skew = more likely to have large negative returns | returns.rolling(self.window).skew()",
    "explanation": "Rolling skewness factor.\n\nNegative skew = more likely to have large negative returns",
    "python_code": "def calculate_skewness(self, returns: pd.Series) -> pd.Series:\n        \"\"\"\n        Rolling skewness factor.\n\n        Negative skew = more likely to have large negative returns\n        \"\"\"\n        return returns.rolling(self.window).skew()",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "HigherOrderMoments"
  },
  {
    "name": "calculate_kurtosis",
    "category": "statistical",
    "formula": "kurtosis = fat tails = more extreme moves | returns.rolling(self.window).kurt()",
    "explanation": "Rolling kurtosis factor.\n\nHigh kurtosis = fat tails = more extreme moves",
    "python_code": "def calculate_kurtosis(self, returns: pd.Series) -> pd.Series:\n        \"\"\"\n        Rolling kurtosis factor.\n\n        High kurtosis = fat tails = more extreme moves\n        \"\"\"\n        return returns.rolling(self.window).kurt()",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "HigherOrderMoments"
  },
  {
    "name": "calculate_downside_skew",
    "category": "reinforcement_learning",
    "formula": "0 | stats.skew(neg) | returns.rolling(self.window).apply(downside_skew, raw=True)",
    "explanation": "Downside skewness - only negative returns.\n\nMore relevant for risk assessment.",
    "python_code": "def calculate_downside_skew(self, returns: pd.Series) -> pd.Series:\n        \"\"\"\n        Downside skewness - only negative returns.\n\n        More relevant for risk assessment.\n        \"\"\"\n        def downside_skew(x):\n            neg = x[x < 0]\n            if len(neg) < 3:\n                return 0\n            return stats.skew(neg)\n\n        return returns.rolling(self.window).apply(downside_skew, raw=True)",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "HigherOrderMoments"
  },
  {
    "name": "calculate_tail_risk",
    "category": "risk",
    "formula": "x[x <= threshold].mean() | returns.rolling(self.window).apply(expected_shortfall, raw=True)",
    "explanation": "Tail risk factor (Expected Shortfall / CVaR).\n\nAverage loss in worst X% of cases.",
    "python_code": "def calculate_tail_risk(self, returns: pd.Series,\n                           percentile: float = 0.05) -> pd.Series:\n        \"\"\"\n        Tail risk factor (Expected Shortfall / CVaR).\n\n        Average loss in worst X% of cases.\n        \"\"\"\n        def expected_shortfall(x):\n            threshold = np.percentile(x, percentile * 100)\n            return x[x <= threshold].mean()\n\n        return returns.rolling(self.window).apply(expected_shortfall, raw=True)",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "HigherOrderMoments"
  },
  {
    "name": "generate_all",
    "category": "reinforcement_learning",
    "formula": "result",
    "explanation": "Generate all higher-order moment factors.",
    "python_code": "def generate_all(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Generate all higher-order moment factors.\"\"\"\n        result = df.copy()\n\n        returns = df['close'].pct_change()\n\n        result['skewness'] = self.calculate_skewness(returns)\n        result['kurtosis'] = self.calculate_kurtosis(returns)\n        result['downside_skew'] = self.calculate_downside_skew(returns)\n        result['tail_risk'] = self.calculate_tail_risk(returns)\n\n        # Normalize skewness for trading signal\n        result['skew_zscore'] = (\n            (result['skewness'] - result['skewness'].rolling(100).mean()) /\n            (result['skewness'].rolling(100).std() + 1e-10)\n        )\n\n        return result",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "HigherOrderMoments"
  },
  {
    "name": "calculate_first_hour_momentum",
    "category": "technical",
    "formula": "pd.Series(0, index=df.index) | returns = df['close'].pct_change() | first_hour.fillna(0)",
    "explanation": "First N minutes momentum.\n\nStrong opens often continue.",
    "python_code": "def calculate_first_hour_momentum(self, df: pd.DataFrame,\n                                      minutes: int = 30) -> pd.Series:\n        \"\"\"\n        First N minutes momentum.\n\n        Strong opens often continue.\n        \"\"\"\n        if 'timestamp' not in df.columns and not isinstance(df.index, pd.DatetimeIndex):\n            return pd.Series(0, index=df.index)\n\n        # Group by date, calculate first hour return\n        returns = df['close'].pct_change()\n\n        # Simplified: use rolling as proxy\n        first_hour = returns.rolling(minutes).sum()\n\n        return first_hour.fillna(0)",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "IntradayMomentum"
  },
  {
    "name": "calculate_last_hour_momentum",
    "category": "technical",
    "formula": "last_hour.fillna(0)",
    "explanation": "Last N minutes momentum.\n\nOften driven by institutional rebalancing.",
    "python_code": "def calculate_last_hour_momentum(self, df: pd.DataFrame,\n                                     minutes: int = 30) -> pd.Series:\n        \"\"\"\n        Last N minutes momentum.\n\n        Often driven by institutional rebalancing.\n        \"\"\"\n        returns = df['close'].pct_change()\n\n        # Shift to get \"last hour\" effect\n        last_hour = returns.shift(-minutes).rolling(minutes).sum()\n\n        return last_hour.fillna(0)",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "IntradayMomentum"
  },
  {
    "name": "calculate_overnight_return",
    "category": "technical",
    "formula": "factor. | pd.Series(overnight_signal, index=df.index)",
    "explanation": "Overnight return factor.\n\nFor forex: Return during low-liquidity hours.",
    "python_code": "def calculate_overnight_return(self, df: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Overnight return factor.\n\n        For forex: Return during low-liquidity hours.\n        \"\"\"\n        # Simplified: gap between sessions\n        returns = df['close'].pct_change()\n\n        # Large gaps often mean reversal\n        gap = returns.abs()\n        threshold = gap.rolling(100).quantile(0.9)\n\n        overnight_signal = np.where(gap > threshold, -np.sign(returns), 0)\n\n        return pd.Series(overnight_signal, index=df.index)",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "IntradayMomentum"
  },
  {
    "name": "calculate_session_momentum",
    "category": "technical",
    "formula": "pd.Series(0, index=df.index) | session_mom.fillna(0)",
    "explanation": "Session-specific momentum.\n\nDifferent sessions have different characteristics.",
    "python_code": "def calculate_session_momentum(self, df: pd.DataFrame,\n                                   session: str = 'london') -> pd.Series:\n        \"\"\"\n        Session-specific momentum.\n\n        Different sessions have different characteristics.\n        \"\"\"\n        if session not in self.session_hours:\n            return pd.Series(0, index=df.index)\n\n        start_hour, end_hour = self.session_hours[session]\n        returns = df['close'].pct_change()\n\n        # Would need timestamp for proper implementation\n        # Simplified: rolling momentum\n        session_mom = returns.rolling(int((end_hour - start_hour) * 60)).sum()\n\n        return session_mom.fillna(0)",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "IntradayMomentum"
  },
  {
    "name": "calculate_ic_weights",
    "category": "statistical",
    "formula": "weights",
    "explanation": "Calculate IC-based weights with exponential decay.\n\nRecent IC matters more than old IC.",
    "python_code": "def calculate_ic_weights(self, factor_df: pd.DataFrame,\n                            returns: pd.Series) -> Dict[str, float]:\n        \"\"\"\n        Calculate IC-based weights with exponential decay.\n\n        Recent IC matters more than old IC.\n        \"\"\"\n        weights = {}\n\n        for col in factor_df.columns:\n            # Rolling IC\n            ic_series = self.ic_engine.rolling_ic(\n                factor_df[col], returns, window=self.lookback\n            )\n\n            if len(ic_series) == 0:\n                weights[col] = 0.0\n                continue\n\n            # Exponentially weighted IC\n            decay_weights = self.decay ** np.arange(len(ic_series))[::-1]\n            weighted_ic = np.average(ic_series, weights=decay_weights)\n\n            # ICIR scaling\n            icir = self.ic_engine.calculate_icir(ic_series)\n\n            # Weight = IC * ICIR (both matter)\n            weights[col] = weighted_ic * max(0, icir)\n\n        # Normalize\n        total = sum(abs(w) for w in weights.values())\n        if total > 0:\n            weights = {k: v / total for k, v in weights.items()}\n\n        self.weights = weights\n        return weights",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "AdaptiveFactorWeighting"
  },
  {
    "name": "combine_factors",
    "category": "reinforcement_learning",
    "formula": "combined",
    "explanation": "Combine factors with adaptive weights.\n\nReturns combined signal.",
    "python_code": "def combine_factors(self, factor_df: pd.DataFrame,\n                       returns: pd.Series) -> pd.Series:\n        \"\"\"\n        Combine factors with adaptive weights.\n\n        Returns combined signal.\n        \"\"\"\n        weights = self.calculate_ic_weights(factor_df, returns)\n\n        combined = pd.Series(0.0, index=factor_df.index)\n        for col, weight in weights.items():\n            combined += weight * factor_df[col].fillna(0)\n\n        return combined",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "AdaptiveFactorWeighting"
  },
  {
    "name": "calculate_momentum_rank",
    "category": "technical",
    "formula": "normalized",
    "explanation": "Rank assets by momentum.\n\nArgs:\n    returns_df: DataFrame with asset returns as columns\n\nReturns:\n    DataFrame with ranks (1 = strongest)",
    "python_code": "def calculate_momentum_rank(self, returns_df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Rank assets by momentum.\n\n        Args:\n            returns_df: DataFrame with asset returns as columns\n\n        Returns:\n            DataFrame with ranks (1 = strongest)\n        \"\"\"\n        # Rolling momentum\n        momentum = returns_df.rolling(self.lookback).sum()\n\n        # Cross-sectional rank (1 = best)\n        ranks = momentum.rank(axis=1, ascending=False)\n\n        # Normalize to [-1, 1]\n        n_assets = returns_df.shape[1]\n        normalized = 2 * (ranks - 1) / (n_assets - 1) - 1\n\n        return normalized",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "CrossSectionalMomentum"
  },
  {
    "name": "calculate_relative_strength",
    "category": "technical",
    "formula": "rel_strength",
    "explanation": "Relative strength vs benchmark.\n\nRS > 1: Outperforming\nRS < 1: Underperforming",
    "python_code": "def calculate_relative_strength(self, prices_df: pd.DataFrame,\n                                    benchmark: pd.Series) -> pd.DataFrame:\n        \"\"\"\n        Relative strength vs benchmark.\n\n        RS > 1: Outperforming\n        RS < 1: Underperforming\n        \"\"\"\n        returns = prices_df.pct_change()\n        bench_ret = benchmark.pct_change()\n\n        # Rolling relative performance\n        rel_perf = returns.subtract(bench_ret, axis=0)\n        rel_strength = rel_perf.rolling(self.lookback).sum()\n\n        return rel_strength",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "CrossSectionalMomentum"
  },
  {
    "name": "generate_all_factors",
    "category": "reinforcement_learning",
    "formula": "result",
    "explanation": "Generate all elite factors.",
    "python_code": "def generate_all_factors(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Generate all elite factors.\"\"\"\n        result = df.copy()\n\n        # Higher-order moments\n        result = self.moments.generate_all(result)\n\n        # Intraday momentum\n        result = self.intraday.generate_all(result)\n\n        # Additional factors\n        returns = df['close'].pct_change()\n\n        # Acceleration (momentum of momentum)\n        mom_20 = returns.rolling(20).sum()\n        result['momentum_acceleration'] = mom_20.diff(5)\n\n        # Volatility-adjusted momentum\n        vol = returns.rolling(20).std()\n        result['vol_adj_momentum'] = mom_20 / (vol + 1e-10)\n\n        # Mean reversion signal\n        zscore = (df['close'] - df['close'].rolling(50).mean()) / (df['close'].rolling(50).std() + 1e-10)\n        result['mean_reversion'] = -zscore  # Contrarian\n\n        # Trend strength (ADX-like)\n        high_low_range = df['high'] - df['low'] if 'high' in df.columns else returns.abs()\n        result['trend_strength'] = high_low_range.rolling(14).mean() / (vol + 1e-10)\n\n        new_factors = [c for c in result.columns if c not in df.columns]\n        logger.info(f\"Generated {len(new_factors)} elite factors\")\n\n        return result",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "EliteFactorEngine"
  },
  {
    "name": "evaluate_all_factors",
    "category": "reinforcement_learning",
    "formula": "metrics. | pd.DataFrame(results).sort_values('icir', ascending=False)",
    "explanation": "Evaluate all factors and return metrics.\n\nReturns DataFrame with IC, ICIR, etc. for each factor.",
    "python_code": "def evaluate_all_factors(self, factor_df: pd.DataFrame,\n                            returns: pd.Series) -> pd.DataFrame:\n        \"\"\"\n        Evaluate all factors and return metrics.\n\n        Returns DataFrame with IC, ICIR, etc. for each factor.\n        \"\"\"\n        results = []\n\n        factor_cols = [c for c in factor_df.columns if c not in ['close', 'open', 'high', 'low', 'volume']]\n\n        for col in factor_cols:\n            try:\n                eval_result = self.ic_engine.evaluate_factor(factor_df[col], returns)\n                results.append({\n                    'factor': col,\n                    'ic': eval_result.ic,\n                    'icir': eval_result.icir,\n                    'rank_ic': eval_result.rank_ic,\n                    't_stat': eval_result.t_stat,\n                    'turnover': eval_result.turnover,\n                    'decay_halflife': eval_result.decay_halflife\n                })\n            except Exception as e:\n                logger.warning(f\"Failed to evaluate {col}: {e}\")\n\n        return pd.DataFrame(results).sort_values('icir', ascending=False)",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "EliteFactorEngine"
  },
  {
    "name": "get_factor_names",
    "category": "reinforcement_learning",
    "formula": "[",
    "explanation": "Return list of elite factor names.",
    "python_code": "def get_factor_names(self) -> List[str]:\n        \"\"\"Return list of elite factor names.\"\"\"\n        return [\n            'skewness', 'kurtosis', 'downside_skew', 'tail_risk', 'skew_zscore',\n            'first_30m_mom', 'first_60m_mom', 'last_30m_mom',\n            'overnight_reversal', 'london_session_mom', 'ny_session_mom',\n            'momentum_acceleration', 'vol_adj_momentum',\n            'mean_reversion', 'trend_strength'\n        ]",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "EliteFactorEngine"
  },
  {
    "name": "compute_all_features",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Compute all Elite Quant features.\nInterface compatible with HFT Feature Engine.",
    "python_code": "def compute_all_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Compute all Elite Quant features.\n        Interface compatible with HFT Feature Engine.\n        \"\"\"\n        return self.generate_all_factors(df)",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": "EliteFactorEngine"
  },
  {
    "name": "downside_skew",
    "category": "reinforcement_learning",
    "formula": "0 | stats.skew(neg)",
    "explanation": "",
    "python_code": "def downside_skew(x):\n            neg = x[x < 0]\n            if len(neg) < 3:\n                return 0\n            return stats.skew(neg)",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": null
  },
  {
    "name": "expected_shortfall",
    "category": "reinforcement_learning",
    "formula": "x[x <= threshold].mean()",
    "explanation": "",
    "python_code": "def expected_shortfall(x):\n            threshold = np.percentile(x, percentile * 100)\n            return x[x <= threshold].mean()",
    "source_file": "core\\_experimental\\elite_quant_factors.py",
    "academic_reference": "Jegadeesh & Titman (1993) 'Momentum Strategies' JF",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Initialize feature importance calculator.\n\nArgs:\n    n_jobs: Number of parallel jobs (-1 for all cores)\n    random_state: Random seed for reproducibility",
    "python_code": "def __init__(self, n_jobs: int = -1, random_state: int = 42):\n        \"\"\"\n        Initialize feature importance calculator.\n\n        Args:\n            n_jobs: Number of parallel jobs (-1 for all cores)\n            random_state: Random seed for reproducibility\n        \"\"\"\n        self.n_jobs = n_jobs\n        self.random_state = random_state",
    "source_file": "core\\_experimental\\feature_importance.py",
    "academic_reference": null,
    "class_name": "FeatureImportance"
  },
  {
    "name": "mean_decrease_accuracy",
    "category": "reinforcement_learning",
    "formula": "Importance = mean score drop across CV folds | FeatureImportanceResult(",
    "explanation": "Mean Decrease Accuracy (MDA) - Permutation-based feature importance.\n\nFor each feature:\n1. Fit model on training data\n2. Measure baseline score on OOS data\n3. Permute feature values in OOS data\n4. Measure score drop after permutation\n5. Importance = mean score drop across CV folds\n\nReference:\n    Lopez de Prado (2018), AFML Chapter 8.3\n\nArgs:\n    model: Fitted sklearn-compatible model\n    X: Feature DataFrame\n    y: Target Series\n    cv: Number of cross-validation folds\n    scoring: Scoring metric ('accuracy', 'f1', 'neg_log_loss', etc.)\n    n_repeats: Number of permutation repeats per feature\n    sample_weight: Optional sample weights\n\nReturns:\n    FeatureImportanceResult with importance scores",
    "python_code": "def mean_decrease_accuracy(\n        self,\n        model: Any,\n        X: pd.DataFrame,\n        y: pd.Series,\n        cv: int = 5,\n        scoring: str = 'accuracy',\n        n_repeats: int = 10,\n        sample_weight: Optional[np.ndarray] = None\n    ) -> FeatureImportanceResult:\n        \"\"\"\n        Mean Decrease Accuracy (MDA) - Permutation-based feature importance.\n\n        For each feature:\n        1. Fit model on training data\n        2. Measure baseline score on OOS data\n        3. Permute feature values in OOS data\n        4. Measure score drop after permutation\n        5. Importance = mean score drop across CV folds\n\n        Reference:\n            Lopez de Prado (2018), AFML Chapter 8.3\n\n        Args:\n            model: Fitted sklearn-compatible model\n            X: Feature DataFrame\n            y: Target Series\n            cv: Number of cross-validation folds\n            scoring: Scoring metric ('accuracy', 'f1', 'neg_log_loss', etc.)\n            n_repeats: Number of permutation repeats per feature\n            sample_weight: Optional sample weights\n\n        Returns:\n            FeatureImportanceResult with importance scores\n        \"\"\"\n        from sklearn.model_selection import KFold\n\n        n_samples, n_features = X.shape\n        feature_names = X.columns.tolist()\n\n        # Initialize importance arrays\n        importance_matrix = np.zeros((cv, n_features))\n\n        # Cross-validation\n        kf = KFold(n_splits=cv, shuffle=True, random_state=self.random_state)\n\n        for fold_idx, (train_idx, test_idx) in enumerate(kf.split(X)):\n            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx].copy()\n            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n\n            # Fit model\n            model_clone = clone(model)\n            if sample_weight is not None:\n                model_clone.fit(X_train, y_train, sample_weight=sample_weight[train_idx])\n            else:\n                model_clone.fit(X_train, y_train)\n\n            # Baseline score\n            baseline_score = self._score(model_clone, X_test, y_test, scoring)\n\n            # Permute each feature\n            for feat_idx, feature in enumerate(feature_names):\n                scores_after_permute = []\n\n                for _ in range(n_repeats):\n                    X_permuted = X_test.copy()\n                    X_permuted[feature] = np.random.permutation(X_permuted[feature].values)\n                    score_permuted = self._score(model_clone, X_permuted, y_test, scoring)\n                    scores_after_permute.append(baseline_score - score_permuted)\n\n                importance_matrix[fold_idx, feat_idx] = np.mean(scores_after_permute)\n\n        # Aggregate across folds\n        importance = pd.Series(\n            np.mean(importance_matrix, axis=0),\n            index=feature_names,\n            name='mda_importance'\n        )\n        std = pd.Series(\n            np.std(importance_matrix, axis=0),\n            index=feature_names,\n            name='mda_std'\n        )\n\n    ",
    "source_file": "core\\_experimental\\feature_importance.py",
    "academic_reference": null,
    "class_name": "FeatureImportance"
  },
  {
    "name": "_score",
    "category": "feature_engineering",
    "formula": "model.score(X, y) | -log_loss(y, model.predict_proba(X)) | f1_score(y, model.predict(), average='weighted')",
    "explanation": "Calculate model score.",
    "python_code": "def _score(self, model, X, y, scoring: str) -> float:\n        \"\"\"Calculate model score.\"\"\"\n        if scoring == 'accuracy':\n            return model.score(X, y)\n        elif scoring == 'neg_log_loss':\n            from sklearn.metrics import log_loss\n            return -log_loss(y, model.predict_proba(X))\n        elif scoring == 'f1':\n            from sklearn.metrics import f1_score\n            return f1_score(y, model.predict(), average='weighted')\n        elif scoring == 'roc_auc':\n            from sklearn.metrics import roc_auc_score\n            return roc_auc_score(y, model.predict_proba(X)[:, 1])\n        else:\n            return model.score(X, y)",
    "source_file": "core\\_experimental\\feature_importance.py",
    "academic_reference": null,
    "class_name": "FeatureImportance"
  },
  {
    "name": "mean_decrease_impurity",
    "category": "statistical",
    "formula": "FeatureImportanceResult(",
    "explanation": "Mean Decrease Impurity (MDI) - Tree-based feature importance.\n\nUses the built-in feature_importances_ from tree-based models.\nFast but can be biased toward high-cardinality features.\n\nReference:\n    Lopez de Prado (2018), AFML Chapter 8.2\n\nArgs:\n    model: Fitted tree-based model (RF, XGBoost, LightGBM, etc.)\n    X: Feature DataFrame\n    normalize: Whether to normalize importances to sum to 1\n\nReturns:\n    FeatureImportanceResult with importance scores",
    "python_code": "def mean_decrease_impurity(\n        self,\n        model: Any,\n        X: pd.DataFrame,\n        normalize: bool = True\n    ) -> FeatureImportanceResult:\n        \"\"\"\n        Mean Decrease Impurity (MDI) - Tree-based feature importance.\n\n        Uses the built-in feature_importances_ from tree-based models.\n        Fast but can be biased toward high-cardinality features.\n\n        Reference:\n            Lopez de Prado (2018), AFML Chapter 8.2\n\n        Args:\n            model: Fitted tree-based model (RF, XGBoost, LightGBM, etc.)\n            X: Feature DataFrame\n            normalize: Whether to normalize importances to sum to 1\n\n        Returns:\n            FeatureImportanceResult with importance scores\n        \"\"\"\n        if not hasattr(model, 'feature_importances_'):\n            raise ValueError(\"Model does not have feature_importances_ attribute\")\n\n        importance = pd.Series(\n            model.feature_importances_,\n            index=X.columns,\n            name='mdi_importance'\n        )\n\n        if normalize:\n            importance = importance / importance.sum()\n\n        return FeatureImportanceResult(\n            importance=importance.sort_values(ascending=False),\n            std=None,\n            method='MDI',\n            n_samples=len(X),\n            n_features=len(X.columns)\n        )",
    "source_file": "core\\_experimental\\feature_importance.py",
    "academic_reference": null,
    "class_name": "FeatureImportance"
  },
  {
    "name": "single_feature_importance",
    "category": "machine_learning",
    "formula": "FeatureImportanceResult(",
    "explanation": "Single Feature Importance (SFI) - Univariate importance.\n\nTrains model on each feature individually and measures OOS score.\nSimple baseline that ignores feature interactions.\n\nReference:\n    Lopez de Prado (2018), AFML Chapter 8.4\n\nArgs:\n    model: sklearn-compatible model\n    X: Feature DataFrame\n    y: Target Series\n    cv: Number of cross-validation folds\n    scoring: Scoring metric\n\nReturns:\n    FeatureImportanceResult with importance scores",
    "python_code": "def single_feature_importance(\n        self,\n        model: Any,\n        X: pd.DataFrame,\n        y: pd.Series,\n        cv: int = 5,\n        scoring: str = 'accuracy'\n    ) -> FeatureImportanceResult:\n        \"\"\"\n        Single Feature Importance (SFI) - Univariate importance.\n\n        Trains model on each feature individually and measures OOS score.\n        Simple baseline that ignores feature interactions.\n\n        Reference:\n            Lopez de Prado (2018), AFML Chapter 8.4\n\n        Args:\n            model: sklearn-compatible model\n            X: Feature DataFrame\n            y: Target Series\n            cv: Number of cross-validation folds\n            scoring: Scoring metric\n\n        Returns:\n            FeatureImportanceResult with importance scores\n        \"\"\"\n        feature_names = X.columns.tolist()\n        scores = {}\n\n        for feature in feature_names:\n            X_single = X[[feature]]\n            model_clone = clone(model)\n            cv_scores = cross_val_score(\n                model_clone, X_single, y,\n                cv=cv, scoring=scoring, n_jobs=self.n_jobs\n            )\n            scores[feature] = np.mean(cv_scores)\n\n        importance = pd.Series(scores, name='sfi_importance')\n\n        return FeatureImportanceResult(\n            importance=importance.sort_values(ascending=False),\n            std=None,\n            method='SFI',\n            n_samples=len(X),\n            n_features=len(X.columns)\n        )",
    "source_file": "core\\_experimental\\feature_importance.py",
    "academic_reference": null,
    "class_name": "FeatureImportance"
  },
  {
    "name": "clustered_mda",
    "category": "statistical",
    "formula": "result, cluster_map",
    "explanation": "Clustered MDA - Correlation-aware feature importance.\n\nClusters correlated features together and permutes entire clusters.\nAvoids redundant importance across correlated features.\n\nReference:\n    Lopez de Prado (2018), AFML Chapter 8.5\n\nArgs:\n    model: sklearn-compatible model\n    X: Feature DataFrame\n    y: Target Series\n    n_clusters: Number of feature clusters\n    cv: Number of CV folds\n    scoring: Scoring metric\n\nReturns:\n    Tuple of (importance result, cluster assignments)",
    "python_code": "def clustered_mda(\n        self,\n        model: Any,\n        X: pd.DataFrame,\n        y: pd.Series,\n        n_clusters: int = 10,\n        cv: int = 5,\n        scoring: str = 'accuracy'\n    ) -> Tuple[FeatureImportanceResult, Dict[int, List[str]]]:\n        \"\"\"\n        Clustered MDA - Correlation-aware feature importance.\n\n        Clusters correlated features together and permutes entire clusters.\n        Avoids redundant importance across correlated features.\n\n        Reference:\n            Lopez de Prado (2018), AFML Chapter 8.5\n\n        Args:\n            model: sklearn-compatible model\n            X: Feature DataFrame\n            y: Target Series\n            n_clusters: Number of feature clusters\n            cv: Number of CV folds\n            scoring: Scoring metric\n\n        Returns:\n            Tuple of (importance result, cluster assignments)\n        \"\"\"\n        from sklearn.model_selection import KFold\n\n        # Cluster features by correlation\n        corr = X.corr()\n        dist = ((1 - corr) / 2).values  # Convert correlation to distance\n        np.fill_diagonal(dist, 0)\n\n        # Hierarchical clustering\n        dist_condensed = squareform(dist, checks=False)\n        Z = linkage(dist_condensed, method='ward')\n        clusters = fcluster(Z, n_clusters, criterion='maxclust')\n\n        # Map features to clusters\n        feature_names = X.columns.tolist()\n        cluster_map = {}\n        for feat, cluster in zip(feature_names, clusters):\n            if cluster not in cluster_map:\n                cluster_map[cluster] = []\n            cluster_map[cluster].append(feat)\n\n        # Calculate clustered importance\n        kf = KFold(n_splits=cv, shuffle=True, random_state=self.random_state)\n        cluster_importance = {c: [] for c in cluster_map.keys()}\n\n        for train_idx, test_idx in kf.split(X):\n            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx].copy()\n            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n\n            model_clone = clone(model)\n            model_clone.fit(X_train, y_train)\n            baseline = self._score(model_clone, X_test, y_test, scoring)\n\n            for cluster_id, features in cluster_map.items():\n                X_permuted = X_test.copy()\n                for feat in features:\n                    X_permuted[feat] = np.random.permutation(X_permuted[feat].values)\n                score_permuted = self._score(model_clone, X_permuted, y_test, scoring)\n                cluster_importance[cluster_id].append(baseline - score_permuted)\n\n        # Average importance per cluster\n        importance = pd.Series(\n            {f\"cluster_{c}\": np.mean(scores) for c, scores in cluster_importance.items()},\n            name='clustered_mda_importance'\n        )\n\n        result = FeatureImportanceResult(\n            importance=importance.sort_values(ascending=False),\n            std=None,\n            method='Clustered_MDA',\n            n_samples=len(X),\n            n_features=len(X.columns)\n        )\n\n        retu",
    "source_file": "core\\_experimental\\feature_importance.py",
    "academic_reference": null,
    "class_name": "FeatureImportance"
  },
  {
    "name": "shap_importance",
    "category": "machine_learning",
    "formula": "FeatureImportanceResult(",
    "explanation": "SHAP (SHapley Additive exPlanations) feature importance.\n\nUses Shapley values for theoretically sound feature attribution.\nRequires 'shap' package to be installed.\n\nReference:\n    Lundberg, S. M., & Lee, S. I. (2017). \"A Unified Approach to\n    Interpreting Model Predictions\". NeurIPS.\n\nArgs:\n    model: Fitted model\n    X: Feature DataFrame\n    n_samples: Number of samples for SHAP calculation\n    check_additivity: Whether to check SHAP additivity\n\nReturns:\n    FeatureImportanceResult with SHAP importance",
    "python_code": "def shap_importance(\n        self,\n        model: Any,\n        X: pd.DataFrame,\n        n_samples: int = 100,\n        check_additivity: bool = False\n    ) -> FeatureImportanceResult:\n        \"\"\"\n        SHAP (SHapley Additive exPlanations) feature importance.\n\n        Uses Shapley values for theoretically sound feature attribution.\n        Requires 'shap' package to be installed.\n\n        Reference:\n            Lundberg, S. M., & Lee, S. I. (2017). \"A Unified Approach to\n            Interpreting Model Predictions\". NeurIPS.\n\n        Args:\n            model: Fitted model\n            X: Feature DataFrame\n            n_samples: Number of samples for SHAP calculation\n            check_additivity: Whether to check SHAP additivity\n\n        Returns:\n            FeatureImportanceResult with SHAP importance\n        \"\"\"\n        try:\n            import shap\n        except ImportError:\n            raise ImportError(\"SHAP package required. Install with: pip install shap\")\n\n        # Subsample for efficiency\n        if len(X) > n_samples:\n            X_sample = X.sample(n_samples, random_state=self.random_state)\n        else:\n            X_sample = X\n\n        # Create explainer based on model type\n        model_type = type(model).__name__.lower()\n\n        if 'tree' in model_type or 'forest' in model_type or 'xgb' in model_type or 'lgb' in model_type or 'catboost' in model_type:\n            explainer = shap.TreeExplainer(model)\n            shap_values = explainer.shap_values(X_sample, check_additivity=check_additivity)\n        else:\n            # Use KernelExplainer for other models\n            background = shap.sample(X_sample, min(50, len(X_sample)))\n            explainer = shap.KernelExplainer(model.predict, background)\n            shap_values = explainer.shap_values(X_sample)\n\n        # Handle multi-class output\n        if isinstance(shap_values, list):\n            # Multi-class: use absolute mean across classes\n            shap_values = np.abs(np.array(shap_values)).mean(axis=0)\n        else:\n            shap_values = np.abs(shap_values)\n\n        # Mean absolute SHAP value per feature\n        importance = pd.Series(\n            shap_values.mean(axis=0),\n            index=X.columns,\n            name='shap_importance'\n        )\n\n        return FeatureImportanceResult(\n            importance=importance.sort_values(ascending=False),\n            std=pd.Series(shap_values.std(axis=0), index=X.columns),\n            method='SHAP',\n            n_samples=len(X_sample),\n            n_features=len(X.columns)\n        )",
    "source_file": "core\\_experimental\\feature_importance.py",
    "academic_reference": null,
    "class_name": "FeatureImportance"
  },
  {
    "name": "compare_methods",
    "category": "machine_learning",
    "formula": "MDA = likely overfit feature | MDI = complex feature interaction | comparison.sort_values('MDA', ascending=False)",
    "explanation": "Compare all feature importance methods.\n\nUseful for identifying potential overfitting:\n- High MDI + Low MDA = likely overfit feature\n- High MDA + Low MDI = complex feature interaction\n\nReturns:\n    DataFrame with importance from all methods",
    "python_code": "def compare_methods(\n        self,\n        model: Any,\n        X: pd.DataFrame,\n        y: pd.Series,\n        cv: int = 5\n    ) -> pd.DataFrame:\n        \"\"\"\n        Compare all feature importance methods.\n\n        Useful for identifying potential overfitting:\n        - High MDI + Low MDA = likely overfit feature\n        - High MDA + Low MDI = complex feature interaction\n\n        Returns:\n            DataFrame with importance from all methods\n        \"\"\"\n        results = {}\n\n        # MDI (if tree-based)\n        if hasattr(model, 'feature_importances_'):\n            mdi = self.mean_decrease_impurity(model, X)\n            results['MDI'] = mdi.importance\n\n        # MDA\n        mda = self.mean_decrease_accuracy(model, X, y, cv=cv)\n        results['MDA'] = mda.importance\n\n        # SFI\n        try:\n            sfi = self.single_feature_importance(model, X, y, cv=cv)\n            results['SFI'] = sfi.importance\n        except Exception:\n            pass\n\n        comparison = pd.DataFrame(results)\n\n        # Normalize for comparison\n        comparison_norm = comparison.apply(lambda x: (x - x.min()) / (x.max() - x.min() + 1e-10))\n\n        # Add overfit indicator: high MDI but low MDA\n        if 'MDI' in comparison_norm.columns and 'MDA' in comparison_norm.columns:\n            comparison['overfit_risk'] = comparison_norm['MDI'] - comparison_norm['MDA']\n\n        return comparison.sort_values('MDA', ascending=False)",
    "source_file": "core\\_experimental\\feature_importance.py",
    "academic_reference": null,
    "class_name": "FeatureImportance"
  },
  {
    "name": "detect_overfit_features",
    "category": "machine_learning",
    "formula": "[] | overfit_features",
    "explanation": "Detect features likely to cause overfitting.\n\nFeatures with high MDI but low MDA are likely memorizing noise.\n\nArgs:\n    model: Fitted tree-based model\n    X: Feature DataFrame\n    y: Target Series\n    threshold: MDI-MDA difference threshold\n\nReturns:\n    List of potentially overfit feature names",
    "python_code": "def detect_overfit_features(\n        self,\n        model: Any,\n        X: pd.DataFrame,\n        y: pd.Series,\n        threshold: float = 0.3\n    ) -> List[str]:\n        \"\"\"\n        Detect features likely to cause overfitting.\n\n        Features with high MDI but low MDA are likely memorizing noise.\n\n        Args:\n            model: Fitted tree-based model\n            X: Feature DataFrame\n            y: Target Series\n            threshold: MDI-MDA difference threshold\n\n        Returns:\n            List of potentially overfit feature names\n        \"\"\"\n        comparison = self.compare_methods(model, X, y)\n\n        if 'overfit_risk' not in comparison.columns:\n            return []\n\n        overfit_features = comparison[comparison['overfit_risk'] > threshold].index.tolist()\n\n        return overfit_features",
    "source_file": "core\\_experimental\\feature_importance.py",
    "academic_reference": null,
    "class_name": "FeatureImportance"
  },
  {
    "name": "get_indicator_matrix",
    "category": "technical",
    "formula": "index = start time) | indicator",
    "explanation": "Create indicator matrix showing which samples span each time point.\n\nArgs:\n    t1: Series with sample end times (index = start time)\n    close_idx: Full datetime index of the price series\n\nReturns:\n    DataFrame with samples as columns, times as rows",
    "python_code": "def get_indicator_matrix(\n        t1: pd.Series,\n        close_idx: pd.DatetimeIndex\n    ) -> pd.DataFrame:\n        \"\"\"\n        Create indicator matrix showing which samples span each time point.\n\n        Args:\n            t1: Series with sample end times (index = start time)\n            close_idx: Full datetime index of the price series\n\n        Returns:\n            DataFrame with samples as columns, times as rows\n        \"\"\"\n        indicator = pd.DataFrame(0, index=close_idx, columns=range(len(t1)))\n\n        for i, (t0, t1_val) in enumerate(t1.items()):\n            indicator.loc[t0:t1_val, i] = 1\n\n        return indicator",
    "source_file": "core\\_experimental\\feature_importance.py",
    "academic_reference": null,
    "class_name": "SampleWeightsByUniqueness"
  },
  {
    "name": "get_average_uniqueness",
    "category": "technical",
    "formula": "Uniqueness = 1 / (number of concurrent samples) | avg_uniqueness",
    "explanation": "Calculate average uniqueness for each sample.\n\nUniqueness = 1 / (number of concurrent samples)\n\nArgs:\n    indicator_matrix: Output from get_indicator_matrix\n\nReturns:\n    Series of uniqueness values per sample",
    "python_code": "def get_average_uniqueness(indicator_matrix: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Calculate average uniqueness for each sample.\n\n        Uniqueness = 1 / (number of concurrent samples)\n\n        Args:\n            indicator_matrix: Output from get_indicator_matrix\n\n        Returns:\n            Series of uniqueness values per sample\n        \"\"\"\n        # Concurrency at each time point\n        concurrency = indicator_matrix.sum(axis=1)\n\n        # Average uniqueness per sample\n        uniqueness = indicator_matrix.div(concurrency, axis=0)\n        avg_uniqueness = uniqueness.sum() / indicator_matrix.sum()\n\n        return avg_uniqueness",
    "source_file": "core\\_experimental\\feature_importance.py",
    "academic_reference": null,
    "class_name": "SampleWeightsByUniqueness"
  },
  {
    "name": "get_sample_weights",
    "category": "feature_engineering",
    "formula": "index = start time) | weights",
    "explanation": "Get sample weights based on uniqueness.\n\nArgs:\n    t1: Series with sample end times (index = start time)\n    close_idx: Full datetime index\n    num_co_events: Optional pre-computed concurrency\n\nReturns:\n    Series of sample weights",
    "python_code": "def get_sample_weights(\n        t1: pd.Series,\n        close_idx: pd.DatetimeIndex,\n        num_co_events: Optional[pd.Series] = None\n    ) -> pd.Series:\n        \"\"\"\n        Get sample weights based on uniqueness.\n\n        Args:\n            t1: Series with sample end times (index = start time)\n            close_idx: Full datetime index\n            num_co_events: Optional pre-computed concurrency\n\n        Returns:\n            Series of sample weights\n        \"\"\"\n        # Get indicator matrix\n        indicator = SampleWeightsByUniqueness.get_indicator_matrix(t1, close_idx)\n\n        # Get uniqueness\n        uniqueness = SampleWeightsByUniqueness.get_average_uniqueness(indicator)\n\n        # Normalize weights to sum to number of samples\n        weights = uniqueness * len(uniqueness) / uniqueness.sum()\n\n        return weights",
    "source_file": "core\\_experimental\\feature_importance.py",
    "academic_reference": null,
    "class_name": "SampleWeightsByUniqueness"
  },
  {
    "name": "get_time_decay_weights",
    "category": "statistical",
    "formula": "recent = higher weight). | pd.Series(decay, index=t1.index)",
    "explanation": "Apply time decay to sample weights (more recent = higher weight).\n\nArgs:\n    t1: Series with sample end times\n    decay_factor: Decay rate (0 = uniform, 1 = linear decay)\n\nReturns:\n    Time-decayed sample weights",
    "python_code": "def get_time_decay_weights(\n        t1: pd.Series,\n        decay_factor: float = 0.5\n    ) -> pd.Series:\n        \"\"\"\n        Apply time decay to sample weights (more recent = higher weight).\n\n        Args:\n            t1: Series with sample end times\n            decay_factor: Decay rate (0 = uniform, 1 = linear decay)\n\n        Returns:\n            Time-decayed sample weights\n        \"\"\"\n        # Linear decay from oldest to newest\n        n = len(t1)\n        decay = np.linspace(1 - decay_factor, 1, n)\n\n        return pd.Series(decay, index=t1.index)",
    "source_file": "core\\_experimental\\feature_importance.py",
    "academic_reference": null,
    "class_name": "SampleWeightsByUniqueness"
  },
  {
    "name": "combine_weights",
    "category": "statistical",
    "formula": "combined * len(combined) / combined.sum()",
    "explanation": "Combine uniqueness and time decay weights.\n\nArgs:\n    uniqueness_weights: From get_sample_weights\n    time_decay_weights: From get_time_decay_weights\n\nReturns:\n    Combined normalized weights",
    "python_code": "def combine_weights(\n        uniqueness_weights: pd.Series,\n        time_decay_weights: pd.Series\n    ) -> pd.Series:\n        \"\"\"\n        Combine uniqueness and time decay weights.\n\n        Args:\n            uniqueness_weights: From get_sample_weights\n            time_decay_weights: From get_time_decay_weights\n\n        Returns:\n            Combined normalized weights\n        \"\"\"\n        combined = uniqueness_weights * time_decay_weights\n        return combined * len(combined) / combined.sum()",
    "source_file": "core\\_experimental\\feature_importance.py",
    "academic_reference": null,
    "class_name": "SampleWeightsByUniqueness"
  },
  {
    "name": "create_feature_importance",
    "category": "reinforcement_learning",
    "formula": "FeatureImportance(n_jobs=n_jobs)",
    "explanation": "Factory function for FeatureImportance.",
    "python_code": "def create_feature_importance(n_jobs: int = -1) -> FeatureImportance:\n    \"\"\"Factory function for FeatureImportance.\"\"\"\n    return FeatureImportance(n_jobs=n_jobs)",
    "source_file": "core\\_experimental\\feature_importance.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "get_sample_weights",
    "category": "deep_learning",
    "formula": "SampleWeightsByUniqueness.get_sample_weights(t1, close_idx)",
    "explanation": "Convenience function for sample weights.",
    "python_code": "def get_sample_weights(\n    t1: pd.Series,\n    close_idx: pd.DatetimeIndex\n) -> pd.Series:\n    \"\"\"Convenience function for sample weights.\"\"\"\n    return SampleWeightsByUniqueness.get_sample_weights(t1, close_idx)",
    "source_file": "core\\_experimental\\feature_importance.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "_ts_delay",
    "category": "reinforcement_learning",
    "formula": "result",
    "explanation": "Time series delay (lag).",
    "python_code": "def _ts_delay(x, period=1):\n    \"\"\"Time series delay (lag).\"\"\"\n    result = np.roll(x, period)\n    result[:period] = np.nan\n    return result",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "_ts_delta",
    "category": "reinforcement_learning",
    "formula": "x - _ts_delay(x, period)",
    "explanation": "Time series difference.",
    "python_code": "def _ts_delta(x, period=1):\n    \"\"\"Time series difference.\"\"\"\n    return x - _ts_delay(x, period)",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "_ts_sum",
    "category": "statistical",
    "formula": "result",
    "explanation": "Rolling sum.",
    "python_code": "def _ts_sum(x, period=5):\n    \"\"\"Rolling sum.\"\"\"\n    result = np.convolve(x, np.ones(period), mode='same')\n    result[:period-1] = np.nan\n    return result",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "_ts_mean",
    "category": "statistical",
    "formula": "_ts_sum(x, period) / period",
    "explanation": "Rolling mean.",
    "python_code": "def _ts_mean(x, period=5):\n    \"\"\"Rolling mean.\"\"\"\n    return _ts_sum(x, period) / period",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "_ts_std",
    "category": "statistical",
    "formula": "np.sqrt(np.abs(variance))",
    "explanation": "Rolling standard deviation.",
    "python_code": "def _ts_std(x, period=5):\n    \"\"\"Rolling standard deviation.\"\"\"\n    mean = _ts_mean(x, period)\n    sq_diff = (x - mean) ** 2\n    variance = _ts_mean(sq_diff, period)\n    return np.sqrt(np.abs(variance))",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "_ts_rank",
    "category": "statistical",
    "formula": "result",
    "explanation": "Rolling rank (percentile position).",
    "python_code": "def _ts_rank(x, period=5):\n    \"\"\"Rolling rank (percentile position).\"\"\"\n    result = np.zeros_like(x)\n    for i in range(period-1, len(x)):\n        window = x[i-period+1:i+1]\n        result[i] = (np.sum(window <= x[i])) / period\n    result[:period-1] = np.nan\n    return result",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "_ts_max",
    "category": "statistical",
    "formula": "result",
    "explanation": "Rolling maximum.",
    "python_code": "def _ts_max(x, period=5):\n    \"\"\"Rolling maximum.\"\"\"\n    result = np.zeros_like(x)\n    for i in range(period-1, len(x)):\n        result[i] = np.max(x[i-period+1:i+1])\n    result[:period-1] = np.nan\n    return result",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "_ts_min",
    "category": "statistical",
    "formula": "result",
    "explanation": "Rolling minimum.",
    "python_code": "def _ts_min(x, period=5):\n    \"\"\"Rolling minimum.\"\"\"\n    result = np.zeros_like(x)\n    for i in range(period-1, len(x)):\n        result[i] = np.min(x[i-period+1:i+1])\n    result[:period-1] = np.nan\n    return result",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "_ts_argmax",
    "category": "reinforcement_learning",
    "formula": "result",
    "explanation": "Days since max in window.",
    "python_code": "def _ts_argmax(x, period=5):\n    \"\"\"Days since max in window.\"\"\"\n    result = np.zeros_like(x)\n    for i in range(period-1, len(x)):\n        window = x[i-period+1:i+1]\n        result[i] = period - 1 - np.argmax(window)\n    result[:period-1] = np.nan\n    return result",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "_ts_argmin",
    "category": "reinforcement_learning",
    "formula": "result",
    "explanation": "Days since min in window.",
    "python_code": "def _ts_argmin(x, period=5):\n    \"\"\"Days since min in window.\"\"\"\n    result = np.zeros_like(x)\n    for i in range(period-1, len(x)):\n        window = x[i-period+1:i+1]\n        result[i] = period - 1 - np.argmin(window)\n    result[:period-1] = np.nan\n    return result",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "_ts_corr",
    "category": "statistical",
    "formula": "result",
    "explanation": "Rolling correlation.",
    "python_code": "def _ts_corr(x, y, period=5):\n    \"\"\"Rolling correlation.\"\"\"\n    result = np.zeros_like(x)\n    for i in range(period-1, len(x)):\n        x_win = x[i-period+1:i+1]\n        y_win = y[i-period+1:i+1]\n        if np.std(x_win) > 0 and np.std(y_win) > 0:\n            result[i] = np.corrcoef(x_win, y_win)[0, 1]\n        else:\n            result[i] = 0\n    result[:period-1] = np.nan\n    return result",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "_protected_div",
    "category": "reinforcement_learning",
    "formula": "result",
    "explanation": "Protected division (avoid div by zero).",
    "python_code": "def _protected_div(x, y):\n    \"\"\"Protected division (avoid div by zero).\"\"\"\n    with np.errstate(divide='ignore', invalid='ignore'):\n        result = np.where(np.abs(y) > 1e-10, x / y, 0.0)\n    return result",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "_protected_log",
    "category": "reinforcement_learning",
    "formula": "result",
    "explanation": "Protected log (avoid log of non-positive).",
    "python_code": "def _protected_log(x):\n    \"\"\"Protected log (avoid log of non-positive).\"\"\"\n    with np.errstate(divide='ignore', invalid='ignore'):\n        result = np.where(x > 0, np.log(x), 0.0)\n    return result",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "_sign",
    "category": "reinforcement_learning",
    "formula": "np.sign(x)",
    "explanation": "Sign function.",
    "python_code": "def _sign(x):\n    \"\"\"Sign function.\"\"\"\n    return np.sign(x)",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "_abs",
    "category": "reinforcement_learning",
    "formula": "np.abs(x)",
    "explanation": "Absolute value.",
    "python_code": "def _abs(x):\n    \"\"\"Absolute value.\"\"\"\n    return np.abs(x)",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(\n        self,\n        population_size: int = 1000,\n        generations: int = 20,\n        tournament_size: int = 20,\n        stopping_criteria: float = 0.01,\n        p_crossover: float = 0.7,\n        p_subtree_mutation: float = 0.1,\n        p_hoist_mutation: float = 0.05,\n        p_point_mutation: float = 0.1,\n        max_samples: float = 0.9,\n        parsimony_coefficient: float = 0.001,\n        random_state: int = 42\n    ):\n        self.population_size = population_size\n        self.generations = generations\n        self.tournament_size = tournament_size\n        self.stopping_criteria = stopping_criteria\n        self.p_crossover = p_crossover\n        self.p_subtree_mutation = p_subtree_mutation\n        self.p_hoist_mutation = p_hoist_mutation\n        self.p_point_mutation = p_point_mutation\n        self.max_samples = max_samples\n        self.parsimony_coefficient = parsimony_coefficient\n        self.random_state = random_state\n\n        self.discovered_factors: List[DiscoveredFactor] = []\n        self.transformer = None",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GeneticFactorMiner"
  },
  {
    "name": "_create_function_set",
    "category": "reinforcement_learning",
    "formula": "[] | function_set",
    "explanation": "Create custom function set for financial time series.",
    "python_code": "def _create_function_set(self) -> List:\n        \"\"\"Create custom function set for financial time series.\"\"\"\n        if not HAS_GPLEARN:\n            return []\n\n        function_set = ['add', 'sub', 'mul', 'div', 'sqrt', 'log', 'abs', 'neg', 'max', 'min']\n\n        # Add time series functions\n        ts_delay_5 = make_function(lambda x: _ts_delay(x, 5), 'delay5', 1)\n        ts_delta_5 = make_function(lambda x: _ts_delta(x, 5), 'delta5', 1)\n        ts_mean_5 = make_function(lambda x: _ts_mean(x, 5), 'mean5', 1)\n        ts_mean_10 = make_function(lambda x: _ts_mean(x, 10), 'mean10', 1)\n        ts_mean_20 = make_function(lambda x: _ts_mean(x, 20), 'mean20', 1)\n        ts_std_5 = make_function(lambda x: _ts_std(x, 5), 'std5', 1)\n        ts_std_10 = make_function(lambda x: _ts_std(x, 10), 'std10', 1)\n        ts_rank_5 = make_function(lambda x: _ts_rank(x, 5), 'rank5', 1)\n        ts_rank_10 = make_function(lambda x: _ts_rank(x, 10), 'rank10', 1)\n        ts_max_5 = make_function(lambda x: _ts_max(x, 5), 'max5', 1)\n        ts_min_5 = make_function(lambda x: _ts_min(x, 5), 'min5', 1)\n\n        function_set.extend([\n            ts_delay_5, ts_delta_5,\n            ts_mean_5, ts_mean_10, ts_mean_20,\n            ts_std_5, ts_std_10,\n            ts_rank_5, ts_rank_10,\n            ts_max_5, ts_min_5\n        ])\n\n        return function_set",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GeneticFactorMiner"
  },
  {
    "name": "mine",
    "category": "reinforcement_learning",
    "formula": "[]",
    "explanation": "Mine factors using genetic programming.\n\nArgs:\n    df: DataFrame with OHLCV data\n    target: Target returns (forward returns)\n    n_factors: Number of factors to discover\n    feature_cols: Columns to use as base features\n\nReturns:\n    List of discovered factors",
    "python_code": "def mine(\n        self,\n        df: pd.DataFrame,\n        target: pd.Series,\n        n_factors: int = 10,\n        feature_cols: Optional[List[str]] = None\n    ) -> List[DiscoveredFactor]:\n        \"\"\"\n        Mine factors using genetic programming.\n\n        Args:\n            df: DataFrame with OHLCV data\n            target: Target returns (forward returns)\n            n_factors: Number of factors to discover\n            feature_cols: Columns to use as base features\n\n        Returns:\n            List of discovered factors\n        \"\"\"\n        if not HAS_GPLEARN:\n            logger.warning(\"gplearn not available, using fallback\")\n            return self._mine_fallback(df, target, n_factors)\n\n        # Prepare features\n        if feature_cols is None:\n            feature_cols = [c for c in df.columns if c in\n                          ['open', 'high', 'low', 'close', 'volume', 'returns']]\n\n        X = df[feature_cols].values\n        y = target.values\n\n        # Remove NaN\n        valid_mask = ~(np.isnan(X).any(axis=1) | np.isnan(y))\n        X = X[valid_mask]\n        y = y[valid_mask]\n\n        if len(X) < 100:\n            logger.warning(\"Not enough data for genetic mining\")\n            return []\n\n        # Create function set\n        function_set = self._create_function_set()\n\n        # Create transformer\n        self.transformer = SymbolicTransformer(\n            population_size=self.population_size,\n            generations=self.generations,\n            tournament_size=self.tournament_size,\n            stopping_criteria=self.stopping_criteria,\n            p_crossover=self.p_crossover,\n            p_subtree_mutation=self.p_subtree_mutation,\n            p_hoist_mutation=self.p_hoist_mutation,\n            p_point_mutation=self.p_point_mutation,\n            max_samples=self.max_samples,\n            parsimony_coefficient=self.parsimony_coefficient,\n            function_set=function_set,\n            n_components=n_factors,\n            hall_of_fame=n_factors * 2,\n            random_state=self.random_state,\n            n_jobs=-1,\n            verbose=1\n        )\n\n        logger.info(f\"Mining {n_factors} factors from {len(X)} samples...\")\n\n        # Fit transformer\n        self.transformer.fit(X, y)\n\n        # Extract discovered factors\n        self.discovered_factors = []\n\n        for i, program in enumerate(self.transformer._best_programs):\n            if program is not None:\n                # Transform data with this program\n                factor_values = program.execute(X)\n\n                # Calculate IC\n                valid_idx = ~np.isnan(factor_values)\n                if np.sum(valid_idx) > 10:\n                    ic = np.corrcoef(factor_values[valid_idx], y[valid_idx])[0, 1]\n                else:\n                    ic = 0.0\n\n                # Calculate ICIR (IC / std of IC)\n                # For simplicity, use rolling IC std estimate\n                icir = ic / 0.1 if abs(ic) > 0.01 else 0.0\n\n                # Calculate turnover\n                facto",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GeneticFactorMiner"
  },
  {
    "name": "_mine_fallback",
    "category": "reinforcement_learning",
    "formula": "top n | factors[:n_factors]",
    "explanation": "Fallback factor mining without gplearn.",
    "python_code": "def _mine_fallback(\n        self,\n        df: pd.DataFrame,\n        target: pd.Series,\n        n_factors: int\n    ) -> List[DiscoveredFactor]:\n        \"\"\"Fallback factor mining without gplearn.\"\"\"\n        logger.info(\"Using fallback factor mining (basic combinations)\")\n\n        factors = []\n\n        # Basic price-volume factors\n        if 'close' in df.columns:\n            close = df['close'].values\n\n            # Momentum factors\n            for period in [5, 10, 20]:\n                ret = close / np.roll(close, period) - 1\n                ret[:period] = np.nan\n\n                valid_mask = ~np.isnan(ret) & ~np.isnan(target.values)\n                if np.sum(valid_mask) > 10:\n                    ic = np.corrcoef(ret[valid_mask], target.values[valid_mask])[0, 1]\n                else:\n                    ic = 0.0\n\n                factors.append(DiscoveredFactor(\n                    formula=f\"momentum_{period}\",\n                    fitness=abs(ic),\n                    ic=ic if not np.isnan(ic) else 0.0,\n                    icir=ic / 0.1 if abs(ic) > 0.01 else 0.0,\n                    turnover=0.1,\n                    complexity=1\n                ))\n\n            # Mean reversion factors\n            for period in [10, 20, 50]:\n                ma = _ts_mean(close, period)\n                mr = (close - ma) / ma\n\n                valid_mask = ~np.isnan(mr) & ~np.isnan(target.values)\n                if np.sum(valid_mask) > 10:\n                    ic = np.corrcoef(mr[valid_mask], target.values[valid_mask])[0, 1]\n                else:\n                    ic = 0.0\n\n                factors.append(DiscoveredFactor(\n                    formula=f\"mean_reversion_{period}\",\n                    fitness=abs(ic),\n                    ic=ic if not np.isnan(ic) else 0.0,\n                    icir=ic / 0.1 if abs(ic) > 0.01 else 0.0,\n                    turnover=0.05,\n                    complexity=2\n                ))\n\n        # Sort and return top n\n        factors.sort(key=lambda x: abs(x.ic), reverse=True)\n        return factors[:n_factors]",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GeneticFactorMiner"
  },
  {
    "name": "transform",
    "category": "reinforcement_learning",
    "formula": "pd.DataFrame() | factor_df",
    "explanation": "Transform data using discovered factors.\n\nArgs:\n    df: DataFrame with same features as training\n    feature_cols: Feature columns (must match training)\n\nReturns:\n    DataFrame with factor columns",
    "python_code": "def transform(self, df: pd.DataFrame, feature_cols: Optional[List[str]] = None) -> pd.DataFrame:\n        \"\"\"\n        Transform data using discovered factors.\n\n        Args:\n            df: DataFrame with same features as training\n            feature_cols: Feature columns (must match training)\n\n        Returns:\n            DataFrame with factor columns\n        \"\"\"\n        if self.transformer is None:\n            logger.warning(\"No transformer fitted, returning empty\")\n            return pd.DataFrame()\n\n        if feature_cols is None:\n            feature_cols = [c for c in df.columns if c in\n                          ['open', 'high', 'low', 'close', 'volume', 'returns']]\n\n        X = df[feature_cols].values\n        factors = self.transformer.transform(X)\n\n        factor_df = pd.DataFrame(\n            factors,\n            index=df.index,\n            columns=[f'gp_factor_{i}' for i in range(factors.shape[1])]\n        )\n\n        return factor_df",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GeneticFactorMiner"
  },
  {
    "name": "get_best_formulas",
    "category": "reinforcement_learning",
    "formula": "[f.formula for f in self.discovered_factors[:n]]",
    "explanation": "Get the best factor formulas.",
    "python_code": "def get_best_formulas(self, n: int = 5) -> List[str]:\n        \"\"\"Get the best factor formulas.\"\"\"\n        return [f.formula for f in self.discovered_factors[:n]]",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GeneticFactorMiner"
  },
  {
    "name": "_setup_primitives",
    "category": "reinforcement_learning",
    "formula": "pset",
    "explanation": "Setup DEAP primitive set.",
    "python_code": "def _setup_primitives(self, n_features: int):\n        \"\"\"Setup DEAP primitive set.\"\"\"\n        if not HAS_DEAP:\n            return None\n\n        # Create primitive set\n        pset = gp.PrimitiveSetTyped(\"MAIN\", [float] * n_features, float)\n\n        # Add operators\n        pset.addPrimitive(np.add, [float, float], float, name=\"add\")\n        pset.addPrimitive(np.subtract, [float, float], float, name=\"sub\")\n        pset.addPrimitive(np.multiply, [float, float], float, name=\"mul\")\n        pset.addPrimitive(_protected_div, [float, float], float, name=\"div\")\n        pset.addPrimitive(np.negative, [float], float, name=\"neg\")\n        pset.addPrimitive(_protected_log, [float], float, name=\"log\")\n        pset.addPrimitive(_abs, [float], float, name=\"abs\")\n        pset.addPrimitive(_sign, [float], float, name=\"sign\")\n\n        # Add constants\n        pset.addEphemeralConstant(\"rand\", lambda: np.random.uniform(-1, 1), float)\n\n        # Rename arguments\n        for i in range(n_features):\n            pset.renameArguments(**{f\"ARG{i}\": f\"x{i}\"})\n\n        return pset",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DEAPFactorMiner"
  },
  {
    "name": "mine",
    "category": "reinforcement_learning",
    "formula": "[] | (0.0,) | (abs(ic) if not np.isnan(ic) else 0.0,)",
    "explanation": "Mine factors using DEAP.",
    "python_code": "def mine(\n        self,\n        df: pd.DataFrame,\n        target: pd.Series,\n        n_factors: int = 10,\n        feature_cols: Optional[List[str]] = None\n    ) -> List[DiscoveredFactor]:\n        \"\"\"Mine factors using DEAP.\"\"\"\n        if not HAS_DEAP:\n            logger.warning(\"DEAP not available\")\n            return []\n\n        # Prepare features\n        if feature_cols is None:\n            feature_cols = [c for c in df.columns if c in\n                          ['open', 'high', 'low', 'close', 'volume', 'returns']]\n\n        X = df[feature_cols].values\n        y = target.values\n\n        # Remove NaN\n        valid_mask = ~(np.isnan(X).any(axis=1) | np.isnan(y))\n        X = X[valid_mask]\n        y = y[valid_mask]\n\n        n_features = X.shape[1]\n\n        # Setup primitives\n        pset = self._setup_primitives(n_features)\n\n        # Create types\n        if not hasattr(creator, \"FitnessMax\"):\n            creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n        if not hasattr(creator, \"Individual\"):\n            creator.create(\"Individual\", gp.PrimitiveTree, fitness=creator.FitnessMax)\n\n        # Create toolbox\n        self.toolbox = base.Toolbox()\n        self.toolbox.register(\"expr\", gp.genHalfAndHalf, pset=pset, min_=1, max_=self.max_depth)\n        self.toolbox.register(\"individual\", tools.initIterate, creator.Individual, self.toolbox.expr)\n        self.toolbox.register(\"population\", tools.initRepeat, list, self.toolbox.individual)\n        self.toolbox.register(\"compile\", gp.compile, pset=pset)\n\n        # Define fitness function\n        def eval_factor(individual):\n            func = self.toolbox.compile(expr=individual)\n            try:\n                factor_values = np.array([func(*row) for row in X])\n                valid_idx = ~np.isnan(factor_values) & ~np.isinf(factor_values)\n                if np.sum(valid_idx) < 10:\n                    return (0.0,)\n                ic = np.corrcoef(factor_values[valid_idx], y[valid_idx])[0, 1]\n                return (abs(ic) if not np.isnan(ic) else 0.0,)\n            except:\n                return (0.0,)\n\n        self.toolbox.register(\"evaluate\", eval_factor)\n        self.toolbox.register(\"select\", tools.selTournament, tournsize=3)\n        self.toolbox.register(\"mate\", gp.cxOnePoint)\n        self.toolbox.register(\"expr_mut\", gp.genFull, min_=0, max_=2)\n        self.toolbox.register(\"mutate\", gp.mutUniform, expr=self.toolbox.expr_mut, pset=pset)\n\n        # Limit tree depth\n        self.toolbox.decorate(\"mate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=self.max_depth))\n        self.toolbox.decorate(\"mutate\", gp.staticLimit(key=operator.attrgetter(\"height\"), max_value=self.max_depth))\n\n        # Set random seed\n        np.random.seed(self.random_state)\n\n        # Run evolution\n        logger.info(f\"Running DEAP evolution for {self.generations} generations...\")\n\n        pop = self.toolbox.population(n=self.population_size)\n        hof = tools.HallOfFame(n_factors * 2)\n\n        stat",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DEAPFactorMiner"
  },
  {
    "name": "mine_factors",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Mine factors from data.",
    "python_code": "def mine_factors(\n        self,\n        df: pd.DataFrame,\n        target: pd.Series,\n        n_factors: int = 10\n    ) -> List[DiscoveredFactor]:\n        \"\"\"Mine factors from data.\"\"\"\n        self.discovered_factors = self.miner.mine(df, target, n_factors)\n        return self.discovered_factors",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GeneticFactorEngine"
  },
  {
    "name": "generate_factor_features",
    "category": "reinforcement_learning",
    "formula": "pd.DataFrame()",
    "explanation": "Generate features using discovered factors.",
    "python_code": "def generate_factor_features(\n        self,\n        df: pd.DataFrame\n    ) -> pd.DataFrame:\n        \"\"\"Generate features using discovered factors.\"\"\"\n        if hasattr(self.miner, 'transform'):\n            return self.miner.transform(df)\n        return pd.DataFrame()",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GeneticFactorEngine"
  },
  {
    "name": "get_factor_report",
    "category": "reinforcement_learning",
    "formula": "pd.DataFrame() | pd.DataFrame([",
    "explanation": "Get report of discovered factors.",
    "python_code": "def get_factor_report(self) -> pd.DataFrame:\n        \"\"\"Get report of discovered factors.\"\"\"\n        if not self.discovered_factors:\n            return pd.DataFrame()\n\n        return pd.DataFrame([\n            {\n                'formula': f.formula,\n                'ic': f.ic,\n                'icir': f.icir,\n                'fitness': f.fitness,\n                'complexity': f.complexity,\n                'turnover': f.turnover\n            }\n            for f in self.discovered_factors\n        ])",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GeneticFactorEngine"
  },
  {
    "name": "compute_all_features",
    "category": "reinforcement_learning",
    "formula": "features | features",
    "explanation": "Compute genetic programming derived features.\nInterface compatible with HFT Feature Engine.\n\nUses pre-defined genetic formulas for speed in HFT context.",
    "python_code": "def compute_all_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Compute genetic programming derived features.\n        Interface compatible with HFT Feature Engine.\n\n        Uses pre-defined genetic formulas for speed in HFT context.\n        \"\"\"\n        features = pd.DataFrame(index=df.index)\n\n        if len(df) < 10:\n            return features\n\n        # Pre-defined genetic formulas from research\n        # These are evolved formulas that showed high IC in backtests\n\n        close = df['close'].values\n        open_ = df['open'].values if 'open' in df.columns else close\n        high = df['high'].values if 'high' in df.columns else close\n        low = df['low'].values if 'low' in df.columns else close\n        volume = df['volume'].values if 'volume' in df.columns else np.ones_like(close)\n\n        # GP Formula 1: Price-Volume Interaction\n        # (close - open) / (high - low + 1e-10) * log(volume + 1)\n        price_range = high - low + 1e-10\n        body = close - open_\n        features['pv_interaction'] = (body / price_range) * np.log(volume + 1)\n\n        # GP Formula 2: Momentum with Volume Weight\n        # Returns(5) * (volume / ma(volume, 20))\n        returns_5 = np.zeros_like(close)\n        returns_5[5:] = (close[5:] / close[:-5]) - 1\n        vol_ma = _ts_mean(volume, 20)\n        vol_ratio = volume / (vol_ma + 1e-10)\n        features['mom_vol'] = returns_5 * vol_ratio\n\n        # GP Formula 3: Range-based Volatility Signal\n        # (high - low) / (ma(high - low, 10))\n        hl_range = high - low\n        range_ma = _ts_mean(hl_range, 10)\n        features['range_signal'] = hl_range / (range_ma + 1e-10)\n\n        # GP Formula 4: Price Position in Range\n        # (close - low) / (high - low + 1e-10)\n        features['price_pos'] = (close - low) / price_range\n\n        # GP Formula 5: Volume-Weighted Price Change\n        # delta(close, 1) * (volume / max(volume, 20))\n        delta_close = _ts_delta(close, 1)\n        vol_max = _ts_max(volume, 20)\n        features['vwpc'] = delta_close * (volume / (vol_max + 1e-10))\n\n        # GP Formula 6: Trend Strength\n        # (close - ma(close, 20)) / std(close, 20)\n        ma_20 = _ts_mean(close, 20)\n        std_20 = _ts_std(close, 20)\n        features['trend_strength'] = (close - ma_20) / (std_20 + 1e-10)\n\n        # GP Formula 7: Acceleration\n        # delta(delta(close, 5), 5)\n        delta_5 = _ts_delta(close, 5)\n        features['acceleration'] = _ts_delta(delta_5, 5)\n\n        # GP Formula 8: Volume Spike\n        # volume / (ma(volume, 10) * 2)\n        vol_ma_10 = _ts_mean(volume, 10)\n        features['vol_spike'] = volume / (vol_ma_10 * 2 + 1e-10)\n\n        # GP Formula 9: Body Ratio\n        # abs(close - open) / (high - low + 1e-10)\n        features['body_ratio'] = np.abs(body) / price_range\n\n        # GP Formula 10: Upper Shadow\n        # (high - max(open, close)) / (high - low + 1e-10)\n        upper_shadow = high - np.maximum(open_, close)\n        features['upper_shadow'] = upper_sh",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GeneticFactorEngine"
  },
  {
    "name": "eval_factor",
    "category": "reinforcement_learning",
    "formula": "(0.0,) | (abs(ic) if not np.isnan(ic) else 0.0,) | (0.0,)",
    "explanation": "",
    "python_code": "def eval_factor(individual):\n            func = self.toolbox.compile(expr=individual)\n            try:\n                factor_values = np.array([func(*row) for row in X])\n                valid_idx = ~np.isnan(factor_values) & ~np.isinf(factor_values)\n                if np.sum(valid_idx) < 10:\n                    return (0.0,)\n                ic = np.corrcoef(factor_values[valid_idx], y[valid_idx])[0, 1]\n                return (abs(ic) if not np.isnan(ic) else 0.0,)\n            except:\n                return (0.0,)",
    "source_file": "core\\_experimental\\genetic_factor_mining.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "deep_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self,\n                 seq_len: int = 96,\n                 pred_len: int = 24,\n                 d_model: int = 512,\n                 n_heads: int = 8,\n                 e_layers: int = 3,\n                 d_ff: int = 2048,\n                 dropout: float = 0.1):\n        self.seq_len = seq_len\n        self.pred_len = pred_len\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.e_layers = e_layers\n        self.d_ff = d_ff\n        self.dropout = dropout\n        self.model = None",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "iTransformerForex"
  },
  {
    "name": "build_model",
    "category": "deep_learning",
    "formula": "x.reshape(b, v, t, d).permute(0, 2, 1, 3) | x.permute(0, 2, 1)  # (batch, pred_len, n_features) | True",
    "explanation": "Build iTransformer model.",
    "python_code": "def build_model(self, n_features: int):\n        \"\"\"Build iTransformer model.\"\"\"\n        try:\n            import torch\n            import torch.nn as nn\n\n            class InvertedAttention(nn.Module):\n                \"\"\"Attention across variates instead of time.\"\"\"\n                def __init__(self, d_model, n_heads, dropout):\n                    super().__init__()\n                    self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n                    self.norm = nn.LayerNorm(d_model)\n                    self.dropout = nn.Dropout(dropout)\n\n                def forward(self, x):\n                    # x: (batch, seq_len, n_variates, d_model) -> transpose for variate attention\n                    b, t, v, d = x.shape\n                    x = x.permute(0, 2, 1, 3).reshape(b * v, t, d)  # (batch*variates, time, d_model)\n                    attn_out, _ = self.attention(x, x, x)\n                    x = self.norm(x + self.dropout(attn_out))\n                    return x.reshape(b, v, t, d).permute(0, 2, 1, 3)\n\n            class iTransformerModel(nn.Module):\n                def __init__(self, seq_len, pred_len, n_features, d_model, n_heads, e_layers, d_ff, dropout):\n                    super().__init__()\n                    self.seq_len = seq_len\n                    self.pred_len = pred_len\n\n                    # Embedding per variate\n                    self.embed = nn.Linear(seq_len, d_model)\n\n                    # Inverted attention layers\n                    self.layers = nn.ModuleList([\n                        InvertedAttention(d_model, n_heads, dropout)\n                        for _ in range(e_layers)\n                    ])\n\n                    # FFN\n                    self.ffn = nn.Sequential(\n                        nn.Linear(d_model, d_ff),\n                        nn.GELU(),\n                        nn.Dropout(dropout),\n                        nn.Linear(d_ff, d_model)\n                    )\n\n                    # Projection\n                    self.projection = nn.Linear(d_model, pred_len)\n\n                def forward(self, x):\n                    # x: (batch, seq_len, n_features)\n                    b, t, v = x.shape\n\n                    # Embed each variate's time series\n                    x = x.permute(0, 2, 1)  # (batch, n_features, seq_len)\n                    x = self.embed(x)  # (batch, n_features, d_model)\n                    x = x.unsqueeze(1).expand(-1, t, -1, -1)  # (batch, seq_len, n_features, d_model)\n\n                    # Apply inverted attention\n                    for layer in self.layers:\n                        x = layer(x)\n\n                    # FFN and project\n                    x = x[:, -1, :, :]  # Take last time step\n                    x = x + self.ffn(x)\n                    x = self.projection(x)  # (batch, n_features, pred_len)\n\n                    return x.permute(0, 2, 1)  # (batch, pred_len, n_features)\n\n            self.model = iTransformerModel(\n                self.seq_len",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "iTransformerForex"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "pred.numpy()",
    "explanation": "Generate predictions.\n\nArgs:\n    data: (batch, seq_len, features) or (seq_len, features)\n\nReturns:\n    predictions: (batch, pred_len, features)",
    "python_code": "def predict(self, data: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Generate predictions.\n\n        Args:\n            data: (batch, seq_len, features) or (seq_len, features)\n\n        Returns:\n            predictions: (batch, pred_len, features)\n        \"\"\"\n        if self.model is None:\n            raise ValueError(\"Model not built. Call build_model() first.\")\n\n        import torch\n\n        if len(data.shape) == 2:\n            data = data[np.newaxis, ...]\n\n        x = torch.FloatTensor(data)\n\n        self.model.eval()\n        with torch.no_grad():\n            pred = self.model(x)\n\n        return pred.numpy()",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "iTransformerForex"
  },
  {
    "name": "build_model",
    "category": "quantitative",
    "formula": "out.reshape(-1, self.pred_len, self.n_endo) | True | False",
    "explanation": "Build TimeXer model.",
    "python_code": "def build_model(self):\n        \"\"\"Build TimeXer model.\"\"\"\n        try:\n            import torch\n            import torch.nn as nn\n\n            class TimeXerModel(nn.Module):\n                def __init__(self, seq_len, pred_len, n_endo, n_exo, d_model):\n                    super().__init__()\n\n                    # Separate embeddings\n                    self.endo_embed = nn.Linear(n_endo, d_model)\n                    self.exo_embed = nn.Linear(n_exo, d_model)\n\n                    # Temporal encoding\n                    self.temporal_enc = nn.Parameter(torch.randn(1, seq_len, d_model))\n\n                    # Cross-attention: how exogenous affects endogenous\n                    self.cross_attn = nn.MultiheadAttention(d_model, 4, batch_first=True)\n\n                    # Self-attention on combined\n                    self.self_attn = nn.MultiheadAttention(d_model, 4, batch_first=True)\n\n                    # Projection\n                    self.projection = nn.Linear(d_model, pred_len * n_endo)\n                    self.pred_len = pred_len\n                    self.n_endo = n_endo\n\n                def forward(self, endo, exo):\n                    # endo: (batch, seq_len, n_endo)\n                    # exo: (batch, seq_len, n_exo)\n\n                    endo_emb = self.endo_embed(endo) + self.temporal_enc\n                    exo_emb = self.exo_embed(exo) + self.temporal_enc\n\n                    # Cross attention: endogenous queries, exogenous keys/values\n                    cross_out, _ = self.cross_attn(endo_emb, exo_emb, exo_emb)\n\n                    # Self attention on enriched endogenous\n                    combined = endo_emb + cross_out\n                    self_out, _ = self.self_attn(combined, combined, combined)\n\n                    # Project to predictions\n                    out = self_out[:, -1, :]  # Last time step\n                    out = self.projection(out)\n                    return out.reshape(-1, self.pred_len, self.n_endo)\n\n            self.model = TimeXerModel(\n                self.seq_len, self.pred_len,\n                self.n_endogenous, self.n_exogenous, self.d_model\n            )\n            logger.info(\"TimeXer built successfully\")\n            return True\n\n        except ImportError:\n            logger.warning(\"PyTorch not available\")\n            return False",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TimeXerForex"
  },
  {
    "name": "prepare_exogenous_features",
    "category": "volatility",
    "formula": "np.column_stack(features) | np.zeros((len(df), 1))",
    "explanation": "Prepare exogenous features for forex trading.\n\nRecommended exogenous variables:\n1. Related pairs (EUR/GBP, GBP/JPY for EUR/USD)\n2. DXY (Dollar Index)\n3. VIX (Volatility Index)\n4. Bond yield spreads\n5. Economic calendar events (binary)\n6. Session indicators (London, NY, Tokyo)\n7. Day of week\n8. Hour of day",
    "python_code": "def prepare_exogenous_features(df: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Prepare exogenous features for forex trading.\n\n        Recommended exogenous variables:\n        1. Related pairs (EUR/GBP, GBP/JPY for EUR/USD)\n        2. DXY (Dollar Index)\n        3. VIX (Volatility Index)\n        4. Bond yield spreads\n        5. Economic calendar events (binary)\n        6. Session indicators (London, NY, Tokyo)\n        7. Day of week\n        8. Hour of day\n        \"\"\"\n        features = []\n\n        # Add related pairs if available\n        for col in ['EURGBP', 'GBPJPY', 'DXY', 'VIX']:\n            if col in df.columns:\n                features.append(df[col].values)\n\n        # Add time features\n        if 'timestamp' in df.columns or isinstance(df.index, pd.DatetimeIndex):\n            idx = df.index if isinstance(df.index, pd.DatetimeIndex) else pd.to_datetime(df['timestamp'])\n            features.append(idx.hour / 24)  # Hour normalized\n            features.append(idx.dayofweek / 7)  # Day normalized\n\n            # Session indicators\n            london = ((idx.hour >= 8) & (idx.hour < 16)).astype(float)\n            ny = ((idx.hour >= 13) & (idx.hour < 21)).astype(float)\n            tokyo = ((idx.hour >= 0) & (idx.hour < 9)).astype(float)\n            features.extend([london, ny, tokyo])\n\n        if features:\n            return np.column_stack(features)\n        return np.zeros((len(df), 1))",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TimeXerForex"
  },
  {
    "name": "build_model",
    "category": "reinforcement_learning",
    "formula": "weights, attn_weights | True | False",
    "explanation": "Build attention factor model.",
    "python_code": "def build_model(self):\n        \"\"\"Build attention factor model.\"\"\"\n        try:\n            import torch\n            import torch.nn as nn\n\n            class AttentionFactorNet(nn.Module):\n                def __init__(self, n_pairs, lookback, d_model, n_heads):\n                    super().__init__()\n\n                    # Embed each pair's returns\n                    self.embed = nn.Linear(lookback, d_model)\n\n                    # Cross-pair attention\n                    self.attention = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n\n                    # Factor weights output\n                    self.weights = nn.Sequential(\n                        nn.Linear(d_model, d_model // 2),\n                        nn.ReLU(),\n                        nn.Linear(d_model // 2, 1),\n                        nn.Tanh()  # Weights between -1 and 1\n                    )\n\n                def forward(self, returns):\n                    # returns: (batch, n_pairs, lookback)\n                    x = self.embed(returns)  # (batch, n_pairs, d_model)\n\n                    # Attention across pairs\n                    attn_out, attn_weights = self.attention(x, x, x)\n\n                    # Generate portfolio weights\n                    weights = self.weights(attn_out).squeeze(-1)  # (batch, n_pairs)\n\n                    # Normalize to sum to zero (market neutral)\n                    weights = weights - weights.mean(dim=1, keepdim=True)\n\n                    return weights, attn_weights\n\n            self.model = AttentionFactorNet(self.n_pairs, self.lookback, self.d_model, self.n_heads)\n            logger.info(\"Attention Factor Model built\")\n            return True\n\n        except ImportError:\n            logger.warning(\"PyTorch not available\")\n            return False",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AttentionFactorModel"
  },
  {
    "name": "get_portfolio_weights",
    "category": "technical",
    "formula": "weights.numpy().squeeze(), attn.numpy().squeeze()",
    "explanation": "Get market-neutral portfolio weights.\n\nArgs:\n    returns_matrix: (lookback, n_pairs) matrix of returns\n\nReturns:\n    weights: Portfolio weights (sum to 0)\n    attention: Attention matrix showing pair relationships",
    "python_code": "def get_portfolio_weights(self, returns_matrix: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Get market-neutral portfolio weights.\n\n        Args:\n            returns_matrix: (lookback, n_pairs) matrix of returns\n\n        Returns:\n            weights: Portfolio weights (sum to 0)\n            attention: Attention matrix showing pair relationships\n        \"\"\"\n        if self.model is None:\n            raise ValueError(\"Model not built\")\n\n        import torch\n\n        # Reshape for batch dimension\n        x = torch.FloatTensor(returns_matrix.T)[np.newaxis, ...]  # (1, n_pairs, lookback)\n\n        self.model.eval()\n        with torch.no_grad():\n            weights, attn = self.model(x)\n\n        return weights.numpy().squeeze(), attn.numpy().squeeze()",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AttentionFactorModel"
  },
  {
    "name": "generate_signals",
    "category": "reinforcement_learning",
    "formula": "signals_df",
    "explanation": "Generate trading signals for a basket of currency pairs.\n\nArgs:\n    prices_df: DataFrame with pair prices as columns\n    pairs: List of pair names to trade\n\nReturns:\n    DataFrame with signals and weights per pair",
    "python_code": "def generate_signals(self, prices_df: pd.DataFrame, pairs: List[str]) -> pd.DataFrame:\n        \"\"\"\n        Generate trading signals for a basket of currency pairs.\n\n        Args:\n            prices_df: DataFrame with pair prices as columns\n            pairs: List of pair names to trade\n\n        Returns:\n            DataFrame with signals and weights per pair\n        \"\"\"\n        returns = prices_df[pairs].pct_change().dropna()\n\n        signals = []\n        for i in range(self.lookback, len(returns)):\n            window = returns.iloc[i-self.lookback:i].values\n            weights, _ = self.get_portfolio_weights(window)\n            signals.append(weights)\n\n        signals_df = pd.DataFrame(\n            signals,\n            index=returns.index[self.lookback:],\n            columns=pairs\n        )\n\n        return signals_df",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AttentionFactorModel"
  },
  {
    "name": "setup",
    "category": "quantitative",
    "formula": "",
    "explanation": "Setup backtest with tick data.\n\nArgs:\n    tick_data: DataFrame with columns [timestamp, bid, ask, bid_size, ask_size]",
    "python_code": "def setup(self, tick_data: pd.DataFrame):\n        \"\"\"\n        Setup backtest with tick data.\n\n        Args:\n            tick_data: DataFrame with columns [timestamp, bid, ask, bid_size, ask_size]\n        \"\"\"\n        try:\n            from hftbacktest import BacktestAsset, HashMapMarketDepthBacktest\n\n            # Convert to hftbacktest format\n            # ... implementation depends on hftbacktest version\n\n            logger.info(\"HftBacktest setup complete\")\n\n        except ImportError:\n            logger.warning(\"hftbacktest not installed: pip install hftbacktest\")",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HftBacktestWrapper"
  },
  {
    "name": "run_backtest",
    "category": "quantitative",
    "formula": "{",
    "explanation": "Run backtest with strategy function.\n\nArgs:\n    strategy_fn: Function that takes (bid, ask, position) and returns action\n\nReturns:\n    Performance metrics",
    "python_code": "def run_backtest(self, strategy_fn) -> Dict[str, float]:\n        \"\"\"\n        Run backtest with strategy function.\n\n        Args:\n            strategy_fn: Function that takes (bid, ask, position) and returns action\n\n        Returns:\n            Performance metrics\n        \"\"\"\n        # Placeholder - actual implementation requires hftbacktest\n        return {\n            'total_pnl': 0.0,\n            'sharpe': 0.0,\n            'max_drawdown': 0.0,\n            'n_trades': 0,\n            'win_rate': 0.0\n        }",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HftBacktestWrapper"
  },
  {
    "name": "optimize_xgboost",
    "category": "quantitative",
    "formula": "np.mean(scores) | { | {}",
    "explanation": "Optimize XGBoost hyperparameters.\n\nReturns best parameters and validation score.",
    "python_code": "def optimize_xgboost(self, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:\n        \"\"\"\n        Optimize XGBoost hyperparameters.\n\n        Returns best parameters and validation score.\n        \"\"\"\n        try:\n            import optuna\n            import xgboost as xgb\n            from sklearn.model_selection import TimeSeriesSplit\n\n            def objective(trial):\n                params = {\n                    'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n                    'max_depth': trial.suggest_int('max_depth', 3, 12),\n                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n                    'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n                    'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n                    'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n                    'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n                }\n\n                tscv = TimeSeriesSplit(n_splits=self.n_splits)\n                scores = []\n\n                for train_idx, val_idx in tscv.split(X):\n                    X_train, X_val = X[train_idx], X[val_idx]\n                    y_train, y_val = y[train_idx], y[val_idx]\n\n                    model = xgb.XGBClassifier(**params, use_label_encoder=False, eval_metric='logloss')\n                    model.fit(X_train, y_train, verbose=False)\n\n                    pred = model.predict(X_val)\n                    acc = (pred == y_val).mean()\n                    scores.append(acc)\n\n                return np.mean(scores)\n\n            self.study = optuna.create_study(direction='maximize')\n            self.study.optimize(objective, n_trials=self.n_trials, show_progress_bar=True)\n\n            return {\n                'best_params': self.study.best_params,\n                'best_score': self.study.best_value,\n                'n_trials': len(self.study.trials)\n            }\n\n        except ImportError:\n            logger.warning(\"Optuna not installed: pip install optuna\")\n            return {}",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OptunaOptimizer"
  },
  {
    "name": "optimize_transformer",
    "category": "deep_learning",
    "formula": "0.0  # validation loss | { | {}",
    "explanation": "Optimize transformer hyperparameters.",
    "python_code": "def optimize_transformer(self, train_data: np.ndarray, val_data: np.ndarray) -> Dict[str, Any]:\n        \"\"\"Optimize transformer hyperparameters.\"\"\"\n        try:\n            import optuna\n\n            def objective(trial):\n                d_model = trial.suggest_categorical('d_model', [64, 128, 256, 512])\n                n_heads = trial.suggest_categorical('n_heads', [2, 4, 8])\n                n_layers = trial.suggest_int('n_layers', 1, 6)\n                dropout = trial.suggest_float('dropout', 0.0, 0.5)\n                lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n\n                # Build and train model (simplified)\n                model = iTransformerForex(d_model=d_model, n_heads=n_heads, e_layers=n_layers, dropout=dropout)\n                # ... training loop ...\n\n                return 0.0  # validation loss\n\n            self.study = optuna.create_study(direction='minimize')\n            self.study.optimize(objective, n_trials=self.n_trials)\n\n            return {\n                'best_params': self.study.best_params,\n                'best_score': self.study.best_value\n            }\n\n        except ImportError:\n            return {}",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OptunaOptimizer"
  },
  {
    "name": "get_triple_barrier_labels",
    "category": "microstructure",
    "formula": "labels",
    "explanation": "Apply triple barrier method.\n\nBarriers:\n1. Upper barrier (profit take)\n2. Lower barrier (stop loss)\n3. Vertical barrier (max holding time)\n\nLabel: +1 (hit upper), -1 (hit lower), 0 (hit vertical)",
    "python_code": "def get_triple_barrier_labels(self, prices: pd.Series, events: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Apply triple barrier method.\n\n        Barriers:\n        1. Upper barrier (profit take)\n        2. Lower barrier (stop loss)\n        3. Vertical barrier (max holding time)\n\n        Label: +1 (hit upper), -1 (hit lower), 0 (hit vertical)\n        \"\"\"\n        labels = pd.Series(index=events.index, dtype=float)\n\n        for idx in events.index:\n            entry_price = prices.loc[idx]\n            upper = entry_price * (1 + self.profit_take)\n            lower = entry_price * (1 - self.stop_loss)\n\n            # Look forward\n            future = prices.loc[idx:][:self.max_holding]\n\n            # Find first barrier touch\n            upper_touch = (future >= upper).idxmax() if (future >= upper).any() else None\n            lower_touch = (future <= lower).idxmax() if (future <= lower).any() else None\n            vertical_touch = future.index[-1] if len(future) == self.max_holding else None\n\n            # Determine which barrier was hit first\n            touches = [(upper_touch, 1), (lower_touch, -1), (vertical_touch, 0)]\n            touches = [(t, l) for t, l in touches if t is not None]\n\n            if touches:\n                first_touch = min(touches, key=lambda x: x[0])\n                labels.loc[idx] = first_touch[1]\n            else:\n                labels.loc[idx] = 0\n\n        return labels",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MetaLabeler"
  },
  {
    "name": "train_meta_model",
    "category": "machine_learning",
    "formula": "meta_model",
    "explanation": "Train meta model to predict if primary model is correct.\n\nArgs:\n    X: Features\n    primary_predictions: Primary model's predictions\n    actual_labels: Actual outcomes\n\nReturns:\n    Trained meta model",
    "python_code": "def train_meta_model(self,\n                         X: np.ndarray,\n                         primary_predictions: np.ndarray,\n                         actual_labels: np.ndarray) -> Any:\n        \"\"\"\n        Train meta model to predict if primary model is correct.\n\n        Args:\n            X: Features\n            primary_predictions: Primary model's predictions\n            actual_labels: Actual outcomes\n\n        Returns:\n            Trained meta model\n        \"\"\"\n        try:\n            from sklearn.ensemble import RandomForestClassifier\n\n            # Meta labels: 1 if primary was correct, 0 otherwise\n            meta_labels = (primary_predictions == actual_labels).astype(int)\n\n            # Add primary prediction as feature\n            X_meta = np.column_stack([X, primary_predictions])\n\n            meta_model = RandomForestClassifier(n_estimators=100)\n            meta_model.fit(X_meta, meta_labels)\n\n            return meta_model\n\n        except ImportError:\n            logger.warning(\"sklearn not available\")\n            return None",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MetaLabeler"
  },
  {
    "name": "get_trade_confidence",
    "category": "quantitative",
    "formula": "meta_model.predict_proba(X_meta)[:, 1]",
    "explanation": "Get confidence for each trade.\n\nReturns probability that primary model is correct.",
    "python_code": "def get_trade_confidence(self,\n                            meta_model,\n                            X: np.ndarray,\n                            primary_prediction: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Get confidence for each trade.\n\n        Returns probability that primary model is correct.\n        \"\"\"\n        X_meta = np.column_stack([X, primary_prediction])\n        return meta_model.predict_proba(X_meta)[:, 1]",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MetaLabeler"
  },
  {
    "name": "reset",
    "category": "quantitative",
    "formula": "",
    "explanation": "Reset environment with new data.",
    "python_code": "def reset(self, data: pd.DataFrame) -> np.ndarray:\n        \"\"\"Reset environment with new data.\"\"\"\n        self.data = data\n        self.balance = self.initial_balance\n        self.position = 0\n        self.entry_price = 0\n        self.step_idx = 0\n\n        return self._get_observation()",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ForexTradingEnv"
  },
  {
    "name": "step",
    "category": "quantitative",
    "formula": "",
    "explanation": "Execute action.\n\nActions:\n0 = Hold\n1 = Buy (go long)\n2 = Sell (go short)\n3 = Close position",
    "python_code": "def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:\n        \"\"\"\n        Execute action.\n\n        Actions:\n        0 = Hold\n        1 = Buy (go long)\n        2 = Sell (go short)\n        3 = Close position\n        \"\"\"\n        if self.data is None:\n            raise ValueError(\"Call reset() first\")\n\n        current_price = self.data.iloc[self.step_idx]['close']\n        reward = 0\n\n        # Calculate spread cost\n        spread_cost = self.spread_pips * 0.0001  # Convert pips to decimal\n\n        if action == 1 and self.position <= 0:  # Buy\n            # Close short if exists\n            if self.position < 0:\n                pnl = (self.entry_price - current_price - spread_cost) * abs(self.position)\n                reward += pnl\n                self.balance += pnl\n\n            # Open long\n            self.position = min(self.max_position, self.balance * self.leverage)\n            self.entry_price = current_price + spread_cost\n            self.balance -= self.position * self.commission_pct\n\n        elif action == 2 and self.position >= 0:  # Sell\n            # Close long if exists\n            if self.position > 0:\n                pnl = (current_price - self.entry_price - spread_cost) * self.position\n                reward += pnl\n                self.balance += pnl\n\n            # Open short\n            self.position = -min(self.max_position, self.balance * self.leverage)\n            self.entry_price = current_price - spread_cost\n            self.balance -= abs(self.position) * self.commission_pct\n\n        elif action == 3:  # Close\n            if self.position > 0:\n                pnl = (current_price - self.entry_price - spread_cost) * self.position\n            elif self.position < 0:\n                pnl = (self.entry_price - current_price - spread_cost) * abs(self.position)\n            else:\n                pnl = 0\n            reward += pnl\n            self.balance += pnl\n            self.position = 0\n\n        # Mark-to-market unrealized PnL\n        if self.position > 0:\n            unrealized = (current_price - self.entry_price) * self.position\n        elif self.position < 0:\n            unrealized = (self.entry_price - current_price) * abs(self.position)\n        else:\n            unrealized = 0\n\n        # Move to next step\n        self.step_idx += 1\n        done = self.step_idx >= len(self.data) - 1 or self.balance <= 0\n\n        info = {\n            'balance': self.balance,\n            'position': self.position,\n            'unrealized_pnl': unrealized,\n            'total_equity': self.balance + unrealized\n        }\n\n        return self._get_observation(), reward, done, info",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ForexTradingEnv"
  },
  {
    "name": "_get_observation",
    "category": "quantitative",
    "formula": "obs",
    "explanation": "Get current observation.",
    "python_code": "def _get_observation(self) -> np.ndarray:\n        \"\"\"Get current observation.\"\"\"\n        row = self.data.iloc[self.step_idx]\n\n        obs = np.array([\n            row['close'],\n            row.get('returns', 0),\n            row.get('volatility', 0),\n            row.get('rsi', 50) / 100,\n            self.position / self.max_position,\n            self.balance / self.initial_balance,\n        ], dtype=np.float32)\n\n        return obs",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "ForexTradingEnv"
  },
  {
    "name": "build_model",
    "category": "quantitative",
    "formula": "x.permute(0, 2, 1) | seasonal_out + trend_out",
    "explanation": "Build DLinear model using PyTorch.",
    "python_code": "def build_model(self, n_features: int):\n        \"\"\"Build DLinear model using PyTorch.\"\"\"\n        try:\n            import torch\n            import torch.nn as nn\n\n            class MovingAvg(nn.Module):\n                \"\"\"Moving average block for trend extraction.\"\"\"\n                def __init__(self, kernel_size, stride=1):\n                    super().__init__()\n                    self.kernel_size = kernel_size\n                    self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n\n                def forward(self, x):\n                    # Padding on both ends\n                    front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n                    end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n                    x = torch.cat([front, x, end], dim=1)\n                    x = self.avg(x.permute(0, 2, 1))\n                    return x.permute(0, 2, 1)\n\n            class DLinearModel(nn.Module):\n                def __init__(self, seq_len, pred_len, n_features, individual, kernel_size):\n                    super().__init__()\n                    self.seq_len = seq_len\n                    self.pred_len = pred_len\n                    self.n_features = n_features\n\n                    # Decomposition\n                    self.decomp = MovingAvg(kernel_size)\n\n                    # Linear layers\n                    if individual:\n                        self.Linear_Seasonal = nn.ModuleList([\n                            nn.Linear(seq_len, pred_len) for _ in range(n_features)\n                        ])\n                        self.Linear_Trend = nn.ModuleList([\n                            nn.Linear(seq_len, pred_len) for _ in range(n_features)\n                        ])\n                    else:\n                        self.Linear_Seasonal = nn.Linear(seq_len, pred_len)\n                        self.Linear_Trend = nn.Linear(seq_len, pred_len)\n\n                    self.individual = individual\n\n                def forward(self, x):\n                    # x: (batch, seq_len, n_features)\n                    trend = self.decomp(x)\n                    seasonal = x - trend\n\n                    if self.individual:\n                        seasonal_out = torch.zeros([x.size(0), self.pred_len, self.n_features],\n                                                   dtype=x.dtype, device=x.device)\n                        trend_out = torch.zeros([x.size(0), self.pred_len, self.n_features],\n                                               dtype=x.dtype, device=x.device)\n                        for i in range(self.n_features):\n                            seasonal_out[:, :, i] = self.Linear_Seasonal[i](seasonal[:, :, i])\n                            trend_out[:, :, i] = self.Linear_Trend[i](trend[:, :, i])\n                    else:\n                        seasonal_out = self.Linear_Seasonal(seasonal.permute(0, 2, 1)).permute(0, 2, 1)\n                        trend_out = self.Linear_Trend(trend.permute(0, 2, 1)).permute(0, 2, 1)\n\n     ",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DLinearForex"
  },
  {
    "name": "fit",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Train DLinear model.",
    "python_code": "def fit(self, X: np.ndarray, y: np.ndarray, epochs: int = 100, lr: float = 0.001):\n        \"\"\"Train DLinear model.\"\"\"\n        if self.model is None:\n            self.build_model(X.shape[-1])\n\n        if self.model is None:\n            return self\n\n        import torch\n        import torch.nn as nn\n        from torch.utils.data import DataLoader, TensorDataset\n\n        X_tensor = torch.FloatTensor(X)\n        y_tensor = torch.FloatTensor(y)\n        dataset = TensorDataset(X_tensor, y_tensor)\n        loader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n        criterion = nn.MSELoss()\n\n        self.model.train()\n        for epoch in range(epochs):\n            total_loss = 0\n            for batch_x, batch_y in loader:\n                optimizer.zero_grad()\n                pred = self.model(batch_x)\n                loss = criterion(pred, batch_y)\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n\n            if (epoch + 1) % 20 == 0:\n                logger.info(f\"DLinear Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(loader):.6f}\")\n\n        return self",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DLinearForex"
  },
  {
    "name": "build_model",
    "category": "quantitative",
    "formula": "x, n_features | x",
    "explanation": "Build PatchTST model using PyTorch.",
    "python_code": "def build_model(self, n_features: int):\n        \"\"\"Build PatchTST model using PyTorch.\"\"\"\n        try:\n            import torch\n            import torch.nn as nn\n\n            class Patching(nn.Module):\n                \"\"\"Patch embedding for time series.\"\"\"\n                def __init__(self, patch_len, stride, d_model):\n                    super().__init__()\n                    self.patch_len = patch_len\n                    self.stride = stride\n                    self.linear = nn.Linear(patch_len, d_model)\n\n                def forward(self, x):\n                    # x: (batch, seq_len, n_features)\n                    batch, seq_len, n_features = x.shape\n                    # Unfold to patches\n                    x = x.permute(0, 2, 1)  # (batch, n_features, seq_len)\n                    x = x.unfold(2, self.patch_len, self.stride)  # (batch, n_features, n_patches, patch_len)\n                    n_patches = x.shape[2]\n                    x = x.reshape(batch * n_features, n_patches, self.patch_len)\n                    x = self.linear(x)  # (batch*n_features, n_patches, d_model)\n                    return x, n_features\n\n            class PatchTSTModel(nn.Module):\n                def __init__(self, seq_len, pred_len, patch_len, stride, n_features,\n                             d_model, n_heads, e_layers, dropout):\n                    super().__init__()\n                    self.n_features = n_features\n                    self.pred_len = pred_len\n\n                    # Patching\n                    self.patching = Patching(patch_len, stride, d_model)\n\n                    # Positional encoding\n                    n_patches = (seq_len - patch_len) // stride + 1\n                    self.pos_embed = nn.Parameter(torch.randn(1, n_patches, d_model) * 0.02)\n\n                    # Transformer encoder\n                    encoder_layer = nn.TransformerEncoderLayer(\n                        d_model=d_model, nhead=n_heads, dim_feedforward=d_model * 4,\n                        dropout=dropout, batch_first=True\n                    )\n                    self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=e_layers)\n\n                    # Prediction head\n                    self.head = nn.Linear(d_model * n_patches, pred_len)\n\n                def forward(self, x):\n                    batch = x.shape[0]\n                    # Patch embedding\n                    x, n_feat = self.patching(x)  # (batch*n_features, n_patches, d_model)\n\n                    # Add positional encoding\n                    x = x + self.pos_embed\n\n                    # Transformer encoding\n                    x = self.encoder(x)  # (batch*n_features, n_patches, d_model)\n\n                    # Flatten and project\n                    x = x.reshape(batch * n_feat, -1)  # (batch*n_features, n_patches*d_model)\n                    x = self.head(x)  # (batch*n_features, pred_len)\n\n                    # Reshape back\n                    x = x.reshape(batch, n_feat, self.pred_len).permute(0, 2, 1)\n\n   ",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "PatchTSTForex"
  },
  {
    "name": "build_model",
    "category": "quantitative",
    "formula": "trend.permute(0, 2, 1), seasonal.permute(0, 2, 1) | out.reshape(batch, self.pred_len, self.n_features)",
    "explanation": "Build TimeMixer model using PyTorch.",
    "python_code": "def build_model(self, n_features: int):\n        \"\"\"Build TimeMixer model using PyTorch.\"\"\"\n        try:\n            import torch\n            import torch.nn as nn\n\n            class MultiScaleDecomp(nn.Module):\n                \"\"\"Multi-scale decomposition via average pooling.\"\"\"\n                def __init__(self, kernel_size):\n                    super().__init__()\n                    self.kernel_size = kernel_size\n                    self.avg_pool = nn.AvgPool1d(kernel_size=kernel_size, stride=kernel_size, padding=0)\n\n                def forward(self, x):\n                    # x: (batch, seq_len, n_features)\n                    x = x.permute(0, 2, 1)  # (batch, n_features, seq_len)\n                    trend = self.avg_pool(x)  # Downsampled\n                    # Upsample back for residual\n                    trend_up = torch.repeat_interleave(trend, self.kernel_size, dim=2)[:, :, :x.shape[2]]\n                    seasonal = x - trend_up\n                    return trend.permute(0, 2, 1), seasonal.permute(0, 2, 1)\n\n            class MixingBlock(nn.Module):\n                \"\"\"Mixing block for multi-scale features.\"\"\"\n                def __init__(self, seq_len, n_features, d_model, d_ff, dropout):\n                    super().__init__()\n                    self.temporal_linear = nn.Linear(seq_len, d_model)\n                    self.channel_linear = nn.Linear(n_features, d_ff)\n                    self.output_linear = nn.Linear(d_ff, n_features)\n                    self.dropout = nn.Dropout(dropout)\n                    self.norm = nn.LayerNorm(n_features)\n\n                def forward(self, x):\n                    # Temporal mixing\n                    x_t = self.temporal_linear(x.permute(0, 2, 1)).permute(0, 2, 1)\n                    # Channel mixing\n                    x_c = self.channel_linear(x_t)\n                    x_c = torch.relu(x_c)\n                    x_c = self.dropout(x_c)\n                    x_c = self.output_linear(x_c)\n                    return self.norm(x + x_c)\n\n            class TimeMixerModel(nn.Module):\n                def __init__(self, seq_len, pred_len, n_features, d_model, d_ff,\n                             e_layers, down_sampling_layers, down_sampling_window, dropout):\n                    super().__init__()\n                    self.pred_len = pred_len\n                    self.n_features = n_features\n\n                    # Multi-scale decomposition\n                    self.decomp_layers = nn.ModuleList([\n                        MultiScaleDecomp(down_sampling_window ** (i + 1))\n                        for i in range(down_sampling_layers)\n                    ])\n\n                    # Mixing blocks for each scale\n                    self.mixing_blocks = nn.ModuleList([\n                        MixingBlock(seq_len // (down_sampling_window ** (i + 1)),\n                                   n_features, d_model, d_ff, dropout)\n                        for i in range(down_sampling_layers)\n                    ])\n\n                    # Final pro",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TimeMixerForex"
  },
  {
    "name": "get_gold_standard_models",
    "category": "quantitative",
    "formula": "{",
    "explanation": "Get all gold standard models.\n\nReturns dict of initialized model classes.",
    "python_code": "def get_gold_standard_models() -> Dict[str, Any]:\n    \"\"\"\n    Get all gold standard models.\n\n    Returns dict of initialized model classes.\n    \"\"\"\n    return {\n        'iTransformer': iTransformerForex(),\n        'TimeXer': TimeXerForex(),\n        'AttentionFactor': AttentionFactorModel(),\n        'HftBacktest': HftBacktestWrapper(),\n        'Optuna': OptunaOptimizer(),\n        'MetaLabeler': MetaLabeler(),\n        'TradingEnv': ForexTradingEnv(),\n    }",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "forward",
    "category": "deep_learning",
    "formula": "x.reshape(b, v, t, d).permute(0, 2, 1, 3)",
    "explanation": "",
    "python_code": "def forward(self, x):\n                    # x: (batch, seq_len, n_variates, d_model) -> transpose for variate attention\n                    b, t, v, d = x.shape\n                    x = x.permute(0, 2, 1, 3).reshape(b * v, t, d)  # (batch*variates, time, d_model)\n                    attn_out, _ = self.attention(x, x, x)\n                    x = self.norm(x + self.dropout(attn_out))\n                    return x.reshape(b, v, t, d).permute(0, 2, 1, 3)",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "InvertedAttention"
  },
  {
    "name": "forward",
    "category": "deep_learning",
    "formula": "x.permute(0, 2, 1)",
    "explanation": "",
    "python_code": "def forward(self, x):\n                    # x: (batch, seq_len, n_features)\n                    b, t, v = x.shape\n\n                    # Embed each variate's time series\n                    x = x.permute(0, 2, 1)  # (batch, n_features, seq_len)\n                    x = self.embed(x)  # (batch, n_features, d_model)\n                    x = x.unsqueeze(1).expand(-1, t, -1, -1)  # (batch, seq_len, n_features, d_model)\n\n                    # Apply inverted attention\n                    for layer in self.layers:\n                        x = layer(x)\n\n                    # FFN and project\n                    x = x[:, -1, :, :]  # Take last time step\n                    x = x + self.ffn(x)\n                    x = self.projection(x)  # (batch, n_features, pred_len)\n\n                    return x.permute(0, 2, 1)",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "iTransformerModel"
  },
  {
    "name": "forward",
    "category": "quantitative",
    "formula": "out.reshape(-1, self.pred_len, self.n_endo)",
    "explanation": "",
    "python_code": "def forward(self, endo, exo):\n                    # endo: (batch, seq_len, n_endo)\n                    # exo: (batch, seq_len, n_exo)\n\n                    endo_emb = self.endo_embed(endo) + self.temporal_enc\n                    exo_emb = self.exo_embed(exo) + self.temporal_enc\n\n                    # Cross attention: endogenous queries, exogenous keys/values\n                    cross_out, _ = self.cross_attn(endo_emb, exo_emb, exo_emb)\n\n                    # Self attention on enriched endogenous\n                    combined = endo_emb + cross_out\n                    self_out, _ = self.self_attn(combined, combined, combined)\n\n                    # Project to predictions\n                    out = self_out[:, -1, :]  # Last time step\n                    out = self.projection(out)\n                    return out.reshape(-1, self.pred_len, self.n_endo)",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TimeXerModel"
  },
  {
    "name": "forward",
    "category": "reinforcement_learning",
    "formula": "weights, attn_weights",
    "explanation": "",
    "python_code": "def forward(self, returns):\n                    # returns: (batch, n_pairs, lookback)\n                    x = self.embed(returns)  # (batch, n_pairs, d_model)\n\n                    # Attention across pairs\n                    attn_out, attn_weights = self.attention(x, x, x)\n\n                    # Generate portfolio weights\n                    weights = self.weights(attn_out).squeeze(-1)  # (batch, n_pairs)\n\n                    # Normalize to sum to zero (market neutral)\n                    weights = weights - weights.mean(dim=1, keepdim=True)\n\n                    return weights, attn_weights",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AttentionFactorNet"
  },
  {
    "name": "objective",
    "category": "quantitative",
    "formula": "np.mean(scores)",
    "explanation": "",
    "python_code": "def objective(trial):\n                params = {\n                    'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n                    'max_depth': trial.suggest_int('max_depth', 3, 12),\n                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n                    'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n                    'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n                    'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n                    'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n                }\n\n                tscv = TimeSeriesSplit(n_splits=self.n_splits)\n                scores = []\n\n                for train_idx, val_idx in tscv.split(X):\n                    X_train, X_val = X[train_idx], X[val_idx]\n                    y_train, y_val = y[train_idx], y[val_idx]\n\n                    model = xgb.XGBClassifier(**params, use_label_encoder=False, eval_metric='logloss')\n                    model.fit(X_train, y_train, verbose=False)\n\n                    pred = model.predict(X_val)\n                    acc = (pred == y_val).mean()\n                    scores.append(acc)\n\n                return np.mean(scores)",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "objective",
    "category": "quantitative",
    "formula": "0.0",
    "explanation": "",
    "python_code": "def objective(trial):\n                d_model = trial.suggest_categorical('d_model', [64, 128, 256, 512])\n                n_heads = trial.suggest_categorical('n_heads', [2, 4, 8])\n                n_layers = trial.suggest_int('n_layers', 1, 6)\n                dropout = trial.suggest_float('dropout', 0.0, 0.5)\n                lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n\n                # Build and train model (simplified)\n                model = iTransformerForex(d_model=d_model, n_heads=n_heads, e_layers=n_layers, dropout=dropout)\n                # ... training loop ...\n\n                return 0.0",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "forward",
    "category": "quantitative",
    "formula": "seasonal_out + trend_out",
    "explanation": "",
    "python_code": "def forward(self, x):\n                    # x: (batch, seq_len, n_features)\n                    trend = self.decomp(x)\n                    seasonal = x - trend\n\n                    if self.individual:\n                        seasonal_out = torch.zeros([x.size(0), self.pred_len, self.n_features],\n                                                   dtype=x.dtype, device=x.device)\n                        trend_out = torch.zeros([x.size(0), self.pred_len, self.n_features],\n                                               dtype=x.dtype, device=x.device)\n                        for i in range(self.n_features):\n                            seasonal_out[:, :, i] = self.Linear_Seasonal[i](seasonal[:, :, i])\n                            trend_out[:, :, i] = self.Linear_Trend[i](trend[:, :, i])\n                    else:\n                        seasonal_out = self.Linear_Seasonal(seasonal.permute(0, 2, 1)).permute(0, 2, 1)\n                        trend_out = self.Linear_Trend(trend.permute(0, 2, 1)).permute(0, 2, 1)\n\n                    return seasonal_out + trend_out",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "DLinearModel"
  },
  {
    "name": "forward",
    "category": "quantitative",
    "formula": "x, n_features",
    "explanation": "",
    "python_code": "def forward(self, x):\n                    # x: (batch, seq_len, n_features)\n                    batch, seq_len, n_features = x.shape\n                    # Unfold to patches\n                    x = x.permute(0, 2, 1)  # (batch, n_features, seq_len)\n                    x = x.unfold(2, self.patch_len, self.stride)  # (batch, n_features, n_patches, patch_len)\n                    n_patches = x.shape[2]\n                    x = x.reshape(batch * n_features, n_patches, self.patch_len)\n                    x = self.linear(x)  # (batch*n_features, n_patches, d_model)\n                    return x, n_features",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "Patching"
  },
  {
    "name": "forward",
    "category": "quantitative",
    "formula": "x",
    "explanation": "",
    "python_code": "def forward(self, x):\n                    batch = x.shape[0]\n                    # Patch embedding\n                    x, n_feat = self.patching(x)  # (batch*n_features, n_patches, d_model)\n\n                    # Add positional encoding\n                    x = x + self.pos_embed\n\n                    # Transformer encoding\n                    x = self.encoder(x)  # (batch*n_features, n_patches, d_model)\n\n                    # Flatten and project\n                    x = x.reshape(batch * n_feat, -1)  # (batch*n_features, n_patches*d_model)\n                    x = self.head(x)  # (batch*n_features, pred_len)\n\n                    # Reshape back\n                    x = x.reshape(batch, n_feat, self.pred_len).permute(0, 2, 1)\n\n                    return x",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "PatchTSTModel"
  },
  {
    "name": "forward",
    "category": "quantitative",
    "formula": "trend.permute(0, 2, 1), seasonal.permute(0, 2, 1)",
    "explanation": "",
    "python_code": "def forward(self, x):\n                    # x: (batch, seq_len, n_features)\n                    x = x.permute(0, 2, 1)  # (batch, n_features, seq_len)\n                    trend = self.avg_pool(x)  # Downsampled\n                    # Upsample back for residual\n                    trend_up = torch.repeat_interleave(trend, self.kernel_size, dim=2)[:, :, :x.shape[2]]\n                    seasonal = x - trend_up\n                    return trend.permute(0, 2, 1), seasonal.permute(0, 2, 1)",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MultiScaleDecomp"
  },
  {
    "name": "forward",
    "category": "quantitative",
    "formula": "",
    "explanation": "",
    "python_code": "def forward(self, x):\n                    # Temporal mixing\n                    x_t = self.temporal_linear(x.permute(0, 2, 1)).permute(0, 2, 1)\n                    # Channel mixing\n                    x_c = self.channel_linear(x_t)\n                    x_c = torch.relu(x_c)\n                    x_c = self.dropout(x_c)\n                    x_c = self.output_linear(x_c)\n                    return self.norm(x + x_c)",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MixingBlock"
  },
  {
    "name": "forward",
    "category": "quantitative",
    "formula": "out.reshape(batch, self.pred_len, self.n_features)",
    "explanation": "",
    "python_code": "def forward(self, x):\n                    batch = x.shape[0]\n                    multi_scale_features = []\n\n                    current = x\n                    for decomp, mixing in zip(self.decomp_layers, self.mixing_blocks):\n                        trend, seasonal = decomp(current)\n                        mixed = mixing(trend)\n                        multi_scale_features.append(mixed)\n                        current = seasonal  # Continue decomposition on seasonal\n\n                    # Concatenate multi-scale features\n                    concat = torch.cat([f.reshape(batch, -1) for f in multi_scale_features], dim=1)\n\n                    # Project to prediction\n                    out = self.projection(concat)\n                    return out.reshape(batch, self.pred_len, self.n_features)",
    "source_file": "core\\_experimental\\gold_standard_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TimeMixerModel"
  },
  {
    "name": "__init__",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Initialize predictor.\n\nArgs:\n    min_confidence: Minimum confidence to trade\n    min_agreement: Minimum model agreement (e.g., 0.7 = 70% must agree)\n    use_meta_labeling: Use meta-labeling for trade filtering",
    "python_code": "def __init__(self,\n                 min_confidence: float = 0.6,\n                 min_agreement: float = 0.7,\n                 use_meta_labeling: bool = True):\n        \"\"\"\n        Initialize predictor.\n\n        Args:\n            min_confidence: Minimum confidence to trade\n            min_agreement: Minimum model agreement (e.g., 0.7 = 70% must agree)\n            use_meta_labeling: Use meta-labeling for trade filtering\n        \"\"\"\n        self.min_confidence = min_confidence\n        self.min_agreement = min_agreement\n        self.use_meta_labeling = use_meta_labeling\n\n        # Model containers\n        self.models: Dict[str, Dict[str, Any]] = {}  # pair -> models\n        self.loaded = False\n\n        # Components\n        self.alpha101 = Alpha101Forex() if 'Alpha101Forex' in dir() else None\n        self.kelly = KellyCriterion() if 'KellyCriterion' in dir() else None\n        self.meta_labeler = MetaLabeler() if 'MetaLabeler' in dir() else None\n        self.renaissance = RenaissanceSignalGenerator() if 'RenaissanceSignalGenerator' in dir() else None\n\n        # Model weights (tuned via backtesting)\n        self.model_weights = {\n            'hmm': 0.10,           # Regime detection (not directional)\n            'kalman': 0.10,        # Mean reversion signal\n            'xgboost': 0.15,       # Strong baseline\n            'lightgbm': 0.15,      # Fast gradient boosting\n            'catboost': 0.15,      # Handles categoricals\n            'itransformer': 0.15,  # SOTA time series\n            'rl_ppo': 0.10,        # RL signal\n            'renaissance': 0.10,   # 50+ weak signals\n        }",
    "source_file": "core\\_experimental\\gold_standard_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "GoldStandardPredictor"
  },
  {
    "name": "load_models",
    "category": "machine_learning",
    "formula": "False",
    "explanation": "Load trained models from directory.\n\nArgs:\n    models_dir: Path to models/gold_standard/\n\nReturns:\n    True if loaded successfully",
    "python_code": "def load_models(self, models_dir: Path) -> bool:\n        \"\"\"\n        Load trained models from directory.\n\n        Args:\n            models_dir: Path to models/gold_standard/\n\n        Returns:\n            True if loaded successfully\n        \"\"\"\n        models_dir = Path(models_dir)\n\n        if not models_dir.exists():\n            logger.error(f\"Models directory not found: {models_dir}\")\n            return False\n\n        # Load pair-specific models\n        for model_file in models_dir.glob(\"*_gold_standard.pkl\"):\n            pair = model_file.stem.replace(\"_gold_standard\", \"\")\n\n            try:\n                with open(model_file, 'rb') as f:\n                    self.models[pair] = pickle.load(f)\n                logger.info(f\"Loaded models for {pair}\")\n            except Exception as e:\n                logger.error(f\"Failed to load {model_file}: {e}\")\n\n        # Load master ensemble if exists\n        master_file = models_dir / \"all_pairs_gold_standard.pkl\"\n        if master_file.exists():\n            try:\n                with open(master_file, 'rb') as f:\n                    self.master_models = pickle.load(f)\n                logger.info(\"Loaded master ensemble\")\n            except Exception as e:\n                logger.warning(f\"Failed to load master ensemble: {e}\")\n\n        self.loaded = len(self.models) > 0\n        logger.info(f\"Loaded models for {len(self.models)} pairs\")\n\n        return self.loaded",
    "source_file": "core\\_experimental\\gold_standard_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "GoldStandardPredictor"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "PredictionResult(",
    "explanation": "Generate prediction using all models.\n\nArgs:\n    pair: Currency pair (e.g., \"EURUSD\")\n    data: DataFrame with OHLCV data\n\nReturns:\n    PredictionResult with signal, confidence, sizing",
    "python_code": "def predict(self, pair: str, data: pd.DataFrame) -> PredictionResult:\n        \"\"\"\n        Generate prediction using all models.\n\n        Args:\n            pair: Currency pair (e.g., \"EURUSD\")\n            data: DataFrame with OHLCV data\n\n        Returns:\n            PredictionResult with signal, confidence, sizing\n        \"\"\"\n        if not self.loaded:\n            logger.warning(\"Models not loaded, using fallback\")\n            return self._fallback_prediction(pair, data)\n\n        if pair not in self.models:\n            logger.warning(f\"No models for {pair}, using fallback\")\n            return self._fallback_prediction(pair, data)\n\n        pair_models = self.models[pair]\n        votes = {}\n        confidences = {}\n\n        # 1. Get regime from HMM\n        regime = self._get_regime(pair_models.get('hmm'), data)\n\n        # 2. Get Kalman mean reversion signal\n        kalman_signal, kalman_conf = self._get_kalman_signal(pair_models.get('kalman'), data)\n        votes['kalman'] = kalman_signal\n        confidences['kalman'] = kalman_conf\n\n        # 3. Get gradient boosting predictions\n        for model_name in ['xgboost', 'lightgbm', 'catboost']:\n            if 'boosting' in pair_models and model_name in pair_models['boosting'].get('models', {}):\n                signal, conf = self._get_boosting_signal(\n                    pair_models['boosting']['models'][model_name], data\n                )\n                votes[model_name] = signal\n                confidences[model_name] = conf\n\n        # 4. Get transformer prediction\n        if 'transformers' in pair_models:\n            trans_signal, trans_conf = self._get_transformer_signal(pair_models['transformers'], data)\n            votes['itransformer'] = trans_signal\n            confidences['itransformer'] = trans_conf\n\n        # 5. Get RL action\n        for algo in ['ppo', 'sac']:\n            key = f'rl_{algo}'\n            if key in pair_models:\n                rl_signal = self._get_rl_signal(pair_models[key], data)\n                votes[key] = rl_signal\n                confidences[key] = 0.5  # RL doesn't give confidence\n\n        # 6. Get Renaissance weak signals\n        if self.renaissance:\n            ren_signal, ren_conf = self._get_renaissance_signal(data)\n            votes['renaissance'] = ren_signal\n            confidences['renaissance'] = ren_conf\n\n        # 7. Combine votes with weights\n        final_signal, final_confidence = self._combine_votes(votes, confidences)\n\n        # 8. Apply meta-labeling filter\n        meta_confidence = 1.0\n        if self.use_meta_labeling and self.meta_labeler:\n            # Meta model predicts if primary signal is correct\n            meta_confidence = self._get_meta_confidence(pair_models, data, final_signal)\n            final_confidence *= meta_confidence\n\n        # 9. Calculate position size with Kelly\n        position_size = self._calculate_position_size(final_confidence, regime)\n\n        # 10. Calculate entry/TP/SL levels\n        current_price = data['close'].iloc[-",
    "source_file": "core\\_experimental\\gold_standard_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "GoldStandardPredictor"
  },
  {
    "name": "_get_regime",
    "category": "machine_learning",
    "formula": "'normal' | 'normal' | 'low_vol'",
    "explanation": "Get current regime from HMM.",
    "python_code": "def _get_regime(self, hmm_data: Optional[Dict], data: pd.DataFrame) -> str:\n        \"\"\"Get current regime from HMM.\"\"\"\n        if hmm_data is None or 'model' not in hmm_data:\n            return 'normal'\n\n        try:\n            returns = data['close'].pct_change().dropna().values[-100:]\n            if len(returns) < 10:\n                return 'normal'\n\n            hmm = hmm_data['model']\n            states = hmm.predict(returns.reshape(-1, 1))\n            current_state = states[-1]\n\n            # Map states to regimes (based on volatility)\n            state_vols = []\n            for s in range(hmm.n_components):\n                mask = states == s\n                if mask.sum() > 0:\n                    state_vols.append((s, np.std(returns[mask])))\n\n            state_vols.sort(key=lambda x: x[1])\n\n            if current_state == state_vols[0][0]:\n                return 'low_vol'\n            elif current_state == state_vols[-1][0]:\n                return 'high_vol'\n            else:\n                return 'normal'\n\n        except Exception as e:\n            logger.debug(f\"HMM regime error: {e}\")\n            return 'normal'",
    "source_file": "core\\_experimental\\gold_standard_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "GoldStandardPredictor"
  },
  {
    "name": "_get_kalman_signal",
    "category": "technical",
    "formula": "0, 0.5 | = returns[-1] | - filtered_mean",
    "explanation": "Get mean reversion signal from Kalman filter.",
    "python_code": "def _get_kalman_signal(self, kalman_data: Optional[Dict], data: pd.DataFrame) -> Tuple[int, float]:\n        \"\"\"Get mean reversion signal from Kalman filter.\"\"\"\n        if kalman_data is None or 'model' not in kalman_data:\n            return 0, 0.5\n\n        try:\n            kf = kalman_data['model']\n            returns = data['close'].pct_change().dropna().values\n\n            filtered_means, _ = kf.filter(returns)\n            current_return = returns[-1]\n            filtered_mean = filtered_means[-1][0] if len(filtered_means[-1].shape) > 0 else filtered_means[-1]\n\n            # Mean reversion: if above mean, expect down; if below, expect up\n            deviation = current_return - filtered_mean\n            threshold = np.std(returns) * 0.5\n\n            if deviation > threshold:\n                return -1, min(abs(deviation) / threshold, 1.0)  # Expect reversion down\n            elif deviation < -threshold:\n                return 1, min(abs(deviation) / threshold, 1.0)  # Expect reversion up\n            else:\n                return 0, 0.3\n\n        except Exception as e:\n            logger.debug(f\"Kalman error: {e}\")\n            return 0, 0.5",
    "source_file": "core\\_experimental\\gold_standard_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "GoldStandardPredictor"
  },
  {
    "name": "_get_boosting_signal",
    "category": "machine_learning",
    "formula": "0, 0.5 | signal, confidence | 0, 0.5",
    "explanation": "Get signal from gradient boosting model.",
    "python_code": "def _get_boosting_signal(self, model: Any, data: pd.DataFrame) -> Tuple[int, float]:\n        \"\"\"Get signal from gradient boosting model.\"\"\"\n        try:\n            X = self._prepare_features(data)\n            if X is None:\n                return 0, 0.5\n\n            proba = model.predict_proba(X[-1:])\n            pred = model.predict(X[-1:])[0]\n\n            confidence = max(proba[0])\n            signal = 1 if pred == 1 else -1\n\n            return signal, confidence\n\n        except Exception as e:\n            logger.debug(f\"Boosting error: {e}\")\n            return 0, 0.5",
    "source_file": "core\\_experimental\\gold_standard_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "GoldStandardPredictor"
  },
  {
    "name": "_get_transformer_signal",
    "category": "machine_learning",
    "formula": "0, 0.5",
    "explanation": "Get signal from transformer model.",
    "python_code": "def _get_transformer_signal(self, trans_data: Dict, data: pd.DataFrame) -> Tuple[int, float]:\n        \"\"\"Get signal from transformer model.\"\"\"\n        # Placeholder - actual implementation depends on model\n        return 0, 0.5",
    "source_file": "core\\_experimental\\gold_standard_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "GoldStandardPredictor"
  },
  {
    "name": "_get_rl_signal",
    "category": "machine_learning",
    "formula": "0 | 1 | -1",
    "explanation": "Get action from RL agent.",
    "python_code": "def _get_rl_signal(self, rl_data: Dict, data: pd.DataFrame) -> int:\n        \"\"\"Get action from RL agent.\"\"\"\n        try:\n            model = rl_data.get('model')\n            if model is None:\n                return 0\n\n            obs = self._prepare_rl_observation(data)\n            action, _ = model.predict(obs)\n\n            # Map action: 0=hold, 1=buy, 2=sell\n            if action == 1:\n                return 1\n            elif action == 2:\n                return -1\n            else:\n                return 0\n\n        except Exception as e:\n            logger.debug(f\"RL error: {e}\")\n            return 0",
    "source_file": "core\\_experimental\\gold_standard_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "GoldStandardPredictor"
  },
  {
    "name": "_get_renaissance_signal",
    "category": "machine_learning",
    "formula": "0, 0.5 | signal, confidence | 0, 0.5",
    "explanation": "Get ensemble signal from Renaissance weak signals.",
    "python_code": "def _get_renaissance_signal(self, data: pd.DataFrame) -> Tuple[int, float]:\n        \"\"\"Get ensemble signal from Renaissance weak signals.\"\"\"\n        try:\n            if self.renaissance is None:\n                return 0, 0.5\n\n            signals_df = self.renaissance.generate_all_signals(data)\n            signals_df = self.renaissance.ensemble_signals(signals_df)\n\n            if 'ensemble_signal' in signals_df.columns:\n                signal = int(signals_df['ensemble_signal'].iloc[-1])\n                confidence = signals_df['ensemble_confidence'].iloc[-1]\n                return signal, confidence\n\n            return 0, 0.5\n\n        except Exception as e:\n            logger.debug(f\"Renaissance error: {e}\")\n            return 0, 0.5",
    "source_file": "core\\_experimental\\gold_standard_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "GoldStandardPredictor"
  },
  {
    "name": "_combine_votes",
    "category": "machine_learning",
    "formula": "0, 0.0 | 0, 0.0 | signal, final_confidence",
    "explanation": "Combine model votes with weights.",
    "python_code": "def _combine_votes(self, votes: Dict[str, int], confidences: Dict[str, float]) -> Tuple[int, float]:\n        \"\"\"Combine model votes with weights.\"\"\"\n        if not votes:\n            return 0, 0.0\n\n        weighted_sum = 0.0\n        total_weight = 0.0\n\n        for model, vote in votes.items():\n            weight = self.model_weights.get(model, 0.1)\n            conf = confidences.get(model, 0.5)\n\n            weighted_sum += vote * weight * conf\n            total_weight += weight\n\n        if total_weight == 0:\n            return 0, 0.0\n\n        # Normalize\n        avg_vote = weighted_sum / total_weight\n\n        # Determine signal\n        if avg_vote > 0.2:\n            signal = 1\n        elif avg_vote < -0.2:\n            signal = -1\n        else:\n            signal = 0\n\n        # Calculate agreement (how many agree with final signal)\n        if signal != 0:\n            agreeing = sum(1 for v in votes.values() if v == signal)\n            agreement = agreeing / len(votes)\n        else:\n            agreement = 0.5\n\n        # Confidence is combination of agreement and average confidence\n        avg_confidence = np.mean(list(confidences.values())) if confidences else 0.5\n        final_confidence = agreement * avg_confidence\n\n        return signal, final_confidence",
    "source_file": "core\\_experimental\\gold_standard_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "GoldStandardPredictor"
  },
  {
    "name": "_get_meta_confidence",
    "category": "machine_learning",
    "formula": "0.8",
    "explanation": "Get meta-labeling confidence.",
    "python_code": "def _get_meta_confidence(self, pair_models: Dict, data: pd.DataFrame, primary_signal: int) -> float:\n        \"\"\"Get meta-labeling confidence.\"\"\"\n        # Placeholder - actual implementation uses trained meta model\n        return 0.8",
    "source_file": "core\\_experimental\\gold_standard_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "GoldStandardPredictor"
  },
  {
    "name": "_calculate_position_size",
    "category": "risk",
    "formula": "confidence * 0.25  # 25% max | min(kelly_fraction, 0.5)",
    "explanation": "Calculate Kelly-sized position.",
    "python_code": "def _calculate_position_size(self, confidence: float, regime: str) -> float:\n        \"\"\"Calculate Kelly-sized position.\"\"\"\n        if self.kelly is None:\n            return confidence * 0.25  # 25% max\n\n        # Assume historical win rate based on confidence\n        # Higher confidence = higher win probability\n        win_prob = 0.5 + (confidence - 0.5) * 0.4  # Maps 0.5-1.0 conf to 0.5-0.7 win prob\n\n        # Win/loss ratio (take profit / stop loss)\n        win_loss_ratio = 1.5  # Default 1.5:1\n\n        # Calculate Kelly fraction\n        kelly_fraction = self.kelly.fractional_kelly(\n            win_prob=win_prob,\n            win_loss_ratio=win_loss_ratio,\n            fraction=0.25  # Quarter Kelly for safety\n        )\n\n        # Adjust for regime\n        if regime == 'high_vol':\n            kelly_fraction *= 0.5  # Reduce in high volatility\n        elif regime == 'low_vol':\n            kelly_fraction *= 1.2  # Increase in low volatility\n\n        return min(kelly_fraction, 0.5)",
    "source_file": "core\\_experimental\\gold_standard_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "GoldStandardPredictor"
  },
  {
    "name": "_calculate_levels",
    "category": "microstructure",
    "formula": "price, price, price | entry, take_profit, stop_loss",
    "explanation": "Calculate entry, take profit, and stop loss levels.",
    "python_code": "def _calculate_levels(self, price: float, signal: int, data: pd.DataFrame) -> Tuple[float, float, float]:\n        \"\"\"Calculate entry, take profit, and stop loss levels.\"\"\"\n        if signal == 0 or price == 0:\n            return price, price, price\n\n        # Calculate ATR for dynamic levels\n        if 'high' in data.columns and 'low' in data.columns:\n            tr = data['high'] - data['low']\n            atr = tr.rolling(14).mean().iloc[-1]\n        else:\n            returns = data['close'].pct_change().dropna()\n            atr = returns.std() * price * 2\n\n        # Set levels\n        if signal == 1:  # Buy\n            entry = price\n            take_profit = price + atr * 2\n            stop_loss = price - atr * 1.5\n        else:  # Sell\n            entry = price\n            take_profit = price - atr * 2\n            stop_loss = price + atr * 1.5\n\n        return entry, take_profit, stop_loss",
    "source_file": "core\\_experimental\\gold_standard_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "GoldStandardPredictor"
  },
  {
    "name": "_prepare_features",
    "category": "machine_learning",
    "formula": "df[feature_cols].values",
    "explanation": "Prepare features for ML models.",
    "python_code": "def _prepare_features(self, data: pd.DataFrame) -> Optional[np.ndarray]:\n        \"\"\"Prepare features for ML models.\"\"\"\n        try:\n            df = data.copy()\n\n            # Returns\n            df['returns'] = df['close'].pct_change()\n            df['volatility'] = df['returns'].rolling(20).std()\n\n            # Momentum\n            for p in [5, 10, 20]:\n                df[f'mom_{p}'] = df['close'].pct_change(p)\n\n            # RSI\n            delta = df['close'].diff()\n            gain = delta.where(delta > 0, 0).rolling(14).mean()\n            loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n            rs = gain / loss\n            df['rsi'] = 100 - (100 / (1 + rs))\n\n            df = df.dropna()\n\n            feature_cols = ['returns', 'volatility', 'mom_5', 'mom_10', 'mom_20', 'rsi']\n            return df[feature_cols].values\n\n        except Exception as e:\n            logger.debug(f\"Feature prep error: {e}\")\n            return None",
    "source_file": "core\\_experimental\\gold_standard_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "GoldStandardPredictor"
  },
  {
    "name": "_prepare_rl_observation",
    "category": "machine_learning",
    "formula": "features[-1].astype(np.float32) | np.zeros(6, dtype=np.float32)",
    "explanation": "Prepare observation for RL agent.",
    "python_code": "def _prepare_rl_observation(self, data: pd.DataFrame) -> np.ndarray:\n        \"\"\"Prepare observation for RL agent.\"\"\"\n        features = self._prepare_features(data)\n        if features is not None:\n            return features[-1].astype(np.float32)\n        return np.zeros(6, dtype=np.float32)",
    "source_file": "core\\_experimental\\gold_standard_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "GoldStandardPredictor"
  },
  {
    "name": "_fallback_prediction",
    "category": "machine_learning",
    "formula": "PredictionResult( | PredictionResult(",
    "explanation": "Fallback prediction when models not available.",
    "python_code": "def _fallback_prediction(self, pair: str, data: pd.DataFrame) -> PredictionResult:\n        \"\"\"Fallback prediction when models not available.\"\"\"\n        # Simple momentum-based fallback\n        if len(data) < 20:\n            return PredictionResult(\n                signal=0, confidence=0, regime='normal',\n                position_size=0, meta_confidence=0, model_votes={}\n            )\n\n        returns = data['close'].pct_change().dropna()\n        momentum = returns.iloc[-5:].mean()\n\n        if momentum > 0.001:\n            signal = 1\n        elif momentum < -0.001:\n            signal = -1\n        else:\n            signal = 0\n\n        confidence = min(abs(momentum) * 100, 0.6)\n\n        return PredictionResult(\n            signal=signal,\n            confidence=confidence,\n            regime='normal',\n            position_size=confidence * 0.1,\n            meta_confidence=0.5,\n            model_votes={'fallback_momentum': signal}\n        )",
    "source_file": "core\\_experimental\\gold_standard_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "GoldStandardPredictor"
  },
  {
    "name": "get_all_signals",
    "category": "machine_learning",
    "formula": "{",
    "explanation": "Get detailed signals from all models.\n\nUseful for debugging and analysis.",
    "python_code": "def get_all_signals(self, pair: str, data: pd.DataFrame) -> Dict[str, Any]:\n        \"\"\"\n        Get detailed signals from all models.\n\n        Useful for debugging and analysis.\n        \"\"\"\n        result = self.predict(pair, data)\n\n        return {\n            'prediction': result,\n            'votes': result.model_votes,\n            'regime': result.regime,\n            'confidence_breakdown': {\n                'final': result.confidence,\n                'meta': result.meta_confidence\n            },\n            'sizing': {\n                'position': result.position_size,\n                'entry': result.entry_price,\n                'tp': result.take_profit,\n                'sl': result.stop_loss\n            }\n        }",
    "source_file": "core\\_experimental\\gold_standard_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "GoldStandardPredictor"
  },
  {
    "name": "create_gold_standard_predictor",
    "category": "machine_learning",
    "formula": "predictor",
    "explanation": "Create and initialize gold standard predictor.\n\nArgs:\n    models_dir: Path to trained models (default: models/gold_standard)",
    "python_code": "def create_gold_standard_predictor(models_dir: Optional[Path] = None) -> GoldStandardPredictor:\n    \"\"\"\n    Create and initialize gold standard predictor.\n\n    Args:\n        models_dir: Path to trained models (default: models/gold_standard)\n    \"\"\"\n    predictor = GoldStandardPredictor()\n\n    if models_dir is None:\n        models_dir = Path(__file__).parent.parent / 'models' / 'gold_standard'\n\n    if models_dir.exists():\n        predictor.load_models(models_dir)\n    else:\n        logger.warning(f\"Models not found at {models_dir}, predictor will use fallback\")\n\n    return predictor",
    "source_file": "core\\_experimental\\gold_standard_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "deep_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, in_features: int, out_features: int, bias: bool = True):\n        super().__init__()\n        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n        self.bias = nn.Parameter(torch.FloatTensor(out_features)) if bias else None\n        self._reset_parameters()",
    "source_file": "core\\_experimental\\graph_neural_network.py",
    "academic_reference": null,
    "class_name": "GraphConvolution"
  },
  {
    "name": "_reset_parameters",
    "category": "deep_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def _reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight)\n        if self.bias is not None:\n            nn.init.zeros_(self.bias)",
    "source_file": "core\\_experimental\\graph_neural_network.py",
    "academic_reference": null,
    "class_name": "GraphConvolution"
  },
  {
    "name": "forward",
    "category": "technical",
    "formula": "output",
    "explanation": "Args:\n    x: [B, N, F] node features\n    adj: [N, N] adjacency matrix\n\nReturns:\n    [B, N, F'] transformed features",
    "python_code": "def forward(self, x: torch.Tensor, adj: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: [B, N, F] node features\n            adj: [N, N] adjacency matrix\n\n        Returns:\n            [B, N, F'] transformed features\n        \"\"\"\n        # Normalize adjacency\n        rowsum = adj.sum(dim=1, keepdim=True) + 1e-6\n        adj_norm = adj / rowsum\n\n        # Graph convolution: H' = A * H * W\n        support = torch.matmul(x, self.weight)\n        output = torch.matmul(adj_norm, support)\n\n        if self.bias is not None:\n            output = output + self.bias\n\n        return output",
    "source_file": "core\\_experimental\\graph_neural_network.py",
    "academic_reference": null,
    "class_name": "GraphConvolution"
  },
  {
    "name": "forward",
    "category": "deep_learning",
    "formula": "N=nodes, T=time, F=features | x",
    "explanation": "Args:\n    x: [B, N, T, F] where N=nodes, T=time, F=features\n\nReturns:\n    [B, N, T, F'] temporal features",
    "python_code": "def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: [B, N, T, F] where N=nodes, T=time, F=features\n\n        Returns:\n            [B, N, T, F'] temporal features\n        \"\"\"\n        B, N, T, F = x.shape\n\n        # Reshape for conv: [B*N, F, T]\n        x = x.reshape(B * N, T, F).transpose(1, 2)\n\n        # Convolve\n        x = self.conv(x)[:, :, :T]  # Causal: keep original length\n        x = self.norm(x)\n        x = self.activation(x)\n\n        # Reshape back: [B, N, T, F']\n        x = x.transpose(1, 2).reshape(B, N, T, -1)\n        return x",
    "source_file": "core\\_experimental\\graph_neural_network.py",
    "academic_reference": null,
    "class_name": "TemporalConvolution"
  },
  {
    "name": "forward",
    "category": "technical",
    "formula": "x",
    "explanation": "Args:\n    x: [B, N, T, F] spatiotemporal tensor\n    adj: [N, N] adjacency matrix\n\nReturns:\n    [B, N, T, F'] processed tensor",
    "python_code": "def forward(self, x: torch.Tensor, adj: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: [B, N, T, F] spatiotemporal tensor\n            adj: [N, N] adjacency matrix\n\n        Returns:\n            [B, N, T, F'] processed tensor\n        \"\"\"\n        residual = x if self.skip is None else self.skip(x)\n\n        # Temporal\n        x = self.temporal(x)\n\n        # Spatial for each time step\n        B, N, T, F = x.shape\n        spatial_out = []\n        for t in range(T):\n            h = self.spatial(x[:, :, t, :], adj)\n            spatial_out.append(h)\n        x = torch.stack(spatial_out, dim=2)\n\n        # Residual\n        x = self.norm(residual + self.dropout(x))\n        return x",
    "source_file": "core\\_experimental\\graph_neural_network.py",
    "academic_reference": null,
    "class_name": "SpatioTemporalBlock"
  },
  {
    "name": "forward",
    "category": "technical",
    "formula": "adj",
    "explanation": "Return learned adjacency matrix.",
    "python_code": "def forward(self) -> torch.Tensor:\n        \"\"\"Return learned adjacency matrix.\"\"\"\n        # Similarity between node embeddings\n        adj = torch.matmul(self.node_embeddings, self.node_embeddings.T)\n        adj = F.softmax(adj, dim=1)\n        return adj",
    "source_file": "core\\_experimental\\graph_neural_network.py",
    "academic_reference": null,
    "class_name": "AdaptiveGraphLearner"
  },
  {
    "name": "get_adjacency",
    "category": "technical",
    "formula": "",
    "explanation": "Get adjacency matrix (fixed or learned).",
    "python_code": "def get_adjacency(self) -> torch.Tensor:\n        \"\"\"Get adjacency matrix (fixed or learned).\"\"\"\n        if self.adaptive_graph is not None:\n            return self.adaptive_graph()\n        return self.adj",
    "source_file": "core\\_experimental\\graph_neural_network.py",
    "academic_reference": null,
    "class_name": "MTGNN"
  },
  {
    "name": "forward",
    "category": "machine_learning",
    "formula": "torch.stack(outputs, dim=1)",
    "explanation": "Args:\n    x: [B, N, T, F] multi-pair time series\n\nReturns:\n    [B, N, output_dim] predictions per pair",
    "python_code": "def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: [B, N, T, F] multi-pair time series\n\n        Returns:\n            [B, N, output_dim] predictions per pair\n        \"\"\"\n        adj = self.get_adjacency()\n\n        # Project input\n        x = self.input_proj(x)\n\n        # Spatiotemporal processing\n        for block in self.blocks:\n            x = block(x, adj)\n\n        # Flatten temporal dimension\n        B, N, T, F = x.shape\n        x = x.reshape(B, N, T * F)\n\n        # Output per node\n        outputs = []\n        for n in range(N):\n            out = self.output(x[:, n, :])\n            outputs.append(out)\n\n        return torch.stack(outputs, dim=1)",
    "source_file": "core\\_experimental\\graph_neural_network.py",
    "academic_reference": null,
    "class_name": "MTGNN"
  },
  {
    "name": "prepare_multi_pair_data",
    "category": "deep_learning",
    "formula": "np.stack(all_features, axis=0).transpose(1, 0, 2)",
    "explanation": "Prepare data for all pairs.\n\nArgs:\n    pair_data: Dict[pair -> DataFrame with OHLCV]\n\nReturns:\n    [T, N, F] tensor",
    "python_code": "def prepare_multi_pair_data(self, pair_data: Dict[str, pd.DataFrame]) -> np.ndarray:\n        \"\"\"\n        Prepare data for all pairs.\n\n        Args:\n            pair_data: Dict[pair -> DataFrame with OHLCV]\n\n        Returns:\n            [T, N, F] tensor\n        \"\"\"\n        all_features = []\n\n        for pair in self.PAIRS:\n            if pair in pair_data:\n                df = pair_data[pair]\n                features = self._prepare_single_pair(df)\n            else:\n                # Missing pair - use zeros\n                features = np.zeros((len(list(pair_data.values())[0]), 10))\n            all_features.append(features)\n\n        # Stack: [N, T, F] -> transpose to [T, N, F]\n        return np.stack(all_features, axis=0).transpose(1, 0, 2)",
    "source_file": "core\\_experimental\\graph_neural_network.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GNNForex"
  },
  {
    "name": "_prepare_single_pair",
    "category": "deep_learning",
    "formula": "features",
    "explanation": "Prepare features for single pair.",
    "python_code": "def _prepare_single_pair(self, df: pd.DataFrame) -> np.ndarray:\n        \"\"\"Prepare features for single pair.\"\"\"\n        mid = df['mid'].values if 'mid' in df.columns else (df['bid'] + df['ask']).values / 2\n\n        features = np.column_stack([\n            pd.Series(mid).pct_change().fillna(0).values * 10000,\n            pd.Series(mid).pct_change(5).fillna(0).values * 10000,\n            pd.Series(mid).pct_change(20).fillna(0).values * 10000,\n            pd.Series(mid).pct_change().rolling(20).std().fillna(0.001).values * 10000,\n            ((mid - pd.Series(mid).rolling(20).mean()) / (pd.Series(mid).rolling(20).std() + 1e-10)).fillna(0).values,\n            pd.Series(mid).pct_change().rolling(10).mean().fillna(0).values * 10000,\n            (df['ask'] - df['bid']).values / mid * 10000 if 'bid' in df.columns else np.zeros(len(df)),\n            df['volume'].values / df['volume'].rolling(20).mean().fillna(1).values if 'volume' in df.columns else np.ones(len(df)),\n            np.zeros(len(df)),  # Placeholder\n            np.zeros(len(df))   # Placeholder\n        ])\n\n        return features",
    "source_file": "core\\_experimental\\graph_neural_network.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GNNForex"
  },
  {
    "name": "fit",
    "category": "machine_learning",
    "formula": "data = self.prepare_multi_pair_data(pair_data)  # [T, N, F] | for each pair",
    "explanation": "Train GNN on multi-pair data.",
    "python_code": "def fit(self, pair_data: Dict[str, pd.DataFrame],\n           epochs: int = 50, lr: float = 0.001, batch_size: int = 32):\n        \"\"\"Train GNN on multi-pair data.\"\"\"\n        if not HAS_TORCH:\n            logger.warning(\"PyTorch not available\")\n            return\n\n        data = self.prepare_multi_pair_data(pair_data)  # [T, N, F]\n        T, N, F = data.shape\n\n        # Create targets: next period return for each pair\n        targets = []\n        for pair in self.PAIRS:\n            if pair in pair_data:\n                mid = pair_data[pair]['mid'].values if 'mid' in pair_data[pair].columns else (pair_data[pair]['bid'] + pair_data[pair]['ask']).values / 2\n                ret = pd.Series(mid).pct_change().shift(-1).fillna(0).values * 10000\n            else:\n                ret = np.zeros(T)\n            targets.append(ret)\n        targets = np.stack(targets, axis=1)  # [T, N]\n\n        # Create sequences\n        X, y = [], []\n        for t in range(self.seq_len, T - 1):\n            X.append(data[t-self.seq_len:t])  # [seq_len, N, F]\n            y.append((targets[t] > 0).astype(float))  # Direction\n\n        X = torch.FloatTensor(np.array(X)).transpose(1, 2)  # [B, N, T, F]\n        y = torch.FloatTensor(np.array(y))  # [B, N]\n\n        # Train\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n        criterion = nn.BCEWithLogitsLoss()\n\n        n_batches = len(X) // batch_size\n\n        self.model.train()\n        for epoch in range(epochs):\n            total_loss = 0\n            for b in range(n_batches):\n                start = b * batch_size\n                end = start + batch_size\n\n                optimizer.zero_grad()\n                pred = self.model(X[start:end])\n                loss = criterion(pred.squeeze(-1), y[start:end])\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n\n            if epoch % 10 == 0:\n                logger.info(f\"Epoch {epoch}, Loss: {total_loss/n_batches:.4f}\")\n\n        self.is_fitted = True\n        logger.info(\"GNN model trained\")",
    "source_file": "core\\_experimental\\graph_neural_network.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GNNForex"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "{pair: 0.5 for pair in self.PAIRS} | {pair: float(probs[i]) for i, pair in enumerate(self.PAIRS)}",
    "explanation": "Predict direction for all pairs.\n\nReturns:\n    Dict[pair -> probability of up move]",
    "python_code": "def predict(self, pair_data: Dict[str, pd.DataFrame]) -> Dict[str, float]:\n        \"\"\"\n        Predict direction for all pairs.\n\n        Returns:\n            Dict[pair -> probability of up move]\n        \"\"\"\n        if not HAS_TORCH or not self.is_fitted:\n            return {pair: 0.5 for pair in self.PAIRS}\n\n        data = self.prepare_multi_pair_data(pair_data)[-self.seq_len:]\n\n        self.model.eval()\n        with torch.no_grad():\n            X = torch.FloatTensor(data).unsqueeze(0).transpose(1, 2)  # [1, N, T, F]\n            logits = self.model(X)\n            probs = torch.sigmoid(logits).squeeze().numpy()\n\n        return {pair: float(probs[i]) for i, pair in enumerate(self.PAIRS)}",
    "source_file": "core\\_experimental\\graph_neural_network.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GNNForex"
  },
  {
    "name": "get_cross_pair_signals",
    "category": "technical",
    "formula": "signals",
    "explanation": "Get cross-pair arbitrage signals.\n\nIdentifies divergent pairs and mean-reversion opportunities.",
    "python_code": "def get_cross_pair_signals(self, pair_data: Dict[str, pd.DataFrame]) -> Dict[str, Dict]:\n        \"\"\"\n        Get cross-pair arbitrage signals.\n\n        Identifies divergent pairs and mean-reversion opportunities.\n        \"\"\"\n        probs = self.predict(pair_data)\n\n        signals = {}\n        for pair in self.PAIRS:\n            # Find correlated pairs\n            corr_pairs = [(p, self.DEFAULT_ADJ[self.PAIRS.index(pair), self.PAIRS.index(p)])\n                         for p in self.PAIRS if p != pair and p in probs]\n            corr_pairs.sort(key=lambda x: x[1], reverse=True)\n\n            # Check for divergence with top correlated pair\n            if corr_pairs:\n                top_corr_pair, corr = corr_pairs[0]\n                divergence = probs[pair] - probs[top_corr_pair]\n\n                signals[pair] = {\n                    'direction_prob': probs[pair],\n                    'top_correlated': top_corr_pair,\n                    'correlation': corr,\n                    'divergence': divergence,\n                    'arbitrage_signal': -divergence if abs(divergence) > 0.2 else 0  # Mean reversion\n                }\n\n        return signals",
    "source_file": "core\\_experimental\\graph_neural_network.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GNNForex"
  },
  {
    "name": "__init__",
    "category": "volatility",
    "formula": "",
    "explanation": "Initialize HAR-RV model.\n\nArgs:\n    daily_window: Window for daily RV (default 1)\n    weekly_window: Window for weekly RV (default 5)\n    monthly_window: Window for monthly RV (default 22)\n    jump_threshold: Z-score threshold for jump detection (default 3.0)\n    annualization_factor: Trading days per year for annualization",
    "python_code": "def __init__(\n        self,\n        daily_window: int = 1,\n        weekly_window: int = 5,\n        monthly_window: int = 22,\n        jump_threshold: float = 3.0,\n        annualization_factor: float = 252\n    ):\n        \"\"\"\n        Initialize HAR-RV model.\n\n        Args:\n            daily_window: Window for daily RV (default 1)\n            weekly_window: Window for weekly RV (default 5)\n            monthly_window: Window for monthly RV (default 22)\n            jump_threshold: Z-score threshold for jump detection (default 3.0)\n            annualization_factor: Trading days per year for annualization\n        \"\"\"\n        self.daily_window = daily_window\n        self.weekly_window = weekly_window\n        self.monthly_window = monthly_window\n        self.jump_threshold = jump_threshold\n        self.annualization_factor = annualization_factor\n\n        # Model coefficients\n        self.beta0 = None  # Intercept\n        self.beta_d = None  # Daily coefficient\n        self.beta_w = None  # Weekly coefficient\n        self.beta_m = None  # Monthly coefficient\n\n        # Jump model coefficients (for HAR-RV-J)\n        self.beta_j = None  # Jump coefficient\n        self.beta_c = None  # Continuous component coefficient\n\n        # Model statistics\n        self.r_squared = None\n        self.adjusted_r_squared = None\n        self.residual_std = None",
    "source_file": "core\\_experimental\\har_rv_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HARRVVolatility"
  },
  {
    "name": "compute_realized_volatility",
    "category": "volatility",
    "formula": "RV = sqrt(sum(r_i^2)) | RV = sqrt(sum(r_i^2)) | RV^2 (variance), else return RV (std)",
    "explanation": "Compute realized volatility from returns.\n\nFormula: RV = sqrt(sum(r_i^2))\n\nArgs:\n    returns: Return series\n    window: Rolling window for aggregation\n    squared: If True, return RV^2 (variance), else return RV (std)\n\nReturns:\n    Realized volatility series",
    "python_code": "def compute_realized_volatility(\n        self,\n        returns: pd.Series,\n        window: int = 1,\n        squared: bool = True\n    ) -> pd.Series:\n        \"\"\"\n        Compute realized volatility from returns.\n\n        Formula: RV = sqrt(sum(r_i^2))\n\n        Args:\n            returns: Return series\n            window: Rolling window for aggregation\n            squared: If True, return RV^2 (variance), else return RV (std)\n\n        Returns:\n            Realized volatility series\n        \"\"\"\n        squared_returns = returns ** 2\n\n        if window == 1:\n            rv = squared_returns\n        else:\n            rv = squared_returns.rolling(window, min_periods=1).sum()\n\n        if squared:\n            return rv\n        else:\n            return np.sqrt(rv)",
    "source_file": "core\\_experimental\\har_rv_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HARRVVolatility"
  },
  {
    "name": "compute_bipower_variation",
    "category": "volatility",
    "formula": "BPV = (/2) * sum(|r_i| * |r_{i-1}|) | BPV = (/2) * sum(|r_i| * |r_{i-1}|) | bpv.rolling(self.daily_window, min_periods=1).sum()",
    "explanation": "Compute Bipower Variation (BPV) - robust to jumps.\n\nFormula: BPV = (/2) * sum(|r_i| * |r_{i-1}|)\n\nUsed to separate continuous volatility from jumps.",
    "python_code": "def compute_bipower_variation(self, returns: pd.Series) -> pd.Series:\n        \"\"\"\n        Compute Bipower Variation (BPV) - robust to jumps.\n\n        Formula: BPV = (/2) * sum(|r_i| * |r_{i-1}|)\n\n        Used to separate continuous volatility from jumps.\n        \"\"\"\n        mu1 = np.sqrt(2 / np.pi)\n        abs_returns = returns.abs()\n        bpv = (np.pi / 2) * (abs_returns * abs_returns.shift(1))\n        return bpv.rolling(self.daily_window, min_periods=1).sum()",
    "source_file": "core\\_experimental\\har_rv_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HARRVVolatility"
  },
  {
    "name": "compute_jump_component",
    "category": "volatility",
    "formula": "jump",
    "explanation": "Compute jump component: RV - BPV (truncated at 0).\n\nJumps are the difference between realized variance and bipower variation.",
    "python_code": "def compute_jump_component(self, returns: pd.Series) -> pd.Series:\n        \"\"\"\n        Compute jump component: RV - BPV (truncated at 0).\n\n        Jumps are the difference between realized variance and bipower variation.\n        \"\"\"\n        rv = self.compute_realized_volatility(returns, self.daily_window, squared=True)\n        bpv = self.compute_bipower_variation(returns)\n        jump = np.maximum(0, rv - bpv)\n        return jump",
    "source_file": "core\\_experimental\\har_rv_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HARRVVolatility"
  },
  {
    "name": "compute_signed_jumps",
    "category": "volatility",
    "formula": "sign on jump days | pd.Series(pos_jump, index=returns.index), pd.Series(neg_jump, index=returns.index)",
    "explanation": "Compute signed jump components (positive and negative).\n\nReturns:\n    Tuple of (positive_jumps, negative_jumps)",
    "python_code": "def compute_signed_jumps(self, returns: pd.Series) -> Tuple[pd.Series, pd.Series]:\n        \"\"\"\n        Compute signed jump components (positive and negative).\n\n        Returns:\n            Tuple of (positive_jumps, negative_jumps)\n        \"\"\"\n        jump = self.compute_jump_component(returns)\n\n        # Detect jump direction based on return sign on jump days\n        rv = self.compute_realized_volatility(returns, self.daily_window, squared=True)\n        bpv = self.compute_bipower_variation(returns)\n\n        # Jump significance test\n        is_jump = (rv - bpv) / (bpv + 1e-10) > self.jump_threshold\n\n        # Positive jumps: up moves with significant jump\n        pos_jump = np.where((returns > 0) & is_jump, jump, 0)\n        # Negative jumps: down moves with significant jump\n        neg_jump = np.where((returns < 0) & is_jump, jump, 0)\n\n        return pd.Series(pos_jump, index=returns.index), pd.Series(neg_jump, index=returns.index)",
    "source_file": "core\\_experimental\\har_rv_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HARRVVolatility"
  },
  {
    "name": "compute_continuous_volatility",
    "category": "volatility",
    "formula": "np.minimum(rv, bpv)",
    "explanation": "Compute continuous component of volatility (BPV capped at RV).",
    "python_code": "def compute_continuous_volatility(self, returns: pd.Series) -> pd.Series:\n        \"\"\"\n        Compute continuous component of volatility (BPV capped at RV).\n        \"\"\"\n        rv = self.compute_realized_volatility(returns, self.daily_window, squared=True)\n        bpv = self.compute_bipower_variation(returns)\n        return np.minimum(rv, bpv)",
    "source_file": "core\\_experimental\\har_rv_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HARRVVolatility"
  },
  {
    "name": "compute_har_components",
    "category": "volatility",
    "formula": "pd.DataFrame({",
    "explanation": "Compute daily, weekly, monthly RV components for HAR model.\n\nArgs:\n    rv: Realized volatility series (squared returns)\n\nReturns:\n    DataFrame with rv_daily, rv_weekly, rv_monthly columns",
    "python_code": "def compute_har_components(self, rv: pd.Series) -> pd.DataFrame:\n        \"\"\"\n        Compute daily, weekly, monthly RV components for HAR model.\n\n        Args:\n            rv: Realized volatility series (squared returns)\n\n        Returns:\n            DataFrame with rv_daily, rv_weekly, rv_monthly columns\n        \"\"\"\n        return pd.DataFrame({\n            'rv_daily': rv,\n            'rv_weekly': rv.rolling(self.weekly_window, min_periods=1).mean(),\n            'rv_monthly': rv.rolling(self.monthly_window, min_periods=1).mean()\n        }, index=rv.index)",
    "source_file": "core\\_experimental\\har_rv_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HARRVVolatility"
  },
  {
    "name": "fit",
    "category": "volatility",
    "formula": "",
    "explanation": "Fit HAR-RV model using OLS.\n\nArgs:\n    returns: Return series\n    method: 'basic' (HAR-RV), 'jump' (HAR-RV-J), 'cj' (HAR-RV-CJ), 'sj' (HAR-RV-SJ)\n\nReturns:\n    Self for method chaining",
    "python_code": "def fit(self, returns: pd.Series, method: str = 'basic') -> 'HARRVVolatility':\n        \"\"\"\n        Fit HAR-RV model using OLS.\n\n        Args:\n            returns: Return series\n            method: 'basic' (HAR-RV), 'jump' (HAR-RV-J), 'cj' (HAR-RV-CJ), 'sj' (HAR-RV-SJ)\n\n        Returns:\n            Self for method chaining\n        \"\"\"\n        # Compute realized volatility\n        rv = self.compute_realized_volatility(returns, self.daily_window, squared=True)\n\n        # Get HAR components\n        components = self.compute_har_components(rv)\n\n        # Target: next-period RV\n        y = rv.shift(-1)\n\n        if method == 'basic':\n            # HAR-RV: RV_t+1 ~ RV_d + RV_w + RV_m\n            X = components.copy()\n            X['intercept'] = 1\n            X = X[['intercept', 'rv_daily', 'rv_weekly', 'rv_monthly']]\n\n        elif method == 'jump':\n            # HAR-RV-J: Add jump component\n            jump = self.compute_jump_component(returns)\n            X = components.copy()\n            X['jump'] = jump\n            X['intercept'] = 1\n            X = X[['intercept', 'rv_daily', 'rv_weekly', 'rv_monthly', 'jump']]\n\n        elif method == 'cj':\n            # HAR-RV-CJ: Continuous + Jump decomposition\n            continuous = self.compute_continuous_volatility(returns)\n            jump = self.compute_jump_component(returns)\n            X = pd.DataFrame({\n                'intercept': 1,\n                'continuous': continuous,\n                'cont_weekly': continuous.rolling(self.weekly_window).mean(),\n                'cont_monthly': continuous.rolling(self.monthly_window).mean(),\n                'jump': jump,\n                'jump_weekly': jump.rolling(self.weekly_window).mean(),\n                'jump_monthly': jump.rolling(self.monthly_window).mean()\n            }, index=returns.index)\n\n        elif method == 'sj':\n            # HAR-RV-SJ: Signed jumps\n            pos_jump, neg_jump = self.compute_signed_jumps(returns)\n            continuous = self.compute_continuous_volatility(returns)\n            X = pd.DataFrame({\n                'intercept': 1,\n                'continuous': continuous,\n                'cont_weekly': continuous.rolling(self.weekly_window).mean(),\n                'cont_monthly': continuous.rolling(self.monthly_window).mean(),\n                'pos_jump': pos_jump,\n                'neg_jump': neg_jump\n            }, index=returns.index)\n\n        # Drop NaN values\n        valid_mask = ~(X.isnull().any(axis=1) | y.isnull())\n        X_clean = X[valid_mask]\n        y_clean = y[valid_mask]\n\n        if len(X_clean) < len(X.columns) + 1:\n            # Not enough data points\n            self.beta0 = 0\n            self.beta_d = 0.3\n            self.beta_w = 0.3\n            self.beta_m = 0.3\n            return self\n\n        # OLS estimation:  = (X'X)^(-1) X'y\n        try:\n            XtX = X_clean.T @ X_clean\n            Xty = X_clean.T @ y_clean\n            betas = np.linalg.solve(XtX, Xty)\n\n            # Store coefficients\n            if me",
    "source_file": "core\\_experimental\\har_rv_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HARRVVolatility"
  },
  {
    "name": "predict",
    "category": "volatility",
    "formula": "pred",
    "explanation": "Predict next-period volatility.\n\nArgs:\n    returns: Return series\n    steps: Number of steps ahead (1 for next period)\n\nReturns:\n    Predicted volatility series",
    "python_code": "def predict(self, returns: pd.Series, steps: int = 1) -> pd.Series:\n        \"\"\"\n        Predict next-period volatility.\n\n        Args:\n            returns: Return series\n            steps: Number of steps ahead (1 for next period)\n\n        Returns:\n            Predicted volatility series\n        \"\"\"\n        rv = self.compute_realized_volatility(returns, self.daily_window, squared=True)\n        components = self.compute_har_components(rv)\n\n        # HAR prediction\n        pred = (\n            self.beta0 +\n            self.beta_d * components['rv_daily'] +\n            self.beta_w * components['rv_weekly'] +\n            self.beta_m * components['rv_monthly']\n        )\n\n        # Multi-step ahead prediction (simple iteration)\n        if steps > 1:\n            for _ in range(steps - 1):\n                # Update components with prediction\n                rv_new = pred\n                weekly_new = (components['rv_weekly'] * (self.weekly_window - 1) + rv_new) / self.weekly_window\n                monthly_new = (components['rv_monthly'] * (self.monthly_window - 1) + rv_new) / self.monthly_window\n\n                pred = (\n                    self.beta0 +\n                    self.beta_d * rv_new +\n                    self.beta_w * weekly_new +\n                    self.beta_m * monthly_new\n                )\n\n        return pred",
    "source_file": "core\\_experimental\\har_rv_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HARRVVolatility"
  },
  {
    "name": "predict_with_jumps",
    "category": "volatility",
    "formula": "pred",
    "explanation": "Predict volatility including jump component (HAR-RV-J).",
    "python_code": "def predict_with_jumps(self, returns: pd.Series) -> pd.Series:\n        \"\"\"\n        Predict volatility including jump component (HAR-RV-J).\n        \"\"\"\n        rv = self.compute_realized_volatility(returns, self.daily_window, squared=True)\n        components = self.compute_har_components(rv)\n        jump = self.compute_jump_component(returns)\n\n        pred = (\n            self.beta0 +\n            self.beta_d * components['rv_daily'] +\n            self.beta_w * components['rv_weekly'] +\n            self.beta_m * components['rv_monthly'] +\n            (self.beta_j or 0) * jump\n        )\n\n        return pred",
    "source_file": "core\\_experimental\\har_rv_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HARRVVolatility"
  },
  {
    "name": "forecast_volatility",
    "category": "volatility",
    "formula": "vol",
    "explanation": "Get point forecast for future volatility.\n\nArgs:\n    returns: Return series\n    horizon: Forecast horizon in periods\n    annualize: Whether to annualize the volatility\n\nReturns:\n    Volatility forecast",
    "python_code": "def forecast_volatility(\n        self,\n        returns: pd.Series,\n        horizon: int = 1,\n        annualize: bool = True\n    ) -> float:\n        \"\"\"\n        Get point forecast for future volatility.\n\n        Args:\n            returns: Return series\n            horizon: Forecast horizon in periods\n            annualize: Whether to annualize the volatility\n\n        Returns:\n            Volatility forecast\n        \"\"\"\n        pred = self.predict(returns, steps=horizon)\n        forecast = pred.iloc[-1] if len(pred) > 0 else 0\n\n        # Convert from variance to volatility\n        vol = np.sqrt(np.maximum(0, forecast))\n\n        if annualize:\n            vol = vol * np.sqrt(self.annualization_factor)\n\n        return vol",
    "source_file": "core\\_experimental\\har_rv_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HARRVVolatility"
  },
  {
    "name": "get_volatility_regime",
    "category": "volatility",
    "formula": "regime",
    "explanation": "Classify volatility regime: low, normal, high.\n\nArgs:\n    returns: Return series\n    low_percentile: Percentile threshold for low volatility\n    high_percentile: Percentile threshold for high volatility\n\nReturns:\n    Series with regime labels ('low', 'normal', 'high')",
    "python_code": "def get_volatility_regime(\n        self,\n        returns: pd.Series,\n        low_percentile: float = 25,\n        high_percentile: float = 75\n    ) -> pd.Series:\n        \"\"\"\n        Classify volatility regime: low, normal, high.\n\n        Args:\n            returns: Return series\n            low_percentile: Percentile threshold for low volatility\n            high_percentile: Percentile threshold for high volatility\n\n        Returns:\n            Series with regime labels ('low', 'normal', 'high')\n        \"\"\"\n        rv = self.compute_realized_volatility(returns, self.daily_window, squared=False)\n\n        low_thresh = rv.quantile(low_percentile / 100)\n        high_thresh = rv.quantile(high_percentile / 100)\n\n        regime = pd.Series('normal', index=returns.index)\n        regime[rv < low_thresh] = 'low'\n        regime[rv > high_thresh] = 'high'\n\n        return regime",
    "source_file": "core\\_experimental\\har_rv_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HARRVVolatility"
  },
  {
    "name": "get_volatility_percentile",
    "category": "volatility",
    "formula": "rv.rolling(window).apply(",
    "explanation": "Get rolling percentile rank of current volatility.\n\nReturns value between 0-100 indicating where current vol ranks historically.",
    "python_code": "def get_volatility_percentile(self, returns: pd.Series, window: int = 252) -> pd.Series:\n        \"\"\"\n        Get rolling percentile rank of current volatility.\n\n        Returns value between 0-100 indicating where current vol ranks historically.\n        \"\"\"\n        rv = self.compute_realized_volatility(returns, self.daily_window, squared=False)\n        return rv.rolling(window).apply(\n            lambda x: stats.percentileofscore(x[:-1], x[-1]) if len(x) > 1 else 50,\n            raw=False\n        )",
    "source_file": "core\\_experimental\\har_rv_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HARRVVolatility"
  },
  {
    "name": "detect_vol_breakout",
    "category": "volatility",
    "formula": "z_score > threshold",
    "explanation": "Detect volatility breakouts (sudden vol spikes).\n\nArgs:\n    returns: Return series\n    lookback: Lookback period for baseline\n    threshold: Number of standard deviations for breakout\n\nReturns:\n    Boolean series indicating breakout",
    "python_code": "def detect_vol_breakout(\n        self,\n        returns: pd.Series,\n        lookback: int = 20,\n        threshold: float = 2.0\n    ) -> pd.Series:\n        \"\"\"\n        Detect volatility breakouts (sudden vol spikes).\n\n        Args:\n            returns: Return series\n            lookback: Lookback period for baseline\n            threshold: Number of standard deviations for breakout\n\n        Returns:\n            Boolean series indicating breakout\n        \"\"\"\n        rv = self.compute_realized_volatility(returns, self.daily_window, squared=False)\n        rv_mean = rv.rolling(lookback).mean()\n        rv_std = rv.rolling(lookback).std()\n\n        z_score = (rv - rv_mean) / (rv_std + 1e-10)\n        return z_score > threshold",
    "source_file": "core\\_experimental\\har_rv_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HARRVVolatility"
  },
  {
    "name": "vol_mean_reversion_signal",
    "category": "volatility",
    "formula": "vol = expect vol to drop = increase position | vol = expect vol to spike = reduce position | signal",
    "explanation": "Generate trading signals based on vol mean reversion.\n\nHigh vol = expect vol to drop = increase position\nLow vol = expect vol to spike = reduce position\n\nArgs:\n    returns: Return series\n    z_entry: Z-score threshold for entry\n    z_exit: Z-score threshold for exit\n\nReturns:\n    Signal series: 1 (increase), -1 (reduce), 0 (neutral)",
    "python_code": "def vol_mean_reversion_signal(\n        self,\n        returns: pd.Series,\n        z_entry: float = 2.0,\n        z_exit: float = 0.5\n    ) -> pd.Series:\n        \"\"\"\n        Generate trading signals based on vol mean reversion.\n\n        High vol = expect vol to drop = increase position\n        Low vol = expect vol to spike = reduce position\n\n        Args:\n            returns: Return series\n            z_entry: Z-score threshold for entry\n            z_exit: Z-score threshold for exit\n\n        Returns:\n            Signal series: 1 (increase), -1 (reduce), 0 (neutral)\n        \"\"\"\n        rv = self.compute_realized_volatility(returns, self.daily_window, squared=False)\n        rv_mean = rv.rolling(22).mean()\n        rv_std = rv.rolling(22).std()\n\n        z_score = (rv - rv_mean) / (rv_std + 1e-10)\n\n        signal = pd.Series(0, index=returns.index)\n        signal[z_score > z_entry] = 1  # High vol, expect reversion down\n        signal[z_score < -z_entry] = -1  # Low vol, expect spike up\n\n        return signal",
    "source_file": "core\\_experimental\\har_rv_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HARRVVolatility"
  },
  {
    "name": "vol_momentum_signal",
    "category": "volatility",
    "formula": "vol = reduce positions | vol = increase positions | signal",
    "explanation": "Generate signals based on volatility momentum.\n\nRising vol = reduce positions\nFalling vol = increase positions\n\nReturns:\n    Signal series: 1 (increasing risk), -1 (decreasing risk), 0 (neutral)",
    "python_code": "def vol_momentum_signal(\n        self,\n        returns: pd.Series,\n        short_window: int = 5,\n        long_window: int = 22\n    ) -> pd.Series:\n        \"\"\"\n        Generate signals based on volatility momentum.\n\n        Rising vol = reduce positions\n        Falling vol = increase positions\n\n        Returns:\n            Signal series: 1 (increasing risk), -1 (decreasing risk), 0 (neutral)\n        \"\"\"\n        rv = self.compute_realized_volatility(returns, self.daily_window, squared=False)\n\n        rv_short = rv.rolling(short_window).mean()\n        rv_long = rv.rolling(long_window).mean()\n\n        # Momentum: short above long = rising vol\n        signal = pd.Series(0, index=returns.index)\n        signal[rv_short > rv_long * 1.1] = -1  # Rising vol, reduce\n        signal[rv_short < rv_long * 0.9] = 1   # Falling vol, increase\n\n        return signal",
    "source_file": "core\\_experimental\\har_rv_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HARRVVolatility"
  },
  {
    "name": "get_model_summary",
    "category": "volatility",
    "formula": "{",
    "explanation": "Get summary of fitted model.",
    "python_code": "def get_model_summary(self) -> Dict:\n        \"\"\"Get summary of fitted model.\"\"\"\n        return {\n            'beta0': self.beta0,\n            'beta_daily': self.beta_d,\n            'beta_weekly': self.beta_w,\n            'beta_monthly': self.beta_m,\n            'beta_jump': self.beta_j,\n            'r_squared': self.r_squared,\n            'adj_r_squared': self.adjusted_r_squared,\n            'residual_std': self.residual_std\n        }",
    "source_file": "core\\_experimental\\har_rv_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HARRVVolatility"
  },
  {
    "name": "get_component_contributions",
    "category": "volatility",
    "formula": "pd.DataFrame({",
    "explanation": "Get contribution of each HAR component to forecast.\n\nReturns DataFrame with daily, weekly, monthly contributions.",
    "python_code": "def get_component_contributions(self, returns: pd.Series) -> pd.DataFrame:\n        \"\"\"\n        Get contribution of each HAR component to forecast.\n\n        Returns DataFrame with daily, weekly, monthly contributions.\n        \"\"\"\n        rv = self.compute_realized_volatility(returns, self.daily_window, squared=True)\n        components = self.compute_har_components(rv)\n\n        return pd.DataFrame({\n            'daily_contrib': self.beta_d * components['rv_daily'],\n            'weekly_contrib': self.beta_w * components['rv_weekly'],\n            'monthly_contrib': self.beta_m * components['rv_monthly'],\n            'intercept': self.beta0,\n            'total_forecast': self.predict(returns)\n        }, index=returns.index)",
    "source_file": "core\\_experimental\\har_rv_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HARRVVolatility"
  },
  {
    "name": "compute_session_rv",
    "category": "volatility",
    "formula": "simple RV | pd.DataFrame({ | pd.DataFrame({",
    "explanation": "Compute realized volatility by trading session.\n\nReturns DataFrame with Asian, London, NY RV components.",
    "python_code": "def compute_session_rv(self, returns: pd.Series) -> pd.DataFrame:\n        \"\"\"\n        Compute realized volatility by trading session.\n\n        Returns DataFrame with Asian, London, NY RV components.\n        \"\"\"\n        if not hasattr(returns.index, 'hour'):\n            # Not datetime index, return simple RV\n            rv = self.compute_realized_volatility(returns)\n            return pd.DataFrame({\n                'rv_asian': rv / 3,\n                'rv_london': rv / 3,\n                'rv_ny': rv / 3\n            }, index=returns.index)\n\n        hour = returns.index.hour\n\n        asian_mask = (hour >= self.asian_hours[0]) & (hour < self.asian_hours[1])\n        london_mask = (hour >= self.london_hours[0]) & (hour < self.london_hours[1])\n        ny_mask = (hour >= self.ny_hours[0]) | (hour < self.ny_hours[0])  # Wrap around\n\n        rv_asian = (returns ** 2).where(asian_mask, 0).rolling(24).sum()\n        rv_london = (returns ** 2).where(london_mask, 0).rolling(24).sum()\n        rv_ny = (returns ** 2).where(ny_mask, 0).rolling(24).sum()\n\n        return pd.DataFrame({\n            'rv_asian': rv_asian,\n            'rv_london': rv_london,\n            'rv_ny': rv_ny\n        }, index=returns.index)",
    "source_file": "core\\_experimental\\har_rv_volatility.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HARRVForex"
  },
  {
    "name": "__init__",
    "category": "machine_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, models_dir: Path):\n        self.models_dir = models_dir\n\n        # Institutional models\n        self.hmm_models = {}      # Hidden Markov Models (regime detection)\n        self.kalman_models = {}   # Kalman filters (dynamic parameters)\n        self.ensemble_models = {} # XGBoost/LightGBM/CatBoost\n        self.scalers = {}\n        self.feature_cols = []\n\n        # Current state\n        self.current_regime = {}  # Per-pair regime state\n        self.kalman_state = {}    # Per-pair Kalman state\n\n        # Gold standard quant formulas\n        if QUANT_FORMULAS_AVAILABLE:\n            self.alpha101 = Alpha101Forex()\n            self.avellaneda_stoikov = AvellanedaStoikov(gamma=0.1, sigma=0.02, k=1.5)\n            self.kelly = KellyCriterion()\n            self.triple_barrier = TripleBarrier()\n            self.frac_diff = FractionalDifferentiation()\n        else:\n            self.alpha101 = None\n            self.avellaneda_stoikov = None\n            self.kelly = None\n            self.triple_barrier = None\n            self.frac_diff = None\n\n        self.loaded = False",
    "source_file": "core\\_experimental\\institutional_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "InstitutionalPredictor"
  },
  {
    "name": "load_models",
    "category": "machine_learning",
    "formula": "False",
    "explanation": "Load all institutional-grade models.",
    "python_code": "def load_models(self) -> bool:\n        \"\"\"Load all institutional-grade models.\"\"\"\n        try:\n            # Load HMM models (Renaissance method)\n            hmm_path = self.models_dir / 'hmm_models.pkl'\n            if hmm_path.exists():\n                with open(hmm_path, 'rb') as f:\n                    self.hmm_models = pickle.load(f)\n                logger.info(f\"Loaded HMM models for {len(self.hmm_models)} pairs\")\n\n            # Load Kalman models (Goldman method)\n            kalman_path = self.models_dir / 'kalman_models.pkl'\n            if kalman_path.exists():\n                with open(kalman_path, 'rb') as f:\n                    self.kalman_models = pickle.load(f)\n                logger.info(f\"Loaded Kalman models for {len(self.kalman_models)} pairs\")\n\n            # Load ensemble models (XGBoost/LightGBM/CatBoost)\n            ensemble_path = self.models_dir / 'ensemble_models.pkl'\n            if ensemble_path.exists():\n                with open(ensemble_path, 'rb') as f:\n                    self.ensemble_models = pickle.load(f)\n                logger.info(f\"Loaded ensemble models for {len(self.ensemble_models)} pairs\")\n\n            # Load scalers\n            scalers_path = self.models_dir / 'scalers.pkl'\n            if scalers_path.exists():\n                with open(scalers_path, 'rb') as f:\n                    self.scalers = pickle.load(f)\n                logger.info(f\"Loaded scalers for {len(self.scalers)} pairs\")\n\n            # Load feature columns\n            features_path = self.models_dir / 'features.pkl'\n            if features_path.exists():\n                with open(features_path, 'rb') as f:\n                    self.feature_cols = pickle.load(f)\n                logger.info(f\"Loaded {len(self.feature_cols)} feature columns\")\n\n            self.loaded = bool(self.ensemble_models)\n\n            if not self.loaded:\n                logger.warning(\"No ensemble models loaded - using fallback signals\")\n\n            return self.loaded\n\n        except Exception as e:\n            logger.error(f\"Error loading models: {e}\")\n            return False",
    "source_file": "core\\_experimental\\institutional_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "InstitutionalPredictor"
  },
  {
    "name": "detect_regime",
    "category": "machine_learning",
    "formula": "{'state': 1, 'confidence': 0.5, 'action': 'normal'} | {'state': 1, 'confidence': 0.5, 'action': 'normal'} | {",
    "explanation": "Detect market regime using Hidden Markov Model.\n\nThis is the Renaissance Technologies method:\n- Uses HMM to identify hidden market states\n- States typically represent: low vol, normal, high vol regimes\n- Adjust position sizing based on regime\n\nReturns:\n    Dict with regime info: state (0,1,2), probabilities, recommended action",
    "python_code": "def detect_regime(self, pair: str, returns: np.ndarray, volatility: np.ndarray) -> Dict:\n        \"\"\"\n        Detect market regime using Hidden Markov Model.\n\n        This is the Renaissance Technologies method:\n        - Uses HMM to identify hidden market states\n        - States typically represent: low vol, normal, high vol regimes\n        - Adjust position sizing based on regime\n\n        Returns:\n            Dict with regime info: state (0,1,2), probabilities, recommended action\n        \"\"\"\n        if pair not in self.hmm_models:\n            return {'state': 1, 'confidence': 0.5, 'action': 'normal'}\n\n        try:\n            hmm = self.hmm_models[pair]\n\n            # Prepare observation data\n            X = np.column_stack([returns[-50:], volatility[-50:]])\n            X = np.nan_to_num(X, nan=0.0)\n\n            if len(X) < 10:\n                return {'state': 1, 'confidence': 0.5, 'action': 'normal'}\n\n            # Get current regime\n            regime = hmm.predict(X)[-1]\n            regime_probs = hmm.predict_proba(X)[-1]\n            confidence = regime_probs[regime]\n\n            # Store current regime\n            self.current_regime[pair] = regime\n\n            # Determine action based on regime\n            # Regime 0: Low volatility - normal trading\n            # Regime 1: Normal - full position size\n            # Regime 2: High volatility - reduce position or wait\n            actions = {\n                0: 'aggressive',  # Low vol - can be more aggressive\n                1: 'normal',      # Normal conditions\n                2: 'conservative' # High vol - reduce risk\n            }\n\n            return {\n                'state': int(regime),\n                'confidence': float(confidence),\n                'probabilities': regime_probs.tolist(),\n                'action': actions.get(regime, 'normal')\n            }\n\n        except Exception as e:\n            logger.warning(f\"HMM regime detection failed for {pair}: {e}\")\n            return {'state': 1, 'confidence': 0.5, 'action': 'normal'}",
    "source_file": "core\\_experimental\\institutional_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "InstitutionalPredictor"
  },
  {
    "name": "get_kalman_estimate",
    "category": "technical",
    "formula": "{'mean': price, 'deviation': 0.0, 'signal': 0.0} | { | {'mean': price, 'deviation': 0.0, 'signal': 0.0}",
    "explanation": "Get Kalman filter estimate for trend/mean.\n\nThis is the Goldman Sachs method for market making:\n- Kalman filter estimates true underlying value\n- Deviation from estimate suggests mean reversion opportunity\n- Updates dynamically with each new observation\n\nReturns:\n    Dict with: estimated_mean, deviation, signal strength",
    "python_code": "def get_kalman_estimate(self, pair: str, price: float) -> Dict:\n        \"\"\"\n        Get Kalman filter estimate for trend/mean.\n\n        This is the Goldman Sachs method for market making:\n        - Kalman filter estimates true underlying value\n        - Deviation from estimate suggests mean reversion opportunity\n        - Updates dynamically with each new observation\n\n        Returns:\n            Dict with: estimated_mean, deviation, signal strength\n        \"\"\"\n        if pair not in self.kalman_models:\n            return {'mean': price, 'deviation': 0.0, 'signal': 0.0}\n\n        try:\n            kf = self.kalman_models[pair]\n\n            # Initialize state if needed\n            if pair not in self.kalman_state:\n                self.kalman_state[pair] = {\n                    'state_mean': np.array([price]),\n                    'state_cov': np.array([[1.0]])\n                }\n\n            # Update with new observation\n            state = self.kalman_state[pair]\n            new_mean, new_cov = kf.filter_update(\n                state['state_mean'],\n                state['state_cov'],\n                observation=price\n            )\n\n            # Store updated state\n            self.kalman_state[pair] = {\n                'state_mean': new_mean,\n                'state_cov': new_cov\n            }\n\n            # Calculate deviation\n            estimated_mean = float(new_mean[0])\n            deviation = (price - estimated_mean) / estimated_mean if estimated_mean != 0 else 0\n\n            # Signal: negative deviation = price below mean = potential long\n            # Positive deviation = price above mean = potential short\n            signal = -deviation * 100  # Scale for usability\n\n            return {\n                'mean': estimated_mean,\n                'deviation': deviation,\n                'signal': signal,\n                'uncertainty': float(new_cov[0, 0])\n            }\n\n        except Exception as e:\n            logger.warning(f\"Kalman filter failed for {pair}: {e}\")\n            return {'mean': price, 'deviation': 0.0, 'signal': 0.0}",
    "source_file": "core\\_experimental\\institutional_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "InstitutionalPredictor"
  },
  {
    "name": "get_ensemble_prediction",
    "category": "machine_learning",
    "formula": "'neutral', 0.5 | signal, confidence | 'neutral', 0.5",
    "explanation": "Get ensemble prediction from XGBoost/LightGBM/CatBoost.\n\nThis is what actually wins in production:\n- Gradient boosting trees (NOT neural networks)\n- Ensemble of 3 models with weighted voting\n- Simple, interpretable, generalizes well\n\nReturns:\n    Tuple of (signal: 'long'/'short'/'neutral', confidence: 0-1)",
    "python_code": "def get_ensemble_prediction(self, pair: str, features: np.ndarray) -> Tuple[str, float]:\n        \"\"\"\n        Get ensemble prediction from XGBoost/LightGBM/CatBoost.\n\n        This is what actually wins in production:\n        - Gradient boosting trees (NOT neural networks)\n        - Ensemble of 3 models with weighted voting\n        - Simple, interpretable, generalizes well\n\n        Returns:\n            Tuple of (signal: 'long'/'short'/'neutral', confidence: 0-1)\n        \"\"\"\n        if pair not in self.ensemble_models:\n            return 'neutral', 0.5\n\n        try:\n            model_data = self.ensemble_models[pair]\n            models = model_data['models']\n            weights = model_data['weights']\n\n            # Scale features if scaler available\n            if pair in self.scalers:\n                features = self.scalers[pair].transform(features.reshape(1, -1))\n            else:\n                features = features.reshape(1, -1)\n\n            # Get predictions from each model\n            ensemble_prob = 0.0\n            total_weight = sum(weights.values())\n\n            predictions = {}\n            for name, model in models.items():\n                try:\n                    pred_proba = model.predict_proba(features)[0]\n                    prob_up = pred_proba[1] if len(pred_proba) > 1 else pred_proba[0]\n                    predictions[name] = prob_up\n                    ensemble_prob += prob_up * (weights[name] / total_weight)\n                except Exception as e:\n                    logger.warning(f\"Model {name} failed for {pair}: {e}\")\n\n            # Determine signal\n            # \"We're right 50.75% of the time\" - Renaissance\n            if ensemble_prob > 0.52:  # Slight edge is enough\n                signal = 'long'\n                confidence = ensemble_prob\n            elif ensemble_prob < 0.48:\n                signal = 'short'\n                confidence = 1 - ensemble_prob\n            else:\n                signal = 'neutral'\n                confidence = 0.5\n\n            return signal, confidence\n\n        except Exception as e:\n            logger.error(f\"Ensemble prediction failed for {pair}: {e}\")\n            return 'neutral', 0.5",
    "source_file": "core\\_experimental\\institutional_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "InstitutionalPredictor"
  },
  {
    "name": "predict",
    "category": "technical",
    "formula": "{ | { | {",
    "explanation": "Full institutional-grade prediction combining all methods.\n\nCombines:\n1. HMM regime detection (position sizing adjustment)\n2. Kalman filter (mean reversion signal)\n3. Ensemble prediction (direction and confidence)\n\nReturns:\n    Dict with signal, confidence, regime, and recommendations",
    "python_code": "def predict(self, pair: str, data: pd.DataFrame) -> Dict:\n        \"\"\"\n        Full institutional-grade prediction combining all methods.\n\n        Combines:\n        1. HMM regime detection (position sizing adjustment)\n        2. Kalman filter (mean reversion signal)\n        3. Ensemble prediction (direction and confidence)\n\n        Returns:\n            Dict with signal, confidence, regime, and recommendations\n        \"\"\"\n        if not self.loaded:\n            return {\n                'signal': 'neutral',\n                'confidence': 0.0,\n                'regime': 'unknown',\n                'position_multiplier': 1.0,\n                'notes': 'Models not loaded'\n            }\n\n        try:\n            # Prepare data\n            returns = data['returns'].values if 'returns' in data.columns else np.diff(data['mid'].values) / data['mid'].values[:-1]\n            volatility = data['vol_20'].values if 'vol_20' in data.columns else pd.Series(returns).rolling(20).std().values\n            current_price = data['mid'].iloc[-1] if 'mid' in data.columns else data.iloc[-1, 0]\n\n            # 1. HMM Regime Detection (Renaissance method)\n            regime_info = self.detect_regime(pair, returns, volatility)\n\n            # 2. Kalman Filter (Goldman method)\n            kalman_info = self.get_kalman_estimate(pair, current_price)\n\n            # 3. Ensemble Prediction (Industry standard)\n            features = self._extract_features(data)\n            ensemble_signal, ensemble_confidence = self.get_ensemble_prediction(pair, features)\n\n            # Combine signals\n            # Adjust position size based on regime\n            position_multipliers = {\n                'aggressive': 1.5,\n                'normal': 1.0,\n                'conservative': 0.5\n            }\n            position_mult = position_multipliers.get(regime_info['action'], 1.0)\n\n            # Combine Kalman and ensemble signals\n            kalman_signal = 'long' if kalman_info['signal'] > 1 else ('short' if kalman_info['signal'] < -1 else 'neutral')\n\n            # Final signal: require agreement or strong ensemble confidence\n            if ensemble_signal == kalman_signal:\n                final_signal = ensemble_signal\n                final_confidence = (ensemble_confidence + abs(kalman_info['signal']) / 10) / 2\n            elif ensemble_confidence > 0.58:  # Strong ensemble signal\n                final_signal = ensemble_signal\n                final_confidence = ensemble_confidence * 0.8  # Slight penalty for disagreement\n            else:\n                final_signal = 'neutral'\n                final_confidence = 0.5\n\n            # Reduce confidence in high volatility regime\n            if regime_info['action'] == 'conservative':\n                final_confidence *= 0.8\n\n            return {\n                'signal': final_signal,\n                'confidence': min(final_confidence, 0.95),  # Cap confidence\n                'regime': {\n                    'state': regime_info['state'],\n                  ",
    "source_file": "core\\_experimental\\institutional_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "InstitutionalPredictor"
  },
  {
    "name": "get_alpha101_signals",
    "category": "alpha_factor",
    "formula": "data | data",
    "explanation": "Generate WorldQuant Alpha101 signals.\n\nThese are 101 formulaic alphas from WorldQuant research.\nEach alpha is a weak signal (51-55% accuracy) that contributes\nto ensemble predictions.\n\nArgs:\n    data: DataFrame with OHLCV or forex tick data\n\nReturns:\n    DataFrame with alpha signals added",
    "python_code": "def get_alpha101_signals(self, data: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate WorldQuant Alpha101 signals.\n\n        These are 101 formulaic alphas from WorldQuant research.\n        Each alpha is a weak signal (51-55% accuracy) that contributes\n        to ensemble predictions.\n\n        Args:\n            data: DataFrame with OHLCV or forex tick data\n\n        Returns:\n            DataFrame with alpha signals added\n        \"\"\"\n        if self.alpha101 is None:\n            logger.warning(\"Alpha101 not available\")\n            return data\n\n        try:\n            return self.alpha101.generate_all_alphas(data)\n        except Exception as e:\n            logger.warning(f\"Alpha101 generation failed: {e}\")\n            return data",
    "source_file": "core\\_experimental\\institutional_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "InstitutionalPredictor"
  },
  {
    "name": "get_market_making_quotes",
    "category": "microstructure",
    "formula": "positive = long) | {'bid': mid_price * 0.9999, 'ask': mid_price * 1.0001} | {",
    "explanation": "Get Avellaneda-Stoikov optimal market making quotes.\n\nThis is the gold standard for HFT market making.\n\nArgs:\n    mid_price: Current mid price\n    inventory: Current position (positive = long)\n    time_remaining: Fraction of trading period remaining\n\nReturns:\n    Dict with bid, ask, and spread info",
    "python_code": "def get_market_making_quotes(self, mid_price: float, inventory: int,\n                                  time_remaining: float = 0.5) -> Dict:\n        \"\"\"\n        Get Avellaneda-Stoikov optimal market making quotes.\n\n        This is the gold standard for HFT market making.\n\n        Args:\n            mid_price: Current mid price\n            inventory: Current position (positive = long)\n            time_remaining: Fraction of trading period remaining\n\n        Returns:\n            Dict with bid, ask, and spread info\n        \"\"\"\n        if self.avellaneda_stoikov is None:\n            return {'bid': mid_price * 0.9999, 'ask': mid_price * 1.0001}\n\n        bid, ask = self.avellaneda_stoikov.optimal_quotes(\n            mid_price, inventory, time_remaining\n        )\n        reservation = self.avellaneda_stoikov.reservation_price(\n            mid_price, inventory, time_remaining\n        )\n\n        return {\n            'bid': bid,\n            'ask': ask,\n            'spread': ask - bid,\n            'spread_pips': (ask - bid) * 10000,\n            'reservation_price': reservation,\n            'inventory_skew': self.avellaneda_stoikov.inventory_skew(inventory)\n        }",
    "source_file": "core\\_experimental\\institutional_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "InstitutionalPredictor"
  },
  {
    "name": "calculate_position_size",
    "category": "risk",
    "formula": "Kelly = 75% optimal growth, 25% variance | Kelly = 43.75% optimal growth, 6.25% variance | {",
    "explanation": "Calculate optimal position size using Kelly Criterion.\n\nUses fractional Kelly (default 25%) for safety:\n- 50% Kelly = 75% optimal growth, 25% variance\n- 25% Kelly = 43.75% optimal growth, 6.25% variance\n\nArgs:\n    account_value: Total account value\n    win_prob: Probability of winning (from model confidence)\n    win_loss_ratio: Average win / Average loss\n    kelly_fraction: Fraction of full Kelly to use (0.25-0.5)\n\nReturns:\n    Dict with position sizing info",
    "python_code": "def calculate_position_size(self, account_value: float, win_prob: float,\n                                 win_loss_ratio: float,\n                                 kelly_fraction: float = 0.25) -> Dict:\n        \"\"\"\n        Calculate optimal position size using Kelly Criterion.\n\n        Uses fractional Kelly (default 25%) for safety:\n        - 50% Kelly = 75% optimal growth, 25% variance\n        - 25% Kelly = 43.75% optimal growth, 6.25% variance\n\n        Args:\n            account_value: Total account value\n            win_prob: Probability of winning (from model confidence)\n            win_loss_ratio: Average win / Average loss\n            kelly_fraction: Fraction of full Kelly to use (0.25-0.5)\n\n        Returns:\n            Dict with position sizing info\n        \"\"\"\n        if self.kelly is None:\n            # Fallback to simple 1% risk\n            return {\n                'position_size': account_value * 0.01,\n                'kelly_pct': 0.01,\n                'method': 'fallback_1pct'\n            }\n\n        full_kelly = self.kelly.kelly_fraction(win_prob, win_loss_ratio)\n        frac_kelly = self.kelly.fractional_kelly(win_prob, win_loss_ratio, kelly_fraction)\n        position = self.kelly.position_size(\n            account_value, win_prob, win_loss_ratio,\n            fraction=kelly_fraction, max_position_pct=0.10  # Max 10% per trade\n        )\n\n        return {\n            'position_size': position,\n            'kelly_pct': frac_kelly,\n            'full_kelly_pct': full_kelly,\n            'fraction_used': kelly_fraction,\n            'method': f'{int(kelly_fraction*100)}%_kelly'\n        }",
    "source_file": "core\\_experimental\\institutional_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "InstitutionalPredictor"
  },
  {
    "name": "_extract_features",
    "category": "machine_learning",
    "formula": "np.zeros(50) | data[cols].iloc[-1].values",
    "explanation": "Extract features for ensemble prediction.",
    "python_code": "def _extract_features(self, data: pd.DataFrame) -> np.ndarray:\n        \"\"\"Extract features for ensemble prediction.\"\"\"\n        # First, generate Alpha101 signals if available\n        if self.alpha101 is not None:\n            try:\n                data = self.get_alpha101_signals(data)\n            except Exception as e:\n                logger.debug(f\"Alpha101 feature extraction failed: {e}\")\n\n        if not self.feature_cols:\n            # Use all numeric columns except excluded ones\n            exclude = ['bid', 'ask', 'mid', 'spread', 'pair', 'timestamp', 'returns', 'target']\n            cols = [c for c in data.columns if c not in exclude and data[c].dtype in [np.float64, np.int64, np.float32, np.int32]]\n        else:\n            cols = [c for c in self.feature_cols if c in data.columns]\n\n        if not cols:\n            return np.zeros(50)\n\n        return data[cols].iloc[-1].values",
    "source_file": "core\\_experimental\\institutional_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "InstitutionalPredictor"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "{ | {",
    "explanation": "Generate prediction from simple signals.",
    "python_code": "def predict(self, pair: str, data: pd.DataFrame) -> Dict:\n        \"\"\"Generate prediction from simple signals.\"\"\"\n        try:\n            mid = data['mid'].values if 'mid' in data.columns else data.iloc[:, 0].values\n\n            # Simple moving average crossover\n            sma_fast = pd.Series(mid).rolling(10).mean().iloc[-1]\n            sma_slow = pd.Series(mid).rolling(50).mean().iloc[-1]\n\n            # RSI\n            returns = np.diff(mid) / mid[:-1]\n            gains = np.where(returns > 0, returns, 0)\n            losses = np.where(returns < 0, -returns, 0)\n            avg_gain = np.mean(gains[-14:])\n            avg_loss = np.mean(losses[-14:])\n            rs = avg_gain / (avg_loss + 1e-10)\n            rsi = 100 - (100 / (1 + rs))\n\n            # Signal logic\n            if sma_fast > sma_slow and rsi < 70:\n                signal = 'long'\n                confidence = 0.52 + (sma_fast - sma_slow) / sma_slow * 10\n            elif sma_fast < sma_slow and rsi > 30:\n                signal = 'short'\n                confidence = 0.52 + (sma_slow - sma_fast) / sma_slow * 10\n            else:\n                signal = 'neutral'\n                confidence = 0.5\n\n            return {\n                'signal': signal,\n                'confidence': min(max(confidence, 0.5), 0.6),  # Cap at 60% for fallback\n                'regime': {'state': 1, 'action': 'normal', 'confidence': 0.5},\n                'position_multiplier': 1.0,\n                'notes': 'Fallback predictor (simple signals)'\n            }\n\n        except Exception as e:\n            return {\n                'signal': 'neutral',\n                'confidence': 0.0,\n                'regime': {'state': 1, 'action': 'normal', 'confidence': 0.5},\n                'position_multiplier': 0.5,\n                'notes': f'Error: {e}'\n            }",
    "source_file": "core\\_experimental\\institutional_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "FallbackPredictor"
  },
  {
    "name": "create_predictor",
    "category": "reinforcement_learning",
    "formula": "predictor | FallbackPredictor()",
    "explanation": "Factory function to create the appropriate predictor.",
    "python_code": "def create_predictor(models_dir: Path) -> InstitutionalPredictor:\n    \"\"\"Factory function to create the appropriate predictor.\"\"\"\n    predictor = InstitutionalPredictor(models_dir)\n\n    if predictor.load_models():\n        logger.info(\"Using institutional-grade predictor (HMM + Kalman + Ensemble)\")\n        return predictor\n    else:\n        logger.warning(\"Falling back to simple predictor\")\n        return FallbackPredictor()",
    "source_file": "core\\_experimental\\institutional_predictor.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "statistical",
    "formula": "",
    "explanation": "Initialize BPV calculator.\n\nArgs:\n    window: Rolling window for RV/BPV calculation\n           (78 = 5-min bars in forex session)",
    "python_code": "def __init__(self, window: int = 78):\n        \"\"\"\n        Initialize BPV calculator.\n\n        Args:\n            window: Rolling window for RV/BPV calculation\n                   (78 = 5-min bars in forex session)\n        \"\"\"\n        self.window = window",
    "source_file": "core\\_experimental\\jump_detection.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BipowerVariation"
  },
  {
    "name": "realized_variance",
    "category": "quantitative",
    "formula": "RV =  r_i | np.sum(returns**2)",
    "explanation": "Compute realized variance (sum of squared returns).\n\nRV =  r_i",
    "python_code": "def realized_variance(self, returns: np.ndarray) -> float:\n        \"\"\"\n        Compute realized variance (sum of squared returns).\n\n        RV =  r_i\n        \"\"\"\n        return np.sum(returns**2)",
    "source_file": "core\\_experimental\\jump_detection.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BipowerVariation"
  },
  {
    "name": "bipower_variance",
    "category": "quantitative",
    "formula": "BPV = (/2) *  |r_i| * |r_{i-1}| | 0.0 | bpv",
    "explanation": "Compute bipower variation (jump-robust).\n\nBPV = (/2) *  |r_i| * |r_{i-1}|",
    "python_code": "def bipower_variance(self, returns: np.ndarray) -> float:\n        \"\"\"\n        Compute bipower variation (jump-robust).\n\n        BPV = (/2) *  |r_i| * |r_{i-1}|\n        \"\"\"\n        if len(returns) < 2:\n            return 0.0\n\n        # Bipower variation\n        bpv = np.sum(np.abs(returns[1:]) * np.abs(returns[:-1]))\n        # Scale factor to make it comparable to RV under no jumps\n        bpv *= np.pi / 2\n\n        return bpv",
    "source_file": "core\\_experimental\\jump_detection.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BipowerVariation"
  },
  {
    "name": "tripower_quarticity",
    "category": "quantitative",
    "formula": "TPQ = n * _{4/3}^{-3} *  |r_i|^{4/3} * |r_{i-1}|^{4/3} * |r_{i-2}|^{4/3} | 0.0 | n * mu_43**(-3) * tpq",
    "explanation": "Tripower quarticity for variance estimation of BPV.\n\nTPQ = n * _{4/3}^{-3} *  |r_i|^{4/3} * |r_{i-1}|^{4/3} * |r_{i-2}|^{4/3}\n\nUsed in jump test statistic variance calculation.",
    "python_code": "def tripower_quarticity(self, returns: np.ndarray) -> float:\n        \"\"\"\n        Tripower quarticity for variance estimation of BPV.\n\n        TPQ = n * _{4/3}^{-3} *  |r_i|^{4/3} * |r_{i-1}|^{4/3} * |r_{i-2}|^{4/3}\n\n        Used in jump test statistic variance calculation.\n        \"\"\"\n        if len(returns) < 3:\n            return 0.0\n\n        n = len(returns)\n        mu_43 = 2**(2/3) * gamma_func(7/6) / gamma_func(1/2)\n\n        tpq = np.sum(\n            np.abs(returns[2:])**(4/3) *\n            np.abs(returns[1:-1])**(4/3) *\n            np.abs(returns[:-2])**(4/3)\n        )\n\n        return n * mu_43**(-3) * tpq",
    "source_file": "core\\_experimental\\jump_detection.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BipowerVariation"
  },
  {
    "name": "quadpower_quarticity",
    "category": "quantitative",
    "formula": "QPQ = n * _1^{-4} *  |r_i| * |r_{i-1}| * |r_{i-2}| * |r_{i-3}| | 0.0 | n * np.pi**2 / 4 * mu_1**(-4) * qpq",
    "explanation": "Quadpower quarticity - alternative variance estimator.\n\nQPQ = n * _1^{-4} *  |r_i| * |r_{i-1}| * |r_{i-2}| * |r_{i-3}|",
    "python_code": "def quadpower_quarticity(self, returns: np.ndarray) -> float:\n        \"\"\"\n        Quadpower quarticity - alternative variance estimator.\n\n        QPQ = n * _1^{-4} *  |r_i| * |r_{i-1}| * |r_{i-2}| * |r_{i-3}|\n        \"\"\"\n        if len(returns) < 4:\n            return 0.0\n\n        n = len(returns)\n        mu_1 = MU_1\n\n        qpq = np.sum(\n            np.abs(returns[3:]) *\n            np.abs(returns[2:-1]) *\n            np.abs(returns[1:-2]) *\n            np.abs(returns[:-3])\n        )\n\n        return n * np.pi**2 / 4 * mu_1**(-4) * qpq",
    "source_file": "core\\_experimental\\jump_detection.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BipowerVariation"
  },
  {
    "name": "rolling_rv_bpv",
    "category": "statistical",
    "formula": "result",
    "explanation": "Compute rolling RV and BPV.\n\nReturns DataFrame with:\n- rv: Realized variance\n- bpv: Bipower variance\n- jump_var: max(0, RV - BPV)\n- relative_jump: jump_var / rv",
    "python_code": "def rolling_rv_bpv(self, returns: pd.Series) -> pd.DataFrame:\n        \"\"\"\n        Compute rolling RV and BPV.\n\n        Returns DataFrame with:\n        - rv: Realized variance\n        - bpv: Bipower variance\n        - jump_var: max(0, RV - BPV)\n        - relative_jump: jump_var / rv\n        \"\"\"\n        result = pd.DataFrame(index=returns.index)\n\n        rv = returns.rolling(self.window).apply(self.realized_variance)\n        bpv = returns.rolling(self.window).apply(self.bipower_variance)\n\n        result['rv'] = rv\n        result['bpv'] = bpv\n        result['jump_var'] = np.maximum(0, rv - bpv)\n        result['relative_jump'] = result['jump_var'] / (result['rv'] + 1e-10)\n\n        return result",
    "source_file": "core\\_experimental\\jump_detection.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "BipowerVariation"
  },
  {
    "name": "bns_test",
    "category": "quantitative",
    "formula": "Z = (RV - BPV) / sqrt(Var(RV - BPV)) | 0.0, 1.0, False | 0.0, 1.0, False",
    "explanation": "Barndorff-Nielsen & Shephard (2006) jump test.\n\nTest statistic:\nZ = (RV - BPV) / sqrt(Var(RV - BPV))\n  = (RV - BPV) / sqrt((/4 +  - 5) * TPQ)\n\nUnder H0 (no jumps): Z  N(0, 1)\n\nReturns: (test_statistic, p_value, has_jump)",
    "python_code": "def bns_test(self, returns: np.ndarray) -> Tuple[float, float, bool]:\n        \"\"\"\n        Barndorff-Nielsen & Shephard (2006) jump test.\n\n        Test statistic:\n        Z = (RV - BPV) / sqrt(Var(RV - BPV))\n          = (RV - BPV) / sqrt((/4 +  - 5) * TPQ)\n\n        Under H0 (no jumps): Z  N(0, 1)\n\n        Returns: (test_statistic, p_value, has_jump)\n        \"\"\"\n        n = len(returns)\n        if n < 10:\n            return 0.0, 1.0, False\n\n        rv = self.bpv_calc.realized_variance(returns)\n        bpv = self.bpv_calc.bipower_variance(returns)\n        tpq = self.bpv_calc.tripower_quarticity(returns)\n\n        # Variance of RV - BPV under no jumps\n        # Var = (/4 +  - 5) * (_1)^{-4} * TPQ / n\n        var_coef = np.pi**2 / 4 + np.pi - 5  #  0.6090\n\n        if tpq <= 0 or bpv <= 0:\n            return 0.0, 1.0, False\n\n        # Test statistic\n        variance = var_coef * tpq / n\n        if variance <= 0:\n            return 0.0, 1.0, False\n\n        z_stat = (rv - bpv) / np.sqrt(variance)\n\n        # One-sided test (we look for positive jumps contribution)\n        p_value = 1 - norm.cdf(z_stat)\n\n        has_jump = p_value < self.significance\n\n        return float(z_stat), float(p_value), has_jump",
    "source_file": "core\\_experimental\\jump_detection.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JumpTest"
  },
  {
    "name": "ratio_test",
    "category": "quantitative",
    "formula": "RJ = 1 - BPV / RV | 0.0, 1.0, False | float(rj), float(p_value), has_jump",
    "explanation": "Ratio-based jump test.\n\nTest statistic: RJ = 1 - BPV / RV\n\nUnder H0: RJ  0\nLarge RJ indicates jumps",
    "python_code": "def ratio_test(self, returns: np.ndarray) -> Tuple[float, float, bool]:\n        \"\"\"\n        Ratio-based jump test.\n\n        Test statistic: RJ = 1 - BPV / RV\n\n        Under H0: RJ  0\n        Large RJ indicates jumps\n        \"\"\"\n        rv = self.bpv_calc.realized_variance(returns)\n        bpv = self.bpv_calc.bipower_variance(returns)\n\n        if rv <= 0:\n            return 0.0, 1.0, False\n\n        rj = 1 - bpv / rv\n        rj = max(0, rj)  # Can't be negative\n\n        # Approximate p-value (empirical critical values)\n        # At 1% level, RJ > 0.4 typically indicates jumps\n        threshold = 0.3 if self.significance <= 0.01 else 0.2\n\n        has_jump = rj > threshold\n        p_value = 1 - min(1, rj / 0.5)  # Rough approximation\n\n        return float(rj), float(p_value), has_jump",
    "source_file": "core\\_experimental\\jump_detection.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JumpTest"
  },
  {
    "name": "detect_jumps",
    "category": "quantitative",
    "formula": "JumpResult( | as potential jump | JumpResult(",
    "explanation": "Combined jump detection using multiple tests.\n\nReturns JumpResult with comprehensive jump information.",
    "python_code": "def detect_jumps(self, returns: np.ndarray) -> JumpResult:\n        \"\"\"\n        Combined jump detection using multiple tests.\n\n        Returns JumpResult with comprehensive jump information.\n        \"\"\"\n        n = len(returns)\n        if n < 10:\n            return JumpResult(\n                has_jump=False, jump_time=None, jump_size=0.0,\n                jump_direction=0, continuous_vol=0.0, jump_vol=0.0,\n                total_vol=0.0, test_statistic=0.0, p_value=1.0\n            )\n\n        # Run tests\n        z_stat, z_pval, z_jump = self.bns_test(returns)\n        rj_stat, rj_pval, rj_jump = self.ratio_test(returns)\n\n        # Combine: jump if either test significant\n        has_jump = z_jump or rj_jump\n\n        # Volatility decomposition\n        rv = self.bpv_calc.realized_variance(returns)\n        bpv = self.bpv_calc.bipower_variance(returns)\n        jump_var = max(0, rv - bpv)\n\n        total_vol = np.sqrt(rv)\n        continuous_vol = np.sqrt(bpv)\n        jump_vol = np.sqrt(jump_var)\n\n        # Find largest jump (if any)\n        jump_time = None\n        jump_size = 0.0\n        jump_direction = 0\n\n        if has_jump:\n            # Identify the largest return as potential jump\n            abs_returns = np.abs(returns)\n            jump_time = int(np.argmax(abs_returns))\n            jump_size = float(returns[jump_time])\n            jump_direction = 1 if jump_size > 0 else -1\n\n        return JumpResult(\n            has_jump=has_jump,\n            jump_time=jump_time,\n            jump_size=jump_size,\n            jump_direction=jump_direction,\n            continuous_vol=continuous_vol,\n            jump_vol=jump_vol,\n            total_vol=total_vol,\n            test_statistic=z_stat,\n            p_value=z_pval\n        )",
    "source_file": "core\\_experimental\\jump_detection.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JumpTest"
  },
  {
    "name": "test_returns",
    "category": "volatility",
    "formula": "for jump significance. | L = returns[t] / sigma | {",
    "explanation": "Test each return for jump significance.\n\nReturns dict with:\n- is_jump: boolean array\n- jump_stats: test statistics\n- local_vol: local volatility estimates",
    "python_code": "def test_returns(self, returns: np.ndarray) -> Dict:\n        \"\"\"\n        Test each return for jump significance.\n\n        Returns dict with:\n        - is_jump: boolean array\n        - jump_stats: test statistics\n        - local_vol: local volatility estimates\n        \"\"\"\n        n = len(returns)\n        is_jump = np.zeros(n, dtype=bool)\n        jump_stats = np.zeros(n)\n        local_vol = np.zeros(n)\n\n        for t in range(self.window, n):\n            # Local volatility from bipower variation\n            window_returns = returns[t - self.window:t]\n            bpv = self.bpv_calc.bipower_variance(window_returns)\n            sigma = np.sqrt(bpv / self.window)\n            local_vol[t] = sigma\n\n            if sigma > 0:\n                # Standardized return\n                L = returns[t] / sigma\n\n                # Critical value based on extreme value distribution\n                c = np.sqrt(2 * np.log(n)) - (np.log(np.pi) + np.log(np.log(n))) / (2 * np.sqrt(2 * np.log(n)))\n                s = 1 / np.sqrt(2 * np.log(n))\n\n                # Test statistic\n                jump_stats[t] = (np.abs(L) - c) / s\n\n                # Is it a jump? (rejection of null)\n                # Under null, test stat follows standard Gumbel\n                # 99% critical value  4.6\n                critical_value = -np.log(-np.log(1 - self.significance))\n                is_jump[t] = jump_stats[t] > critical_value\n\n        return {\n            'is_jump': is_jump,\n            'jump_stats': jump_stats,\n            'local_vol': local_vol,\n            'jump_times': np.where(is_jump)[0],\n            'n_jumps': np.sum(is_jump)\n        }",
    "source_file": "core\\_experimental\\jump_detection.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "LeeMyklandJumpTest"
  },
  {
    "name": "intensity",
    "category": "quantitative",
    "formula": "intensity",
    "explanation": "Compute intensity at time t.\n\n(t) =  +  exp(-(t - t_i))",
    "python_code": "def intensity(self, t: int) -> float:\n        \"\"\"\n        Compute intensity at time t.\n\n        (t) =  +  exp(-(t - t_i))\n        \"\"\"\n        intensity = self.lambda_0\n\n        for t_i in self.jump_times:\n            if t_i < t:\n                intensity += self.alpha * np.exp(-self.beta * (t - t_i))\n\n        return intensity",
    "source_file": "core\\_experimental\\jump_detection.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JumpHawkesProcess"
  },
  {
    "name": "update",
    "category": "quantitative",
    "formula": "",
    "explanation": "Update process with new observation.",
    "python_code": "def update(self, t: int, is_jump: bool):\n        \"\"\"Update process with new observation.\"\"\"\n        self.current_time = t\n        if is_jump:\n            self.jump_times.append(t)",
    "source_file": "core\\_experimental\\jump_detection.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JumpHawkesProcess"
  },
  {
    "name": "jump_probability",
    "category": "quantitative",
    "formula": "1 - np.exp(-lambda_t)",
    "explanation": "Probability of jump in next period.\n\nP(jump)  1 - exp(-(t) * t)",
    "python_code": "def jump_probability(self, t: int) -> float:\n        \"\"\"\n        Probability of jump in next period.\n\n        P(jump)  1 - exp(-(t) * t)\n        \"\"\"\n        lambda_t = self.intensity(t)\n        return 1 - np.exp(-lambda_t)",
    "source_file": "core\\_experimental\\jump_detection.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JumpHawkesProcess"
  },
  {
    "name": "simulate",
    "category": "quantitative",
    "formula": "jumps",
    "explanation": "Simulate Hawkes process.\n\nReturns array of jump indicators.",
    "python_code": "def simulate(self, n: int) -> np.ndarray:\n        \"\"\"\n        Simulate Hawkes process.\n\n        Returns array of jump indicators.\n        \"\"\"\n        jumps = np.zeros(n, dtype=bool)\n\n        for t in range(n):\n            p = self.jump_probability(t)\n            if np.random.random() < p:\n                jumps[t] = True\n                self.update(t, True)\n            else:\n                self.update(t, False)\n\n        return jumps",
    "source_file": "core\\_experimental\\jump_detection.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JumpHawkesProcess"
  },
  {
    "name": "decompose_volatility",
    "category": "volatility",
    "formula": "VolatilityDecomposition(",
    "explanation": "Full volatility decomposition into continuous and jump components.",
    "python_code": "def decompose_volatility(self, returns: pd.Series) -> VolatilityDecomposition:\n        \"\"\"\n        Full volatility decomposition into continuous and jump components.\n        \"\"\"\n        returns_arr = returns.values\n        n = len(returns_arr)\n\n        # Rolling decomposition\n        rv = np.zeros(n)\n        bpv = np.zeros(n)\n        jump_var = np.zeros(n)\n        jump_ind = np.zeros(n)\n        rj = np.zeros(n)\n\n        window = self.bpv_calc.window\n\n        for t in range(window, n):\n            window_returns = returns_arr[t - window:t]\n\n            rv[t] = self.bpv_calc.realized_variance(window_returns)\n            bpv[t] = self.bpv_calc.bipower_variance(window_returns)\n            jump_var[t] = max(0, rv[t] - bpv[t])\n            rj[t] = jump_var[t] / (rv[t] + 1e-10)\n\n            # Test for jump\n            result = self.jump_test.detect_jumps(window_returns)\n            jump_ind[t] = 1 if result.has_jump else 0\n\n        return VolatilityDecomposition(\n            realized_variance=rv,\n            bipower_variance=bpv,\n            jump_variance=jump_var,\n            jump_indicator=jump_ind,\n            relative_jump=rj\n        )",
    "source_file": "core\\_experimental\\jump_detection.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JumpVolatilityModel"
  },
  {
    "name": "compute_features",
    "category": "volatility",
    "formula": "features",
    "explanation": "Compute jump-related features for ML.",
    "python_code": "def compute_features(self, returns: pd.Series) -> pd.DataFrame:\n        \"\"\"\n        Compute jump-related features for ML.\n        \"\"\"\n        decomp = self.decompose_volatility(returns)\n\n        # Identify individual jumps\n        lm_result = self.lm_test.test_returns(returns.values)\n\n        features = pd.DataFrame(index=returns.index)\n\n        # Volatility components\n        features['rv'] = decomp.realized_variance\n        features['bpv'] = decomp.bipower_variance\n        features['jump_var'] = decomp.jump_variance\n        features['relative_jump'] = decomp.relative_jump\n        features['jump_indicator'] = decomp.jump_indicator\n\n        # Derived features\n        features['continuous_vol'] = np.sqrt(decomp.bipower_variance)\n        features['jump_vol'] = np.sqrt(decomp.jump_variance)\n        features['total_vol'] = np.sqrt(decomp.realized_variance)\n\n        # Jump intensity from Hawkes\n        jump_times = lm_result['jump_times']\n        self.hawkes.jump_times = list(jump_times)\n\n        features['jump_intensity'] = [\n            self.hawkes.intensity(t) for t in range(len(returns))\n        ]\n        features['jump_probability'] = [\n            self.hawkes.jump_probability(t) for t in range(len(returns))\n        ]\n\n        # Recent jump count\n        features['recent_jumps'] = pd.Series(lm_result['is_jump']).rolling(20).sum().values\n\n        # Lee-Mykland statistics\n        features['lm_stat'] = lm_result['jump_stats']\n        features['is_jump'] = lm_result['is_jump'].astype(int)\n\n        return features",
    "source_file": "core\\_experimental\\jump_detection.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "JumpVolatilityModel"
  },
  {
    "name": "compute_jump_features",
    "category": "volatility",
    "formula": "model.compute_features(returns)",
    "explanation": "Convenience function to compute all jump-related features.\n\nArgs:\n    returns: Return series\n    window: Window for volatility estimation\n    significance: Significance level for jump tests\n\nReturns:\n    DataFrame with jump features",
    "python_code": "def compute_jump_features(returns: pd.Series,\n                         window: int = 78,\n                         significance: float = 0.01) -> pd.DataFrame:\n    \"\"\"\n    Convenience function to compute all jump-related features.\n\n    Args:\n        returns: Return series\n        window: Window for volatility estimation\n        significance: Significance level for jump tests\n\n    Returns:\n        DataFrame with jump features\n    \"\"\"\n    model = JumpVolatilityModel(window, significance)\n    return model.compute_features(returns)",
    "source_file": "core\\_experimental\\jump_detection.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "detect_jumps_simple",
    "category": "quantitative",
    "formula": "is a jump if |r| > threshold *  | is_jump",
    "explanation": "Simple threshold-based jump detection.\n\nA return is a jump if |r| > threshold * \n\nArgs:\n    returns: Return array\n    threshold_std: Number of standard deviations for jump threshold\n\nReturns:\n    Boolean array of jump indicators",
    "python_code": "def detect_jumps_simple(returns: np.ndarray,\n                       threshold_std: float = 3.0) -> np.ndarray:\n    \"\"\"\n    Simple threshold-based jump detection.\n\n    A return is a jump if |r| > threshold * \n\n    Args:\n        returns: Return array\n        threshold_std: Number of standard deviations for jump threshold\n\n    Returns:\n        Boolean array of jump indicators\n    \"\"\"\n    # Robust volatility estimate using MAD\n    mad = np.median(np.abs(returns - np.median(returns)))\n    sigma = mad * 1.4826  # Scale to match std dev\n\n    threshold = threshold_std * sigma\n    is_jump = np.abs(returns) > threshold\n\n    return is_jump",
    "source_file": "core\\_experimental\\jump_detection.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "feature_engineering",
    "formula": "rule = 0 for HFT)",
    "explanation": "Args:\n    delay_quotes: Quote delay in ticks (5-second rule = 0 for HFT)",
    "python_code": "def __init__(self, delay_quotes: int = 0):\n        \"\"\"\n        Args:\n            delay_quotes: Quote delay in ticks (5-second rule = 0 for HFT)\n        \"\"\"\n        self.delay_quotes = delay_quotes\n        self.prev_direction = TradeDirection.UNKNOWN",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": null,
    "class_name": "LeeReadyClassifier"
  },
  {
    "name": "classify",
    "category": "execution",
    "formula": "direction",
    "explanation": "Classify single trade.\n\nArgs:\n    trade_price: Trade execution price\n    bid: Best bid at trade time\n    ask: Best ask at trade time\n    prev_trade_price: Previous trade price for tick rule\n\nReturns:\n    TradeDirection",
    "python_code": "def classify(self, trade_price: float, bid: float, ask: float,\n                prev_trade_price: Optional[float] = None) -> TradeDirection:\n        \"\"\"\n        Classify single trade.\n\n        Args:\n            trade_price: Trade execution price\n            bid: Best bid at trade time\n            ask: Best ask at trade time\n            prev_trade_price: Previous trade price for tick rule\n\n        Returns:\n            TradeDirection\n        \"\"\"\n        mid = (bid + ask) / 2\n\n        # Quote rule\n        if trade_price > mid:\n            direction = TradeDirection.BUY\n        elif trade_price < mid:\n            direction = TradeDirection.SELL\n        else:\n            # Tick rule for trades at midpoint\n            if prev_trade_price is not None:\n                if trade_price > prev_trade_price:\n                    direction = TradeDirection.BUY\n                elif trade_price < prev_trade_price:\n                    direction = TradeDirection.SELL\n                else:\n                    # Zero tick - use previous\n                    direction = self.prev_direction\n            else:\n                direction = TradeDirection.UNKNOWN\n\n        self.prev_direction = direction\n        return direction",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": null,
    "class_name": "LeeReadyClassifier"
  },
  {
    "name": "classify_series",
    "category": "feature_engineering",
    "formula": "pd.Series(directions, index=df.index, name='trade_direction')",
    "explanation": "Classify all trades in DataFrame.\n\nArgs:\n    df: DataFrame with 'price', 'bid', 'ask' columns\n\nReturns:\n    Series of trade directions (-1, 0, 1)",
    "python_code": "def classify_series(self, df: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Classify all trades in DataFrame.\n\n        Args:\n            df: DataFrame with 'price', 'bid', 'ask' columns\n\n        Returns:\n            Series of trade directions (-1, 0, 1)\n        \"\"\"\n        directions = []\n        prev_price = None\n\n        for idx, row in df.iterrows():\n            price = row.get('price', row.get('close'))\n            bid = row.get('bid', price * 0.9999)\n            ask = row.get('ask', price * 1.0001)\n\n            direction = self.classify(price, bid, ask, prev_price)\n            directions.append(direction.value)\n            prev_price = price\n\n        return pd.Series(directions, index=df.index, name='trade_direction')",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": null,
    "class_name": "LeeReadyClassifier"
  },
  {
    "name": "__init__",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, n_levels: int = 5, decay: float = 0.5):\n        self.n_levels = n_levels\n        self.decay = decay",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": null,
    "class_name": "BookImbalance"
  },
  {
    "name": "calculate_level1",
    "category": "feature_engineering",
    "formula": "0.0 | (bid_size - ask_size) / total",
    "explanation": "Level 1 (BBO) imbalance.",
    "python_code": "def calculate_level1(self, bid_size: float, ask_size: float) -> float:\n        \"\"\"Level 1 (BBO) imbalance.\"\"\"\n        total = bid_size + ask_size\n        if total == 0:\n            return 0.0\n        return (bid_size - ask_size) / total",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": null,
    "class_name": "BookImbalance"
  },
  {
    "name": "calculate_weighted",
    "category": "statistical",
    "formula": "0.0 | numerator / denominator",
    "explanation": "Weighted multi-level imbalance.\n\nDeeper levels have exponentially decaying weight.",
    "python_code": "def calculate_weighted(self, bids: List[Tuple[float, float]],\n                          asks: List[Tuple[float, float]]) -> float:\n        \"\"\"\n        Weighted multi-level imbalance.\n\n        Deeper levels have exponentially decaying weight.\n        \"\"\"\n        numerator = 0.0\n        denominator = 0.0\n\n        for i in range(min(self.n_levels, len(bids), len(asks))):\n            weight = self.decay ** i\n            bid_size = bids[i][1]\n            ask_size = asks[i][1]\n\n            numerator += weight * (bid_size - ask_size)\n            denominator += weight * (bid_size + ask_size)\n\n        if denominator == 0:\n            return 0.0\n        return numerator / denominator",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": null,
    "class_name": "BookImbalance"
  },
  {
    "name": "calculate_volume_imbalance",
    "category": "feature_engineering",
    "formula": "0.0 | (total_bid - total_ask) / total",
    "explanation": "Total volume imbalance across all levels.",
    "python_code": "def calculate_volume_imbalance(self, bids: List[Tuple[float, float]],\n                                   asks: List[Tuple[float, float]]) -> float:\n        \"\"\"Total volume imbalance across all levels.\"\"\"\n        total_bid = sum(size for _, size in bids[:self.n_levels])\n        total_ask = sum(size for _, size in asks[:self.n_levels])\n\n        total = total_bid + total_ask\n        if total == 0:\n            return 0.0\n        return (total_bid - total_ask) / total",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": null,
    "class_name": "BookImbalance"
  },
  {
    "name": "calculate_depth_ratio",
    "category": "feature_engineering",
    "formula": "0.0 | (bid_depth - ask_depth) / total",
    "explanation": "Depth ratio within price range.\n\nMeasures liquidity asymmetry.",
    "python_code": "def calculate_depth_ratio(self, bids: List[Tuple[float, float]],\n                             asks: List[Tuple[float, float]],\n                             price_range_pct: float = 0.001) -> float:\n        \"\"\"\n        Depth ratio within price range.\n\n        Measures liquidity asymmetry.\n        \"\"\"\n        mid = (bids[0][0] + asks[0][0]) / 2\n        range_size = mid * price_range_pct\n\n        bid_depth = sum(size for price, size in bids if price >= mid - range_size)\n        ask_depth = sum(size for price, size in asks if price <= mid + range_size)\n\n        total = bid_depth + ask_depth\n        if total == 0:\n            return 0.0\n        return (bid_depth - ask_depth) / total",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": null,
    "class_name": "BookImbalance"
  },
  {
    "name": "calculate",
    "category": "feature_engineering",
    "formula": "0.0 | 0.0 | (bid_contrib + ask_contrib) / total",
    "explanation": "Calculate queue imbalance from quote changes.\n\nReturns imbalance in [-1, 1] range.",
    "python_code": "def calculate(self, bid: float, ask: float,\n                 bid_size: float, ask_size: float) -> float:\n        \"\"\"\n        Calculate queue imbalance from quote changes.\n\n        Returns imbalance in [-1, 1] range.\n        \"\"\"\n        if self.prev_bid_price is None:\n            self.prev_bid_price = bid\n            self.prev_ask_price = ask\n            self.prev_bid_size = bid_size\n            self.prev_ask_size = ask_size\n            return 0.0\n\n        # Bid side contribution\n        if bid > self.prev_bid_price:\n            bid_contrib = bid_size  # New aggressive bid\n        elif bid == self.prev_bid_price:\n            bid_contrib = bid_size - self.prev_bid_size  # Size change\n        else:\n            bid_contrib = -self.prev_bid_size  # Bid dropped\n\n        # Ask side contribution\n        if ask < self.prev_ask_price:\n            ask_contrib = -ask_size  # New aggressive ask\n        elif ask == self.prev_ask_price:\n            ask_contrib = -(ask_size - self.prev_ask_size)  # Size change\n        else:\n            ask_contrib = self.prev_ask_size  # Ask lifted\n\n        # Update state\n        self.prev_bid_price = bid\n        self.prev_ask_price = ask\n        self.prev_bid_size = bid_size\n        self.prev_ask_size = ask_size\n\n        # Normalize\n        total = abs(bid_contrib) + abs(ask_contrib)\n        if total == 0:\n            return 0.0\n        return (bid_contrib + ask_contrib) / total",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": null,
    "class_name": "QueueImbalance"
  },
  {
    "name": "calculate_realized_spread",
    "category": "microstructure",
    "formula": "Realized_Spread = 2 * direction * (trade_price - mid_future) | realized_spread.fillna(0)",
    "explanation": "Realized spread - actual profit to liquidity provider.\n\nRealized_Spread = 2 * direction * (trade_price - mid_future)",
    "python_code": "def calculate_realized_spread(self, df: pd.DataFrame,\n                                  horizon: int = 10) -> pd.Series:\n        \"\"\"\n        Realized spread - actual profit to liquidity provider.\n\n        Realized_Spread = 2 * direction * (trade_price - mid_future)\n        \"\"\"\n        mid = (df['bid'] + df['ask']) / 2\n        mid_future = mid.shift(-horizon)\n        trade_price = df['close']\n\n        # Estimate direction\n        direction = np.sign(trade_price - mid)\n\n        realized_spread = 2 * direction * (trade_price - mid_future)\n        return realized_spread.fillna(0)",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "SpreadDecomposition"
  },
  {
    "name": "calculate_effective_spread",
    "category": "microstructure",
    "formula": "Effective_Spread = 2 * |trade_price - mid| | effective_spread",
    "explanation": "Effective spread - actual cost to liquidity taker.\n\nEffective_Spread = 2 * |trade_price - mid|",
    "python_code": "def calculate_effective_spread(self, df: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Effective spread - actual cost to liquidity taker.\n\n        Effective_Spread = 2 * |trade_price - mid|\n        \"\"\"\n        mid = (df['bid'] + df['ask']) / 2\n        trade_price = df['close']\n\n        effective_spread = 2 * np.abs(trade_price - mid)\n        return effective_spread",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": null,
    "class_name": "SpreadDecomposition"
  },
  {
    "name": "calculate_adverse_selection",
    "category": "microstructure",
    "formula": "AS = Effective_Spread - Realized_Spread | effective - realized",
    "explanation": "Adverse selection component.\n\nAS = Effective_Spread - Realized_Spread",
    "python_code": "def calculate_adverse_selection(self, df: pd.DataFrame,\n                                   horizon: int = 10) -> pd.Series:\n        \"\"\"\n        Adverse selection component.\n\n        AS = Effective_Spread - Realized_Spread\n        \"\"\"\n        effective = self.calculate_effective_spread(df)\n        realized = self.calculate_realized_spread(df, horizon)\n\n        return effective - realized",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": null,
    "class_name": "SpreadDecomposition"
  },
  {
    "name": "decompose",
    "category": "microstructure",
    "formula": "{",
    "explanation": "Full spread decomposition.",
    "python_code": "def decompose(self, df: pd.DataFrame) -> Dict[str, pd.Series]:\n        \"\"\"Full spread decomposition.\"\"\"\n        return {\n            'quoted_spread': df['ask'] - df['bid'],\n            'effective_spread': self.calculate_effective_spread(df),\n            'realized_spread': self.calculate_realized_spread(df),\n            'adverse_selection': self.calculate_adverse_selection(df)\n        }",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": null,
    "class_name": "SpreadDecomposition"
  },
  {
    "name": "calculate_slope",
    "category": "feature_engineering",
    "formula": "Slope = d(cumulative_size) / d(price_distance) | 0.0 | float('inf')  # All at same price",
    "explanation": "Calculate slope using linear regression.\n\nSlope = d(cumulative_size) / d(price_distance)",
    "python_code": "def calculate_slope(self, prices: List[float], sizes: List[float]) -> float:\n        \"\"\"\n        Calculate slope using linear regression.\n\n        Slope = d(cumulative_size) / d(price_distance)\n        \"\"\"\n        if len(prices) < 2:\n            return 0.0\n\n        cumulative_sizes = np.cumsum(sizes)\n        price_distances = np.abs(np.array(prices) - prices[0])\n\n        if price_distances[-1] == 0:\n            return float('inf')  # All at same price\n\n        # Simple slope\n        slope = cumulative_sizes[-1] / price_distances[-1]\n        return slope",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": null,
    "class_name": "LOBSlope"
  },
  {
    "name": "calculate_bid_slope",
    "category": "feature_engineering",
    "formula": "positive = thick book).",
    "explanation": "Bid side slope (positive = thick book).",
    "python_code": "def calculate_bid_slope(self, bids: List[Tuple[float, float]]) -> float:\n        \"\"\"Bid side slope (positive = thick book).\"\"\"\n        prices = [p for p, _ in bids[:self.n_levels]]\n        sizes = [s for _, s in bids[:self.n_levels]]\n        return self.calculate_slope(prices, sizes)",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": null,
    "class_name": "LOBSlope"
  },
  {
    "name": "calculate_ask_slope",
    "category": "feature_engineering",
    "formula": "positive = thick book).",
    "explanation": "Ask side slope (positive = thick book).",
    "python_code": "def calculate_ask_slope(self, asks: List[Tuple[float, float]]) -> float:\n        \"\"\"Ask side slope (positive = thick book).\"\"\"\n        prices = [p for p, _ in asks[:self.n_levels]]\n        sizes = [s for _, s in asks[:self.n_levels]]\n        return self.calculate_slope(prices, sizes)",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": null,
    "class_name": "LOBSlope"
  },
  {
    "name": "calculate_slope_imbalance",
    "category": "feature_engineering",
    "formula": "Positive = bid side thicker = buy pressure | 0.0 | (bid_slope - ask_slope) / (bid_slope + ask_slope)",
    "explanation": "Slope imbalance.\n\nPositive = bid side thicker = buy pressure",
    "python_code": "def calculate_slope_imbalance(self, bids: List[Tuple[float, float]],\n                                  asks: List[Tuple[float, float]]) -> float:\n        \"\"\"\n        Slope imbalance.\n\n        Positive = bid side thicker = buy pressure\n        \"\"\"\n        bid_slope = self.calculate_bid_slope(bids)\n        ask_slope = self.calculate_ask_slope(asks)\n\n        if bid_slope + ask_slope == 0:\n            return 0.0\n\n        return (bid_slope - ask_slope) / (bid_slope + ask_slope)",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": null,
    "class_name": "LOBSlope"
  },
  {
    "name": "calculate",
    "category": "microstructure",
    "formula": "Resilience = Speed of spread recovery after widening | pd.Series(resilience, index=df.index).rolling(50).mean().fillna(1.0)",
    "explanation": "Calculate resilience metric.\n\nResilience = Speed of spread recovery after widening",
    "python_code": "def calculate(self, df: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Calculate resilience metric.\n\n        Resilience = Speed of spread recovery after widening\n        \"\"\"\n        spread = df['ask'] - df['bid']\n        spread_ma = spread.rolling(50).mean()\n\n        # Detect spread widening events\n        spread_deviation = (spread - spread_ma) / (spread_ma + 1e-10)\n        widening = spread_deviation > 0.5  # 50% above average\n\n        # Measure recovery time\n        resilience = []\n        for i in range(len(df)):\n            if widening.iloc[i]:\n                # Look forward for recovery\n                recovery_time = self.recovery_window\n                for j in range(1, min(self.recovery_window + 1, len(df) - i)):\n                    if spread_deviation.iloc[i + j] < 0.2:\n                        recovery_time = j\n                        break\n                # Faster recovery = higher resilience\n                resilience.append(1.0 / recovery_time)\n            else:\n                resilience.append(1.0)  # Normal conditions\n\n        return pd.Series(resilience, index=df.index).rolling(50).mean().fillna(1.0)",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": null,
    "class_name": "Resilience"
  },
  {
    "name": "calibrate",
    "category": "microstructure",
    "formula": "",
    "explanation": "Calibrate impact model from historical data.\n\nReturns estimated sigma.",
    "python_code": "def calibrate(self, df: pd.DataFrame, window: int = 100) -> float:\n        \"\"\"\n        Calibrate impact model from historical data.\n\n        Returns estimated sigma.\n        \"\"\"\n        returns = df['close'].pct_change().abs()\n        volume = df['volume'] if 'volume' in df.columns else pd.Series(1, index=df.index)\n\n        # Regression: |return| = sigma * volume^delta\n        log_returns = np.log(returns + 1e-10)\n        log_volume = np.log(volume + 1)\n\n        # Rolling estimation\n        cov = log_returns.rolling(window).cov(log_volume)\n        var = log_volume.rolling(window).var()\n\n        estimated_delta = cov / (var + 1e-10)\n\n        # Use median delta for sigma estimation\n        self.delta = estimated_delta.median()\n        self.sigma = (returns / (volume ** self.delta + 1e-10)).rolling(window).mean().iloc[-1]\n\n        return self.sigma",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "PriceImpactCurve"
  },
  {
    "name": "estimate_impact",
    "category": "volatility",
    "formula": "volatility * self.sigma * np.sign(order_size) * (abs(order_size) ** self.delta)",
    "explanation": "Estimate price impact for given order size.\n\nArgs:\n    order_size: Order size (can be negative for sells)\n    volatility: Current volatility estimate\n\nReturns:\n    Expected price impact in price units",
    "python_code": "def estimate_impact(self, order_size: float, volatility: float) -> float:\n        \"\"\"\n        Estimate price impact for given order size.\n\n        Args:\n            order_size: Order size (can be negative for sells)\n            volatility: Current volatility estimate\n\n        Returns:\n            Expected price impact in price units\n        \"\"\"\n        if self.sigma is None:\n            self.sigma = 0.1  # Default\n\n        return volatility * self.sigma * np.sign(order_size) * (abs(order_size) ** self.delta)",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": "Almgren & Chriss (2001) 'Optimal Execution' J. Risk",
    "class_name": "PriceImpactCurve"
  },
  {
    "name": "generate_all_features",
    "category": "feature_engineering",
    "formula": "result",
    "explanation": "Generate all LOB features.\n\nArgs:\n    df: DataFrame with bid, ask, volume columns\n\nReturns:\n    DataFrame with LOB feature columns added",
    "python_code": "def generate_all_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all LOB features.\n\n        Args:\n            df: DataFrame with bid, ask, volume columns\n\n        Returns:\n            DataFrame with LOB feature columns added\n        \"\"\"\n        result = df.copy()\n\n        # Trade direction (Lee-Ready)\n        result['trade_direction'] = self.lee_ready.classify_series(df)\n\n        # Trade imbalance\n        volume = df['volume'] if 'volume' in df.columns else pd.Series(1, index=df.index)\n        result['trade_imbalance'] = (result['trade_direction'] * volume).rolling(20).sum()\n\n        # Book imbalance (L1)\n        if 'bid_size' in df.columns and 'ask_size' in df.columns:\n            result['book_imbalance_l1'] = df.apply(\n                lambda row: self.book_imbalance.calculate_level1(\n                    row['bid_size'], row['ask_size']\n                ), axis=1\n            )\n\n        # Queue imbalance\n        if 'bid' in df.columns and 'ask' in df.columns:\n            bid_size = df['bid_size'] if 'bid_size' in df.columns else pd.Series(1, index=df.index)\n            ask_size = df['ask_size'] if 'ask_size' in df.columns else pd.Series(1, index=df.index)\n\n            queue_imb = []\n            for bid, ask, bs, ask_s in zip(df['bid'], df['ask'], bid_size, ask_size):\n                queue_imb.append(self.queue_imbalance.calculate(bid, ask, bs, ask_s))\n            result['queue_imbalance'] = queue_imb\n\n        # Spread decomposition\n        if 'bid' in df.columns and 'ask' in df.columns:\n            spread_features = self.spread_decomp.decompose(df)\n            for name, series in spread_features.items():\n                result[f'spread_{name}'] = series\n\n        # Resilience\n        if 'bid' in df.columns and 'ask' in df.columns:\n            result['resilience'] = self.resilience.calculate(df)\n\n        # Calibrate price impact\n        self.price_impact.calibrate(df)\n        result['impact_sigma'] = self.price_impact.sigma\n\n        logger.info(f\"Generated {len([c for c in result.columns if c not in df.columns])} LOB features\")\n\n        return result",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": null,
    "class_name": "LOBFeatureEngine"
  },
  {
    "name": "get_feature_names",
    "category": "feature_engineering",
    "formula": "[",
    "explanation": "Return list of feature names.",
    "python_code": "def get_feature_names(self) -> List[str]:\n        \"\"\"Return list of feature names.\"\"\"\n        return [\n            'trade_direction', 'trade_imbalance',\n            'book_imbalance_l1', 'queue_imbalance',\n            'spread_quoted_spread', 'spread_effective_spread',\n            'spread_realized_spread', 'spread_adverse_selection',\n            'resilience', 'impact_sigma'\n        ]",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": null,
    "class_name": "LOBFeatureEngine"
  },
  {
    "name": "compute_all_features",
    "category": "feature_engineering",
    "formula": "",
    "explanation": "Compute all LOB features.\nInterface compatible with HFT Feature Engine.",
    "python_code": "def compute_all_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Compute all LOB features.\n        Interface compatible with HFT Feature Engine.\n        \"\"\"\n        return self.generate_all_features(df)",
    "source_file": "core\\_experimental\\lob_features.py",
    "academic_reference": null,
    "class_name": "LOBFeatureEngine"
  },
  {
    "name": "__init__",
    "category": "microstructure",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self):\n        self.lambda_ = None\n        self.sigma_v = None\n        self.sigma_u = None",
    "source_file": "core\\_experimental\\market_impact_models.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "KyleModel"
  },
  {
    "name": "estimate_lambda",
    "category": "microstructure",
    "formula": "Lambda = Cov(P, V) / Var(V) | np.nan | cov / var",
    "explanation": "Estimate Kyle's lambda from price and volume data.\n\nLambda = Cov(P, V) / Var(V)\n\nArgs:\n    prices: Price series\n    volumes: Volume series (signed: + for buy, - for sell)\n    window: Rolling window for estimation\n\nReturns:\n    Rolling lambda estimates",
    "python_code": "def estimate_lambda(\n        self,\n        prices: pd.Series,\n        volumes: pd.Series,\n        window: int = 20\n    ) -> pd.Series:\n        \"\"\"\n        Estimate Kyle's lambda from price and volume data.\n\n        Lambda = Cov(P, V) / Var(V)\n\n        Args:\n            prices: Price series\n            volumes: Volume series (signed: + for buy, - for sell)\n            window: Rolling window for estimation\n\n        Returns:\n            Rolling lambda estimates\n        \"\"\"\n        price_changes = prices.diff()\n\n        # Estimate lambda via regression: P = lambda * V + epsilon\n        def estimate_single(dp, vol):\n            if len(dp) < 5:\n                return np.nan\n            # OLS: lambda = Cov(P, V) / Var(V)\n            cov = np.cov(dp, vol)[0, 1]\n            var = np.var(vol)\n            if var > 0:\n                return cov / var\n            return np.nan\n\n        lambda_series = price_changes.rolling(window).apply(\n            lambda x: estimate_single(x.values, volumes.loc[x.index].values),\n            raw=False\n        )\n\n        return lambda_series",
    "source_file": "core\\_experimental\\market_impact_models.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "KyleModel"
  },
  {
    "name": "estimate_from_trades",
    "category": "volatility",
    "formula": "P = lambda * sign * size + epsilon | MarketImpactEstimate(",
    "explanation": "Estimate Kyle model parameters from trade data.\n\nUses regression: P = lambda * sign * size + epsilon\n\nArgs:\n    trade_prices: Transaction prices\n    trade_signs: +1 for buyer-initiated, -1 for seller-initiated\n    trade_sizes: Trade sizes\n\nReturns:\n    MarketImpactEstimate with lambda and volatility parameters",
    "python_code": "def estimate_from_trades(\n        self,\n        trade_prices: pd.Series,\n        trade_signs: pd.Series,\n        trade_sizes: pd.Series\n    ) -> MarketImpactEstimate:\n        \"\"\"\n        Estimate Kyle model parameters from trade data.\n\n        Uses regression: P = lambda * sign * size + epsilon\n\n        Args:\n            trade_prices: Transaction prices\n            trade_signs: +1 for buyer-initiated, -1 for seller-initiated\n            trade_sizes: Trade sizes\n\n        Returns:\n            MarketImpactEstimate with lambda and volatility parameters\n        \"\"\"\n        price_changes = trade_prices.diff().dropna()\n        signed_volume = (trade_signs * trade_sizes).iloc[1:]\n\n        # Align indices\n        idx = price_changes.index.intersection(signed_volume.index)\n        dp = price_changes.loc[idx].values\n        sv = signed_volume.loc[idx].values\n\n        # OLS regression\n        lambda_ = np.cov(dp, sv)[0, 1] / np.var(sv) if np.var(sv) > 0 else 0\n\n        # Estimate volatilities\n        sigma_dp = np.std(dp)\n        sigma_sv = np.std(sv)\n\n        # From Kyle model: sigma_dp^2 = lambda^2 * sigma_sv^2\n        # and lambda = sigma_v / (2 * sigma_u)\n        sigma_u = sigma_sv / 2 if sigma_sv > 0 else 0\n        sigma_v = 2 * lambda_ * sigma_u if lambda_ > 0 else 0\n\n        self.lambda_ = lambda_\n        self.sigma_v = sigma_v\n        self.sigma_u = sigma_u\n\n        return MarketImpactEstimate(\n            lambda_=lambda_,\n            sigma_u=sigma_u,\n            sigma_v=sigma_v,\n            mu=0,  # Not directly estimated\n            method='Kyle_OLS'\n        )",
    "source_file": "core\\_experimental\\market_impact_models.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "KyleModel"
  },
  {
    "name": "expected_price_impact",
    "category": "microstructure",
    "formula": "P = lambda * order_size",
    "explanation": "Calculate expected price impact for given order size.\n\nP = lambda * order_size\n\nArgs:\n    order_size: Signed order size (+ for buy, - for sell)\n\nReturns:\n    Expected price change",
    "python_code": "def expected_price_impact(self, order_size: float) -> float:\n        \"\"\"\n        Calculate expected price impact for given order size.\n\n        P = lambda * order_size\n\n        Args:\n            order_size: Signed order size (+ for buy, - for sell)\n\n        Returns:\n            Expected price change\n        \"\"\"\n        if self.lambda_ is None:\n            raise ValueError(\"Model not fitted. Call estimate_from_trades first.\")\n        return self.lambda_ * order_size",
    "source_file": "core\\_experimental\\market_impact_models.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "KyleModel"
  },
  {
    "name": "optimal_trade_intensity",
    "category": "microstructure",
    "formula": "total_order / time_horizon",
    "explanation": "Calculate optimal trading rate (from Kyle model).\n\nOptimal intensity minimizes total execution cost.\n\nArgs:\n    total_order: Total order to execute\n    time_horizon: Time available for execution\n\nReturns:\n    Optimal order size per period",
    "python_code": "def optimal_trade_intensity(self, total_order: float, time_horizon: float) -> float:\n        \"\"\"\n        Calculate optimal trading rate (from Kyle model).\n\n        Optimal intensity minimizes total execution cost.\n\n        Args:\n            total_order: Total order to execute\n            time_horizon: Time available for execution\n\n        Returns:\n            Optimal order size per period\n        \"\"\"\n        # In Kyle model, informed trader trades at constant rate\n        return total_order / time_horizon",
    "source_file": "core\\_experimental\\market_impact_models.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "KyleModel"
  },
  {
    "name": "estimate_adverse_selection",
    "category": "microstructure",
    "formula": "P = c * sign + z * sign * size + epsilon | c = adverse selection, z = inventory/order processing | 0.0, 0.0",
    "explanation": "Estimate adverse selection component of spread.\n\nUses Glosten-Harris (1988) decomposition:\nP = c * sign + z * sign * size + epsilon\n\nwhere c = adverse selection, z = inventory/order processing\n\nArgs:\n    trades: DataFrame with 'price', 'sign', 'size' columns\n    mid_prices: Mid-price series\n\nReturns:\n    Tuple of (adverse_selection, inventory_component)",
    "python_code": "def estimate_adverse_selection(\n        self,\n        trades: pd.DataFrame,\n        mid_prices: pd.Series\n    ) -> Tuple[float, float]:\n        \"\"\"\n        Estimate adverse selection component of spread.\n\n        Uses Glosten-Harris (1988) decomposition:\n        P = c * sign + z * sign * size + epsilon\n\n        where c = adverse selection, z = inventory/order processing\n\n        Args:\n            trades: DataFrame with 'price', 'sign', 'size' columns\n            mid_prices: Mid-price series\n\n        Returns:\n            Tuple of (adverse_selection, inventory_component)\n        \"\"\"\n        # Price changes relative to mid\n        price_change = trades['price'].diff()\n\n        # Features\n        sign = trades['sign']\n        sign_size = trades['sign'] * trades['size']\n\n        # Stack features\n        X = pd.DataFrame({\n            'sign': sign.iloc[1:],\n            'sign_size': sign_size.iloc[1:]\n        })\n        y = price_change.iloc[1:]\n\n        # Align and clean\n        idx = X.dropna().index.intersection(y.dropna().index)\n        X_clean = X.loc[idx].values\n        y_clean = y.loc[idx].values\n\n        if len(y_clean) < 10:\n            return 0.0, 0.0\n\n        # OLS\n        X_aug = np.column_stack([np.ones(len(X_clean)), X_clean])\n        try:\n            beta = np.linalg.lstsq(X_aug, y_clean, rcond=None)[0]\n            adverse_selection = beta[1]  # Coefficient on sign\n            inventory = beta[2]  # Coefficient on sign * size\n        except:\n            adverse_selection = 0.0\n            inventory = 0.0\n\n        self.spread = 2 * adverse_selection  # Full spread\n\n        return adverse_selection, inventory",
    "source_file": "core\\_experimental\\market_impact_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GlostenMilgromModel"
  },
  {
    "name": "probability_informed",
    "category": "volatility",
    "formula": "0.0",
    "explanation": "Estimate probability of informed trading (mu).\n\nFrom Glosten-Milgrom model:\nSpread  2 * mu * delta_v\n\nwhere delta_v  volatility / sqrt(trade_frequency)\n\nArgs:\n    spread: Bid-ask spread\n    volatility: Price volatility\n    trade_frequency: Number of trades per period\n\nReturns:\n    Estimated probability of informed trading",
    "python_code": "def probability_informed(\n        self,\n        spread: float,\n        volatility: float,\n        trade_frequency: float\n    ) -> float:\n        \"\"\"\n        Estimate probability of informed trading (mu).\n\n        From Glosten-Milgrom model:\n        Spread  2 * mu * delta_v\n\n        where delta_v  volatility / sqrt(trade_frequency)\n\n        Args:\n            spread: Bid-ask spread\n            volatility: Price volatility\n            trade_frequency: Number of trades per period\n\n        Returns:\n            Estimated probability of informed trading\n        \"\"\"\n        if trade_frequency <= 0 or volatility <= 0:\n            return 0.0\n\n        delta_v = volatility / np.sqrt(trade_frequency)\n        mu = spread / (2 * delta_v) if delta_v > 0 else 0\n\n        self.mu = np.clip(mu, 0, 1)\n        self.delta_v = delta_v\n\n        return self.mu",
    "source_file": "core\\_experimental\\market_impact_models.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "GlostenMilgromModel"
  },
  {
    "name": "estimate_spread",
    "category": "microstructure",
    "formula": "Spread = 2 * sqrt(-Cov(P_t, P_{t-1})) | np.nan | 0  # Positive autocov implies no valid spread",
    "explanation": "Estimate effective spread using Roll model.\n\nSpread = 2 * sqrt(-Cov(P_t, P_{t-1}))\n\nArgs:\n    prices: Transaction price series\n    window: Rolling window for estimation\n\nReturns:\n    Rolling spread estimates",
    "python_code": "def estimate_spread(\n        prices: pd.Series,\n        window: int = 50\n    ) -> pd.Series:\n        \"\"\"\n        Estimate effective spread using Roll model.\n\n        Spread = 2 * sqrt(-Cov(P_t, P_{t-1}))\n\n        Args:\n            prices: Transaction price series\n            window: Rolling window for estimation\n\n        Returns:\n            Rolling spread estimates\n        \"\"\"\n        returns = prices.diff()\n\n        def calc_spread(rets):\n            if len(rets) < 3:\n                return np.nan\n            # First-order autocovariance\n            autocov = np.cov(rets[:-1], rets[1:])[0, 1]\n            if autocov >= 0:\n                return 0  # Positive autocov implies no valid spread\n            return 2 * np.sqrt(-autocov)\n\n        spread = returns.rolling(window).apply(calc_spread, raw=True)\n\n        return spread",
    "source_file": "core\\_experimental\\market_impact_models.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "RollModel"
  },
  {
    "name": "effective_spread_single",
    "category": "microstructure",
    "formula": "np.nan | 0 | 2 * np.sqrt(-autocov)",
    "explanation": "Single-period Roll spread estimate.\n\nArgs:\n    price_changes: Array of price changes\n\nReturns:\n    Effective spread estimate",
    "python_code": "def effective_spread_single(price_changes: np.ndarray) -> float:\n        \"\"\"\n        Single-period Roll spread estimate.\n\n        Args:\n            price_changes: Array of price changes\n\n        Returns:\n            Effective spread estimate\n        \"\"\"\n        if len(price_changes) < 3:\n            return np.nan\n        autocov = np.cov(price_changes[:-1], price_changes[1:])[0, 1]\n        if autocov >= 0:\n            return 0\n        return 2 * np.sqrt(-autocov)",
    "source_file": "core\\_experimental\\market_impact_models.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "RollModel"
  },
  {
    "name": "decompose_spread",
    "category": "microstructure",
    "formula": "Q_t = S/2 * (Q_t - *Q_{t-1}) + _t | S = total spread | selection = S * (1 - )",
    "explanation": "Decompose spread into components.\n\nModel:\nQ_t = S/2 * (Q_t - *Q_{t-1}) + _t\n\nwhere:\n- S = total spread\n-  = probability of trade reversal\n- Adverse selection = S * (1 - )\n- Inventory + Order processing = S * \n\nArgs:\n    trades: DataFrame with 'price', 'time' columns\n    quotes: DataFrame with 'bid', 'ask', 'time' columns\n\nReturns:\n    Dict with spread components",
    "python_code": "def decompose_spread(\n        self,\n        trades: pd.DataFrame,\n        quotes: pd.DataFrame\n    ) -> Dict[str, float]:\n        \"\"\"\n        Decompose spread into components.\n\n        Model:\n        Q_t = S/2 * (Q_t - *Q_{t-1}) + _t\n\n        where:\n        - S = total spread\n        -  = probability of trade reversal\n        - Adverse selection = S * (1 - )\n        - Inventory + Order processing = S * \n\n        Args:\n            trades: DataFrame with 'price', 'time' columns\n            quotes: DataFrame with 'bid', 'ask', 'time' columns\n\n        Returns:\n            Dict with spread components\n        \"\"\"\n        # Merge trades with quotes\n        merged = pd.merge_asof(\n            trades.sort_values('time'),\n            quotes.sort_values('time'),\n            on='time',\n            direction='backward'\n        )\n\n        # Quote midpoint\n        merged['mid'] = (merged['bid'] + merged['ask']) / 2\n        merged['spread'] = merged['ask'] - merged['bid']\n\n        # Trade indicator: +1 if above mid, -1 if below\n        merged['sign'] = np.sign(merged['price'] - merged['mid'])\n\n        # Lagged sign\n        merged['sign_lag'] = merged['sign'].shift(1)\n\n        # Quote revision\n        merged['quote_change'] = merged['mid'].diff()\n\n        # Regression: Q = *S/2*sign + (1-)*S/2*sign_lag + \n        # where  = adverse selection proportion\n\n        X = merged[['sign', 'sign_lag']].iloc[1:].dropna()\n        y = merged['quote_change'].iloc[1:].loc[X.index]\n        spread_mean = merged['spread'].mean() / 2  # Half spread\n\n        if len(X) < 10:\n            return {'adverse_selection': 0, 'inventory': 0, 'order_processing': 0}\n\n        try:\n            X_aug = np.column_stack([np.ones(len(X)), X.values])\n            beta = np.linalg.lstsq(X_aug, y.values, rcond=None)[0]\n\n            # Coefficients represent proportions of half-spread\n            adverse_selection = beta[1] / spread_mean if spread_mean > 0 else 0\n            inventory = (1 - adverse_selection) * 0.5  # Split remaining\n            order_processing = (1 - adverse_selection) * 0.5\n\n            # Clip to valid range\n            adverse_selection = np.clip(adverse_selection, 0, 1)\n\n        except:\n            adverse_selection = 0.33\n            inventory = 0.33\n            order_processing = 0.34\n\n        self.adverse_selection = adverse_selection\n        self.inventory_cost = inventory\n        self.order_processing = order_processing\n        self.total_spread = merged['spread'].mean()\n\n        return {\n            'adverse_selection': adverse_selection,\n            'inventory': inventory,\n            'order_processing': order_processing,\n            'total_spread': self.total_spread\n        }",
    "source_file": "core\\_experimental\\market_impact_models.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "HuangStollModel"
  },
  {
    "name": "estimate_var",
    "category": "microstructure",
    "formula": "r_t = (a_j * r_{t-j}) + (b_j * x_{t-j}) + _t | x_t = (c_j * r_{t-j}) + (d_j * x_{t-j}) + _t | x_t = trade sign (+1 buy, -1 sell)",
    "explanation": "Estimate VAR model for returns and trade signs.\n\nr_t = (a_j * r_{t-j}) + (b_j * x_{t-j}) + _t\nx_t = (c_j * r_{t-j}) + (d_j * x_{t-j}) + _t\n\nwhere x_t = trade sign (+1 buy, -1 sell)\n\nArgs:\n    returns: Return series\n    trade_signs: Trade sign series\n    lags: Number of VAR lags\n\nReturns:\n    Dict with VAR coefficients",
    "python_code": "def estimate_var(\n        self,\n        returns: pd.Series,\n        trade_signs: pd.Series,\n        lags: int = 5\n    ) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Estimate VAR model for returns and trade signs.\n\n        r_t = (a_j * r_{t-j}) + (b_j * x_{t-j}) + _t\n        x_t = (c_j * r_{t-j}) + (d_j * x_{t-j}) + _t\n\n        where x_t = trade sign (+1 buy, -1 sell)\n\n        Args:\n            returns: Return series\n            trade_signs: Trade sign series\n            lags: Number of VAR lags\n\n        Returns:\n            Dict with VAR coefficients\n        \"\"\"\n        from scipy.linalg import lstsq\n\n        # Build lag matrix\n        n = len(returns)\n        Y = np.column_stack([returns.values, trade_signs.values])[lags:]\n\n        X_list = [np.ones(n - lags)]\n        for lag in range(1, lags + 1):\n            X_list.append(returns.shift(lag).values[lags:])\n            X_list.append(trade_signs.shift(lag).values[lags:])\n\n        X = np.column_stack(X_list)\n\n        # Estimate VAR\n        try:\n            coef, _, _, _ = lstsq(X, Y)\n            self.var_coef = coef\n        except:\n            self.var_coef = np.zeros((2 * lags + 1, 2))\n\n        return {'coefficients': self.var_coef}",
    "source_file": "core\\_experimental\\market_impact_models.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "HasbrouckModel"
  },
  {
    "name": "information_share",
    "category": "microstructure",
    "formula": "share = variance of permanent component / total variance | 0.5 | 0.5",
    "explanation": "Calculate information share of trades.\n\nInformation share = variance of permanent component / total variance\n\nArgs:\n    returns: Return series\n    trade_signs: Trade sign series\n\nReturns:\n    Information share (0 to 1)",
    "python_code": "def information_share(\n        self,\n        returns: pd.Series,\n        trade_signs: pd.Series\n    ) -> float:\n        \"\"\"\n        Calculate information share of trades.\n\n        Information share = variance of permanent component / total variance\n\n        Args:\n            returns: Return series\n            trade_signs: Trade sign series\n\n        Returns:\n            Information share (0 to 1)\n        \"\"\"\n        # Estimate VAR first\n        self.estimate_var(returns, trade_signs)\n\n        # Calculate impulse response to trade shock\n        # Long-run impact of a trade = sum of b coefficients\n        if self.var_coef is None:\n            return 0.5\n\n        # b coefficients are in odd columns (1, 3, 5, ...)\n        b_coefs = self.var_coef[2::2, 0]  # Coefficients on lagged trade signs\n        long_run_impact = np.sum(b_coefs)\n\n        # Normalize by total variance\n        total_var = np.var(returns)\n        if total_var == 0:\n            return 0.5\n\n        # Information share approximation\n        info_share = np.abs(long_run_impact) / np.sqrt(total_var)\n        return np.clip(info_share, 0, 1)",
    "source_file": "core\\_experimental\\market_impact_models.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "HasbrouckModel"
  },
  {
    "name": "compute_features",
    "category": "microstructure",
    "formula": "pd.DataFrame(features)",
    "explanation": "Compute all market impact features.\n\nArgs:\n    prices: Transaction prices\n    volumes: Trade volumes\n    trade_signs: +1 for buy, -1 for sell\n    bid: Bid prices (optional)\n    ask: Ask prices (optional)\n    window: Rolling window\n\nReturns:\n    DataFrame with market impact features",
    "python_code": "def compute_features(\n        self,\n        prices: pd.Series,\n        volumes: pd.Series,\n        trade_signs: pd.Series,\n        bid: Optional[pd.Series] = None,\n        ask: Optional[pd.Series] = None,\n        window: int = 50\n    ) -> pd.DataFrame:\n        \"\"\"\n        Compute all market impact features.\n\n        Args:\n            prices: Transaction prices\n            volumes: Trade volumes\n            trade_signs: +1 for buy, -1 for sell\n            bid: Bid prices (optional)\n            ask: Ask prices (optional)\n            window: Rolling window\n\n        Returns:\n            DataFrame with market impact features\n        \"\"\"\n        features = {}\n\n        # Kyle's lambda\n        signed_volume = trade_signs * volumes\n        features['kyle_lambda'] = self.kyle.estimate_lambda(prices, signed_volume, window)\n\n        # Roll spread\n        features['roll_spread'] = self.roll.estimate_spread(prices, window)\n\n        # If quotes available\n        if bid is not None and ask is not None:\n            spread = ask - bid\n            mid = (bid + ask) / 2\n\n            # Quoted spread\n            features['quoted_spread'] = spread\n            features['quoted_spread_pct'] = spread / mid * 10000  # bps\n\n            # Effective spread (2 * |price - mid|)\n            features['effective_spread'] = 2 * np.abs(prices - mid)\n\n            # Realized spread (price impact)\n            future_mid = mid.shift(-5)  # 5 periods ahead\n            features['realized_spread'] = trade_signs * (future_mid - prices) * 2\n\n            # Price impact (adverse selection proxy)\n            features['price_impact'] = trade_signs * (mid.shift(-5) - mid)\n\n        # Order flow imbalance impact\n        ofi = signed_volume.rolling(window).sum()\n        features['ofi_price_corr'] = prices.rolling(window).corr(ofi)\n\n        # Amihud illiquidity\n        returns = prices.pct_change()\n        features['amihud_illiquidity'] = (\n            np.abs(returns) / (volumes * prices)\n        ).rolling(window).mean() * 1e6\n\n        return pd.DataFrame(features)",
    "source_file": "core\\_experimental\\market_impact_models.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": "MarketImpactFeatures"
  },
  {
    "name": "create_kyle_model",
    "category": "microstructure",
    "formula": "KyleModel()",
    "explanation": "Factory for Kyle model.",
    "python_code": "def create_kyle_model() -> KyleModel:\n    \"\"\"Factory for Kyle model.\"\"\"\n    return KyleModel()",
    "source_file": "core\\_experimental\\market_impact_models.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": null
  },
  {
    "name": "create_market_impact_features",
    "category": "microstructure",
    "formula": "MarketImpactFeatures()",
    "explanation": "Factory for market impact features.",
    "python_code": "def create_market_impact_features() -> MarketImpactFeatures:\n    \"\"\"Factory for market impact features.\"\"\"\n    return MarketImpactFeatures()",
    "source_file": "core\\_experimental\\market_impact_models.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": null
  },
  {
    "name": "estimate_single",
    "category": "microstructure",
    "formula": "np.nan | cov / var | np.nan",
    "explanation": "",
    "python_code": "def estimate_single(dp, vol):\n            if len(dp) < 5:\n                return np.nan\n            # OLS: lambda = Cov(P, V) / Var(V)\n            cov = np.cov(dp, vol)[0, 1]\n            var = np.var(vol)\n            if var > 0:\n                return cov / var\n            return np.nan",
    "source_file": "core\\_experimental\\market_impact_models.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": null
  },
  {
    "name": "calc_spread",
    "category": "microstructure",
    "formula": "np.nan | 0  # Positive autocov implies no valid spread | 2 * np.sqrt(-autocov)",
    "explanation": "",
    "python_code": "def calc_spread(rets):\n            if len(rets) < 3:\n                return np.nan\n            # First-order autocovariance\n            autocov = np.cov(rets[:-1], rets[1:])[0, 1]\n            if autocov >= 0:\n                return 0  # Positive autocov implies no valid spread\n            return 2 * np.sqrt(-autocov)",
    "source_file": "core\\_experimental\\market_impact_models.py",
    "academic_reference": "Kyle (1985) 'Continuous Auctions' Econometrica",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "volatility",
    "formula": "",
    "explanation": "Initialize volatility estimator.",
    "python_code": "def __init__(self):\n        \"\"\"Initialize volatility estimator.\"\"\"\n        pass",
    "source_file": "core\\_experimental\\microstructure_vol.py",
    "academic_reference": null,
    "class_name": "MicrostructureVolatility"
  },
  {
    "name": "realized_variance",
    "category": "volatility",
    "formula": "RV = (r_i)^2 | 0.0 | np.sum(returns ** 2)",
    "explanation": "Standard Realized Variance (biased at high frequency).\n\nRV = (r_i)^2\n\nArgs:\n    prices: Price series\n\nReturns:\n    Realized variance",
    "python_code": "def realized_variance(self, prices: np.ndarray) -> float:\n        \"\"\"\n        Standard Realized Variance (biased at high frequency).\n\n        RV = (r_i)^2\n\n        Args:\n            prices: Price series\n\n        Returns:\n            Realized variance\n        \"\"\"\n        if len(prices) < 2:\n            return 0.0\n\n        returns = np.diff(np.log(prices))\n        return np.sum(returns ** 2)",
    "source_file": "core\\_experimental\\microstructure_vol.py",
    "academic_reference": null,
    "class_name": "MicrostructureVolatility"
  },
  {
    "name": "realized_volatility",
    "category": "volatility",
    "formula": "vol",
    "explanation": "Standard Realized Volatility.\n\nArgs:\n    prices: Price series\n    annualize: If True, annualize (assumes daily data)\n\nReturns:\n    Realized volatility",
    "python_code": "def realized_volatility(self, prices: np.ndarray, annualize: bool = True) -> float:\n        \"\"\"\n        Standard Realized Volatility.\n\n        Args:\n            prices: Price series\n            annualize: If True, annualize (assumes daily data)\n\n        Returns:\n            Realized volatility\n        \"\"\"\n        rv = self.realized_variance(prices)\n        vol = np.sqrt(rv)\n\n        if annualize:\n            vol *= np.sqrt(252)  # Trading days\n\n        return vol",
    "source_file": "core\\_experimental\\microstructure_vol.py",
    "academic_reference": null,
    "class_name": "MicrostructureVolatility"
  },
  {
    "name": "tsrv",
    "category": "volatility",
    "formula": "TSRV = (1/K)  RV_k - (n_bar / n) * RV_all | max(0, tsrv)",
    "explanation": "Two Scales Realized Variance.\n\nUnbiased estimator that accounts for microstructure noise.\n\nTSRV = (1/K)  RV_k - (n_bar / n) * RV_all\n\nwhere:\n- RV_k is realized variance on subsampled grid k\n- n_bar is average sample size per grid\n- n is total observations\n\nArgs:\n    prices: Price series\n    K: Number of subsampling grids (default: optimal)\n\nReturns:\n    Noise-robust realized variance",
    "python_code": "def tsrv(self, prices: np.ndarray, K: int = None) -> float:\n        \"\"\"\n        Two Scales Realized Variance.\n\n        Unbiased estimator that accounts for microstructure noise.\n\n        TSRV = (1/K)  RV_k - (n_bar / n) * RV_all\n\n        where:\n        - RV_k is realized variance on subsampled grid k\n        - n_bar is average sample size per grid\n        - n is total observations\n\n        Args:\n            prices: Price series\n            K: Number of subsampling grids (default: optimal)\n\n        Returns:\n            Noise-robust realized variance\n        \"\"\"\n        n = len(prices)\n        if n < 10:\n            return self.realized_variance(prices)\n\n        # Optimal K (Zhang et al. 2005)\n        if K is None:\n            K = int(np.ceil(n ** (2/3)))\n\n        K = min(K, n // 2)\n        if K < 2:\n            K = 2\n\n        # Subsampled RVs\n        rv_subsampled = 0.0\n        counts = []\n\n        for k in range(K):\n            # Subsample starting at offset k\n            subsample = prices[k::K]\n            if len(subsample) > 1:\n                rv_k = self.realized_variance(subsample)\n                rv_subsampled += rv_k\n                counts.append(len(subsample))\n\n        rv_subsampled /= K\n\n        # Full sample RV (for bias correction)\n        rv_full = self.realized_variance(prices)\n\n        # Bias correction\n        n_bar = np.mean(counts) if counts else n / K\n        bias_correction = (n_bar / n) * rv_full\n\n        tsrv = rv_subsampled - bias_correction\n\n        return max(0, tsrv)",
    "source_file": "core\\_experimental\\microstructure_vol.py",
    "academic_reference": null,
    "class_name": "MicrostructureVolatility"
  },
  {
    "name": "tsrv_volatility",
    "category": "volatility",
    "formula": "vol",
    "explanation": "Two Scales Realized Volatility.\n\nArgs:\n    prices: Price series\n    K: Subsampling grids\n    annualize: Annualize volatility\n\nReturns:\n    TSRV-based volatility",
    "python_code": "def tsrv_volatility(self, prices: np.ndarray, K: int = None,\n                        annualize: bool = True) -> float:\n        \"\"\"\n        Two Scales Realized Volatility.\n\n        Args:\n            prices: Price series\n            K: Subsampling grids\n            annualize: Annualize volatility\n\n        Returns:\n            TSRV-based volatility\n        \"\"\"\n        tsrv = self.tsrv(prices, K)\n        vol = np.sqrt(tsrv)\n\n        if annualize:\n            vol *= np.sqrt(252)\n\n        return vol",
    "source_file": "core\\_experimental\\microstructure_vol.py",
    "academic_reference": null,
    "class_name": "MicrostructureVolatility"
  },
  {
    "name": "estimate_noise_variance",
    "category": "volatility",
    "formula": "0.0 | 0.0 | max(0, noise_var)",
    "explanation": "Estimate microstructure noise variance.\n\nUses first-order autocorrelation of returns.\n\n_noise  -Cov(r_t, r_{t-1})\n\nArgs:\n    prices: Price series\n\nReturns:\n    Estimated noise variance",
    "python_code": "def estimate_noise_variance(self, prices: np.ndarray) -> float:\n        \"\"\"\n        Estimate microstructure noise variance.\n\n        Uses first-order autocorrelation of returns.\n\n        _noise  -Cov(r_t, r_{t-1})\n\n        Args:\n            prices: Price series\n\n        Returns:\n            Estimated noise variance\n        \"\"\"\n        if len(prices) < 3:\n            return 0.0\n\n        returns = np.diff(np.log(prices))\n\n        # Autocovariance at lag 1\n        if len(returns) < 2:\n            return 0.0\n\n        autocov = np.cov(returns[:-1], returns[1:])[0, 1]\n\n        # Noise variance is negative of first autocovariance\n        noise_var = -autocov\n\n        return max(0, noise_var)",
    "source_file": "core\\_experimental\\microstructure_vol.py",
    "academic_reference": null,
    "class_name": "MicrostructureVolatility"
  },
  {
    "name": "signal_to_noise_ratio",
    "category": "volatility",
    "formula": "SNR = true_variance / noise_variance | higher = less noisy) | float('inf')",
    "explanation": "Calculate signal-to-noise ratio.\n\nSNR = true_variance / noise_variance\n\nArgs:\n    prices: Price series\n\nReturns:\n    SNR (higher = less noisy)",
    "python_code": "def signal_to_noise_ratio(self, prices: np.ndarray) -> float:\n        \"\"\"\n        Calculate signal-to-noise ratio.\n\n        SNR = true_variance / noise_variance\n\n        Args:\n            prices: Price series\n\n        Returns:\n            SNR (higher = less noisy)\n        \"\"\"\n        true_var = self.tsrv(prices)\n        noise_var = self.estimate_noise_variance(prices)\n\n        if noise_var <= 0:\n            return float('inf')\n\n        return true_var / noise_var",
    "source_file": "core\\_experimental\\microstructure_vol.py",
    "academic_reference": null,
    "class_name": "MicrostructureVolatility"
  },
  {
    "name": "optimal_sampling_interval",
    "category": "volatility",
    "formula": "samples = more information | samples = less noise | 1",
    "explanation": "Find optimal sampling interval to minimize MSE.\n\nTrades off:\n- More samples = more information\n- Fewer samples = less noise\n\nArgs:\n    prices: High-frequency price series\n    sampling_intervals: Intervals to test\n\nReturns:\n    Optimal sampling interval",
    "python_code": "def optimal_sampling_interval(self, prices: np.ndarray,\n                                  sampling_intervals: List[int] = None) -> int:\n        \"\"\"\n        Find optimal sampling interval to minimize MSE.\n\n        Trades off:\n        - More samples = more information\n        - Fewer samples = less noise\n\n        Args:\n            prices: High-frequency price series\n            sampling_intervals: Intervals to test\n\n        Returns:\n            Optimal sampling interval\n        \"\"\"\n        n = len(prices)\n\n        if sampling_intervals is None:\n            # Test intervals from 1 to n/10\n            max_interval = max(10, n // 10)\n            sampling_intervals = list(range(1, min(max_interval, 100)))\n\n        if not sampling_intervals:\n            return 1\n\n        mse_estimates = []\n\n        for interval in sampling_intervals:\n            if interval >= n // 2:\n                continue\n\n            # Subsample\n            subsampled = prices[::interval]\n\n            if len(subsampled) < 3:\n                continue\n\n            # TSRV on subsampled data\n            var_estimate = self.tsrv(subsampled)\n\n            # Noise on subsampled data (should be lower)\n            noise_var = self.estimate_noise_variance(subsampled)\n\n            # Approximate MSE = variance of estimator + noise impact\n            # More samples = lower variance, but more noise\n            n_sub = len(subsampled)\n            estimator_variance = 2 * var_estimate ** 2 / n_sub if var_estimate > 0 else 0\n\n            mse = estimator_variance + noise_var\n            mse_estimates.append((interval, mse))\n\n        if not mse_estimates:\n            return 1\n\n        # Return interval with lowest MSE\n        optimal = min(mse_estimates, key=lambda x: x[1])\n        return optimal[0]",
    "source_file": "core\\_experimental\\microstructure_vol.py",
    "academic_reference": null,
    "class_name": "MicrostructureVolatility"
  },
  {
    "name": "filter_bidask_bounce",
    "category": "volatility",
    "formula": "prices | filtered.values | prices",
    "explanation": "Filter bid-ask bounce from prices.\n\nArgs:\n    prices: Price series with bid-ask bounce\n    method: 'ma' (moving average) or 'kalman'\n\nReturns:\n    Filtered price series",
    "python_code": "def filter_bidask_bounce(self, prices: np.ndarray,\n                             method: str = 'ma') -> np.ndarray:\n        \"\"\"\n        Filter bid-ask bounce from prices.\n\n        Args:\n            prices: Price series with bid-ask bounce\n            method: 'ma' (moving average) or 'kalman'\n\n        Returns:\n            Filtered price series\n        \"\"\"\n        if len(prices) < 3:\n            return prices\n\n        if method == 'ma':\n            # Simple moving average filter\n            window = 3\n            filtered = pd.Series(prices).rolling(window=window, center=True).mean()\n            filtered = filtered.fillna(method='bfill').fillna(method='ffill')\n            return filtered.values\n\n        elif method == 'kalman':\n            # Simple Kalman filter\n            return self._kalman_filter(prices)\n\n        return prices",
    "source_file": "core\\_experimental\\microstructure_vol.py",
    "academic_reference": "Kalman (1960) 'Filtering and Prediction' J. Basic Engineering",
    "class_name": "MicrostructureVolatility"
  },
  {
    "name": "_kalman_filter",
    "category": "volatility",
    "formula": "filtered",
    "explanation": "Simple Kalman filter for price smoothing.",
    "python_code": "def _kalman_filter(self, prices: np.ndarray) -> np.ndarray:\n        \"\"\"Simple Kalman filter for price smoothing.\"\"\"\n        n = len(prices)\n        filtered = np.zeros(n)\n\n        # Initialize\n        x_hat = prices[0]  # State estimate\n        P = 1.0  # Error covariance\n\n        # Process and measurement noise\n        Q = 0.0001  # Process noise\n        R = self.estimate_noise_variance(prices)\n        if R <= 0:\n            R = 0.0001\n\n        for i in range(n):\n            # Predict\n            x_hat_minus = x_hat\n            P_minus = P + Q\n\n            # Update\n            K = P_minus / (P_minus + R)  # Kalman gain\n            x_hat = x_hat_minus + K * (prices[i] - x_hat_minus)\n            P = (1 - K) * P_minus\n\n            filtered[i] = x_hat\n\n        return filtered",
    "source_file": "core\\_experimental\\microstructure_vol.py",
    "academic_reference": "Kalman (1960) 'Filtering and Prediction' J. Basic Engineering",
    "class_name": "MicrostructureVolatility"
  },
  {
    "name": "multi_scale_variance",
    "category": "volatility",
    "formula": "result",
    "explanation": "Calculate variance at multiple time scales.\n\nArgs:\n    prices: Price series\n    scales: Time scales (sampling intervals)\n\nReturns:\n    Dict with variance at each scale",
    "python_code": "def multi_scale_variance(self, prices: np.ndarray,\n                             scales: List[int] = None) -> Dict[str, float]:\n        \"\"\"\n        Calculate variance at multiple time scales.\n\n        Args:\n            prices: Price series\n            scales: Time scales (sampling intervals)\n\n        Returns:\n            Dict with variance at each scale\n        \"\"\"\n        if scales is None:\n            scales = [1, 5, 15, 30, 60]\n\n        result = {}\n        for scale in scales:\n            if scale >= len(prices) // 2:\n                continue\n\n            subsampled = prices[::scale]\n            if len(subsampled) < 3:\n                continue\n\n            rv = self.realized_variance(subsampled)\n            tsrv = self.tsrv(subsampled)\n\n            result[f'rv_scale_{scale}'] = rv\n            result[f'tsrv_scale_{scale}'] = tsrv\n\n        return result",
    "source_file": "core\\_experimental\\microstructure_vol.py",
    "academic_reference": null,
    "class_name": "MicrostructureVolatility"
  },
  {
    "name": "volatility_signature",
    "category": "volatility",
    "formula": "signature = unbiased estimator. | pd.DataFrame(data)",
    "explanation": "Generate volatility signature plot data.\n\nShows how volatility estimate varies with sampling frequency.\nFlat signature = unbiased estimator.\n\nArgs:\n    prices: Price series\n    max_scale: Maximum sampling interval\n\nReturns:\n    DataFrame with scale and volatility columns",
    "python_code": "def volatility_signature(self, prices: np.ndarray,\n                             max_scale: int = 100) -> pd.DataFrame:\n        \"\"\"\n        Generate volatility signature plot data.\n\n        Shows how volatility estimate varies with sampling frequency.\n        Flat signature = unbiased estimator.\n\n        Args:\n            prices: Price series\n            max_scale: Maximum sampling interval\n\n        Returns:\n            DataFrame with scale and volatility columns\n        \"\"\"\n        scales = list(range(1, min(max_scale, len(prices) // 3)))\n\n        data = []\n        for scale in scales:\n            subsampled = prices[::scale]\n            if len(subsampled) < 3:\n                continue\n\n            rv = self.realized_volatility(subsampled, annualize=False)\n            tsrv = self.tsrv_volatility(subsampled, annualize=False)\n\n            data.append({\n                'scale': scale,\n                'rv': rv,\n                'tsrv': tsrv\n            })\n\n        return pd.DataFrame(data)",
    "source_file": "core\\_experimental\\microstructure_vol.py",
    "academic_reference": null,
    "class_name": "MicrostructureVolatility"
  },
  {
    "name": "bipower_variation",
    "category": "volatility",
    "formula": "BV = (/2)  |r_i| |r_{i-1}| | 0.0 | bv",
    "explanation": "Bipower Variation - robust to jumps.\n\nBV = (/2)  |r_i| |r_{i-1}|\n\nArgs:\n    prices: Price series\n\nReturns:\n    Bipower variation",
    "python_code": "def bipower_variation(self, prices: np.ndarray) -> float:\n        \"\"\"\n        Bipower Variation - robust to jumps.\n\n        BV = (/2)  |r_i| |r_{i-1}|\n\n        Args:\n            prices: Price series\n\n        Returns:\n            Bipower variation\n        \"\"\"\n        if len(prices) < 3:\n            return 0.0\n\n        returns = np.diff(np.log(prices))\n        abs_returns = np.abs(returns)\n\n        bv = (np.pi / 2) * np.sum(abs_returns[1:] * abs_returns[:-1])\n        return bv",
    "source_file": "core\\_experimental\\microstructure_vol.py",
    "academic_reference": null,
    "class_name": "JumpRobustVolatility"
  },
  {
    "name": "detect_jumps",
    "category": "volatility",
    "formula": "np.zeros(len(prices), dtype=bool) | np.concatenate([[False], jumps])",
    "explanation": "Detect jumps in price series.\n\nArgs:\n    prices: Price series\n\nReturns:\n    Boolean array indicating jumps",
    "python_code": "def detect_jumps(self, prices: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Detect jumps in price series.\n\n        Args:\n            prices: Price series\n\n        Returns:\n            Boolean array indicating jumps\n        \"\"\"\n        if len(prices) < 3:\n            return np.zeros(len(prices), dtype=bool)\n\n        returns = np.diff(np.log(prices))\n\n        # Local volatility estimate (rolling std)\n        window = min(20, len(returns) // 3)\n        local_vol = pd.Series(returns).rolling(window=window, center=True).std()\n        local_vol = local_vol.fillna(method='bfill').fillna(method='ffill').values\n\n        # Threshold\n        jumps = np.abs(returns) > self.threshold * local_vol\n\n        # Pad to match price length\n        return np.concatenate([[False], jumps])",
    "source_file": "core\\_experimental\\microstructure_vol.py",
    "academic_reference": null,
    "class_name": "JumpRobustVolatility"
  },
  {
    "name": "remove_jumps",
    "category": "volatility",
    "formula": "filtered",
    "explanation": "Remove jumps from price series.\n\nArgs:\n    prices: Price series with jumps\n\nReturns:\n    Price series with jumps removed",
    "python_code": "def remove_jumps(self, prices: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Remove jumps from price series.\n\n        Args:\n            prices: Price series with jumps\n\n        Returns:\n            Price series with jumps removed\n        \"\"\"\n        jumps = self.detect_jumps(prices)\n        filtered = prices.copy()\n\n        for i in np.where(jumps)[0]:\n            if i > 0:\n                filtered[i] = filtered[i-1]\n\n        return filtered",
    "source_file": "core\\_experimental\\microstructure_vol.py",
    "academic_reference": null,
    "class_name": "JumpRobustVolatility"
  },
  {
    "name": "continuous_variation",
    "category": "volatility",
    "formula": "mv.tsrv(filtered)",
    "explanation": "Estimate continuous variation (excluding jumps).\n\nArgs:\n    prices: Price series\n\nReturns:\n    Continuous variation",
    "python_code": "def continuous_variation(self, prices: np.ndarray) -> float:\n        \"\"\"\n        Estimate continuous variation (excluding jumps).\n\n        Args:\n            prices: Price series\n\n        Returns:\n            Continuous variation\n        \"\"\"\n        filtered = self.remove_jumps(prices)\n        mv = MicrostructureVolatility()\n        return mv.tsrv(filtered)",
    "source_file": "core\\_experimental\\microstructure_vol.py",
    "academic_reference": null,
    "class_name": "JumpRobustVolatility"
  },
  {
    "name": "jump_variation",
    "category": "volatility",
    "formula": "max(0, total_rv - continuous)",
    "explanation": "Estimate jump variation.\n\nArgs:\n    prices: Price series\n\nReturns:\n    Jump variation (RV - continuous variation)",
    "python_code": "def jump_variation(self, prices: np.ndarray) -> float:\n        \"\"\"\n        Estimate jump variation.\n\n        Args:\n            prices: Price series\n\n        Returns:\n            Jump variation (RV - continuous variation)\n        \"\"\"\n        mv = MicrostructureVolatility()\n        total_rv = mv.realized_variance(prices)\n        continuous = self.continuous_variation(prices)\n\n        return max(0, total_rv - continuous)",
    "source_file": "core\\_experimental\\microstructure_vol.py",
    "academic_reference": null,
    "class_name": "JumpRobustVolatility"
  },
  {
    "name": "add_microstructure_features",
    "category": "volatility",
    "formula": "result",
    "explanation": "Add microstructure volatility features to DataFrame.\n\nArgs:\n    df: DataFrame with price data\n    price_col: Name of price column\n\nReturns:\n    DataFrame with additional volatility features",
    "python_code": "def add_microstructure_features(df: pd.DataFrame,\n                                price_col: str = 'close') -> pd.DataFrame:\n    \"\"\"\n    Add microstructure volatility features to DataFrame.\n\n    Args:\n        df: DataFrame with price data\n        price_col: Name of price column\n\n    Returns:\n        DataFrame with additional volatility features\n    \"\"\"\n    result = df.copy()\n    mv = MicrostructureVolatility()\n    jrv = JumpRobustVolatility()\n\n    prices = df[price_col].values\n\n    # Calculate features on rolling window\n    window = 100\n\n    result['tsrv'] = np.nan\n    result['noise_var'] = np.nan\n    result['snr'] = np.nan\n    result['bipower_var'] = np.nan\n    result['jump_var'] = np.nan\n\n    for i in range(window, len(df)):\n        window_prices = prices[i-window:i]\n\n        result.iloc[i, result.columns.get_loc('tsrv')] = mv.tsrv(window_prices)\n        result.iloc[i, result.columns.get_loc('noise_var')] = mv.estimate_noise_variance(window_prices)\n        result.iloc[i, result.columns.get_loc('snr')] = mv.signal_to_noise_ratio(window_prices)\n        result.iloc[i, result.columns.get_loc('bipower_var')] = jrv.bipower_variation(window_prices)\n        result.iloc[i, result.columns.get_loc('jump_var')] = jrv.jump_variation(window_prices)\n\n    return result",
    "source_file": "core\\_experimental\\microstructure_vol.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "machine_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, model_name: str, model_path: Optional[Path] = None):\n        self.model_name = model_name\n        self.model_path = model_path or project_root / \"models\"\n        self.model = None",
    "source_file": "core\\_experimental\\ml_integration.py",
    "academic_reference": null,
    "class_name": "MLPredictor"
  },
  {
    "name": "load_model",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Load trained model from disk",
    "python_code": "def load_model(self):\n        \"\"\"Load trained model from disk\"\"\"\n        raise NotImplementedError",
    "source_file": "core\\_experimental\\ml_integration.py",
    "academic_reference": null,
    "class_name": "MLPredictor"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Generate predictions",
    "python_code": "def predict(self, features: pd.DataFrame) -> np.ndarray:\n        \"\"\"Generate predictions\"\"\"\n        raise NotImplementedError",
    "source_file": "core\\_experimental\\ml_integration.py",
    "academic_reference": null,
    "class_name": "MLPredictor"
  },
  {
    "name": "load_model",
    "category": "machine_learning",
    "formula": "True | False | False",
    "explanation": "Load Time-Series-Library model",
    "python_code": "def load_model(self):\n        \"\"\"Load Time-Series-Library model\"\"\"\n        try:\n            # Import from Time-Series-Library\n            ts_lib_path = project_root / \"Time-Series-Library\"\n            if ts_lib_path.exists():\n                sys.path.insert(0, str(ts_lib_path))\n\n                # Dynamic import based on model type\n                from models import Autoformer, Informer, Transformer  # etc\n\n                print(f\"[OK] Time-Series-Library model {self.model_type} loaded\")\n                self.model = \"loaded\"  # Placeholder\n                return True\n            else:\n                print(f\"[WARNING] Time-Series-Library not found at {ts_lib_path}\")\n                return False\n\n        except ImportError as e:\n            print(f\"[WARNING] Cannot import Time-Series-Library: {e}\")\n            return False",
    "source_file": "core\\_experimental\\ml_integration.py",
    "academic_reference": null,
    "class_name": "TimeSeriesPredictor"
  },
  {
    "name": "_fallback_predict",
    "category": "machine_learning",
    "formula": "signals",
    "explanation": "Fallback prediction using technical indicators",
    "python_code": "def _fallback_predict(self, features: pd.DataFrame) -> np.ndarray:\n        \"\"\"Fallback prediction using technical indicators\"\"\"\n        signals = np.zeros(len(features))\n\n        if 'ma_20' in features.columns and 'ma_50' in features.columns:\n            # Trend following\n            bullish = (features['price'] > features['ma_20']) & (features['ma_20'] > features['ma_50'])\n            bearish = (features['price'] < features['ma_20']) & (features['ma_20'] < features['ma_50'])\n\n            signals[bullish] = 1\n            signals[bearish] = -1\n\n        return signals",
    "source_file": "core\\_experimental\\ml_integration.py",
    "academic_reference": null,
    "class_name": "TimeSeriesPredictor"
  },
  {
    "name": "load_model",
    "category": "machine_learning",
    "formula": "True | False",
    "explanation": "Load Qlib workflow",
    "python_code": "def load_model(self):\n        \"\"\"Load Qlib workflow\"\"\"\n        try:\n            import qlib\n            from qlib.constant import REG_CN, REG_US\n            from qlib.utils import init_instance_by_config\n            from qlib.workflow import R\n            from qlib.workflow.record_temp import SignalRecord, PortAnaRecord\n\n            print(\"[OK] Qlib loaded\")\n            self.model = \"qlib\"\n            return True\n\n        except ImportError:\n            print(\"[WARNING] Qlib not installed (pip install pyqlib)\")\n            return False",
    "source_file": "core\\_experimental\\ml_integration.py",
    "academic_reference": null,
    "class_name": "QlibPredictor"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "np.zeros(len(features))",
    "explanation": "Generate predictions using Qlib",
    "python_code": "def predict(self, features: pd.DataFrame) -> np.ndarray:\n        \"\"\"Generate predictions using Qlib\"\"\"\n        # TODO: Implement Qlib workflow prediction\n        # This requires:\n        # 1. Convert features to Qlib format\n        # 2. Run Qlib model inference\n        # 3. Extract signals\n\n        print(\"[FALLBACK] Qlib prediction not yet implemented\")\n        return np.zeros(len(features))",
    "source_file": "core\\_experimental\\ml_integration.py",
    "academic_reference": null,
    "class_name": "QlibPredictor"
  },
  {
    "name": "_initialize_predictors",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Initialize all predictors",
    "python_code": "def _initialize_predictors(self):\n        \"\"\"Initialize all predictors\"\"\"\n        for model_name in self.models:\n            if model_name == 'timeseries':\n                predictor = TimeSeriesPredictor(model_type='Autoformer')\n            elif model_name == 'qlib':\n                predictor = QlibPredictor()\n            elif model_name == 'finrl':\n                predictor = FinRLPredictor(agent_type='ppo')\n            elif model_name == 'chinese':\n                predictor = ChineseQuantPredictor(framework='quantaxis')\n            else:\n                continue\n\n            # Try to load model\n            if predictor.load_model():\n                self.predictors[model_name] = predictor\n                self.weights[model_name] = 1.0 / len(self.models)  # Equal weight\n            else:\n                print(f\"[WARNING] Skipping {model_name} (not available)\")\n\n        # Normalize weights\n        if self.predictors:\n            total_weight = sum(self.weights.values())\n            for model in self.weights:\n                self.weights[model] /= total_weight\n\n        print(f\"[INFO] Ensemble initialized with {len(self.predictors)} models\")",
    "source_file": "core\\_experimental\\ml_integration.py",
    "academic_reference": null,
    "class_name": "MLEnsemble"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "np.zeros(len(features)), np.zeros(len(features)) | signals, confidence",
    "explanation": "Generate ensemble predictions\n\nReturns:\n    signals: Array of trading signals (1=buy, -1=sell, 0=hold)\n    confidence: Array of confidence scores (0-1)",
    "python_code": "def predict(self, features: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Generate ensemble predictions\n\n        Returns:\n            signals: Array of trading signals (1=buy, -1=sell, 0=hold)\n            confidence: Array of confidence scores (0-1)\n        \"\"\"\n        if not self.predictors:\n            print(\"[ERROR] No models available for ensemble\")\n            return np.zeros(len(features)), np.zeros(len(features))\n\n        # Get predictions from each model\n        all_predictions = {}\n        for model_name, predictor in self.predictors.items():\n            predictions = predictor.predict(features)\n            all_predictions[model_name] = predictions\n\n        # Weighted average\n        ensemble_signal = np.zeros(len(features))\n        for model_name, predictions in all_predictions.items():\n            weight = self.weights[model_name]\n            ensemble_signal += predictions * weight\n\n        # Convert to discrete signals\n        signals = np.where(ensemble_signal > 0.3, 1,\n                          np.where(ensemble_signal < -0.3, -1, 0))\n\n        # Calculate confidence (agreement among models)\n        predictions_matrix = np.column_stack(list(all_predictions.values()))\n        agreement = np.abs(predictions_matrix.sum(axis=1)) / len(self.predictors)\n        confidence = agreement\n\n        return signals, confidence",
    "source_file": "core\\_experimental\\ml_integration.py",
    "academic_reference": null,
    "class_name": "MLEnsemble"
  },
  {
    "name": "update_weights",
    "category": "risk",
    "formula": "",
    "explanation": "Update model weights based on recent performance\n\nArgs:\n    performance: Dict of {model_name: sharpe_ratio or win_rate}",
    "python_code": "def update_weights(self, performance: Dict[str, float]):\n        \"\"\"\n        Update model weights based on recent performance\n\n        Args:\n            performance: Dict of {model_name: sharpe_ratio or win_rate}\n        \"\"\"\n        for model_name, score in performance.items():\n            if model_name in self.weights:\n                self.weights[model_name] = score\n\n        # Normalize\n        total = sum(self.weights.values())\n        if total > 0:\n            for model in self.weights:\n                self.weights[model] /= total\n\n        print(f\"[INFO] Updated ensemble weights: {self.weights}\")",
    "source_file": "core\\_experimental\\ml_integration.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "MLEnsemble"
  },
  {
    "name": "test_ml_integration",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Test ML integration",
    "python_code": "def test_ml_integration():\n    \"\"\"Test ML integration\"\"\"\n    print(\"=\"*70)\n    print(\"ML INTEGRATION TEST\")\n    print(\"=\"*70)\n\n    # Create sample features\n    dates = pd.date_range('2026-01-01', periods=1000, freq='1min')\n    price = 1.16 + np.cumsum(np.random.randn(1000) * 0.0001)\n\n    features = pd.DataFrame({\n        'timestamp': dates,\n        'price': price,\n        'ma_20': pd.Series(price).rolling(20).mean(),\n        'ma_50': pd.Series(price).rolling(50).mean(),\n        'returns': pd.Series(price).pct_change()\n    })\n\n    # Test ensemble\n    ensemble = MLEnsemble(models=['timeseries'])\n\n    signals, confidence = ensemble.predict(features)\n\n    print(f\"\\nPredictions generated: {len(signals)}\")\n    print(f\"Buy signals:  {(signals == 1).sum()}\")\n    print(f\"Sell signals: {(signals == -1).sum()}\")\n    print(f\"Hold signals: {(signals == 0).sum()}\")\n    print(f\"Avg confidence: {confidence.mean():.2f}\")\n    print()",
    "source_file": "core\\_experimental\\ml_integration.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "quantitative",
    "formula": "",
    "explanation": "Initialize executor.\n\nArgs:\n    mode: 'simulation' or 'live'\n    ib_host: IB Gateway host\n    ib_port: IB Gateway port\n    client_id: IB client ID",
    "python_code": "def __init__(self,\n                 mode: str = 'simulation',\n                 ib_host: str = 'localhost',\n                 ib_port: int = 4001,\n                 client_id: int = 1):\n        \"\"\"\n        Initialize executor.\n\n        Args:\n            mode: 'simulation' or 'live'\n            ib_host: IB Gateway host\n            ib_port: IB Gateway port\n            client_id: IB client ID\n        \"\"\"\n        self.mode = mode\n        self.ib_host = ib_host\n        self.ib_port = ib_port\n        self.client_id = client_id\n\n        # State\n        self.orders: Dict[str, Order] = {}\n        self.positions: Dict[str, Position] = {}\n        self.fills: List[Fill] = []\n        self.order_counter = 0\n\n        # Callbacks\n        self.on_fill: Optional[Callable] = None\n        self.on_order_update: Optional[Callable] = None\n\n        # IB connection\n        self.ib = None\n\n        if mode == 'live':\n            self._connect_ib()",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "NautilusExecutor"
  },
  {
    "name": "_connect_ib",
    "category": "quantitative",
    "formula": "",
    "explanation": "Connect to IB Gateway.",
    "python_code": "def _connect_ib(self):\n        \"\"\"Connect to IB Gateway.\"\"\"\n        try:\n            from ib_insync import IB\n\n            self.ib = IB()\n            self.ib.connect(self.ib_host, self.ib_port, clientId=self.client_id)\n            logger.info(f\"Connected to IB Gateway at {self.ib_host}:{self.ib_port}\")\n\n        except ImportError:\n            logger.error(\"ib_insync not installed: pip install ib_insync\")\n        except Exception as e:\n            logger.error(f\"Failed to connect to IB: {e}\")",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "NautilusExecutor"
  },
  {
    "name": "_generate_order_id",
    "category": "quantitative",
    "formula": "f\"ORD-{datetime.now().strftime('%Y%m%d%H%M%S')}-{self.order_counter:04d}\"",
    "explanation": "Generate unique order ID.",
    "python_code": "def _generate_order_id(self) -> str:\n        \"\"\"Generate unique order ID.\"\"\"\n        self.order_counter += 1\n        return f\"ORD-{datetime.now().strftime('%Y%m%d%H%M%S')}-{self.order_counter:04d}\"",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "NautilusExecutor"
  },
  {
    "name": "submit_order",
    "category": "execution",
    "formula": "order.id",
    "explanation": "Submit order for execution.\n\nReturns order ID.",
    "python_code": "def submit_order(self, order: Order) -> str:\n        \"\"\"\n        Submit order for execution.\n\n        Returns order ID.\n        \"\"\"\n        order.id = self._generate_order_id()\n        order.status = OrderStatus.SUBMITTED\n        self.orders[order.id] = order\n\n        if self.mode == 'simulation':\n            self._simulate_execution(order)\n        else:\n            self._execute_ib(order)\n\n        return order.id",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "NautilusExecutor"
  },
  {
    "name": "market_order",
    "category": "quantitative",
    "formula": "",
    "explanation": "Submit market order.",
    "python_code": "def market_order(self, symbol: str, side: OrderSide, quantity: float) -> str:\n        \"\"\"Submit market order.\"\"\"\n        order = Order(\n            id=\"\",\n            symbol=symbol,\n            side=side,\n            order_type=OrderType.MARKET,\n            quantity=quantity\n        )\n        return self.submit_order(order)",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "NautilusExecutor"
  },
  {
    "name": "limit_order",
    "category": "quantitative",
    "formula": "",
    "explanation": "Submit limit order.",
    "python_code": "def limit_order(self, symbol: str, side: OrderSide, quantity: float, price: float) -> str:\n        \"\"\"Submit limit order.\"\"\"\n        order = Order(\n            id=\"\",\n            symbol=symbol,\n            side=side,\n            order_type=OrderType.LIMIT,\n            quantity=quantity,\n            price=price\n        )\n        return self.submit_order(order)",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "NautilusExecutor"
  },
  {
    "name": "stop_order",
    "category": "quantitative",
    "formula": "",
    "explanation": "Submit stop order.",
    "python_code": "def stop_order(self, symbol: str, side: OrderSide, quantity: float, stop_price: float) -> str:\n        \"\"\"Submit stop order.\"\"\"\n        order = Order(\n            id=\"\",\n            symbol=symbol,\n            side=side,\n            order_type=OrderType.STOP,\n            quantity=quantity,\n            stop_price=stop_price\n        )\n        return self.submit_order(order)",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "NautilusExecutor"
  },
  {
    "name": "bracket_order",
    "category": "quantitative",
    "formula": "{",
    "explanation": "Submit bracket order (entry + TP + SL).\n\nReturns dict with order IDs for each leg.",
    "python_code": "def bracket_order(self,\n                      symbol: str,\n                      side: OrderSide,\n                      quantity: float,\n                      entry_price: float,\n                      take_profit: float,\n                      stop_loss: float) -> Dict[str, str]:\n        \"\"\"\n        Submit bracket order (entry + TP + SL).\n\n        Returns dict with order IDs for each leg.\n        \"\"\"\n        # Entry order\n        entry_id = self.limit_order(symbol, side, quantity, entry_price)\n\n        # Take profit (opposite side)\n        tp_side = OrderSide.SELL if side == OrderSide.BUY else OrderSide.BUY\n        tp_id = self.limit_order(symbol, tp_side, quantity, take_profit)\n\n        # Stop loss (opposite side)\n        sl_id = self.stop_order(symbol, tp_side, quantity, stop_loss)\n\n        return {\n            'entry': entry_id,\n            'take_profit': tp_id,\n            'stop_loss': sl_id\n        }",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "NautilusExecutor"
  },
  {
    "name": "cancel_order",
    "category": "quantitative",
    "formula": "False | False | True",
    "explanation": "Cancel order.",
    "python_code": "def cancel_order(self, order_id: str) -> bool:\n        \"\"\"Cancel order.\"\"\"\n        if order_id not in self.orders:\n            return False\n\n        order = self.orders[order_id]\n        if order.status in [OrderStatus.FILLED, OrderStatus.CANCELLED]:\n            return False\n\n        order.status = OrderStatus.CANCELLED\n        order.updated_at = datetime.now()\n\n        if self.on_order_update:\n            self.on_order_update(order)\n\n        return True",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "NautilusExecutor"
  },
  {
    "name": "get_position",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get current position for symbol.",
    "python_code": "def get_position(self, symbol: str) -> Optional[Position]:\n        \"\"\"Get current position for symbol.\"\"\"\n        return self.positions.get(symbol)",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "NautilusExecutor"
  },
  {
    "name": "get_all_positions",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get all positions.",
    "python_code": "def get_all_positions(self) -> Dict[str, Position]:\n        \"\"\"Get all positions.\"\"\"\n        return self.positions.copy()",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "NautilusExecutor"
  },
  {
    "name": "get_order",
    "category": "quantitative",
    "formula": "",
    "explanation": "Get order by ID.",
    "python_code": "def get_order(self, order_id: str) -> Optional[Order]:\n        \"\"\"Get order by ID.\"\"\"\n        return self.orders.get(order_id)",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "NautilusExecutor"
  },
  {
    "name": "get_open_orders",
    "category": "quantitative",
    "formula": "[o for o in self.orders.values()",
    "explanation": "Get all open orders.",
    "python_code": "def get_open_orders(self) -> List[Order]:\n        \"\"\"Get all open orders.\"\"\"\n        return [o for o in self.orders.values()\n                if o.status in [OrderStatus.PENDING, OrderStatus.SUBMITTED, OrderStatus.ACCEPTED]]",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "NautilusExecutor"
  },
  {
    "name": "_simulate_execution",
    "category": "execution",
    "formula": "",
    "explanation": "Simulate order execution.",
    "python_code": "def _simulate_execution(self, order: Order, current_price: Optional[float] = None):\n        \"\"\"Simulate order execution.\"\"\"\n        # For simulation, fill immediately at market price (or limit price)\n        if order.order_type == OrderType.MARKET:\n            fill_price = current_price or 1.0  # Placeholder\n        elif order.order_type == OrderType.LIMIT:\n            fill_price = order.price\n        else:\n            fill_price = order.stop_price or order.price or 1.0\n\n        # Add slippage for market orders\n        if order.order_type == OrderType.MARKET:\n            slippage = 0.0001  # 1 pip\n            if order.side == OrderSide.BUY:\n                fill_price += slippage\n            else:\n                fill_price -= slippage\n\n        # Create fill\n        fill = Fill(\n            order_id=order.id,\n            symbol=order.symbol,\n            side=order.side,\n            quantity=order.quantity,\n            price=fill_price,\n            timestamp=datetime.now(),\n            commission=order.quantity * 0.00002  # 2 pips commission\n        )\n        self.fills.append(fill)\n\n        # Update order\n        order.status = OrderStatus.FILLED\n        order.filled_quantity = order.quantity\n        order.avg_fill_price = fill_price\n        order.updated_at = datetime.now()\n\n        # Update position\n        self._update_position(fill)\n\n        # Callbacks\n        if self.on_fill:\n            self.on_fill(fill)\n        if self.on_order_update:\n            self.on_order_update(order)",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "NautilusExecutor"
  },
  {
    "name": "_update_position",
    "category": "quantitative",
    "formula": "",
    "explanation": "Update position after fill.",
    "python_code": "def _update_position(self, fill: Fill):\n        \"\"\"Update position after fill.\"\"\"\n        symbol = fill.symbol\n\n        if symbol not in self.positions:\n            self.positions[symbol] = Position(symbol=symbol, quantity=0, avg_price=0)\n\n        pos = self.positions[symbol]\n\n        if fill.side == OrderSide.BUY:\n            # Add to long or reduce short\n            if pos.quantity >= 0:\n                # Adding to long\n                total_cost = pos.quantity * pos.avg_price + fill.quantity * fill.price\n                pos.quantity += fill.quantity\n                pos.avg_price = total_cost / pos.quantity if pos.quantity > 0 else 0\n            else:\n                # Reducing short\n                if fill.quantity >= abs(pos.quantity):\n                    # Close short and go long\n                    pnl = (pos.avg_price - fill.price) * abs(pos.quantity)\n                    pos.realized_pnl += pnl\n                    pos.quantity = fill.quantity - abs(pos.quantity)\n                    pos.avg_price = fill.price if pos.quantity > 0 else 0\n                else:\n                    # Partial close of short\n                    pnl = (pos.avg_price - fill.price) * fill.quantity\n                    pos.realized_pnl += pnl\n                    pos.quantity += fill.quantity\n        else:\n            # Sell: reduce long or add to short\n            if pos.quantity <= 0:\n                # Adding to short\n                total_cost = abs(pos.quantity) * pos.avg_price + fill.quantity * fill.price\n                pos.quantity -= fill.quantity\n                pos.avg_price = total_cost / abs(pos.quantity) if pos.quantity < 0 else 0\n            else:\n                # Reducing long\n                if fill.quantity >= pos.quantity:\n                    # Close long and go short\n                    pnl = (fill.price - pos.avg_price) * pos.quantity\n                    pos.realized_pnl += pnl\n                    pos.quantity = pos.quantity - fill.quantity\n                    pos.avg_price = fill.price if pos.quantity < 0 else 0\n                else:\n                    # Partial close of long\n                    pnl = (fill.price - pos.avg_price) * fill.quantity\n                    pos.realized_pnl += pnl\n                    pos.quantity -= fill.quantity",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "NautilusExecutor"
  },
  {
    "name": "_execute_ib",
    "category": "quantitative",
    "formula": "try: | # Submit to IB",
    "explanation": "Execute order via IB Gateway.",
    "python_code": "def _execute_ib(self, order: Order):\n        \"\"\"Execute order via IB Gateway.\"\"\"\n        if self.ib is None:\n            logger.error(\"IB not connected\")\n            order.status = OrderStatus.REJECTED\n            return\n\n        try:\n            from ib_insync import Forex, MarketOrder, LimitOrder, StopOrder\n\n            # Create IB contract\n            contract = Forex(order.symbol)\n\n            # Create IB order\n            if order.order_type == OrderType.MARKET:\n                ib_order = MarketOrder(\n                    'BUY' if order.side == OrderSide.BUY else 'SELL',\n                    order.quantity\n                )\n            elif order.order_type == OrderType.LIMIT:\n                ib_order = LimitOrder(\n                    'BUY' if order.side == OrderSide.BUY else 'SELL',\n                    order.quantity,\n                    order.price\n                )\n            elif order.order_type == OrderType.STOP:\n                ib_order = StopOrder(\n                    'BUY' if order.side == OrderSide.BUY else 'SELL',\n                    order.quantity,\n                    order.stop_price\n                )\n            else:\n                logger.error(f\"Unsupported order type: {order.order_type}\")\n                return\n\n            # Submit to IB\n            trade = self.ib.placeOrder(contract, ib_order)\n            order.status = OrderStatus.SUBMITTED\n\n            logger.info(f\"Order submitted to IB: {order.id}\")\n\n        except Exception as e:\n            logger.error(f\"IB execution error: {e}\")\n            order.status = OrderStatus.REJECTED",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "NautilusExecutor"
  },
  {
    "name": "update_market_data",
    "category": "quantitative",
    "formula": "",
    "explanation": "Update market data and check pending orders.\n\nCall this with each tick for realistic simulation.",
    "python_code": "def update_market_data(self, symbol: str, bid: float, ask: float):\n        \"\"\"\n        Update market data and check pending orders.\n\n        Call this with each tick for realistic simulation.\n        \"\"\"\n        mid_price = (bid + ask) / 2\n\n        # Check limit orders\n        for order in self.get_open_orders():\n            if order.symbol != symbol:\n                continue\n\n            if order.order_type == OrderType.LIMIT:\n                if order.side == OrderSide.BUY and ask <= order.price:\n                    self._simulate_execution(order, ask)\n                elif order.side == OrderSide.SELL and bid >= order.price:\n                    self._simulate_execution(order, bid)\n\n            elif order.order_type == OrderType.STOP:\n                if order.side == OrderSide.BUY and ask >= order.stop_price:\n                    self._simulate_execution(order, ask)\n                elif order.side == OrderSide.SELL and bid <= order.stop_price:\n                    self._simulate_execution(order, bid)\n\n        # Update unrealized PnL\n        if symbol in self.positions:\n            pos = self.positions[symbol]\n            if pos.quantity > 0:\n                pos.unrealized_pnl = (mid_price - pos.avg_price) * pos.quantity\n            elif pos.quantity < 0:\n                pos.unrealized_pnl = (pos.avg_price - mid_price) * abs(pos.quantity)",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "NautilusExecutor"
  },
  {
    "name": "get_statistics",
    "category": "execution",
    "formula": "{} | {",
    "explanation": "Get execution statistics.",
    "python_code": "def get_statistics(self) -> Dict[str, float]:\n        \"\"\"Get execution statistics.\"\"\"\n        if not self.fills:\n            return {}\n\n        total_volume = sum(f.quantity for f in self.fills)\n        total_commission = sum(f.commission for f in self.fills)\n        realized_pnl = sum(p.realized_pnl for p in self.positions.values())\n        unrealized_pnl = sum(p.unrealized_pnl for p in self.positions.values())\n\n        return {\n            'total_trades': len(self.fills),\n            'total_volume': total_volume,\n            'total_commission': total_commission,\n            'realized_pnl': realized_pnl,\n            'unrealized_pnl': unrealized_pnl,\n            'net_pnl': realized_pnl + unrealized_pnl - total_commission\n        }",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "NautilusExecutor"
  },
  {
    "name": "close_all_positions",
    "category": "quantitative",
    "formula": "",
    "explanation": "Close all open positions.",
    "python_code": "def close_all_positions(self):\n        \"\"\"Close all open positions.\"\"\"\n        for symbol, pos in self.positions.items():\n            if pos.quantity > 0:\n                self.market_order(symbol, OrderSide.SELL, pos.quantity)\n            elif pos.quantity < 0:\n                self.market_order(symbol, OrderSide.BUY, abs(pos.quantity))",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "NautilusExecutor"
  },
  {
    "name": "on_tick",
    "category": "quantitative",
    "formula": "",
    "explanation": "Called on each tick. Override in subclass.",
    "python_code": "def on_tick(self, symbol: str, bid: float, ask: float, timestamp: datetime):\n        \"\"\"Called on each tick. Override in subclass.\"\"\"\n        pass",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "StrategyBase"
  },
  {
    "name": "on_bar",
    "category": "quantitative",
    "formula": "",
    "explanation": "Called on each bar close. Override in subclass.",
    "python_code": "def on_bar(self, symbol: str, bar: Dict[str, float], timestamp: datetime):\n        \"\"\"Called on each bar close. Override in subclass.\"\"\"\n        pass",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "StrategyBase"
  },
  {
    "name": "on_fill",
    "category": "quantitative",
    "formula": "",
    "explanation": "Called when order is filled. Override in subclass.",
    "python_code": "def on_fill(self, fill: Fill):\n        \"\"\"Called when order is filled. Override in subclass.\"\"\"\n        pass",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "StrategyBase"
  },
  {
    "name": "on_order_update",
    "category": "quantitative",
    "formula": "",
    "explanation": "Called when order status changes. Override in subclass.",
    "python_code": "def on_order_update(self, order: Order):\n        \"\"\"Called when order status changes. Override in subclass.\"\"\"\n        pass",
    "source_file": "core\\_experimental\\nautilus_executor.py",
    "academic_reference": "Schulman (2017) 'PPO Algorithms' arXiv:1707.06347",
    "class_name": "StrategyBase"
  },
  {
    "name": "__init__",
    "category": "quantitative",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, n_trials: int = 100, timeout: int = 3600):\n        self.n_trials = n_trials\n        self.timeout = timeout\n        self.study = None\n        self.best_params = None",
    "source_file": "core\\_experimental\\optuna_tuning.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "XGBoostOptimizer"
  },
  {
    "name": "optimize",
    "category": "machine_learning",
    "formula": "score | OptimizationResult(",
    "explanation": "Optimize XGBoost hyperparameters.\n\nArgs:\n    X_train, y_train: Training data\n    X_val, y_val: Validation data\n    objective: XGBoost objective\n    metric: Optimization metric\n\nReturns:\n    OptimizationResult with best parameters",
    "python_code": "def optimize(self, X_train: np.ndarray, y_train: np.ndarray,\n                X_val: np.ndarray, y_val: np.ndarray,\n                objective: str = 'binary:logistic',\n                metric: str = 'auc') -> OptimizationResult:\n        \"\"\"\n        Optimize XGBoost hyperparameters.\n\n        Args:\n            X_train, y_train: Training data\n            X_val, y_val: Validation data\n            objective: XGBoost objective\n            metric: Optimization metric\n\n        Returns:\n            OptimizationResult with best parameters\n        \"\"\"\n        if not HAS_OPTUNA:\n            logger.warning(\"Optuna not available, returning defaults\")\n            return self._default_xgb_params()\n\n        try:\n            import xgboost as xgb\n        except ImportError:\n            logger.warning(\"XGBoost not available\")\n            return self._default_xgb_params()\n\n        def objective_fn(trial):\n            params = {\n                'objective': objective,\n                'eval_metric': metric,\n                'max_depth': trial.suggest_int('max_depth', 3, 10),\n                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n                'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n            }\n\n            dtrain = xgb.DMatrix(X_train, label=y_train)\n            dval = xgb.DMatrix(X_val, label=y_val)\n\n            model = xgb.train(\n                params,\n                dtrain,\n                num_boost_round=params.pop('n_estimators'),\n                evals=[(dval, 'val')],\n                early_stopping_rounds=50,\n                verbose_eval=False\n            )\n\n            preds = model.predict(dval)\n            if metric == 'auc':\n                from sklearn.metrics import roc_auc_score\n                score = roc_auc_score(y_val, preds)\n            else:\n                from sklearn.metrics import accuracy_score\n                score = accuracy_score(y_val, (preds > 0.5).astype(int))\n\n            return score\n\n        self.study = optuna.create_study(\n            direction='maximize',\n            sampler=TPESampler(seed=42),\n            study_name='xgboost_optimization'\n        )\n\n        self.study.optimize(\n            objective_fn,\n            n_trials=self.n_trials,\n            timeout=self.timeout,\n            show_progress_bar=True\n        )\n\n        self.best_params = self.study.best_params\n        logger.info(f\"Best XGBoost params: {self.best_params}\")\n        logger.info(f\"Best score: {self.study.best_value:.4f}",
    "source_file": "core\\_experimental\\optuna_tuning.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "XGBoostOptimizer"
  },
  {
    "name": "_default_xgb_params",
    "category": "quantitative",
    "formula": "OptimizationResult(",
    "explanation": "Return default XGBoost parameters.",
    "python_code": "def _default_xgb_params(self) -> OptimizationResult:\n        \"\"\"Return default XGBoost parameters.\"\"\"\n        return OptimizationResult(\n            best_params={\n                'max_depth': 6,\n                'learning_rate': 0.1,\n                'n_estimators': 500,\n                'min_child_weight': 1,\n                'subsample': 0.8,\n                'colsample_bytree': 0.8,\n                'reg_alpha': 0.01,\n                'reg_lambda': 1.0,\n                'gamma': 0.01\n            },\n            best_value=0.0,\n            n_trials=0,\n            study_name='default'\n        )",
    "source_file": "core\\_experimental\\optuna_tuning.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "XGBoostOptimizer"
  },
  {
    "name": "_default_lgb_params",
    "category": "quantitative",
    "formula": "OptimizationResult(",
    "explanation": "",
    "python_code": "def _default_lgb_params(self) -> OptimizationResult:\n        return OptimizationResult(\n            best_params={\n                'max_depth': 6,\n                'learning_rate': 0.1,\n                'n_estimators': 500,\n                'num_leaves': 31,\n                'min_child_samples': 20,\n                'subsample': 0.8,\n                'colsample_bytree': 0.8\n            },\n            best_value=0.0,\n            n_trials=0,\n            study_name='default'\n        )",
    "source_file": "core\\_experimental\\optuna_tuning.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "LightGBMOptimizer"
  },
  {
    "name": "optimize",
    "category": "risk",
    "formula": "sharpe | OptimizationResult(",
    "explanation": "Optimize signal weights to maximize Sharpe ratio.\n\nArgs:\n    signals: Dict of signal name -> signal array\n    returns: Forward returns to predict\n\nReturns:\n    OptimizationResult with optimal weights",
    "python_code": "def optimize(self, signals: Dict[str, np.ndarray],\n                returns: np.ndarray) -> OptimizationResult:\n        \"\"\"\n        Optimize signal weights to maximize Sharpe ratio.\n\n        Args:\n            signals: Dict of signal name -> signal array\n            returns: Forward returns to predict\n\n        Returns:\n            OptimizationResult with optimal weights\n        \"\"\"\n        if not HAS_OPTUNA:\n            return self._equal_weights(list(signals.keys()))\n\n        signal_names = list(signals.keys())\n        signal_matrix = np.column_stack([signals[name] for name in signal_names])\n\n        def objective_fn(trial):\n            # Suggest weights\n            weights = []\n            for name in signal_names:\n                w = trial.suggest_float(f'w_{name}', 0.0, 1.0)\n                weights.append(w)\n\n            weights = np.array(weights)\n            weights = weights / (weights.sum() + 1e-10)  # Normalize\n\n            # Combined signal\n            combined = signal_matrix @ weights\n\n            # Calculate Sharpe (assuming signals predict direction)\n            position = np.sign(combined)\n            pnl = position * returns\n\n            # Sharpe ratio\n            sharpe = np.mean(pnl) / (np.std(pnl) + 1e-10) * np.sqrt(252 * 24 * 12)  # Annualized for 5-min bars\n\n            return sharpe\n\n        self.study = optuna.create_study(\n            direction='maximize',\n            sampler=TPESampler(seed=42),\n            study_name='signal_weight_optimization'\n        )\n\n        self.study.optimize(objective_fn, n_trials=self.n_trials)\n\n        # Extract normalized weights\n        best_weights = {}\n        total = 0\n        for name in signal_names:\n            w = self.study.best_params[f'w_{name}']\n            best_weights[name] = w\n            total += w\n\n        for name in signal_names:\n            best_weights[name] /= (total + 1e-10)\n\n        logger.info(f\"Optimal signal weights: {best_weights}\")\n        logger.info(f\"Best Sharpe: {self.study.best_value:.2f}\")\n\n        return OptimizationResult(\n            best_params=best_weights,\n            best_value=self.study.best_value,\n            n_trials=len(self.study.trials),\n            study_name='signal_weight_optimization'\n        )",
    "source_file": "core\\_experimental\\optuna_tuning.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "SignalWeightOptimizer"
  },
  {
    "name": "_equal_weights",
    "category": "quantitative",
    "formula": "OptimizationResult(",
    "explanation": "",
    "python_code": "def _equal_weights(self, signal_names: List[str]) -> OptimizationResult:\n        n = len(signal_names)\n        return OptimizationResult(\n            best_params={name: 1.0/n for name in signal_names},\n            best_value=0.0,\n            n_trials=0,\n            study_name='default'\n        )",
    "source_file": "core\\_experimental\\optuna_tuning.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "SignalWeightOptimizer"
  },
  {
    "name": "optimize",
    "category": "volatility",
    "formula": "= 1.0 | *= (1 + move) | -10.0  # Not enough trades",
    "explanation": "Optimize risk parameters.\n\nArgs:\n    signals: Trading signals (-1, 0, 1)\n    returns: Realized returns\n    volatility: Volatility estimates (for position sizing)\n\nReturns:\n    OptimizationResult with optimal risk parameters",
    "python_code": "def optimize(self, signals: np.ndarray, returns: np.ndarray,\n                volatility: np.ndarray) -> OptimizationResult:\n        \"\"\"\n        Optimize risk parameters.\n\n        Args:\n            signals: Trading signals (-1, 0, 1)\n            returns: Realized returns\n            volatility: Volatility estimates (for position sizing)\n\n        Returns:\n            OptimizationResult with optimal risk parameters\n        \"\"\"\n        if not HAS_OPTUNA:\n            return self._default_risk_params()\n\n        def objective_fn(trial):\n            kelly_frac = trial.suggest_float('kelly_fraction', 0.1, 0.5)\n            stop_atr = trial.suggest_float('stop_loss_atr', 1.0, 5.0)\n            tp_atr = trial.suggest_float('take_profit_atr', 1.0, 5.0)\n\n            # Simulate trading with these parameters\n            pnl = []\n            position = 0\n            entry_price = 0\n            cumulative_return = 1.0\n\n            for t in range(len(signals)):\n                if signals[t] != 0 and position == 0:\n                    # Enter position\n                    position = signals[t]\n                    entry_price = 1.0  # Normalized\n                    size = kelly_frac / (volatility[t] + 1e-10)\n                    size = min(size, 1.0)  # Cap at 100%\n\n                elif position != 0:\n                    # Check exit conditions\n                    move = returns[t] * position * size\n                    stop = -stop_atr * volatility[t]\n                    tp = tp_atr * volatility[t]\n\n                    if move <= stop or move >= tp or signals[t] == -position:\n                        pnl.append(move)\n                        cumulative_return *= (1 + move)\n                        position = 0\n\n            if len(pnl) < 10:\n                return -10.0  # Not enough trades\n\n            # Objective: Maximize risk-adjusted return\n            total_return = cumulative_return - 1\n            vol = np.std(pnl)\n            sharpe = (np.mean(pnl) / (vol + 1e-10)) * np.sqrt(252)\n            max_dd = self._max_drawdown(np.cumprod(1 + np.array(pnl)))\n\n            # Penalize high drawdown\n            score = sharpe - max_dd * 2\n\n            return score\n\n        self.study = optuna.create_study(\n            direction='maximize',\n            sampler=TPESampler(seed=42)\n        )\n\n        self.study.optimize(objective_fn, n_trials=self.n_trials)\n\n        return OptimizationResult(\n            best_params=self.study.best_params,\n            best_value=self.study.best_value,\n            n_trials=len(self.study.trials),\n            study_name='risk_optimization'\n        )",
    "source_file": "core\\_experimental\\optuna_tuning.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "RiskParameterOptimizer"
  },
  {
    "name": "_max_drawdown",
    "category": "risk",
    "formula": "np.max(drawdown)",
    "explanation": "",
    "python_code": "def _max_drawdown(self, cumulative: np.ndarray) -> float:\n        peak = np.maximum.accumulate(cumulative)\n        drawdown = (peak - cumulative) / peak\n        return np.max(drawdown)",
    "source_file": "core\\_experimental\\optuna_tuning.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "RiskParameterOptimizer"
  },
  {
    "name": "_default_risk_params",
    "category": "quantitative",
    "formula": "OptimizationResult(",
    "explanation": "",
    "python_code": "def _default_risk_params(self) -> OptimizationResult:\n        return OptimizationResult(\n            best_params={\n                'kelly_fraction': 0.25,\n                'stop_loss_atr': 2.0,\n                'take_profit_atr': 3.0\n            },\n            best_value=0.0,\n            n_trials=0,\n            study_name='default'\n        )",
    "source_file": "core\\_experimental\\optuna_tuning.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "RiskParameterOptimizer"
  },
  {
    "name": "save_optimization_results",
    "category": "quantitative",
    "formula": "",
    "explanation": "Save optimization results to JSON.",
    "python_code": "def save_optimization_results(results: Dict[str, OptimizationResult],\n                             filepath: Path):\n    \"\"\"Save optimization results to JSON.\"\"\"\n    data = {\n        name: {\n            'best_params': result.best_params,\n            'best_value': result.best_value,\n            'n_trials': result.n_trials\n        }\n        for name, result in results.items()\n    }\n\n    with open(filepath, 'w') as f:\n        json.dump(data, f, indent=2)\n\n    logger.info(f\"Saved optimization results to {filepath}\")",
    "source_file": "core\\_experimental\\optuna_tuning.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": null
  },
  {
    "name": "load_optimization_results",
    "category": "quantitative",
    "formula": "json.load(f)",
    "explanation": "Load optimization results from JSON.",
    "python_code": "def load_optimization_results(filepath: Path) -> Dict[str, Dict]:\n    \"\"\"Load optimization results from JSON.\"\"\"\n    with open(filepath) as f:\n        return json.load(f)",
    "source_file": "core\\_experimental\\optuna_tuning.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": null
  },
  {
    "name": "objective_fn",
    "category": "quantitative",
    "formula": "score",
    "explanation": "",
    "python_code": "def objective_fn(trial):\n            params = {\n                'objective': objective,\n                'eval_metric': metric,\n                'max_depth': trial.suggest_int('max_depth', 3, 10),\n                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n                'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n            }\n\n            dtrain = xgb.DMatrix(X_train, label=y_train)\n            dval = xgb.DMatrix(X_val, label=y_val)\n\n            model = xgb.train(\n                params,\n                dtrain,\n                num_boost_round=params.pop('n_estimators'),\n                evals=[(dval, 'val')],\n                early_stopping_rounds=50,\n                verbose_eval=False\n            )\n\n            preds = model.predict(dval)\n            if metric == 'auc':\n                from sklearn.metrics import roc_auc_score\n                score = roc_auc_score(y_val, preds)\n            else:\n                from sklearn.metrics import accuracy_score\n                score = accuracy_score(y_val, (preds > 0.5).astype(int))\n\n            return score",
    "source_file": "core\\_experimental\\optuna_tuning.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": null
  },
  {
    "name": "objective_fn",
    "category": "quantitative",
    "formula": "sharpe",
    "explanation": "",
    "python_code": "def objective_fn(trial):\n            # Suggest weights\n            weights = []\n            for name in signal_names:\n                w = trial.suggest_float(f'w_{name}', 0.0, 1.0)\n                weights.append(w)\n\n            weights = np.array(weights)\n            weights = weights / (weights.sum() + 1e-10)  # Normalize\n\n            # Combined signal\n            combined = signal_matrix @ weights\n\n            # Calculate Sharpe (assuming signals predict direction)\n            position = np.sign(combined)\n            pnl = position * returns\n\n            # Sharpe ratio\n            sharpe = np.mean(pnl) / (np.std(pnl) + 1e-10) * np.sqrt(252 * 24 * 12)  # Annualized for 5-min bars\n\n            return sharpe",
    "source_file": "core\\_experimental\\optuna_tuning.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": null
  },
  {
    "name": "objective_fn",
    "category": "quantitative",
    "formula": "= 1.0 | *= (1 + move) | -10.0  # Not enough trades",
    "explanation": "",
    "python_code": "def objective_fn(trial):\n            kelly_frac = trial.suggest_float('kelly_fraction', 0.1, 0.5)\n            stop_atr = trial.suggest_float('stop_loss_atr', 1.0, 5.0)\n            tp_atr = trial.suggest_float('take_profit_atr', 1.0, 5.0)\n\n            # Simulate trading with these parameters\n            pnl = []\n            position = 0\n            entry_price = 0\n            cumulative_return = 1.0\n\n            for t in range(len(signals)):\n                if signals[t] != 0 and position == 0:\n                    # Enter position\n                    position = signals[t]\n                    entry_price = 1.0  # Normalized\n                    size = kelly_frac / (volatility[t] + 1e-10)\n                    size = min(size, 1.0)  # Cap at 100%\n\n                elif position != 0:\n                    # Check exit conditions\n                    move = returns[t] * position * size\n                    stop = -stop_atr * volatility[t]\n                    tp = tp_atr * volatility[t]\n\n                    if move <= stop or move >= tp or signals[t] == -position:\n                        pnl.append(move)\n                        cumulative_return *= (1 + move)\n                        position = 0\n\n            if len(pnl) < 10:\n                return -10.0  # Not enough trades\n\n            # Objective: Maximize risk-adjusted return\n            total_return = cumulative_return - 1\n            vol = np.std(pnl)\n            sharpe = (np.mean(pnl) / (vol + 1e-10)) * np.sqrt(252)\n            max_dd = self._max_drawdown(np.cumprod(1 + np.array(pnl)))\n\n            # Penalize high drawdown\n            score = sharpe - max_dd * 2\n\n            return score",
    "source_file": "core\\_experimental\\optuna_tuning.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "microstructure",
    "formula": "",
    "explanation": "Initialize order flow feature generator.\n\nArgs:\n    lookback: Number of updates for rolling calculations\n    vpin_buckets: Number of volume buckets for VPIN",
    "python_code": "def __init__(self, lookback: int = 100, vpin_buckets: int = 50):\n        \"\"\"\n        Initialize order flow feature generator.\n\n        Args:\n            lookback: Number of updates for rolling calculations\n            vpin_buckets: Number of volume buckets for VPIN\n        \"\"\"\n        self.lookback = lookback\n        self.vpin_buckets = vpin_buckets\n\n        # Order book history\n        self.book_history: deque = deque(maxlen=lookback)\n\n        # Trade history\n        self.trade_history: deque = deque(maxlen=lookback * 10)\n\n        # VPIN volume buckets\n        self.volume_buckets: List[Dict] = []\n        self.bucket_volume: float = 1000.0  # Volume per bucket\n\n        # Hawkes process parameters\n        self.hawkes_mu = 0.1  # Base intensity\n        self.hawkes_alpha = 0.5  # Self-excitation\n        self.hawkes_beta = 1.0  # Decay rate\n\n        # Previous state\n        self.prev_bid = None\n        self.prev_ask = None\n        self.prev_bid_size = None\n        self.prev_ask_size = None",
    "source_file": "core\\_experimental\\order_flow_features.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OrderFlowFeatures"
  },
  {
    "name": "update_book",
    "category": "microstructure",
    "formula": "",
    "explanation": "Update with new book state.\n\nArgs:\n    bid: Best bid price\n    ask: Best ask price\n    bid_size: Quantity at best bid\n    ask_size: Quantity at best ask\n    timestamp: Update timestamp",
    "python_code": "def update_book(self, bid: float, ask: float,\n                    bid_size: float, ask_size: float,\n                    timestamp: datetime = None) -> None:\n        \"\"\"\n        Update with new book state.\n\n        Args:\n            bid: Best bid price\n            ask: Best ask price\n            bid_size: Quantity at best bid\n            ask_size: Quantity at best ask\n            timestamp: Update timestamp\n        \"\"\"\n        if timestamp is None:\n            timestamp = datetime.now()\n\n        # Calculate OFI changes\n        ofi_delta = 0.0\n\n        if self.prev_bid is not None:\n            # Bid side OFI\n            if bid > self.prev_bid:\n                ofi_delta += bid_size\n            elif bid == self.prev_bid:\n                ofi_delta += bid_size - self.prev_bid_size\n            else:\n                ofi_delta -= self.prev_bid_size\n\n            # Ask side OFI\n            if ask < self.prev_ask:\n                ofi_delta -= ask_size\n            elif ask == self.prev_ask:\n                ofi_delta -= ask_size - self.prev_ask_size\n            else:\n                ofi_delta += self.prev_ask_size\n\n        self.book_history.append({\n            'timestamp': timestamp,\n            'bid': bid,\n            'ask': ask,\n            'bid_size': bid_size,\n            'ask_size': ask_size,\n            'ofi_delta': ofi_delta,\n            'mid': (bid + ask) / 2,\n            'spread': ask - bid\n        })\n\n        # Update previous state\n        self.prev_bid = bid\n        self.prev_ask = ask\n        self.prev_bid_size = bid_size\n        self.prev_ask_size = ask_size",
    "source_file": "core\\_experimental\\order_flow_features.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OrderFlowFeatures"
  },
  {
    "name": "update_trade",
    "category": "microstructure",
    "formula": "",
    "explanation": "Update with new trade.\n\nArgs:\n    price: Trade price\n    size: Trade size\n    side: 'buy' or 'sell' (aggressor side)\n    timestamp: Trade timestamp",
    "python_code": "def update_trade(self, price: float, size: float, side: str,\n                     timestamp: datetime = None) -> None:\n        \"\"\"\n        Update with new trade.\n\n        Args:\n            price: Trade price\n            size: Trade size\n            side: 'buy' or 'sell' (aggressor side)\n            timestamp: Trade timestamp\n        \"\"\"\n        if timestamp is None:\n            timestamp = datetime.now()\n\n        direction = 1 if side.lower() == 'buy' else -1\n\n        self.trade_history.append({\n            'timestamp': timestamp,\n            'price': price,\n            'size': size,\n            'side': side,\n            'direction': direction,\n            'signed_volume': size * direction\n        })\n\n        # Update VPIN\n        self._update_vpin(size, direction)\n\n        # Update Hawkes\n        self._update_hawkes(timestamp)",
    "source_file": "core\\_experimental\\order_flow_features.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OrderFlowFeatures"
  },
  {
    "name": "_update_vpin",
    "category": "microstructure",
    "formula": "",
    "explanation": "Update VPIN calculation.",
    "python_code": "def _update_vpin(self, volume: float, direction: int) -> None:\n        \"\"\"Update VPIN calculation.\"\"\"\n        if not self.volume_buckets:\n            self.volume_buckets.append({\n                'buy_volume': 0,\n                'sell_volume': 0,\n                'total_volume': 0\n            })\n\n        current_bucket = self.volume_buckets[-1]\n\n        if direction > 0:\n            current_bucket['buy_volume'] += volume\n        else:\n            current_bucket['sell_volume'] += volume\n\n        current_bucket['total_volume'] += volume\n\n        # Start new bucket if full\n        if current_bucket['total_volume'] >= self.bucket_volume:\n            self.volume_buckets.append({\n                'buy_volume': 0,\n                'sell_volume': 0,\n                'total_volume': 0\n            })\n\n            # Keep only recent buckets\n            if len(self.volume_buckets) > self.vpin_buckets:\n                self.volume_buckets.pop(0)",
    "source_file": "core\\_experimental\\order_flow_features.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OrderFlowFeatures"
  },
  {
    "name": "_update_hawkes",
    "category": "microstructure",
    "formula": "# Hawkes intensity: (t) =  +   * exp(- * (t - ti))",
    "explanation": "Update Hawkes process intensity estimate.",
    "python_code": "def _update_hawkes(self, timestamp: datetime) -> None:\n        \"\"\"Update Hawkes process intensity estimate.\"\"\"\n        # Calculate intensity from recent trades\n        now = timestamp\n        recent_trades = [t for t in self.trade_history\n                         if (now - t['timestamp']).total_seconds() < 60]\n\n        if not recent_trades:\n            return\n\n        # Hawkes intensity: (t) =  +   * exp(- * (t - ti))\n        intensity = self.hawkes_mu\n\n        for trade in recent_trades:\n            time_diff = (now - trade['timestamp']).total_seconds()\n            intensity += self.hawkes_alpha * np.exp(-self.hawkes_beta * time_diff)\n\n        # Store for retrieval\n        if self.trade_history:\n            self.trade_history[-1]['hawkes_intensity'] = intensity",
    "source_file": "core\\_experimental\\order_flow_features.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OrderFlowFeatures"
  },
  {
    "name": "get_ofi",
    "category": "microstructure",
    "formula": "OFI = (bid_size_change - ask_size_change) weighted by price changes | positive = buying pressure) | 0.0",
    "explanation": "Calculate Order Flow Imbalance.\n\nOFI = (bid_size_change - ask_size_change) weighted by price changes\n\nReturns:\n    OFI value (positive = buying pressure)",
    "python_code": "def get_ofi(self, window: int = None) -> float:\n        \"\"\"\n        Calculate Order Flow Imbalance.\n\n        OFI = (bid_size_change - ask_size_change) weighted by price changes\n\n        Returns:\n            OFI value (positive = buying pressure)\n        \"\"\"\n        if not self.book_history:\n            return 0.0\n\n        window = window or len(self.book_history)\n        recent = list(self.book_history)[-window:]\n\n        ofi = sum(update['ofi_delta'] for update in recent)\n\n        # Normalize by total volume\n        total_vol = sum(update['bid_size'] + update['ask_size'] for update in recent)\n        if total_vol > 0:\n            ofi /= total_vol\n\n        return ofi",
    "source_file": "core\\_experimental\\order_flow_features.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OrderFlowFeatures"
  },
  {
    "name": "get_oei",
    "category": "microstructure",
    "formula": "positive = more efficient buying) | 0.0 | 0.0",
    "explanation": "Calculate Order Execution Imbalance.\n\nOEI measures efficiency of bid/ask execution.\n\nReturns:\n    OEI value (positive = more efficient buying)",
    "python_code": "def get_oei(self, window: int = None) -> float:\n        \"\"\"\n        Calculate Order Execution Imbalance.\n\n        OEI measures efficiency of bid/ask execution.\n\n        Returns:\n            OEI value (positive = more efficient buying)\n        \"\"\"\n        if not self.trade_history:\n            return 0.0\n\n        window = window or len(self.trade_history)\n        recent = list(self.trade_history)[-window:]\n\n        buy_volume = sum(t['size'] for t in recent if t['direction'] > 0)\n        sell_volume = sum(t['size'] for t in recent if t['direction'] < 0)\n\n        total = buy_volume + sell_volume\n        if total == 0:\n            return 0.0\n\n        return (buy_volume - sell_volume) / total",
    "source_file": "core\\_experimental\\order_flow_features.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OrderFlowFeatures"
  },
  {
    "name": "get_depth_ratio",
    "category": "microstructure",
    "formula": "1.0 | float('inf') if bid_size > 0 else 1.0 | bid_size / ask_size",
    "explanation": "Calculate depth ratio.\n\nReturns:\n    bid_depth / ask_depth (>1 = more buy interest)",
    "python_code": "def get_depth_ratio(self) -> float:\n        \"\"\"\n        Calculate depth ratio.\n\n        Returns:\n            bid_depth / ask_depth (>1 = more buy interest)\n        \"\"\"\n        if not self.book_history:\n            return 1.0\n\n        current = self.book_history[-1]\n        bid_size = current['bid_size']\n        ask_size = current['ask_size']\n\n        if ask_size == 0:\n            return float('inf') if bid_size > 0 else 1.0\n\n        return bid_size / ask_size",
    "source_file": "core\\_experimental\\order_flow_features.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OrderFlowFeatures"
  },
  {
    "name": "get_trade_imbalance",
    "category": "microstructure",
    "formula": "0.0 | 0.0 | (buy_trades - sell_trades) / total",
    "explanation": "Calculate trade imbalance.\n\nReturns:\n    (buy_trades - sell_trades) / total_trades",
    "python_code": "def get_trade_imbalance(self, window: int = 20) -> float:\n        \"\"\"\n        Calculate trade imbalance.\n\n        Returns:\n            (buy_trades - sell_trades) / total_trades\n        \"\"\"\n        if not self.trade_history:\n            return 0.0\n\n        recent = list(self.trade_history)[-window:]\n\n        buy_trades = sum(1 for t in recent if t['direction'] > 0)\n        sell_trades = sum(1 for t in recent if t['direction'] < 0)\n\n        total = buy_trades + sell_trades\n        if total == 0:\n            return 0.0\n\n        return (buy_trades - sell_trades) / total",
    "source_file": "core\\_experimental\\order_flow_features.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OrderFlowFeatures"
  },
  {
    "name": "get_vpin",
    "category": "microstructure",
    "formula": "VPIN = |buy_vol - sell_vol| / (buy_vol + sell_vol) | Higher = more informed trading (toxic flow) | 0.5  # Neutral default",
    "explanation": "Calculate Volume-Synchronized Probability of Informed Trading.\n\nVPIN = |buy_vol - sell_vol| / (buy_vol + sell_vol)\n\nHigher = more informed trading (toxic flow)",
    "python_code": "def get_vpin(self) -> float:\n        \"\"\"\n        Calculate Volume-Synchronized Probability of Informed Trading.\n\n        VPIN = |buy_vol - sell_vol| / (buy_vol + sell_vol)\n\n        Higher = more informed trading (toxic flow)\n        \"\"\"\n        if len(self.volume_buckets) < 10:\n            return 0.5  # Neutral default\n\n        # Use last N buckets\n        buckets = self.volume_buckets[-self.vpin_buckets:]\n\n        abs_imbalance = sum(\n            abs(b['buy_volume'] - b['sell_volume'])\n            for b in buckets\n        )\n        total_volume = sum(b['total_volume'] for b in buckets)\n\n        if total_volume == 0:\n            return 0.5\n\n        return abs_imbalance / total_volume",
    "source_file": "core\\_experimental\\order_flow_features.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OrderFlowFeatures"
  },
  {
    "name": "get_hawkes_intensity",
    "category": "microstructure",
    "formula": "Higher = more self-exciting (momentum)",
    "explanation": "Get current Hawkes process intensity.\n\nHigher = more self-exciting (momentum)",
    "python_code": "def get_hawkes_intensity(self) -> float:\n        \"\"\"\n        Get current Hawkes process intensity.\n\n        Higher = more self-exciting (momentum)\n        \"\"\"\n        if not self.trade_history:\n            return self.hawkes_mu\n\n        return self.trade_history[-1].get('hawkes_intensity', self.hawkes_mu)",
    "source_file": "core\\_experimental\\order_flow_features.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OrderFlowFeatures"
  },
  {
    "name": "get_spread_percentile",
    "category": "microstructure",
    "formula": "50.0 | percentile",
    "explanation": "Calculate current spread percentile vs history.\n\nReturns:\n    Percentile (0-100)",
    "python_code": "def get_spread_percentile(self, lookback: int = 100) -> float:\n        \"\"\"\n        Calculate current spread percentile vs history.\n\n        Returns:\n            Percentile (0-100)\n        \"\"\"\n        if len(self.book_history) < 10:\n            return 50.0\n\n        spreads = [update['spread'] for update in self.book_history][-lookback:]\n        current = spreads[-1]\n\n        percentile = sum(1 for s in spreads[:-1] if s < current) / len(spreads[:-1]) * 100\n        return percentile",
    "source_file": "core\\_experimental\\order_flow_features.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OrderFlowFeatures"
  },
  {
    "name": "get_microprice_momentum",
    "category": "microstructure",
    "formula": "0.0 | 0.0 | (microprices[-1] / microprices[0] - 1) * 10000",
    "explanation": "Calculate microprice momentum in basis points.",
    "python_code": "def get_microprice_momentum(self, window: int = 20) -> float:\n        \"\"\"\n        Calculate microprice momentum in basis points.\n        \"\"\"\n        if len(self.book_history) < window:\n            return 0.0\n\n        recent = list(self.book_history)[-window:]\n\n        # Calculate microprices\n        microprices = []\n        for update in recent:\n            bid = update['bid']\n            ask = update['ask']\n            bid_size = update['bid_size']\n            ask_size = update['ask_size']\n\n            total = bid_size + ask_size\n            if total > 0:\n                microprice = (bid * ask_size + ask * bid_size) / total\n                microprices.append(microprice)\n\n        if len(microprices) < 2:\n            return 0.0\n\n        return (microprices[-1] / microprices[0] - 1) * 10000",
    "source_file": "core\\_experimental\\order_flow_features.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OrderFlowFeatures"
  },
  {
    "name": "get_signals",
    "category": "microstructure",
    "formula": "{",
    "explanation": "Get all order flow signals.\n\nReturns:\n    Dict with signal names and values",
    "python_code": "def get_signals(self) -> Dict[str, float]:\n        \"\"\"\n        Get all order flow signals.\n\n        Returns:\n            Dict with signal names and values\n        \"\"\"\n        return {\n            'ofi': self.get_ofi(),\n            'ofi_short': self.get_ofi(window=10),\n            'ofi_long': self.get_ofi(window=50),\n            'oei': self.get_oei(),\n            'depth_ratio': self.get_depth_ratio(),\n            'trade_imbalance': self.get_trade_imbalance(),\n            'vpin': self.get_vpin(),\n            'hawkes_intensity': self.get_hawkes_intensity(),\n            'spread_percentile': self.get_spread_percentile(),\n            'microprice_momentum': self.get_microprice_momentum(),\n            'combined_signal': self._combined_signal()\n        }",
    "source_file": "core\\_experimental\\order_flow_features.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OrderFlowFeatures"
  },
  {
    "name": "_combined_signal",
    "category": "microstructure",
    "formula": "positive = bullish) | max(-1, min(1, signal))",
    "explanation": "Generate combined order flow signal.\n\nReturns:\n    Signal in range [-1, 1] (positive = bullish)",
    "python_code": "def _combined_signal(self) -> float:\n        \"\"\"\n        Generate combined order flow signal.\n\n        Returns:\n            Signal in range [-1, 1] (positive = bullish)\n        \"\"\"\n        # Get component signals\n        ofi = self.get_ofi()\n        oei = self.get_oei()\n        depth = self.get_depth_ratio()\n        trade_imb = self.get_trade_imbalance()\n\n        # Normalize depth ratio\n        depth_signal = np.tanh(depth - 1)  # Center around 1\n\n        # Weight components\n        signal = (\n            0.30 * ofi +\n            0.25 * oei +\n            0.20 * depth_signal +\n            0.25 * trade_imb\n        )\n\n        # Clip to [-1, 1]\n        return max(-1, min(1, signal))",
    "source_file": "core\\_experimental\\order_flow_features.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OrderFlowFeatures"
  },
  {
    "name": "get_state",
    "category": "microstructure",
    "formula": "OrderFlowState(",
    "explanation": "Get current order flow state.",
    "python_code": "def get_state(self) -> OrderFlowState:\n        \"\"\"Get current order flow state.\"\"\"\n        return OrderFlowState(\n            ofi=self.get_ofi(),\n            oei=self.get_oei(),\n            depth_ratio=self.get_depth_ratio(),\n            trade_imbalance=self.get_trade_imbalance(),\n            hawkes_intensity=self.get_hawkes_intensity(),\n            vpin=self.get_vpin()\n        )",
    "source_file": "core\\_experimental\\order_flow_features.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OrderFlowFeatures"
  },
  {
    "name": "generate_features",
    "category": "microstructure",
    "formula": "result",
    "explanation": "Generate order flow features from tick DataFrame.\n\nExpects columns:\n- timestamp\n- bid, ask\n- bid_size, ask_size (optional)\n- last_price, last_size (for trades)\n- side (for trades, 'buy' or 'sell')\n\nReturns:\n    DataFrame with additional OFI/OEI columns",
    "python_code": "def generate_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate order flow features from tick DataFrame.\n\n        Expects columns:\n        - timestamp\n        - bid, ask\n        - bid_size, ask_size (optional)\n        - last_price, last_size (for trades)\n        - side (for trades, 'buy' or 'sell')\n\n        Returns:\n            DataFrame with additional OFI/OEI columns\n        \"\"\"\n        result = df.copy()\n\n        # Initialize feature columns\n        result['ofi'] = 0.0\n        result['oei'] = 0.0\n        result['depth_ratio'] = 1.0\n        result['trade_imbalance'] = 0.0\n        result['vpin'] = 0.5\n        result['hawkes_intensity'] = 0.0\n        result['combined_flow_signal'] = 0.0\n\n        # Reset state\n        self.book_history.clear()\n        self.trade_history.clear()\n        self.volume_buckets.clear()\n        self.prev_bid = None\n        self.prev_ask = None\n\n        # Process each row\n        for idx, row in df.iterrows():\n            # Update book if quotes present\n            if 'bid' in row and 'ask' in row:\n                bid_size = row.get('bid_size', 1.0)\n                ask_size = row.get('ask_size', 1.0)\n                self.update_book(\n                    row['bid'], row['ask'],\n                    bid_size, ask_size,\n                    row.get('timestamp', datetime.now())\n                )\n\n            # Update trade if present\n            if 'last_price' in row and pd.notna(row.get('last_price')):\n                side = row.get('side', 'buy')  # Default to buy if not specified\n                self.update_trade(\n                    row['last_price'],\n                    row.get('last_size', 1.0),\n                    side,\n                    row.get('timestamp', datetime.now())\n                )\n\n            # Get signals\n            signals = self.get_signals()\n            result.loc[idx, 'ofi'] = signals['ofi']\n            result.loc[idx, 'oei'] = signals['oei']\n            result.loc[idx, 'depth_ratio'] = signals['depth_ratio']\n            result.loc[idx, 'trade_imbalance'] = signals['trade_imbalance']\n            result.loc[idx, 'vpin'] = signals['vpin']\n            result.loc[idx, 'hawkes_intensity'] = signals['hawkes_intensity']\n            result.loc[idx, 'combined_flow_signal'] = signals['combined_signal']\n\n        return result",
    "source_file": "core\\_experimental\\order_flow_features.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "OrderFlowFeatures"
  },
  {
    "name": "infer_direction",
    "category": "microstructure",
    "formula": "'buy' | 'sell' | 'buy'",
    "explanation": "Infer trade direction.\n\nArgs:\n    price: Trade price\n    bid: Best bid\n    ask: Best ask\n    prev_price: Previous trade price (for tick rule)\n\nReturns:\n    'buy' or 'sell'",
    "python_code": "def infer_direction(self, price: float, bid: float, ask: float,\n                        prev_price: float = None) -> str:\n        \"\"\"\n        Infer trade direction.\n\n        Args:\n            price: Trade price\n            bid: Best bid\n            ask: Best ask\n            prev_price: Previous trade price (for tick rule)\n\n        Returns:\n            'buy' or 'sell'\n        \"\"\"\n        mid = (bid + ask) / 2\n\n        if self.method == 'lee_ready':\n            # Quote rule: above mid = buy, below mid = sell\n            if price > mid:\n                return 'buy'\n            elif price < mid:\n                return 'sell'\n            else:\n                # At mid, use tick rule\n                if prev_price is not None:\n                    if price > prev_price:\n                        return 'buy'\n                    elif price < prev_price:\n                        return 'sell'\n                return 'buy'  # Default\n\n        elif self.method == 'tick':\n            # Pure tick rule\n            if prev_price is not None:\n                if price > prev_price:\n                    return 'buy'\n                elif price < prev_price:\n                    return 'sell'\n            return 'buy'\n\n        elif self.method == 'bulk':\n            # Bulk classification: closer to ask = buy\n            if abs(price - ask) < abs(price - bid):\n                return 'buy'\n            else:\n                return 'sell'\n\n        return 'buy'",
    "source_file": "core\\_experimental\\order_flow_features.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TradeDirectionInference"
  },
  {
    "name": "__init__",
    "category": "microstructure",
    "formula": "",
    "explanation": "Initialize OU model.",
    "python_code": "def __init__(self):\n        \"\"\"Initialize OU model.\"\"\"\n        self.theta = None   # Mean reversion speed\n        self.mu = None      # Long-term mean\n        self.sigma = None   # Volatility\n\n        # Cointegration results\n        self.hedge_ratio = None\n        self.is_cointegrated = False\n        self.adf_pvalue = None",
    "source_file": "core\\_experimental\\ou_spread_trading.py",
    "academic_reference": null,
    "class_name": "OUSpreadTrading"
  },
  {
    "name": "estimate_parameters",
    "category": "microstructure",
    "formula": "",
    "explanation": "Estimate OU parameters using Maximum Likelihood Estimation.\n\nThe MLE estimates for OU process:\n     = -log() / dt  where  is AR(1) coefficient\n     = mean(S)\n     = std(dS) / sqrt(dt)\n\nArgs:\n    spread: Spread time series\n    dt: Time step (default 1/252 for daily data)\n    method: 'mle' or 'ols'\n\nReturns:\n    Dict with theta, mu, sigma",
    "python_code": "def estimate_parameters(\n        self,\n        spread: pd.Series,\n        dt: float = 1/252,\n        method: str = 'mle'\n    ) -> Dict[str, float]:\n        \"\"\"\n        Estimate OU parameters using Maximum Likelihood Estimation.\n\n        The MLE estimates for OU process:\n             = -log() / dt  where  is AR(1) coefficient\n             = mean(S)\n             = std(dS) / sqrt(dt)\n\n        Args:\n            spread: Spread time series\n            dt: Time step (default 1/252 for daily data)\n            method: 'mle' or 'ols'\n\n        Returns:\n            Dict with theta, mu, sigma\n        \"\"\"\n        spread = spread.dropna()\n\n        if len(spread) < 10:\n            self.theta = 0.1\n            self.mu = spread.mean() if len(spread) > 0 else 0\n            self.sigma = spread.std() if len(spread) > 0 else 1\n            return self._get_params_dict()\n\n        # Long-term mean\n        self.mu = spread.mean()\n\n        if method == 'mle':\n            # MLE estimation via AR(1) regression\n            # S_t - S_{t-1} = ( - S_{t-1})dt + dt * \n            # Rearranging: S_t = (1 - dt)S_{t-1} + dt + dt * \n\n            y = spread.values[1:]\n            x = spread.values[:-1]\n\n            # AR(1) coefficient estimation\n            x_mean = x.mean()\n            y_mean = y.mean()\n\n            cov_xy = np.sum((x - x_mean) * (y - y_mean))\n            var_x = np.sum((x - x_mean) ** 2)\n\n            if var_x == 0:\n                rho = 0.99\n            else:\n                rho = cov_xy / var_x\n\n            # Bound rho to prevent numerical issues\n            rho = np.clip(rho, 0.001, 0.999)\n\n            # theta from rho\n            self.theta = -np.log(rho) / dt\n\n            # Residual standard deviation\n            residuals = y - rho * x\n            self.sigma = residuals.std() / np.sqrt(dt)\n\n        elif method == 'ols':\n            # OLS on differences\n            dS = spread.diff().dropna()\n            S_lag = spread.shift(1).dropna()\n\n            # Align\n            min_len = min(len(dS), len(S_lag))\n            dS = dS.iloc[:min_len]\n            S_lag = S_lag.iloc[:min_len]\n\n            # Regression: dS = ( - S)dt + \n            #             dS = dt - S*dt + \n            # As: dS = a + bS +  where a = dt, b = -dt\n\n            X = np.column_stack([np.ones(len(S_lag)), S_lag.values])\n            y = dS.values\n\n            try:\n                beta = np.linalg.lstsq(X, y, rcond=None)[0]\n                a, b = beta\n\n                if b < 0:\n                    self.theta = -b / dt\n                    self.mu = a / (-b) if b != 0 else spread.mean()\n                else:\n                    # Not mean-reverting\n                    self.theta = 0.01\n                    self.mu = spread.mean()\n\n                residuals = y - X @ beta\n                self.sigma = residuals.std() / np.sqrt(dt)\n\n            except np.linalg.LinAlgError:\n                self.theta = 0.1\n                self.sigma = spread.std()\n\n        return self._get_pa",
    "source_file": "core\\_experimental\\ou_spread_trading.py",
    "academic_reference": null,
    "class_name": "OUSpreadTrading"
  },
  {
    "name": "_get_params_dict",
    "category": "microstructure",
    "formula": "{",
    "explanation": "Get parameters as dictionary.",
    "python_code": "def _get_params_dict(self) -> Dict[str, float]:\n        \"\"\"Get parameters as dictionary.\"\"\"\n        return {\n            'theta': self.theta,\n            'mu': self.mu,\n            'sigma': self.sigma,\n            'half_life': self.half_life()\n        }",
    "source_file": "core\\_experimental\\ou_spread_trading.py",
    "academic_reference": null,
    "class_name": "OUSpreadTrading"
  },
  {
    "name": "fit_mle_optimizer",
    "category": "microstructure",
    "formula": "1e10 | 1e10 | -ll",
    "explanation": "Fit OU parameters using numerical MLE optimization.\n\nMore accurate but slower than closed-form estimates.",
    "python_code": "def fit_mle_optimizer(self, spread: pd.Series, dt: float = 1/252) -> Dict[str, float]:\n        \"\"\"\n        Fit OU parameters using numerical MLE optimization.\n\n        More accurate but slower than closed-form estimates.\n        \"\"\"\n        spread = spread.dropna().values\n\n        if len(spread) < 10:\n            return self._get_params_dict()\n\n        def neg_log_likelihood(params):\n            theta, mu, sigma = params\n            if theta <= 0 or sigma <= 0:\n                return 1e10\n\n            n = len(spread)\n            S = spread\n\n            # OU transition density is Normal\n            # Mean: S_t * exp(-dt) + (1 - exp(-dt))\n            # Var:  / (2) * (1 - exp(-2dt))\n\n            exp_neg_theta_dt = np.exp(-theta * dt)\n            exp_neg_2theta_dt = np.exp(-2 * theta * dt)\n\n            mean_next = S[:-1] * exp_neg_theta_dt + mu * (1 - exp_neg_theta_dt)\n            var_next = (sigma ** 2) / (2 * theta) * (1 - exp_neg_2theta_dt)\n\n            if var_next <= 0:\n                return 1e10\n\n            # Log-likelihood\n            ll = -0.5 * (n - 1) * np.log(2 * np.pi * var_next)\n            ll -= 0.5 * np.sum((S[1:] - mean_next) ** 2) / var_next\n\n            return -ll\n\n        # Initial guesses\n        mu0 = spread.mean()\n        sigma0 = np.std(np.diff(spread))\n        theta0 = 0.1\n\n        try:\n            result = minimize(\n                neg_log_likelihood,\n                x0=[theta0, mu0, sigma0],\n                bounds=[(0.001, 100), (-np.inf, np.inf), (0.001, np.inf)],\n                method='L-BFGS-B'\n            )\n\n            if result.success:\n                self.theta, self.mu, self.sigma = result.x\n        except Exception:\n            pass\n\n        return self._get_params_dict()",
    "source_file": "core\\_experimental\\ou_spread_trading.py",
    "academic_reference": null,
    "class_name": "OUSpreadTrading"
  },
  {
    "name": "half_life",
    "category": "microstructure",
    "formula": "np.inf | np.log(2) / self.theta",
    "explanation": "Compute half-life of mean reversion: ln(2) / .\n\nThe half-life is the expected time for the spread to move\nhalfway back to its long-term mean.\n\nReturns:\n    Half-life in same units as dt (typically days)",
    "python_code": "def half_life(self) -> float:\n        \"\"\"\n        Compute half-life of mean reversion: ln(2) / .\n\n        The half-life is the expected time for the spread to move\n        halfway back to its long-term mean.\n\n        Returns:\n            Half-life in same units as dt (typically days)\n        \"\"\"\n        if self.theta is None or self.theta <= 0:\n            return np.inf\n        return np.log(2) / self.theta",
    "source_file": "core\\_experimental\\ou_spread_trading.py",
    "academic_reference": null,
    "class_name": "OUSpreadTrading"
  },
  {
    "name": "expected_reversion_time",
    "category": "microstructure",
    "formula": "np.inf | -np.log(1 - target_pct) / self.theta",
    "explanation": "Time to revert given percentage toward mean.\n\nArgs:\n    target_pct: Percentage reversion (e.g., 0.9 = 90% reversion)\n\nReturns:\n    Expected time in periods",
    "python_code": "def expected_reversion_time(self, target_pct: float = 0.9) -> float:\n        \"\"\"\n        Time to revert given percentage toward mean.\n\n        Args:\n            target_pct: Percentage reversion (e.g., 0.9 = 90% reversion)\n\n        Returns:\n            Expected time in periods\n        \"\"\"\n        if self.theta is None or self.theta <= 0:\n            return np.inf\n        return -np.log(1 - target_pct) / self.theta",
    "source_file": "core\\_experimental\\ou_spread_trading.py",
    "academic_reference": null,
    "class_name": "OUSpreadTrading"
  },
  {
    "name": "zscore",
    "category": "microstructure",
    "formula": "score = (S - ) /  | (spread - roll_mean) / (roll_std + 1e-10) | (spread - spread.mean()) / (spread.std() + 1e-10)",
    "explanation": "Compute z-score for entry/exit signals.\n\nZ-score = (S - ) / \n\nArgs:\n    spread: Spread series\n    window: Rolling window for dynamic mean/std. If None, use fitted params.\n\nReturns:\n    Z-score series",
    "python_code": "def zscore(self, spread: pd.Series, window: Optional[int] = None) -> pd.Series:\n        \"\"\"\n        Compute z-score for entry/exit signals.\n\n        Z-score = (S - ) / \n\n        Args:\n            spread: Spread series\n            window: Rolling window for dynamic mean/std. If None, use fitted params.\n\n        Returns:\n            Z-score series\n        \"\"\"\n        if window is not None:\n            # Rolling z-score\n            roll_mean = spread.rolling(window, min_periods=1).mean()\n            roll_std = spread.rolling(window, min_periods=1).std()\n            return (spread - roll_mean) / (roll_std + 1e-10)\n        else:\n            # Use fitted parameters\n            if self.mu is None or self.sigma is None:\n                return (spread - spread.mean()) / (spread.std() + 1e-10)\n            return (spread - self.mu) / (self.sigma + 1e-10)",
    "source_file": "core\\_experimental\\ou_spread_trading.py",
    "academic_reference": null,
    "class_name": "OUSpreadTrading"
  },
  {
    "name": "generate_signals",
    "category": "microstructure",
    "formula": "signals",
    "explanation": "Generate trading signals based on z-score thresholds.\n\nStrategy:\n    - Enter short when z > entry_z (spread above mean)\n    - Enter long when z < -entry_z (spread below mean)\n    - Exit when |z| < exit_z (spread near mean)\n    - Stop loss when |z| > stop_z\n\nArgs:\n    spread: Spread series\n    entry_z: Z-score threshold for entry (default 2.0)\n    exit_z: Z-score threshold for exit (default 0.5)\n    stop_z: Z-score threshold for stop loss (default 4.0)\n\nReturns:\n    Signal series: 1 (long spread), -1 (short spread), 0 (flat)",
    "python_code": "def generate_signals(\n        self,\n        spread: pd.Series,\n        entry_z: float = 2.0,\n        exit_z: float = 0.5,\n        stop_z: float = 4.0\n    ) -> pd.Series:\n        \"\"\"\n        Generate trading signals based on z-score thresholds.\n\n        Strategy:\n            - Enter short when z > entry_z (spread above mean)\n            - Enter long when z < -entry_z (spread below mean)\n            - Exit when |z| < exit_z (spread near mean)\n            - Stop loss when |z| > stop_z\n\n        Args:\n            spread: Spread series\n            entry_z: Z-score threshold for entry (default 2.0)\n            exit_z: Z-score threshold for exit (default 0.5)\n            stop_z: Z-score threshold for stop loss (default 4.0)\n\n        Returns:\n            Signal series: 1 (long spread), -1 (short spread), 0 (flat)\n        \"\"\"\n        z = self.zscore(spread)\n\n        signals = pd.Series(0, index=spread.index)\n        position = 0\n\n        for i in range(len(z)):\n            z_val = z.iloc[i]\n\n            if position == 0:\n                # No position, look for entry\n                if z_val > entry_z:\n                    position = -1  # Short spread (expect reversion down)\n                elif z_val < -entry_z:\n                    position = 1  # Long spread (expect reversion up)\n\n            elif position == 1:\n                # Long position, look for exit\n                if abs(z_val) < exit_z:\n                    position = 0  # Exit near mean\n                elif z_val < -stop_z:\n                    position = 0  # Stop loss\n\n            elif position == -1:\n                # Short position, look for exit\n                if abs(z_val) < exit_z:\n                    position = 0  # Exit near mean\n                elif z_val > stop_z:\n                    position = 0  # Stop loss\n\n            signals.iloc[i] = position\n\n        return signals",
    "source_file": "core\\_experimental\\ou_spread_trading.py",
    "academic_reference": null,
    "class_name": "OUSpreadTrading"
  },
  {
    "name": "generate_continuous_signals",
    "category": "microstructure",
    "formula": "deviation = larger position (up to max_position). | signals",
    "explanation": "Generate continuous position signals based on z-score.\n\nPosition size proportional to z-score deviation.\nLarger deviation = larger position (up to max_position).\n\nArgs:\n    spread: Spread series\n    max_position: Maximum position size\n\nReturns:\n    Continuous signal series between -max_position and +max_position",
    "python_code": "def generate_continuous_signals(\n        self,\n        spread: pd.Series,\n        max_position: float = 1.0\n    ) -> pd.Series:\n        \"\"\"\n        Generate continuous position signals based on z-score.\n\n        Position size proportional to z-score deviation.\n        Larger deviation = larger position (up to max_position).\n\n        Args:\n            spread: Spread series\n            max_position: Maximum position size\n\n        Returns:\n            Continuous signal series between -max_position and +max_position\n        \"\"\"\n        z = self.zscore(spread)\n\n        # Sigmoid-like scaling: position = -tanh(z) * max_position\n        # Negative because we short when z is high (spread above mean)\n        signals = -np.tanh(z) * max_position\n\n        return signals",
    "source_file": "core\\_experimental\\ou_spread_trading.py",
    "academic_reference": null,
    "class_name": "OUSpreadTrading"
  },
  {
    "name": "compute_hedge_ratio",
    "category": "microstructure",
    "formula": "Spread = asset1 - hedge_ratio * asset2",
    "explanation": "Compute optimal hedge ratio for spread construction.\n\nSpread = asset1 - hedge_ratio * asset2\n\nArgs:\n    asset1: First asset price series\n    asset2: Second asset price series\n    method: 'ols', 'tls' (total least squares), or 'rolling'\n\nReturns:\n    Optimal hedge ratio",
    "python_code": "def compute_hedge_ratio(\n        self,\n        asset1: pd.Series,\n        asset2: pd.Series,\n        method: str = 'ols'\n    ) -> float:\n        \"\"\"\n        Compute optimal hedge ratio for spread construction.\n\n        Spread = asset1 - hedge_ratio * asset2\n\n        Args:\n            asset1: First asset price series\n            asset2: Second asset price series\n            method: 'ols', 'tls' (total least squares), or 'rolling'\n\n        Returns:\n            Optimal hedge ratio\n        \"\"\"\n        # Align series\n        asset1 = asset1.dropna()\n        asset2 = asset2.dropna()\n        common_idx = asset1.index.intersection(asset2.index)\n        asset1 = asset1.loc[common_idx]\n        asset2 = asset2.loc[common_idx]\n\n        if len(asset1) < 10:\n            self.hedge_ratio = 1.0\n            return self.hedge_ratio\n\n        if method == 'ols':\n            # OLS: asset1 = alpha + beta * asset2\n            X = np.column_stack([np.ones(len(asset2)), asset2.values])\n            y = asset1.values\n\n            try:\n                beta = np.linalg.lstsq(X, y, rcond=None)[0]\n                self.hedge_ratio = beta[1]\n            except np.linalg.LinAlgError:\n                self.hedge_ratio = 1.0\n\n        elif method == 'tls':\n            # Total Least Squares (Deming regression)\n            # More robust when both series have measurement error\n            x = asset2.values\n            y = asset1.values\n\n            x_mean = x.mean()\n            y_mean = y.mean()\n\n            sxx = np.sum((x - x_mean) ** 2)\n            syy = np.sum((y - y_mean) ** 2)\n            sxy = np.sum((x - x_mean) * (y - y_mean))\n\n            # TLS solution\n            self.hedge_ratio = (syy - sxx + np.sqrt((syy - sxx) ** 2 + 4 * sxy ** 2)) / (2 * sxy + 1e-10)\n\n        elif method == 'rolling':\n            # Use last 60 periods for rolling estimate\n            window = min(60, len(asset1) // 2)\n            x = asset2.iloc[-window:].values\n            y = asset1.iloc[-window:].values\n\n            X = np.column_stack([np.ones(len(x)), x])\n            try:\n                beta = np.linalg.lstsq(X, y, rcond=None)[0]\n                self.hedge_ratio = beta[1]\n            except:\n                self.hedge_ratio = 1.0\n\n        return self.hedge_ratio",
    "source_file": "core\\_experimental\\ou_spread_trading.py",
    "academic_reference": null,
    "class_name": "OUSpreadTrading"
  },
  {
    "name": "construct_spread",
    "category": "microstructure",
    "formula": "Spread = asset1 - hedge_ratio * asset2 | asset1.loc[common_idx] - hedge_ratio * asset2.loc[common_idx]",
    "explanation": "Construct spread from two asset series.\n\nSpread = asset1 - hedge_ratio * asset2\n\nArgs:\n    asset1: First asset series\n    asset2: Second asset series\n    hedge_ratio: If None, compute optimal ratio\n\nReturns:\n    Spread series",
    "python_code": "def construct_spread(\n        self,\n        asset1: pd.Series,\n        asset2: pd.Series,\n        hedge_ratio: Optional[float] = None\n    ) -> pd.Series:\n        \"\"\"\n        Construct spread from two asset series.\n\n        Spread = asset1 - hedge_ratio * asset2\n\n        Args:\n            asset1: First asset series\n            asset2: Second asset series\n            hedge_ratio: If None, compute optimal ratio\n\n        Returns:\n            Spread series\n        \"\"\"\n        if hedge_ratio is None:\n            hedge_ratio = self.compute_hedge_ratio(asset1, asset2)\n\n        # Align series\n        common_idx = asset1.index.intersection(asset2.index)\n        return asset1.loc[common_idx] - hedge_ratio * asset2.loc[common_idx]",
    "source_file": "core\\_experimental\\ou_spread_trading.py",
    "academic_reference": null,
    "class_name": "OUSpreadTrading"
  },
  {
    "name": "adf_test",
    "category": "microstructure",
    "formula": "value = reject null = spread is stationary = mean-reverting | { | {",
    "explanation": "Augmented Dickey-Fuller test for stationarity (mean-reversion).\n\nNull hypothesis: spread has a unit root (not stationary)\nLow p-value = reject null = spread is stationary = mean-reverting\n\nArgs:\n    spread: Spread series\n    regression: 'c' (constant), 'ct' (constant+trend), 'ctt', 'n'\n\nReturns:\n    Dict with test statistic, p-value, critical values",
    "python_code": "def adf_test(self, spread: pd.Series, regression: str = 'c') -> Dict:\n        \"\"\"\n        Augmented Dickey-Fuller test for stationarity (mean-reversion).\n\n        Null hypothesis: spread has a unit root (not stationary)\n        Low p-value = reject null = spread is stationary = mean-reverting\n\n        Args:\n            spread: Spread series\n            regression: 'c' (constant), 'ct' (constant+trend), 'ctt', 'n'\n\n        Returns:\n            Dict with test statistic, p-value, critical values\n        \"\"\"\n        spread = spread.dropna()\n\n        if len(spread) < 20:\n            return {\n                'statistic': 0,\n                'pvalue': 1.0,\n                'critical_values': {},\n                'is_stationary': False\n            }\n\n        # ADF test implementation (simplified)\n        # Full version would use statsmodels.tsa.stattools.adfuller\n\n        # Regression: S_t =  + S_{t-1} + _i S_{t-i} + _t\n\n        dS = spread.diff().dropna()\n        S_lag = spread.shift(1).dropna()\n\n        # Align\n        min_len = min(len(dS), len(S_lag))\n        dS = dS.iloc[:min_len].values\n        S_lag = S_lag.iloc[:min_len].values\n\n        # Simple OLS for ADF\n        if regression == 'c':\n            X = np.column_stack([np.ones(len(S_lag)), S_lag])\n        else:\n            X = np.column_stack([np.ones(len(S_lag)), np.arange(len(S_lag)), S_lag])\n\n        try:\n            beta = np.linalg.lstsq(X, dS, rcond=None)[0]\n            residuals = dS - X @ beta\n\n            # t-statistic for beta coefficient on S_lag\n            if regression == 'c':\n                beta_S = beta[1]\n                idx = 1\n            else:\n                beta_S = beta[2]\n                idx = 2\n\n            XtX_inv = np.linalg.inv(X.T @ X)\n            sigma_sq = np.sum(residuals ** 2) / (len(dS) - len(beta))\n            se = np.sqrt(sigma_sq * XtX_inv[idx, idx])\n\n            adf_stat = beta_S / (se + 1e-10)\n\n            # Critical values (approximate for n=250)\n            critical_values = {\n                '1%': -3.43,\n                '5%': -2.86,\n                '10%': -2.57\n            }\n\n            # Approximate p-value\n            if adf_stat < -3.43:\n                pvalue = 0.01\n            elif adf_stat < -2.86:\n                pvalue = 0.05\n            elif adf_stat < -2.57:\n                pvalue = 0.10\n            else:\n                pvalue = 0.5\n\n            self.adf_pvalue = pvalue\n            self.is_cointegrated = pvalue < 0.05\n\n            return {\n                'statistic': adf_stat,\n                'pvalue': pvalue,\n                'critical_values': critical_values,\n                'is_stationary': pvalue < 0.05\n            }\n\n        except Exception:\n            return {\n                'statistic': 0,\n                'pvalue': 1.0,\n                'critical_values': {},\n                'is_stationary': False\n            }",
    "source_file": "core\\_experimental\\ou_spread_trading.py",
    "academic_reference": null,
    "class_name": "OUSpreadTrading"
  },
  {
    "name": "johansen_test",
    "category": "microstructure",
    "formula": "{",
    "explanation": "Simplified Johansen cointegration test for two series.\n\nTests for cointegrating relationship between asset1 and asset2.\n\nReturns:\n    Dict with test results",
    "python_code": "def johansen_test(\n        self,\n        asset1: pd.Series,\n        asset2: pd.Series\n    ) -> Dict:\n        \"\"\"\n        Simplified Johansen cointegration test for two series.\n\n        Tests for cointegrating relationship between asset1 and asset2.\n\n        Returns:\n            Dict with test results\n        \"\"\"\n        # For two series, we test if there's a cointegrating vector\n        # This is a simplified version - full Johansen requires matrix eigenvalue analysis\n\n        # Method: Engle-Granger two-step approach\n        # 1. Regress asset1 on asset2 to get residuals\n        # 2. Test residuals for stationarity\n\n        hedge_ratio = self.compute_hedge_ratio(asset1, asset2)\n        spread = self.construct_spread(asset1, asset2, hedge_ratio)\n        adf_result = self.adf_test(spread)\n\n        return {\n            'hedge_ratio': hedge_ratio,\n            'spread_adf_stat': adf_result['statistic'],\n            'spread_pvalue': adf_result['pvalue'],\n            'is_cointegrated': adf_result['is_stationary']\n        }",
    "source_file": "core\\_experimental\\ou_spread_trading.py",
    "academic_reference": null,
    "class_name": "OUSpreadTrading"
  },
  {
    "name": "simulate_ou",
    "category": "microstructure",
    "formula": "S_t = S_{t-1} * exp(-dt) + (1 - exp(-dt)) + ((1-exp(-2dt))/(2)) * Z | pd.Series(S)",
    "explanation": "Simulate OU process path.\n\nUses exact discretization:\n    S_t = S_{t-1} * exp(-dt) + (1 - exp(-dt)) + ((1-exp(-2dt))/(2)) * Z\n\nArgs:\n    n_steps: Number of time steps\n    dt: Time step size\n    S0: Initial value (default: mu)\n\nReturns:\n    Simulated spread series",
    "python_code": "def simulate_ou(\n        self,\n        n_steps: int,\n        dt: float = 1/252,\n        S0: Optional[float] = None\n    ) -> pd.Series:\n        \"\"\"\n        Simulate OU process path.\n\n        Uses exact discretization:\n            S_t = S_{t-1} * exp(-dt) + (1 - exp(-dt)) + ((1-exp(-2dt))/(2)) * Z\n\n        Args:\n            n_steps: Number of time steps\n            dt: Time step size\n            S0: Initial value (default: mu)\n\n        Returns:\n            Simulated spread series\n        \"\"\"\n        if self.theta is None or self.mu is None or self.sigma is None:\n            raise ValueError(\"Model not fitted. Call estimate_parameters first.\")\n\n        if S0 is None:\n            S0 = self.mu\n\n        S = np.zeros(n_steps)\n        S[0] = S0\n\n        exp_neg_theta_dt = np.exp(-self.theta * dt)\n        mean_reversion = self.mu * (1 - exp_neg_theta_dt)\n        vol_factor = self.sigma * np.sqrt((1 - np.exp(-2 * self.theta * dt)) / (2 * self.theta))\n\n        for t in range(1, n_steps):\n            Z = np.random.randn()\n            S[t] = S[t-1] * exp_neg_theta_dt + mean_reversion + vol_factor * Z\n\n        return pd.Series(S)",
    "source_file": "core\\_experimental\\ou_spread_trading.py",
    "academic_reference": null,
    "class_name": "OUSpreadTrading"
  },
  {
    "name": "backtest_strategy",
    "category": "microstructure",
    "formula": "= (1 + net_returns).prod() - 1 | { | / (num_trades + 1e-10)",
    "explanation": "Backtest pairs trading strategy.\n\nArgs:\n    spread: Spread series\n    entry_z: Z-score entry threshold\n    exit_z: Z-score exit threshold\n    transaction_cost: Cost per trade as fraction\n\nReturns:\n    Dict with performance metrics",
    "python_code": "def backtest_strategy(\n        self,\n        spread: pd.Series,\n        entry_z: float = 2.0,\n        exit_z: float = 0.5,\n        transaction_cost: float = 0.0001\n    ) -> Dict:\n        \"\"\"\n        Backtest pairs trading strategy.\n\n        Args:\n            spread: Spread series\n            entry_z: Z-score entry threshold\n            exit_z: Z-score exit threshold\n            transaction_cost: Cost per trade as fraction\n\n        Returns:\n            Dict with performance metrics\n        \"\"\"\n        signals = self.generate_signals(spread, entry_z, exit_z)\n\n        # Calculate returns\n        spread_returns = spread.pct_change().fillna(0)\n\n        # Strategy returns\n        strat_returns = signals.shift(1) * spread_returns\n\n        # Transaction costs\n        trades = signals.diff().abs().fillna(0)\n        costs = trades * transaction_cost\n\n        net_returns = strat_returns - costs\n\n        # Metrics\n        total_return = (1 + net_returns).prod() - 1\n        sharpe = net_returns.mean() / (net_returns.std() + 1e-10) * np.sqrt(252)\n        max_dd = (net_returns.cumsum() - net_returns.cumsum().cummax()).min()\n        win_rate = (net_returns > 0).sum() / ((net_returns != 0).sum() + 1e-10)\n        num_trades = (trades > 0).sum()\n\n        return {\n            'total_return': total_return,\n            'sharpe_ratio': sharpe,\n            'max_drawdown': max_dd,\n            'win_rate': win_rate,\n            'num_trades': num_trades,\n            'avg_return_per_trade': total_return / (num_trades + 1e-10)\n        }",
    "source_file": "core\\_experimental\\ou_spread_trading.py",
    "academic_reference": "Haarnoja (2018) 'Soft Actor-Critic' ICML",
    "class_name": "OUSpreadTrading"
  },
  {
    "name": "get_forex_pairs_candidates",
    "category": "microstructure",
    "formula": "[",
    "explanation": "Get list of candidate forex pairs for pairs trading.\n\nReturns:\n    List of (pair1, pair2, rationale) tuples",
    "python_code": "def get_forex_pairs_candidates() -> List[Tuple[str, str, str]]:\n        \"\"\"\n        Get list of candidate forex pairs for pairs trading.\n\n        Returns:\n            List of (pair1, pair2, rationale) tuples\n        \"\"\"\n        return [\n            ('EURUSD', 'GBPUSD', 'European correlation'),\n            ('AUDUSD', 'NZDUSD', 'Oceania correlation'),\n            ('USDCAD', 'USDNOK', 'Commodity currencies'),\n            ('EURUSD', 'EURGBP', 'EUR base pairs'),\n            ('USDJPY', 'EURJPY', 'JPY cross correlation'),\n            ('AUDUSD', 'AUDCAD', 'AUD cross correlation'),\n            ('GBPUSD', 'GBPJPY', 'GBP base pairs'),\n            ('USDCHF', 'USDJPY', 'Safe haven correlation'),\n        ]",
    "source_file": "core\\_experimental\\ou_spread_trading.py",
    "academic_reference": null,
    "class_name": "OUSpreadTrading"
  },
  {
    "name": "scan_pairs",
    "category": "microstructure",
    "formula": "pd.DataFrame(results)",
    "explanation": "Scan candidate pairs for tradeable relationships.\n\nArgs:\n    price_data: Dict of symbol -> price series\n    candidate_pairs: List of (symbol1, symbol2) pairs to test\n\nReturns:\n    DataFrame with pair statistics",
    "python_code": "def scan_pairs(\n        self,\n        price_data: Dict[str, pd.Series],\n        candidate_pairs: Optional[List[Tuple[str, str]]] = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Scan candidate pairs for tradeable relationships.\n\n        Args:\n            price_data: Dict of symbol -> price series\n            candidate_pairs: List of (symbol1, symbol2) pairs to test\n\n        Returns:\n            DataFrame with pair statistics\n        \"\"\"\n        if candidate_pairs is None:\n            # Generate all combinations\n            symbols = list(price_data.keys())\n            candidate_pairs = [\n                (symbols[i], symbols[j])\n                for i in range(len(symbols))\n                for j in range(i + 1, len(symbols))\n            ]\n\n        results = []\n\n        for sym1, sym2 in candidate_pairs:\n            if sym1 not in price_data or sym2 not in price_data:\n                continue\n\n            asset1 = price_data[sym1]\n            asset2 = price_data[sym2]\n\n            ou = OUSpreadTrading()\n\n            try:\n                # Test cointegration\n                coint_result = ou.johansen_test(asset1, asset2)\n\n                # Construct spread and estimate OU params\n                spread = ou.construct_spread(asset1, asset2, coint_result['hedge_ratio'])\n                ou.estimate_parameters(spread)\n\n                half_life = ou.half_life()\n\n                results.append({\n                    'pair': f\"{sym1}/{sym2}\",\n                    'symbol1': sym1,\n                    'symbol2': sym2,\n                    'hedge_ratio': coint_result['hedge_ratio'],\n                    'adf_stat': coint_result['spread_adf_stat'],\n                    'pvalue': coint_result['spread_pvalue'],\n                    'is_cointegrated': coint_result['is_cointegrated'],\n                    'theta': ou.theta,\n                    'mu': ou.mu,\n                    'sigma': ou.sigma,\n                    'half_life': half_life,\n                    'is_tradeable': (\n                        coint_result['is_cointegrated'] and\n                        self.min_half_life < half_life < self.max_half_life\n                    )\n                })\n\n                if results[-1]['is_tradeable']:\n                    self.pairs[f\"{sym1}/{sym2}\"] = ou\n\n            except Exception as e:\n                continue\n\n        return pd.DataFrame(results)",
    "source_file": "core\\_experimental\\ou_spread_trading.py",
    "academic_reference": null,
    "class_name": "PairsTradingSystem"
  },
  {
    "name": "get_signals",
    "category": "microstructure",
    "formula": "pd.DataFrame(signals)",
    "explanation": "Get trading signals for all registered pairs.\n\nReturns:\n    DataFrame with signals for each pair",
    "python_code": "def get_signals(\n        self,\n        price_data: Dict[str, pd.Series],\n        entry_z: float = 2.0,\n        exit_z: float = 0.5\n    ) -> pd.DataFrame:\n        \"\"\"\n        Get trading signals for all registered pairs.\n\n        Returns:\n            DataFrame with signals for each pair\n        \"\"\"\n        signals = {}\n\n        for pair_name, ou in self.pairs.items():\n            sym1, sym2 = pair_name.split('/')\n\n            if sym1 in price_data and sym2 in price_data:\n                spread = ou.construct_spread(price_data[sym1], price_data[sym2])\n                signals[pair_name] = ou.generate_signals(spread, entry_z, exit_z)\n\n        return pd.DataFrame(signals)",
    "source_file": "core\\_experimental\\ou_spread_trading.py",
    "academic_reference": null,
    "class_name": "PairsTradingSystem"
  },
  {
    "name": "neg_log_likelihood",
    "category": "microstructure",
    "formula": "1e10 | 1e10 | -ll",
    "explanation": "",
    "python_code": "def neg_log_likelihood(params):\n            theta, mu, sigma = params\n            if theta <= 0 or sigma <= 0:\n                return 1e10\n\n            n = len(spread)\n            S = spread\n\n            # OU transition density is Normal\n            # Mean: S_t * exp(-dt) + (1 - exp(-dt))\n            # Var:  / (2) * (1 - exp(-2dt))\n\n            exp_neg_theta_dt = np.exp(-theta * dt)\n            exp_neg_2theta_dt = np.exp(-2 * theta * dt)\n\n            mean_next = S[:-1] * exp_neg_theta_dt + mu * (1 - exp_neg_theta_dt)\n            var_next = (sigma ** 2) / (2 * theta) * (1 - exp_neg_2theta_dt)\n\n            if var_next <= 0:\n                return 1e10\n\n            # Log-likelihood\n            ll = -0.5 * (n - 1) * np.log(2 * np.pi * var_next)\n            ll -= 0.5 * np.sum((S[1:] - mean_next) ** 2) / var_next\n\n            return -ll",
    "source_file": "core\\_experimental\\ou_spread_trading.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "rank",
    "category": "alpha_factor",
    "formula": "x.rank(pct=True)",
    "explanation": "Cross-sectional rank (percentile).",
    "python_code": "def rank(x: pd.Series) -> pd.Series:\n        \"\"\"Cross-sectional rank (percentile).\"\"\"\n        return x.rank(pct=True)",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "delta",
    "category": "alpha_factor",
    "formula": "x.diff(period)",
    "explanation": "Difference from period ago.",
    "python_code": "def delta(x: pd.Series, period: int = 1) -> pd.Series:\n        \"\"\"Difference from period ago.\"\"\"\n        return x.diff(period)",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "delay",
    "category": "alpha_factor",
    "formula": "x.shift(period)",
    "explanation": "Lag by period.",
    "python_code": "def delay(x: pd.Series, period: int = 1) -> pd.Series:\n        \"\"\"Lag by period.\"\"\"\n        return x.shift(period)",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "correlation",
    "category": "alpha_factor",
    "formula": "x.rolling(window).corr(y)",
    "explanation": "Rolling correlation.",
    "python_code": "def correlation(x: pd.Series, y: pd.Series, window: int) -> pd.Series:\n        \"\"\"Rolling correlation.\"\"\"\n        return x.rolling(window).corr(y)",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "covariance",
    "category": "alpha_factor",
    "formula": "x.rolling(window).cov(y)",
    "explanation": "Rolling covariance.",
    "python_code": "def covariance(x: pd.Series, y: pd.Series, window: int) -> pd.Series:\n        \"\"\"Rolling covariance.\"\"\"\n        return x.rolling(window).cov(y)",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "ts_rank",
    "category": "alpha_factor",
    "formula": "x.rolling(window).apply(lambda arr: stats.rankdata(arr)[-1] / len(arr))",
    "explanation": "Time-series rank over window.",
    "python_code": "def ts_rank(x: pd.Series, window: int) -> pd.Series:\n        \"\"\"Time-series rank over window.\"\"\"\n        return x.rolling(window).apply(lambda arr: stats.rankdata(arr)[-1] / len(arr))",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "ts_max",
    "category": "alpha_factor",
    "formula": "x.rolling(window).max()",
    "explanation": "Rolling max.",
    "python_code": "def ts_max(x: pd.Series, window: int) -> pd.Series:\n        \"\"\"Rolling max.\"\"\"\n        return x.rolling(window).max()",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "ts_min",
    "category": "alpha_factor",
    "formula": "x.rolling(window).min()",
    "explanation": "Rolling min.",
    "python_code": "def ts_min(x: pd.Series, window: int) -> pd.Series:\n        \"\"\"Rolling min.\"\"\"\n        return x.rolling(window).min()",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "ts_argmax",
    "category": "alpha_factor",
    "formula": "x.rolling(window).apply(lambda arr: np.argmax(arr) + 1)",
    "explanation": "Position of max in window.",
    "python_code": "def ts_argmax(x: pd.Series, window: int) -> pd.Series:\n        \"\"\"Position of max in window.\"\"\"\n        return x.rolling(window).apply(lambda arr: np.argmax(arr) + 1)",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "ts_argmin",
    "category": "alpha_factor",
    "formula": "x.rolling(window).apply(lambda arr: np.argmin(arr) + 1)",
    "explanation": "Position of min in window.",
    "python_code": "def ts_argmin(x: pd.Series, window: int) -> pd.Series:\n        \"\"\"Position of min in window.\"\"\"\n        return x.rolling(window).apply(lambda arr: np.argmin(arr) + 1)",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "stddev",
    "category": "alpha_factor",
    "formula": "x.rolling(window).std()",
    "explanation": "Rolling standard deviation.",
    "python_code": "def stddev(x: pd.Series, window: int) -> pd.Series:\n        \"\"\"Rolling standard deviation.\"\"\"\n        return x.rolling(window).std()",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "sum_rolling",
    "category": "alpha_factor",
    "formula": "x.rolling(window).sum()",
    "explanation": "Rolling sum.",
    "python_code": "def sum_rolling(x: pd.Series, window: int) -> pd.Series:\n        \"\"\"Rolling sum.\"\"\"\n        return x.rolling(window).sum()",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "product",
    "category": "alpha_factor",
    "formula": "x.rolling(window).apply(np.prod)",
    "explanation": "Rolling product.",
    "python_code": "def product(x: pd.Series, window: int) -> pd.Series:\n        \"\"\"Rolling product.\"\"\"\n        return x.rolling(window).apply(np.prod)",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "alpha001",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Alpha#1: (rank(Ts_ArgMax(SignedPower(((returns < 0) ? stddev(returns, 20) : close), 2.), 5)) - 0.5)\nMomentum reversal signal.",
    "python_code": "def alpha001(self, close: pd.Series, returns: pd.Series) -> pd.Series:\n        \"\"\"\n        Alpha#1: (rank(Ts_ArgMax(SignedPower(((returns < 0) ? stddev(returns, 20) : close), 2.), 5)) - 0.5)\n        Momentum reversal signal.\n        \"\"\"\n        cond = returns < 0\n        inner = pd.Series(np.where(cond, self.stddev(returns, 20), close))\n        return self.rank(self.ts_argmax(inner ** 2, 5)) - 0.5",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "alpha002",
    "category": "alpha_factor",
    "formula": "-1 * self.correlation(",
    "explanation": "Alpha#2: (-1 * correlation(rank(delta(log(volume), 2)), rank(((close - open) / open)), 6))\nVolume-price divergence.",
    "python_code": "def alpha002(self, open_: pd.Series, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"\n        Alpha#2: (-1 * correlation(rank(delta(log(volume), 2)), rank(((close - open) / open)), 6))\n        Volume-price divergence.\n        \"\"\"\n        return -1 * self.correlation(\n            self.rank(self.delta(np.log(volume + 1), 2)),\n            self.rank((close - open_) / (open_ + 0.0001)),\n            6\n        )",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "alpha003",
    "category": "alpha_factor",
    "formula": "-1 * self.correlation(self.rank(open_), self.rank(volume), 10)",
    "explanation": "Alpha#3: (-1 * correlation(rank(open), rank(volume), 10))\nOpen-volume correlation.",
    "python_code": "def alpha003(self, open_: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"\n        Alpha#3: (-1 * correlation(rank(open), rank(volume), 10))\n        Open-volume correlation.\n        \"\"\"\n        return -1 * self.correlation(self.rank(open_), self.rank(volume), 10)",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "alpha004",
    "category": "alpha_factor",
    "formula": "-1 * self.ts_rank(self.rank(low), 9)",
    "explanation": "Alpha#4: (-1 * Ts_Rank(rank(low), 9))\nLow price momentum.",
    "python_code": "def alpha004(self, low: pd.Series) -> pd.Series:\n        \"\"\"\n        Alpha#4: (-1 * Ts_Rank(rank(low), 9))\n        Low price momentum.\n        \"\"\"\n        return -1 * self.ts_rank(self.rank(low), 9)",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "alpha006",
    "category": "alpha_factor",
    "formula": "-1 * self.correlation(open_, volume, 10)",
    "explanation": "Alpha#6: (-1 * correlation(open, volume, 10))\nSimple open-volume correlation.",
    "python_code": "def alpha006(self, open_: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"\n        Alpha#6: (-1 * correlation(open, volume, 10))\n        Simple open-volume correlation.\n        \"\"\"\n        return -1 * self.correlation(open_, volume, 10)",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "alpha012",
    "category": "alpha_factor",
    "formula": "np.sign(self.delta(volume, 1)) * (-1 * self.delta(close, 1))",
    "explanation": "Alpha#12: (sign(delta(volume, 1)) * (-1 * delta(close, 1)))\nVolume-confirmed price reversal.",
    "python_code": "def alpha012(self, volume: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"\n        Alpha#12: (sign(delta(volume, 1)) * (-1 * delta(close, 1)))\n        Volume-confirmed price reversal.\n        \"\"\"\n        return np.sign(self.delta(volume, 1)) * (-1 * self.delta(close, 1))",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "alpha013",
    "category": "alpha_factor",
    "formula": "-1 * self.rank(self.covariance(self.rank(close), self.rank(volume), 5))",
    "explanation": "Alpha#13: (-1 * rank(covariance(rank(close), rank(volume), 5)))\nPrice-volume covariance.",
    "python_code": "def alpha013(self, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"\n        Alpha#13: (-1 * rank(covariance(rank(close), rank(volume), 5)))\n        Price-volume covariance.\n        \"\"\"\n        return -1 * self.rank(self.covariance(self.rank(close), self.rank(volume), 5))",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "alpha014",
    "category": "alpha_factor",
    "formula": "(-1 * self.rank(self.delta(returns, 3))) * self.correlation(open_, volume, 10)",
    "explanation": "Alpha#14: ((-1 * rank(delta(returns, 3))) * correlation(open, volume, 10))\nReturn momentum with volume confirmation.",
    "python_code": "def alpha014(self, open_: pd.Series, volume: pd.Series, returns: pd.Series) -> pd.Series:\n        \"\"\"\n        Alpha#14: ((-1 * rank(delta(returns, 3))) * correlation(open, volume, 10))\n        Return momentum with volume confirmation.\n        \"\"\"\n        return (-1 * self.rank(self.delta(returns, 3))) * self.correlation(open_, volume, 10)",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "alpha017",
    "category": "alpha_factor",
    "formula": "(",
    "explanation": "Alpha#17: (((-1 * rank(ts_rank(close, 10))) * rank(delta(delta(close, 1), 1))) * rank(ts_rank((volume / adv20), 5)))\nComplex momentum signal.",
    "python_code": "def alpha017(self, close: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"\n        Alpha#17: (((-1 * rank(ts_rank(close, 10))) * rank(delta(delta(close, 1), 1))) * rank(ts_rank((volume / adv20), 5)))\n        Complex momentum signal.\n        \"\"\"\n        adv20 = volume.rolling(20).mean()\n        return (\n            (-1 * self.rank(self.ts_rank(close, 10))) *\n            self.rank(self.delta(self.delta(close, 1), 1)) *\n            self.rank(self.ts_rank(volume / (adv20 + 0.0001), 5))\n        )",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "alpha020",
    "category": "alpha_factor",
    "formula": "(",
    "explanation": "Alpha#20: (((-1 * rank((open - delay(high, 1)))) * rank((open - delay(close, 1)))) * rank((open - delay(low, 1))))\nGap analysis.",
    "python_code": "def alpha020(self, open_: pd.Series, high: pd.Series, close: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"\n        Alpha#20: (((-1 * rank((open - delay(high, 1)))) * rank((open - delay(close, 1)))) * rank((open - delay(low, 1))))\n        Gap analysis.\n        \"\"\"\n        return (\n            (-1 * self.rank(open_ - self.delay(high, 1))) *\n            self.rank(open_ - self.delay(close, 1)) *\n            self.rank(open_ - self.delay(low, 1))\n        )",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "alpha033",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Alpha#33: rank((-1 * ((1 - (open / close))^1)))\nOpen-close ratio.",
    "python_code": "def alpha033(self, open_: pd.Series, close: pd.Series) -> pd.Series:\n        \"\"\"\n        Alpha#33: rank((-1 * ((1 - (open / close))^1)))\n        Open-close ratio.\n        \"\"\"\n        return self.rank(-1 * (1 - (open_ / (close + 0.0001))))",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "alpha034",
    "category": "alpha_factor",
    "formula": "",
    "explanation": "Alpha#34: rank(((1 - rank((stddev(returns, 2) / stddev(returns, 5)))) + (1 - rank(delta(close, 1)))))\nVolatility regime signal.",
    "python_code": "def alpha034(self, close: pd.Series, returns: pd.Series) -> pd.Series:\n        \"\"\"\n        Alpha#34: rank(((1 - rank((stddev(returns, 2) / stddev(returns, 5)))) + (1 - rank(delta(close, 1)))))\n        Volatility regime signal.\n        \"\"\"\n        return self.rank(\n            (1 - self.rank(self.stddev(returns, 2) / (self.stddev(returns, 5) + 0.0001))) +\n            (1 - self.rank(self.delta(close, 1)))\n        )",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "alpha038",
    "category": "alpha_factor",
    "formula": "(-1 * self.rank(self.ts_rank(close, 10))) * self.rank(close / (open_ + 0.0001))",
    "explanation": "Alpha#38: ((-1 * rank(Ts_Rank(close, 10))) * rank((close / open)))\nClose momentum with intraday ratio.",
    "python_code": "def alpha038(self, close: pd.Series, open_: pd.Series) -> pd.Series:\n        \"\"\"\n        Alpha#38: ((-1 * rank(Ts_Rank(close, 10))) * rank((close / open)))\n        Close momentum with intraday ratio.\n        \"\"\"\n        return (-1 * self.rank(self.ts_rank(close, 10))) * self.rank(close / (open_ + 0.0001))",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "alpha041",
    "category": "alpha_factor",
    "formula": "np.sqrt(high * low) - vwap",
    "explanation": "Alpha#41: (((high * low)^0.5) - vwap)\nGeometric mean vs VWAP approximation.",
    "python_code": "def alpha041(self, high: pd.Series, low: pd.Series, volume: pd.Series) -> pd.Series:\n        \"\"\"\n        Alpha#41: (((high * low)^0.5) - vwap)\n        Geometric mean vs VWAP approximation.\n        \"\"\"\n        vwap = (high + low + volume.rolling(5).mean()) / 3  # Simplified VWAP\n        return np.sqrt(high * low) - vwap",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "alpha053",
    "category": "alpha_factor",
    "formula": "-1 * self.delta(inner, 9)",
    "explanation": "Alpha#53: ((-1 * delta((((close - low) - (high - close)) / (close - low)), 9)))\nPrice position within range.",
    "python_code": "def alpha053(self, close: pd.Series, high: pd.Series, low: pd.Series) -> pd.Series:\n        \"\"\"\n        Alpha#53: ((-1 * delta((((close - low) - (high - close)) / (close - low)), 9)))\n        Price position within range.\n        \"\"\"\n        inner = ((close - low) - (high - close)) / (close - low + 0.0001)\n        return -1 * self.delta(inner, 9)",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "generate_all_alphas",
    "category": "alpha_factor",
    "formula": "result",
    "explanation": "Generate all Alpha101 signals for forex data.\n\nArgs:\n    df: DataFrame with columns: open, high, low, close, volume (or tick_count for forex)\n\nReturns:\n    DataFrame with alpha signals added",
    "python_code": "def generate_all_alphas(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Generate all Alpha101 signals for forex data.\n\n        Args:\n            df: DataFrame with columns: open, high, low, close, volume (or tick_count for forex)\n\n        Returns:\n            DataFrame with alpha signals added\n        \"\"\"\n        result = df.copy()\n\n        # Ensure required columns exist\n        open_ = df['open'] if 'open' in df.columns else df['mid']\n        high = df['high'] if 'high' in df.columns else df['ask']\n        low = df['low'] if 'low' in df.columns else df['bid']\n        close = df['close'] if 'close' in df.columns else df['mid']\n        volume = df['volume'] if 'volume' in df.columns else df.get('tick_count', pd.Series(1, index=df.index))\n\n        returns = close.pct_change()\n\n        try:\n            result['alpha001'] = self.alpha001(close, returns)\n            result['alpha004'] = self.alpha004(low)\n            result['alpha012'] = self.alpha012(volume, close)\n            result['alpha033'] = self.alpha033(open_, close)\n            result['alpha034'] = self.alpha034(close, returns)\n            result['alpha038'] = self.alpha038(close, open_)\n            result['alpha053'] = self.alpha053(close, high, low)\n\n            # Only add volume-dependent alphas if volume is meaningful\n            if volume.std() > 0:\n                result['alpha002'] = self.alpha002(open_, close, volume)\n                result['alpha003'] = self.alpha003(open_, volume)\n                result['alpha006'] = self.alpha006(open_, volume)\n                result['alpha013'] = self.alpha013(close, volume)\n                result['alpha014'] = self.alpha014(open_, volume, returns)\n                result['alpha017'] = self.alpha017(close, volume)\n\n            result['alpha020'] = self.alpha020(open_, high, close, low)\n            result['alpha041'] = self.alpha041(high, low, volume)\n\n        except Exception as e:\n            logger.warning(f\"Error generating some alphas: {e}\")\n\n        return result",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "Alpha101Forex"
  },
  {
    "name": "__init__",
    "category": "volatility",
    "formula": "higher = more conservative)",
    "explanation": "Initialize Avellaneda-Stoikov parameters.\n\nArgs:\n    gamma: Risk aversion parameter (higher = more conservative)\n    sigma: Volatility of the asset\n    k: Order book liquidity parameter",
    "python_code": "def __init__(self, gamma: float = 0.1, sigma: float = 0.02, k: float = 1.5):\n        \"\"\"\n        Initialize Avellaneda-Stoikov parameters.\n\n        Args:\n            gamma: Risk aversion parameter (higher = more conservative)\n            sigma: Volatility of the asset\n            k: Order book liquidity parameter\n        \"\"\"\n        self.gamma = gamma\n        self.sigma = sigma\n        self.k = k",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "AvellanedaStoikov"
  },
  {
    "name": "reservation_price",
    "category": "technical",
    "formula": "positive = long, negative = short) | mid_price - inventory * self.gamma * (self.sigma ** 2) * time_remaining",
    "explanation": "Calculate the reservation (indifference) price.\n\nr(s,q,t) = s - q * gamma * sigma^2 * (T - t)\n\nThe reservation price adjusts the mid price based on inventory:\n- Positive inventory (long) -> lower reservation price (encourage selling)\n- Negative inventory (short) -> higher reservation price (encourage buying)\n\nArgs:\n    mid_price: Current mid price\n    inventory: Current inventory position (positive = long, negative = short)\n    time_remaining: Time remaining until end of trading period (0 to 1)\n\nReturns:\n    Reservation price",
    "python_code": "def reservation_price(self, mid_price: float, inventory: int, time_remaining: float) -> float:\n        \"\"\"\n        Calculate the reservation (indifference) price.\n\n        r(s,q,t) = s - q * gamma * sigma^2 * (T - t)\n\n        The reservation price adjusts the mid price based on inventory:\n        - Positive inventory (long) -> lower reservation price (encourage selling)\n        - Negative inventory (short) -> higher reservation price (encourage buying)\n\n        Args:\n            mid_price: Current mid price\n            inventory: Current inventory position (positive = long, negative = short)\n            time_remaining: Time remaining until end of trading period (0 to 1)\n\n        Returns:\n            Reservation price\n        \"\"\"\n        return mid_price - inventory * self.gamma * (self.sigma ** 2) * time_remaining",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "AvellanedaStoikov"
  },
  {
    "name": "optimal_spread",
    "category": "microstructure",
    "formula": "term1 + term2",
    "explanation": "Calculate the optimal bid-ask spread.\n\ndelta(t) = gamma * sigma^2 * (T - t) + (2/gamma) * ln(1 + gamma/k)\n\nArgs:\n    time_remaining: Time remaining until end of trading period\n\nReturns:\n    Optimal spread (divide by 2 for each side)",
    "python_code": "def optimal_spread(self, time_remaining: float) -> float:\n        \"\"\"\n        Calculate the optimal bid-ask spread.\n\n        delta(t) = gamma * sigma^2 * (T - t) + (2/gamma) * ln(1 + gamma/k)\n\n        Args:\n            time_remaining: Time remaining until end of trading period\n\n        Returns:\n            Optimal spread (divide by 2 for each side)\n        \"\"\"\n        term1 = self.gamma * (self.sigma ** 2) * time_remaining\n        term2 = (2 / self.gamma) * np.log(1 + self.gamma / self.k)\n        return term1 + term2",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "AvellanedaStoikov"
  },
  {
    "name": "optimal_quotes",
    "category": "technical",
    "formula": "bid, ask",
    "explanation": "Calculate optimal bid and ask quotes.\n\nArgs:\n    mid_price: Current mid price\n    inventory: Current inventory position\n    time_remaining: Time remaining (0 to 1)\n\nReturns:\n    Tuple of (bid_price, ask_price)",
    "python_code": "def optimal_quotes(self, mid_price: float, inventory: int, time_remaining: float) -> Tuple[float, float]:\n        \"\"\"\n        Calculate optimal bid and ask quotes.\n\n        Args:\n            mid_price: Current mid price\n            inventory: Current inventory position\n            time_remaining: Time remaining (0 to 1)\n\n        Returns:\n            Tuple of (bid_price, ask_price)\n        \"\"\"\n        r = self.reservation_price(mid_price, inventory, time_remaining)\n        spread = self.optimal_spread(time_remaining)\n        half_spread = spread / 2\n\n        bid = r - half_spread\n        ask = r + half_spread\n\n        return bid, ask",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "AvellanedaStoikov"
  },
  {
    "name": "inventory_skew",
    "category": "microstructure",
    "formula": "np.clip(inventory / max_inventory, -1, 1)",
    "explanation": "Calculate inventory skew factor.\n\nUsed to adjust spreads based on inventory levels:\n- At max inventory, widen the side we want to exit\n- At zero inventory, symmetric spreads\n\nArgs:\n    inventory: Current position\n    max_inventory: Maximum allowed inventory\n\nReturns:\n    Skew factor (-1 to 1)",
    "python_code": "def inventory_skew(self, inventory: int, max_inventory: int = 100) -> float:\n        \"\"\"\n        Calculate inventory skew factor.\n\n        Used to adjust spreads based on inventory levels:\n        - At max inventory, widen the side we want to exit\n        - At zero inventory, symmetric spreads\n\n        Args:\n            inventory: Current position\n            max_inventory: Maximum allowed inventory\n\n        Returns:\n            Skew factor (-1 to 1)\n        \"\"\"\n        return np.clip(inventory / max_inventory, -1, 1)",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "AvellanedaStoikov"
  },
  {
    "name": "get_signals",
    "category": "quantitative",
    "formula": "result",
    "explanation": "Generate market making signals for a DataFrame.\n\nArgs:\n    df: DataFrame with 'mid' or 'close' price column\n    inventory: Current inventory position\n    trading_hours: Total trading hours in period\n\nReturns:\n    DataFrame with bid, ask, reservation price columns",
    "python_code": "def get_signals(self, df: pd.DataFrame, inventory: int = 0,\n                    trading_hours: float = 24.0) -> pd.DataFrame:\n        \"\"\"\n        Generate market making signals for a DataFrame.\n\n        Args:\n            df: DataFrame with 'mid' or 'close' price column\n            inventory: Current inventory position\n            trading_hours: Total trading hours in period\n\n        Returns:\n            DataFrame with bid, ask, reservation price columns\n        \"\"\"\n        result = df.copy()\n\n        mid = df['mid'] if 'mid' in df.columns else df['close']\n\n        # Calculate time remaining (assuming uniform time steps)\n        n = len(df)\n        time_remaining = np.linspace(1, 0.01, n)  # Never reach exactly 0\n\n        bids = []\n        asks = []\n        reservations = []\n\n        for i, (price, t) in enumerate(zip(mid.values, time_remaining)):\n            r = self.reservation_price(price, inventory, t)\n            bid, ask = self.optimal_quotes(price, inventory, t)\n\n            bids.append(bid)\n            asks.append(ask)\n            reservations.append(r)\n\n        result['as_reservation'] = reservations\n        result['as_bid'] = bids\n        result['as_ask'] = asks\n        result['as_spread'] = result['as_ask'] - result['as_bid']\n\n        return result",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "AvellanedaStoikov"
  },
  {
    "name": "kelly_fraction",
    "category": "risk",
    "formula": "max(kelly, 0)",
    "explanation": "Calculate Kelly fraction for binary outcomes.\n\nArgs:\n    win_prob: Probability of winning (0 to 1)\n    win_loss_ratio: Average win / Average loss\n\nReturns:\n    Optimal fraction to bet (0 to 1, can be negative for short)",
    "python_code": "def kelly_fraction(win_prob: float, win_loss_ratio: float) -> float:\n        \"\"\"\n        Calculate Kelly fraction for binary outcomes.\n\n        Args:\n            win_prob: Probability of winning (0 to 1)\n            win_loss_ratio: Average win / Average loss\n\n        Returns:\n            Optimal fraction to bet (0 to 1, can be negative for short)\n        \"\"\"\n        q = 1 - win_prob\n        b = win_loss_ratio\n\n        kelly = (win_prob * b - q) / b\n        return max(kelly, 0)",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "KellyCriterion"
  },
  {
    "name": "fractional_kelly",
    "category": "risk",
    "formula": "fraction=0.5) gives: | fraction=0.25) gives: | full_kelly * fraction",
    "explanation": "Calculate fractional Kelly (more conservative).\n\nHalf-Kelly (fraction=0.5) gives:\n- 75% of optimal growth rate\n- 25% of the variance\n\nQuarter-Kelly (fraction=0.25) gives:\n- 43.75% of optimal growth rate\n- 6.25% of the variance\n\nArgs:\n    win_prob: Probability of winning\n    win_loss_ratio: Average win / Average loss\n    fraction: Fraction of Kelly to use (0.25 to 0.5 recommended)\n\nReturns:\n    Fractional Kelly bet size",
    "python_code": "def fractional_kelly(win_prob: float, win_loss_ratio: float,\n                         fraction: float = 0.5) -> float:\n        \"\"\"\n        Calculate fractional Kelly (more conservative).\n\n        Half-Kelly (fraction=0.5) gives:\n        - 75% of optimal growth rate\n        - 25% of the variance\n\n        Quarter-Kelly (fraction=0.25) gives:\n        - 43.75% of optimal growth rate\n        - 6.25% of the variance\n\n        Args:\n            win_prob: Probability of winning\n            win_loss_ratio: Average win / Average loss\n            fraction: Fraction of Kelly to use (0.25 to 0.5 recommended)\n\n        Returns:\n            Fractional Kelly bet size\n        \"\"\"\n        full_kelly = KellyCriterion.kelly_fraction(win_prob, win_loss_ratio)\n        return full_kelly * fraction",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "KellyCriterion"
  },
  {
    "name": "kelly_from_returns",
    "category": "risk",
    "formula": "= returns.mean() | 0 | mean_return / variance",
    "explanation": "Calculate Kelly fraction from historical returns.\n\nf* = mean(returns) / var(returns)\n\nArgs:\n    returns: Series of historical returns\n\nReturns:\n    Optimal Kelly fraction",
    "python_code": "def kelly_from_returns(returns: pd.Series) -> float:\n        \"\"\"\n        Calculate Kelly fraction from historical returns.\n\n        f* = mean(returns) / var(returns)\n\n        Args:\n            returns: Series of historical returns\n\n        Returns:\n            Optimal Kelly fraction\n        \"\"\"\n        mean_return = returns.mean()\n        variance = returns.var()\n\n        if variance <= 0:\n            return 0\n\n        return mean_return / variance",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "KellyCriterion"
  },
  {
    "name": "optimal_leverage",
    "category": "volatility",
    "formula": "mu = expected return | r = risk-free rate | sigma = volatility",
    "explanation": "Calculate optimal leverage using Kelly.\n\nf* = (mu - r) / sigma^2\n\nWhere:\n    mu = expected return\n    r = risk-free rate\n    sigma = volatility\n\nArgs:\n    returns: Historical returns\n    risk_free_rate: Risk-free rate (same frequency as returns)\n\nReturns:\n    Optimal leverage (>1 means leverage, <1 means partial investment)",
    "python_code": "def optimal_leverage(returns: pd.Series, risk_free_rate: float = 0.0) -> float:\n        \"\"\"\n        Calculate optimal leverage using Kelly.\n\n        f* = (mu - r) / sigma^2\n\n        Where:\n            mu = expected return\n            r = risk-free rate\n            sigma = volatility\n\n        Args:\n            returns: Historical returns\n            risk_free_rate: Risk-free rate (same frequency as returns)\n\n        Returns:\n            Optimal leverage (>1 means leverage, <1 means partial investment)\n        \"\"\"\n        excess_return = returns.mean() - risk_free_rate\n        variance = returns.var()\n\n        if variance <= 0:\n            return 0\n\n        return excess_return / variance",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "KellyCriterion"
  },
  {
    "name": "position_size",
    "category": "risk",
    "formula": "account_value * kelly_pct",
    "explanation": "Calculate actual position size in currency.\n\nArgs:\n    account_value: Total account value\n    win_prob: Probability of winning\n    win_loss_ratio: Average win / Average loss\n    fraction: Kelly fraction (0.5 = half Kelly)\n    max_position_pct: Maximum position as % of account\n\nReturns:\n    Position size in currency units",
    "python_code": "def position_size(account_value: float, win_prob: float,\n                      win_loss_ratio: float, fraction: float = 0.5,\n                      max_position_pct: float = 0.25) -> float:\n        \"\"\"\n        Calculate actual position size in currency.\n\n        Args:\n            account_value: Total account value\n            win_prob: Probability of winning\n            win_loss_ratio: Average win / Average loss\n            fraction: Kelly fraction (0.5 = half Kelly)\n            max_position_pct: Maximum position as % of account\n\n        Returns:\n            Position size in currency units\n        \"\"\"\n        kelly_pct = KellyCriterion.fractional_kelly(win_prob, win_loss_ratio, fraction)\n\n        # Cap at maximum position\n        kelly_pct = min(kelly_pct, max_position_pct)\n\n        return account_value * kelly_pct",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "KellyCriterion"
  },
  {
    "name": "get_vertical_barrier",
    "category": "quantitative",
    "formula": "t1",
    "explanation": "Get vertical barrier timestamps.\n\nArgs:\n    timestamps: Full price bar timestamps\n    t_events: Event timestamps (when signals occur)\n    num_bars: Number of bars for vertical barrier\n\nReturns:\n    Series with vertical barrier timestamps",
    "python_code": "def get_vertical_barrier(timestamps: pd.DatetimeIndex,\n                             t_events: pd.DatetimeIndex,\n                             num_bars: int) -> pd.Series:\n        \"\"\"\n        Get vertical barrier timestamps.\n\n        Args:\n            timestamps: Full price bar timestamps\n            t_events: Event timestamps (when signals occur)\n            num_bars: Number of bars for vertical barrier\n\n        Returns:\n            Series with vertical barrier timestamps\n        \"\"\"\n        t1 = pd.Series(pd.NaT, index=t_events)\n\n        for i, t0 in enumerate(t_events):\n            try:\n                loc = timestamps.get_loc(t0)\n                if loc + num_bars < len(timestamps):\n                    t1.iloc[i] = timestamps[loc + num_bars]\n            except KeyError:\n                continue\n\n        return t1",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "TripleBarrier"
  },
  {
    "name": "apply_triple_barrier",
    "category": "microstructure",
    "formula": "threshold | if touch_time in close.index: | pd.DataFrame(results)",
    "explanation": "Apply triple barrier method.\n\nArgs:\n    close: Close price series\n    t_events: Event timestamps\n    pt_sl: Tuple of (profit_taking_mult, stop_loss_mult) relative to daily vol\n    vertical_barrier: Vertical barrier timestamps\n    min_return: Minimum return threshold\n\nReturns:\n    DataFrame with barrier touch information",
    "python_code": "def apply_triple_barrier(close: pd.Series,\n                            t_events: pd.DatetimeIndex,\n                            pt_sl: Tuple[float, float],\n                            vertical_barrier: pd.Series,\n                            min_return: float = 0.0) -> pd.DataFrame:\n        \"\"\"\n        Apply triple barrier method.\n\n        Args:\n            close: Close price series\n            t_events: Event timestamps\n            pt_sl: Tuple of (profit_taking_mult, stop_loss_mult) relative to daily vol\n            vertical_barrier: Vertical barrier timestamps\n            min_return: Minimum return threshold\n\n        Returns:\n            DataFrame with barrier touch information\n        \"\"\"\n        # Calculate daily volatility\n        daily_vol = close.pct_change().rolling(20).std()\n\n        results = []\n\n        for t0 in t_events:\n            if t0 not in close.index:\n                continue\n\n            entry_price = close.loc[t0]\n            vol = daily_vol.loc[t0] if t0 in daily_vol.index else daily_vol.mean()\n\n            # Set barriers\n            pt = entry_price * (1 + pt_sl[0] * vol) if pt_sl[0] > 0 else np.inf\n            sl = entry_price * (1 - pt_sl[1] * vol) if pt_sl[1] > 0 else 0\n\n            # Get vertical barrier\n            t1 = vertical_barrier.loc[t0] if t0 in vertical_barrier.index else pd.NaT\n\n            # Find path after event\n            if pd.isna(t1):\n                t1 = close.index[-1]\n\n            path = close.loc[t0:t1]\n\n            # Find first barrier touch\n            touch_pt = path[path >= pt].index.min() if (path >= pt).any() else pd.NaT\n            touch_sl = path[path <= sl].index.min() if (path <= sl).any() else pd.NaT\n\n            # Determine which barrier was touched first\n            if pd.isna(touch_pt) and pd.isna(touch_sl):\n                # Vertical barrier\n                touch_time = t1\n                label = 0  # Neutral\n            elif pd.isna(touch_pt):\n                touch_time = touch_sl\n                label = -1  # Stop loss\n            elif pd.isna(touch_sl):\n                touch_time = touch_pt\n                label = 1  # Profit taking\n            else:\n                if touch_pt <= touch_sl:\n                    touch_time = touch_pt\n                    label = 1\n                else:\n                    touch_time = touch_sl\n                    label = -1\n\n            # Calculate return\n            if touch_time in close.index:\n                ret = (close.loc[touch_time] - entry_price) / entry_price\n            else:\n                ret = 0\n\n            results.append({\n                't0': t0,\n                't1': touch_time,\n                'ret': ret,\n                'label': label,\n                'pt': pt,\n                'sl': sl\n            })\n\n        return pd.DataFrame(results)",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "TripleBarrier"
  },
  {
    "name": "meta_labeling",
    "category": "microstructure",
    "formula": "meta_labels",
    "explanation": "Apply meta-labeling: filter primary model signals.\n\nThe meta-label is 1 if the primary signal would have been profitable,\n0 otherwise. This is used to train a secondary model that learns\nwhen to trust the primary model.\n\nArgs:\n    primary_signal: Primary model's directional signal (1, 0, -1)\n    triple_barrier_labels: Output from apply_triple_barrier\n\nReturns:\n    Meta-labels (1 = take the trade, 0 = skip)",
    "python_code": "def meta_labeling(primary_signal: pd.Series,\n                      triple_barrier_labels: pd.DataFrame) -> pd.Series:\n        \"\"\"\n        Apply meta-labeling: filter primary model signals.\n\n        The meta-label is 1 if the primary signal would have been profitable,\n        0 otherwise. This is used to train a secondary model that learns\n        when to trust the primary model.\n\n        Args:\n            primary_signal: Primary model's directional signal (1, 0, -1)\n            triple_barrier_labels: Output from apply_triple_barrier\n\n        Returns:\n            Meta-labels (1 = take the trade, 0 = skip)\n        \"\"\"\n        meta_labels = pd.Series(0, index=primary_signal.index)\n\n        for _, row in triple_barrier_labels.iterrows():\n            t0 = row['t0']\n            if t0 not in primary_signal.index:\n                continue\n\n            signal = primary_signal.loc[t0]\n            label = row['label']\n\n            # Meta-label is 1 if signal direction matches outcome\n            if signal > 0 and label > 0:\n                meta_labels.loc[t0] = 1\n            elif signal < 0 and label < 0:\n                meta_labels.loc[t0] = 1\n            elif signal == 0:\n                meta_labels.loc[t0] = 0\n\n        return meta_labels",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "TripleBarrier"
  },
  {
    "name": "get_weights",
    "category": "quantitative",
    "formula": "w_k = -w_{k-1} * (d - k + 1) / k | np.array(weights[::-1])",
    "explanation": "Calculate weights for fractional differentiation.\n\nw_k = -w_{k-1} * (d - k + 1) / k\n\nArgs:\n    d: Fractional differentiation order (0 to 1)\n    size: Number of weights to compute\n    threshold: Minimum weight to include\n\nReturns:\n    Array of weights",
    "python_code": "def get_weights(d: float, size: int, threshold: float = 1e-5) -> np.ndarray:\n        \"\"\"\n        Calculate weights for fractional differentiation.\n\n        w_k = -w_{k-1} * (d - k + 1) / k\n\n        Args:\n            d: Fractional differentiation order (0 to 1)\n            size: Number of weights to compute\n            threshold: Minimum weight to include\n\n        Returns:\n            Array of weights\n        \"\"\"\n        weights = [1.0]\n        k = 1\n\n        while k < size:\n            w = -weights[-1] * (d - k + 1) / k\n            if abs(w) < threshold:\n                break\n            weights.append(w)\n            k += 1\n\n        return np.array(weights[::-1])",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "FractionalDifferentiation"
  },
  {
    "name": "frac_diff",
    "category": "quantitative",
    "formula": "result",
    "explanation": "Apply fractional differentiation to a series.\n\nArgs:\n    series: Price series\n    d: Fractional differentiation order\n    threshold: Minimum weight threshold\n\nReturns:\n    Fractionally differentiated series",
    "python_code": "def frac_diff(series: pd.Series, d: float, threshold: float = 1e-5) -> pd.Series:\n        \"\"\"\n        Apply fractional differentiation to a series.\n\n        Args:\n            series: Price series\n            d: Fractional differentiation order\n            threshold: Minimum weight threshold\n\n        Returns:\n            Fractionally differentiated series\n        \"\"\"\n        weights = FractionalDifferentiation.get_weights(d, len(series), threshold)\n        width = len(weights)\n\n        result = pd.Series(index=series.index, dtype=float)\n\n        for i in range(width, len(series)):\n            result.iloc[i] = np.dot(weights, series.iloc[i-width+1:i+1].values)\n\n        return result",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "FractionalDifferentiation"
  },
  {
    "name": "get_optimal_d",
    "category": "quantitative",
    "formula": "d | d_range[1]",
    "explanation": "Find minimum d that achieves stationarity.\n\nArgs:\n    series: Price series\n    d_range: Range of d values to test\n    adf_threshold: ADF statistic threshold for stationarity\n\nReturns:\n    Optimal d value",
    "python_code": "def get_optimal_d(series: pd.Series,\n                      d_range: Tuple[float, float] = (0.1, 1.0),\n                      adf_threshold: float = -2.86) -> float:\n        \"\"\"\n        Find minimum d that achieves stationarity.\n\n        Args:\n            series: Price series\n            d_range: Range of d values to test\n            adf_threshold: ADF statistic threshold for stationarity\n\n        Returns:\n            Optimal d value\n        \"\"\"\n        from statsmodels.tsa.stattools import adfuller\n\n        for d in np.arange(d_range[0], d_range[1], 0.05):\n            frac_series = FractionalDifferentiation.frac_diff(series, d)\n            frac_series = frac_series.dropna()\n\n            if len(frac_series) < 20:\n                continue\n\n            adf_stat = adfuller(frac_series)[0]\n\n            if adf_stat < adf_threshold:\n                return d\n\n        return d_range[1]",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "FractionalDifferentiation"
  },
  {
    "name": "__init__",
    "category": "volatility",
    "formula": "",
    "explanation": "Initialize Almgren-Chriss parameters.\n\nArgs:\n    sigma: Volatility (daily)\n    eta: Temporary market impact coefficient\n    gamma: Permanent market impact coefficient\n    epsilon: Fixed transaction cost\n    lambda_: Risk aversion parameter",
    "python_code": "def __init__(self, sigma: float, eta: float, gamma: float,\n                 epsilon: float = 0.0, lambda_: float = 1e-6):\n        \"\"\"\n        Initialize Almgren-Chriss parameters.\n\n        Args:\n            sigma: Volatility (daily)\n            eta: Temporary market impact coefficient\n            gamma: Permanent market impact coefficient\n            epsilon: Fixed transaction cost\n            lambda_: Risk aversion parameter\n        \"\"\"\n        self.sigma = sigma\n        self.eta = eta\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.lambda_ = lambda_",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "AlmgrenChriss"
  },
  {
    "name": "optimal_trajectory",
    "category": "execution",
    "formula": "trades",
    "explanation": "Calculate optimal execution trajectory.\n\nArgs:\n    X: Total shares to execute\n    T: Total time horizon\n    n_steps: Number of trading intervals\n\nReturns:\n    Array of shares to trade at each step",
    "python_code": "def optimal_trajectory(self, X: float, T: int, n_steps: int) -> np.ndarray:\n        \"\"\"\n        Calculate optimal execution trajectory.\n\n        Args:\n            X: Total shares to execute\n            T: Total time horizon\n            n_steps: Number of trading intervals\n\n        Returns:\n            Array of shares to trade at each step\n        \"\"\"\n        tau = T / n_steps  # Time per interval\n\n        # Calculate kappa\n        kappa_sq = self.lambda_ * (self.sigma ** 2) / (self.eta * tau)\n        kappa = np.sqrt(kappa_sq)\n\n        # Time grid\n        t = np.linspace(0, T, n_steps + 1)\n\n        # Holdings at each time\n        x = X * np.sinh(kappa * (T - t)) / np.sinh(kappa * T)\n\n        # Trades at each interval (negative of change in holdings)\n        trades = -np.diff(x)\n\n        return trades",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "AlmgrenChriss"
  },
  {
    "name": "execution_cost",
    "category": "execution",
    "formula": "{",
    "explanation": "Calculate expected execution cost and variance.\n\nArgs:\n    X: Total shares to execute\n    T: Total time horizon\n    n_steps: Number of trading intervals\n\nReturns:\n    Dict with 'expected_cost' and 'cost_variance'",
    "python_code": "def execution_cost(self, X: float, T: int, n_steps: int) -> Dict[str, float]:\n        \"\"\"\n        Calculate expected execution cost and variance.\n\n        Args:\n            X: Total shares to execute\n            T: Total time horizon\n            n_steps: Number of trading intervals\n\n        Returns:\n            Dict with 'expected_cost' and 'cost_variance'\n        \"\"\"\n        trades = self.optimal_trajectory(X, T, n_steps)\n        tau = T / n_steps\n\n        # Permanent impact cost\n        permanent_cost = 0.5 * self.gamma * X ** 2\n\n        # Temporary impact cost\n        temporary_cost = self.eta * np.sum(trades ** 2) / tau\n\n        # Fixed costs\n        fixed_cost = self.epsilon * n_steps\n\n        # Variance (timing risk)\n        t = np.linspace(0, T, n_steps + 1)\n        x = X * np.sinh(np.sqrt(self.lambda_) * (T - t)) / np.sinh(np.sqrt(self.lambda_) * T)\n        variance = (self.sigma ** 2) * tau * np.sum(x[:-1] ** 2)\n\n        total_cost = permanent_cost + temporary_cost + fixed_cost\n\n        return {\n            'expected_cost': total_cost,\n            'cost_variance': variance,\n            'permanent_impact': permanent_cost,\n            'temporary_impact': temporary_cost,\n            'fixed_cost': fixed_cost\n        }",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "AlmgrenChriss"
  },
  {
    "name": "twap_trajectory",
    "category": "execution",
    "formula": "lambda = 0). | np.full(n_steps, X / n_steps)",
    "explanation": "Calculate TWAP (Time-Weighted Average Price) trajectory.\n\nTWAP corresponds to zero risk aversion (lambda = 0).\n\nArgs:\n    X: Total shares to execute\n    n_steps: Number of trading intervals\n\nReturns:\n    Array of uniform trades",
    "python_code": "def twap_trajectory(self, X: float, n_steps: int) -> np.ndarray:\n        \"\"\"\n        Calculate TWAP (Time-Weighted Average Price) trajectory.\n\n        TWAP corresponds to zero risk aversion (lambda = 0).\n\n        Args:\n            X: Total shares to execute\n            n_steps: Number of trading intervals\n\n        Returns:\n            Array of uniform trades\n        \"\"\"\n        return np.full(n_steps, X / n_steps)",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "AlmgrenChriss"
  },
  {
    "name": "vwap_adjustment",
    "category": "microstructure",
    "formula": "total * volume_profile / volume_profile.sum()",
    "explanation": "Adjust trajectory to follow VWAP profile.\n\nArgs:\n    trades: Base trading trajectory\n    volume_profile: Expected volume at each interval (should sum to 1)\n\nReturns:\n    Volume-adjusted trades",
    "python_code": "def vwap_adjustment(self, trades: np.ndarray,\n                        volume_profile: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Adjust trajectory to follow VWAP profile.\n\n        Args:\n            trades: Base trading trajectory\n            volume_profile: Expected volume at each interval (should sum to 1)\n\n        Returns:\n            Volume-adjusted trades\n        \"\"\"\n        total = trades.sum()\n        return total * volume_profile / volume_profile.sum()",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "AlmgrenChriss"
  },
  {
    "name": "heston_variance_path",
    "category": "volatility",
    "formula": "dv_t = kappa * (theta - v_t) * dt + sigma_v * sqrt(v_t) * dW_t | v",
    "explanation": "Simulate Heston variance path.\n\ndv_t = kappa * (theta - v_t) * dt + sigma_v * sqrt(v_t) * dW_t\n\nArgs:\n    v0: Initial variance\n    kappa: Mean reversion speed\n    theta: Long-run variance\n    sigma_v: Vol of vol\n    T: Time horizon\n    n_steps: Number of steps\n    rng: Random number generator\n\nReturns:\n    Simulated variance path",
    "python_code": "def heston_variance_path(v0: float, kappa: float, theta: float,\n                             sigma_v: float, T: float, n_steps: int,\n                             rng: np.random.Generator = None) -> np.ndarray:\n        \"\"\"\n        Simulate Heston variance path.\n\n        dv_t = kappa * (theta - v_t) * dt + sigma_v * sqrt(v_t) * dW_t\n\n        Args:\n            v0: Initial variance\n            kappa: Mean reversion speed\n            theta: Long-run variance\n            sigma_v: Vol of vol\n            T: Time horizon\n            n_steps: Number of steps\n            rng: Random number generator\n\n        Returns:\n            Simulated variance path\n        \"\"\"\n        if rng is None:\n            rng = np.random.default_rng()\n\n        dt = T / n_steps\n        v = np.zeros(n_steps + 1)\n        v[0] = v0\n\n        for i in range(n_steps):\n            dW = rng.standard_normal() * np.sqrt(dt)\n            v[i+1] = v[i] + kappa * (theta - v[i]) * dt + sigma_v * np.sqrt(max(v[i], 0)) * dW\n            v[i+1] = max(v[i+1], 0)  # Ensure non-negative\n\n        return v",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "StochasticVolatility"
  },
  {
    "name": "sabr_implied_vol",
    "category": "alpha_factor",
    "formula": "factor1 * x_z * factor2",
    "explanation": "SABR implied volatility approximation.\n\nArgs:\n    F: Forward price\n    K: Strike price\n    T: Time to expiry\n    alpha: Initial volatility\n    beta: CEV exponent (0 to 1)\n    rho: Correlation between price and vol\n    nu: Vol of vol\n\nReturns:\n    Implied volatility",
    "python_code": "def sabr_implied_vol(F: float, K: float, T: float,\n                         alpha: float, beta: float, rho: float,\n                         nu: float) -> float:\n        \"\"\"\n        SABR implied volatility approximation.\n\n        Args:\n            F: Forward price\n            K: Strike price\n            T: Time to expiry\n            alpha: Initial volatility\n            beta: CEV exponent (0 to 1)\n            rho: Correlation between price and vol\n            nu: Vol of vol\n\n        Returns:\n            Implied volatility\n        \"\"\"\n        if abs(F - K) < 1e-10:\n            # ATM approximation\n            logFK = 0\n            FK_mid = F\n        else:\n            logFK = np.log(F / K)\n            FK_mid = np.sqrt(F * K)\n\n        FK_beta = (F * K) ** ((1 - beta) / 2)\n\n        # z calculation\n        z = (nu / alpha) * FK_beta * logFK\n\n        # x(z) calculation\n        if abs(z) < 1e-10:\n            x_z = 1\n        else:\n            x_z = z / np.log((np.sqrt(1 - 2*rho*z + z**2) + z - rho) / (1 - rho))\n\n        # First factor\n        factor1 = alpha / (FK_beta * (1 + ((1-beta)**2 / 24) * logFK**2 +\n                                       ((1-beta)**4 / 1920) * logFK**4))\n\n        # Second factor\n        factor2 = 1 + T * (((1-beta)**2 / 24) * (alpha**2 / FK_beta**2) +\n                           0.25 * rho * beta * nu * alpha / FK_beta +\n                           (2 - 3*rho**2) * nu**2 / 24)\n\n        return factor1 * x_z * factor2",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "StochasticVolatility"
  },
  {
    "name": "estimate_heston_params",
    "category": "volatility",
    "formula": "{",
    "explanation": "Estimate Heston model parameters from data.\n\nUses method of moments estimation.\n\nArgs:\n    returns: Return series\n    vol: Realized volatility series\n\nReturns:\n    Dict with kappa, theta, sigma_v estimates",
    "python_code": "def estimate_heston_params(returns: pd.Series,\n                               vol: pd.Series) -> Dict[str, float]:\n        \"\"\"\n        Estimate Heston model parameters from data.\n\n        Uses method of moments estimation.\n\n        Args:\n            returns: Return series\n            vol: Realized volatility series\n\n        Returns:\n            Dict with kappa, theta, sigma_v estimates\n        \"\"\"\n        variance = vol ** 2\n\n        # Long-run variance\n        theta = variance.mean()\n\n        # Mean reversion (from AR(1) on variance)\n        var_diff = variance.diff().dropna()\n        var_lag = variance.shift(1).dropna()\n\n        # Simple regression: dv = kappa * (theta - v) * dt\n        # Approximate kappa from autocorrelation\n        autocorr = variance.autocorr(lag=1)\n        kappa = -np.log(autocorr) if autocorr > 0 else 1.0\n\n        # Vol of vol\n        sigma_v = var_diff.std() / np.sqrt(variance.mean())\n\n        return {\n            'kappa': kappa,\n            'theta': theta,\n            'sigma_v': sigma_v\n        }",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": "StochasticVolatility"
  },
  {
    "name": "create_quant_formulas",
    "category": "quantitative",
    "formula": "{",
    "explanation": "Create all quant formula calculators.\n\nReturns:\n    Dict with all formula calculators",
    "python_code": "def create_quant_formulas() -> Dict:\n    \"\"\"\n    Create all quant formula calculators.\n\n    Returns:\n        Dict with all formula calculators\n    \"\"\"\n    return {\n        'alpha101': Alpha101Forex(),\n        'avellaneda_stoikov': AvellanedaStoikov(),\n        'kelly': KellyCriterion(),\n        'triple_barrier': TripleBarrier(),\n        'frac_diff': FractionalDifferentiation(),\n        'almgren_chriss': AlmgrenChriss(sigma=0.02, eta=2.5e-6, gamma=2.5e-7),\n        'stochastic_vol': StochasticVolatility()\n    }",
    "source_file": "core\\_experimental\\quant_formulas.py",
    "academic_reference": "Kakushadze (2016) '101 Formulaic Alphas' arXiv:1601.00991",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "volatility",
    "formula": "",
    "explanation": "Initialize range volatility calculator.\n\nArgs:\n    annualize: Whether to annualize volatility\n    trading_periods: Periods per year (252 for daily, 252*24*60 for minute)\n    min_periods: Minimum periods for rolling calculations",
    "python_code": "def __init__(\n        self,\n        annualize: bool = True,\n        trading_periods: int = 252,  # 252 for daily, 252*24 for hourly forex\n        min_periods: int = 2\n    ):\n        \"\"\"\n        Initialize range volatility calculator.\n\n        Args:\n            annualize: Whether to annualize volatility\n            trading_periods: Periods per year (252 for daily, 252*24*60 for minute)\n            min_periods: Minimum periods for rolling calculations\n        \"\"\"\n        self.annualize = annualize\n        self.trading_periods = trading_periods\n        self.min_periods = min_periods\n\n        # Annualization factor\n        self.ann_factor = np.sqrt(trading_periods) if annualize else 1.0",
    "source_file": "core\\_experimental\\range_volatility.py",
    "academic_reference": "Parkinson (1980) 'Extreme Value Variance' J. Business",
    "class_name": "RangeVolatility"
  },
  {
    "name": "parkinson",
    "category": "volatility",
    "formula": " = (1 / 4*ln(2)) * E[(ln(H/L))] | np.sqrt(variance) * self.ann_factor",
    "explanation": "Parkinson (1980) volatility estimator.\n\nUses high-low range only. ~5x more efficient than close-to-close.\n\nFormula:\n     = (1 / 4*ln(2)) * E[(ln(H/L))]\n\nReference:\n    Parkinson, M. (1980). \"The Extreme Value Method for Estimating\n    the Variance of the Rate of Return\". Journal of Business, 53(1), 61-65.\n\nArgs:\n    high: High prices\n    low: Low prices\n    window: Rolling window size\n\nReturns:\n    Parkinson volatility series",
    "python_code": "def parkinson(\n        self,\n        high: pd.Series,\n        low: pd.Series,\n        window: int = 20\n    ) -> pd.Series:\n        \"\"\"\n        Parkinson (1980) volatility estimator.\n\n        Uses high-low range only. ~5x more efficient than close-to-close.\n\n        Formula:\n             = (1 / 4*ln(2)) * E[(ln(H/L))]\n\n        Reference:\n            Parkinson, M. (1980). \"The Extreme Value Method for Estimating\n            the Variance of the Rate of Return\". Journal of Business, 53(1), 61-65.\n\n        Args:\n            high: High prices\n            low: Low prices\n            window: Rolling window size\n\n        Returns:\n            Parkinson volatility series\n        \"\"\"\n        # Constant: 1 / (4 * ln(2))  0.3607\n        k = 1.0 / (4.0 * np.log(2))\n\n        log_hl = np.log(high / low) ** 2\n\n        variance = k * log_hl.rolling(window, min_periods=self.min_periods).mean()\n\n        return np.sqrt(variance) * self.ann_factor",
    "source_file": "core\\_experimental\\range_volatility.py",
    "academic_reference": "Parkinson (1980) 'Extreme Value Variance' J. Business",
    "class_name": "RangeVolatility"
  },
  {
    "name": "parkinson_single",
    "category": "volatility",
    "formula": "np.sqrt(k * np.log(high / low) ** 2) * self.ann_factor",
    "explanation": "Single-period Parkinson volatility.",
    "python_code": "def parkinson_single(self, high: float, low: float) -> float:\n        \"\"\"Single-period Parkinson volatility.\"\"\"\n        k = 1.0 / (4.0 * np.log(2))\n        return np.sqrt(k * np.log(high / low) ** 2) * self.ann_factor",
    "source_file": "core\\_experimental\\range_volatility.py",
    "academic_reference": "Parkinson (1980) 'Extreme Value Variance' J. Business",
    "class_name": "RangeVolatility"
  },
  {
    "name": "garman_klass",
    "category": "volatility",
    "formula": " = 0.5 * (ln(H/L)) - (2*ln(2) - 1) * (ln(C/O)) | np.sqrt(variance) * self.ann_factor",
    "explanation": "Garman-Klass (1980) volatility estimator.\n\nUses OHLC data. ~7.4x more efficient than close-to-close.\nAssumes no drift (zero mean return).\n\nFormula:\n     = 0.5 * (ln(H/L)) - (2*ln(2) - 1) * (ln(C/O))\n\nReference:\n    Garman, M. B., & Klass, M. J. (1980). \"On the Estimation of Security\n    Price Volatilities from Historical Data\". Journal of Business, 53(1), 67-78.\n\nArgs:\n    open_: Open prices\n    high: High prices\n    low: Low prices\n    close: Close prices\n    window: Rolling window size\n\nReturns:\n    Garman-Klass volatility series",
    "python_code": "def garman_klass(\n        self,\n        open_: pd.Series,\n        high: pd.Series,\n        low: pd.Series,\n        close: pd.Series,\n        window: int = 20\n    ) -> pd.Series:\n        \"\"\"\n        Garman-Klass (1980) volatility estimator.\n\n        Uses OHLC data. ~7.4x more efficient than close-to-close.\n        Assumes no drift (zero mean return).\n\n        Formula:\n             = 0.5 * (ln(H/L)) - (2*ln(2) - 1) * (ln(C/O))\n\n        Reference:\n            Garman, M. B., & Klass, M. J. (1980). \"On the Estimation of Security\n            Price Volatilities from Historical Data\". Journal of Business, 53(1), 67-78.\n\n        Args:\n            open_: Open prices\n            high: High prices\n            low: Low prices\n            close: Close prices\n            window: Rolling window size\n\n        Returns:\n            Garman-Klass volatility series\n        \"\"\"\n        log_hl_sq = np.log(high / low) ** 2\n        log_co_sq = np.log(close / open_) ** 2\n\n        # Coefficient: 2*ln(2) - 1  0.3863\n        k = 2.0 * np.log(2) - 1.0\n\n        variance = 0.5 * log_hl_sq - k * log_co_sq\n\n        # Rolling mean\n        variance = variance.rolling(window, min_periods=self.min_periods).mean()\n\n        # Handle negative variance (can happen due to noise)\n        variance = variance.clip(lower=0)\n\n        return np.sqrt(variance) * self.ann_factor",
    "source_file": "core\\_experimental\\range_volatility.py",
    "academic_reference": "Parkinson (1980) 'Extreme Value Variance' J. Business",
    "class_name": "RangeVolatility"
  },
  {
    "name": "garman_klass_single",
    "category": "volatility",
    "formula": "np.sqrt(variance) * self.ann_factor",
    "explanation": "Single-period Garman-Klass volatility.",
    "python_code": "def garman_klass_single(\n        self,\n        open_: float,\n        high: float,\n        low: float,\n        close: float\n    ) -> float:\n        \"\"\"Single-period Garman-Klass volatility.\"\"\"\n        log_hl_sq = np.log(high / low) ** 2\n        log_co_sq = np.log(close / open_) ** 2\n        k = 2.0 * np.log(2) - 1.0\n        variance = max(0, 0.5 * log_hl_sq - k * log_co_sq)\n        return np.sqrt(variance) * self.ann_factor",
    "source_file": "core\\_experimental\\range_volatility.py",
    "academic_reference": "Parkinson (1980) 'Extreme Value Variance' J. Business",
    "class_name": "RangeVolatility"
  },
  {
    "name": "rogers_satchell",
    "category": "volatility",
    "formula": " = ln(H/C) * ln(H/O) + ln(L/C) * ln(L/O) | np.sqrt(variance) * self.ann_factor",
    "explanation": "Rogers-Satchell (1991) volatility estimator.\n\nDrift-independent estimator using OHLC. Unbiased even with non-zero drift.\n\nFormula:\n     = ln(H/C) * ln(H/O) + ln(L/C) * ln(L/O)\n\nReference:\n    Rogers, L. C. G., & Satchell, S. E. (1991). \"Estimating Variance from\n    High, Low and Closing Prices\". Annals of Applied Probability, 1(4), 504-512.\n\nArgs:\n    open_: Open prices\n    high: High prices\n    low: Low prices\n    close: Close prices\n    window: Rolling window size\n\nReturns:\n    Rogers-Satchell volatility series",
    "python_code": "def rogers_satchell(\n        self,\n        open_: pd.Series,\n        high: pd.Series,\n        low: pd.Series,\n        close: pd.Series,\n        window: int = 20\n    ) -> pd.Series:\n        \"\"\"\n        Rogers-Satchell (1991) volatility estimator.\n\n        Drift-independent estimator using OHLC. Unbiased even with non-zero drift.\n\n        Formula:\n             = ln(H/C) * ln(H/O) + ln(L/C) * ln(L/O)\n\n        Reference:\n            Rogers, L. C. G., & Satchell, S. E. (1991). \"Estimating Variance from\n            High, Low and Closing Prices\". Annals of Applied Probability, 1(4), 504-512.\n\n        Args:\n            open_: Open prices\n            high: High prices\n            low: Low prices\n            close: Close prices\n            window: Rolling window size\n\n        Returns:\n            Rogers-Satchell volatility series\n        \"\"\"\n        log_hc = np.log(high / close)\n        log_ho = np.log(high / open_)\n        log_lc = np.log(low / close)\n        log_lo = np.log(low / open_)\n\n        variance = log_hc * log_ho + log_lc * log_lo\n\n        # Rolling mean\n        variance = variance.rolling(window, min_periods=self.min_periods).mean()\n\n        # Handle negative variance\n        variance = variance.clip(lower=0)\n\n        return np.sqrt(variance) * self.ann_factor",
    "source_file": "core\\_experimental\\range_volatility.py",
    "academic_reference": "Parkinson (1980) 'Extreme Value Variance' J. Business",
    "class_name": "RangeVolatility"
  },
  {
    "name": "rogers_satchell_single",
    "category": "volatility",
    "formula": "np.sqrt(max(0, variance)) * self.ann_factor",
    "explanation": "Single-period Rogers-Satchell volatility.",
    "python_code": "def rogers_satchell_single(\n        self,\n        open_: float,\n        high: float,\n        low: float,\n        close: float\n    ) -> float:\n        \"\"\"Single-period Rogers-Satchell volatility.\"\"\"\n        variance = (np.log(high/close) * np.log(high/open_) +\n                   np.log(low/close) * np.log(low/open_))\n        return np.sqrt(max(0, variance)) * self.ann_factor",
    "source_file": "core\\_experimental\\range_volatility.py",
    "academic_reference": "Parkinson (1980) 'Extreme Value Variance' J. Business",
    "class_name": "RangeVolatility"
  },
  {
    "name": "yang_zhang",
    "category": "volatility",
    "formula": " = _overnight + k*_open_close + (1-k)*_RS | k = 0.34 is optimal for  = 1.34 (market open/close timing) | np.sqrt(variance) * self.ann_factor",
    "explanation": "Yang-Zhang (2000) volatility estimator.\n\nMost efficient estimator (~14x vs close-to-close).\nHandles overnight jumps (open != previous close).\nCombines overnight, open-to-close, and Rogers-Satchell components.\n\nFormula:\n     = _overnight + k*_open_close + (1-k)*_RS\n\nwhere k = 0.34 is optimal for  = 1.34 (market open/close timing)\n\nReference:\n    Yang, D., & Zhang, Q. (2000). \"Drift Independent Volatility Estimation\n    Based on High, Low, Open, and Close Prices\". Journal of Business, 73(3), 477-492.\n\nArgs:\n    open_: Open prices\n    high: High prices\n    low: Low prices\n    close: Close prices\n    window: Rolling window size\n    k: Weighting parameter (default 0.34 is optimal)\n\nReturns:\n    Yang-Zhang volatility series",
    "python_code": "def yang_zhang(\n        self,\n        open_: pd.Series,\n        high: pd.Series,\n        low: pd.Series,\n        close: pd.Series,\n        window: int = 20,\n        k: float = 0.34\n    ) -> pd.Series:\n        \"\"\"\n        Yang-Zhang (2000) volatility estimator.\n\n        Most efficient estimator (~14x vs close-to-close).\n        Handles overnight jumps (open != previous close).\n        Combines overnight, open-to-close, and Rogers-Satchell components.\n\n        Formula:\n             = _overnight + k*_open_close + (1-k)*_RS\n\n        where k = 0.34 is optimal for  = 1.34 (market open/close timing)\n\n        Reference:\n            Yang, D., & Zhang, Q. (2000). \"Drift Independent Volatility Estimation\n            Based on High, Low, Open, and Close Prices\". Journal of Business, 73(3), 477-492.\n\n        Args:\n            open_: Open prices\n            high: High prices\n            low: Low prices\n            close: Close prices\n            window: Rolling window size\n            k: Weighting parameter (default 0.34 is optimal)\n\n        Returns:\n            Yang-Zhang volatility series\n        \"\"\"\n        # Component 1: Overnight variance (close-to-open)\n        log_co = np.log(open_ / close.shift(1))\n        overnight_var = log_co.rolling(window, min_periods=self.min_periods).var()\n\n        # Component 2: Open-to-close variance\n        log_oc = np.log(close / open_)\n        open_close_var = log_oc.rolling(window, min_periods=self.min_periods).var()\n\n        # Component 3: Rogers-Satchell variance (intraday)\n        log_hc = np.log(high / close)\n        log_ho = np.log(high / open_)\n        log_lc = np.log(low / close)\n        log_lo = np.log(low / open_)\n        rs_var = (log_hc * log_ho + log_lc * log_lo).rolling(\n            window, min_periods=self.min_periods\n        ).mean()\n\n        # Combined variance\n        variance = overnight_var + k * open_close_var + (1 - k) * rs_var\n\n        # Handle negative variance\n        variance = variance.clip(lower=0)\n\n        return np.sqrt(variance) * self.ann_factor",
    "source_file": "core\\_experimental\\range_volatility.py",
    "academic_reference": "Parkinson (1980) 'Extreme Value Variance' J. Business",
    "class_name": "RangeVolatility"
  },
  {
    "name": "yang_zhang_single",
    "category": "volatility",
    "formula": "np.sqrt(max(0, variance)) * self.ann_factor",
    "explanation": "Single-period Yang-Zhang volatility (requires previous close).",
    "python_code": "def yang_zhang_single(\n        self,\n        open_: float,\n        high: float,\n        low: float,\n        close: float,\n        prev_close: float,\n        k: float = 0.34\n    ) -> float:\n        \"\"\"Single-period Yang-Zhang volatility (requires previous close).\"\"\"\n        overnight = np.log(open_ / prev_close) ** 2\n        open_close = np.log(close / open_) ** 2\n        rs = (np.log(high/close) * np.log(high/open_) +\n              np.log(low/close) * np.log(low/open_))\n        variance = overnight + k * open_close + (1 - k) * rs\n        return np.sqrt(max(0, variance)) * self.ann_factor",
    "source_file": "core\\_experimental\\range_volatility.py",
    "academic_reference": "Parkinson (1980) 'Extreme Value Variance' J. Business",
    "class_name": "RangeVolatility"
  },
  {
    "name": "close_to_close",
    "category": "volatility",
    "formula": " = std(ln(C_t / C_{t-1})) | log_returns.rolling(window, min_periods=self.min_periods).std() * self.ann_factor",
    "explanation": "Standard close-to-close volatility (baseline for comparison).\n\nFormula:\n     = std(ln(C_t / C_{t-1}))\n\nArgs:\n    close: Close prices\n    window: Rolling window size\n\nReturns:\n    Close-to-close volatility series",
    "python_code": "def close_to_close(\n        self,\n        close: pd.Series,\n        window: int = 20\n    ) -> pd.Series:\n        \"\"\"\n        Standard close-to-close volatility (baseline for comparison).\n\n        Formula:\n             = std(ln(C_t / C_{t-1}))\n\n        Args:\n            close: Close prices\n            window: Rolling window size\n\n        Returns:\n            Close-to-close volatility series\n        \"\"\"\n        log_returns = np.log(close / close.shift(1))\n        return log_returns.rolling(window, min_periods=self.min_periods).std() * self.ann_factor",
    "source_file": "core\\_experimental\\range_volatility.py",
    "academic_reference": "Parkinson (1980) 'Extreme Value Variance' J. Business",
    "class_name": "RangeVolatility"
  },
  {
    "name": "ensemble",
    "category": "volatility",
    "formula": "ensemble_vol",
    "explanation": "Ensemble of all volatility estimators.\n\nCombines multiple estimators for robust volatility estimation.\n\nArgs:\n    open_: Open prices\n    high: High prices\n    low: Low prices\n    close: Close prices\n    window: Rolling window size\n    weights: Dict of estimator weights (default: efficiency-weighted)\n\nReturns:\n    Ensemble volatility series",
    "python_code": "def ensemble(\n        self,\n        open_: pd.Series,\n        high: pd.Series,\n        low: pd.Series,\n        close: pd.Series,\n        window: int = 20,\n        weights: Optional[dict] = None\n    ) -> pd.Series:\n        \"\"\"\n        Ensemble of all volatility estimators.\n\n        Combines multiple estimators for robust volatility estimation.\n\n        Args:\n            open_: Open prices\n            high: High prices\n            low: Low prices\n            close: Close prices\n            window: Rolling window size\n            weights: Dict of estimator weights (default: efficiency-weighted)\n\n        Returns:\n            Ensemble volatility series\n        \"\"\"\n        if weights is None:\n            # Default: weight by relative efficiency\n            weights = {\n                'yang_zhang': 0.40,      # 14x efficiency\n                'garman_klass': 0.25,    # 7.4x efficiency\n                'rogers_satchell': 0.20, # 6x efficiency\n                'parkinson': 0.15        # 5x efficiency\n            }\n\n        vols = {\n            'yang_zhang': self.yang_zhang(open_, high, low, close, window),\n            'garman_klass': self.garman_klass(open_, high, low, close, window),\n            'rogers_satchell': self.rogers_satchell(open_, high, low, close, window),\n            'parkinson': self.parkinson(high, low, window)\n        }\n\n        ensemble_vol = sum(weights[k] * vols[k] for k in weights)\n\n        return ensemble_vol",
    "source_file": "core\\_experimental\\range_volatility.py",
    "academic_reference": "Parkinson (1980) 'Extreme Value Variance' J. Business",
    "class_name": "RangeVolatility"
  },
  {
    "name": "compute_all_features",
    "category": "volatility",
    "formula": "pd.DataFrame(features)",
    "explanation": "Compute all volatility features for ML.\n\nReturns DataFrame with volatility features at multiple windows.",
    "python_code": "def compute_all_features(\n        self,\n        open_: pd.Series,\n        high: pd.Series,\n        low: pd.Series,\n        close: pd.Series,\n        windows: list = [5, 10, 20, 50]\n    ) -> pd.DataFrame:\n        \"\"\"\n        Compute all volatility features for ML.\n\n        Returns DataFrame with volatility features at multiple windows.\n        \"\"\"\n        features = {}\n\n        for w in windows:\n            features[f'vol_parkinson_{w}'] = self.parkinson(high, low, w)\n            features[f'vol_garman_klass_{w}'] = self.garman_klass(open_, high, low, close, w)\n            features[f'vol_rogers_satchell_{w}'] = self.rogers_satchell(open_, high, low, close, w)\n            features[f'vol_yang_zhang_{w}'] = self.yang_zhang(open_, high, low, close, w)\n            features[f'vol_close_to_close_{w}'] = self.close_to_close(close, w)\n\n        # Volatility ratios (regime indicators)\n        features['vol_ratio_short_long'] = features[f'vol_yang_zhang_{windows[0]}'] / features[f'vol_yang_zhang_{windows[-1]}']\n        features['vol_ratio_yz_cc'] = features[f'vol_yang_zhang_{windows[1]}'] / features[f'vol_close_to_close_{windows[1]}']\n\n        # Volatility percentile (relative to history)\n        vol_ref = features[f'vol_yang_zhang_{windows[1]}']\n        features['vol_percentile'] = vol_ref.rolling(252, min_periods=20).apply(\n            lambda x: (x < x.iloc[-1]).mean(), raw=False\n        )\n\n        return pd.DataFrame(features)",
    "source_file": "core\\_experimental\\range_volatility.py",
    "academic_reference": "Parkinson (1980) 'Extreme Value Variance' J. Business",
    "class_name": "RangeVolatility"
  },
  {
    "name": "volatility_regime",
    "category": "volatility",
    "formula": "regime",
    "explanation": "Classify volatility regime: low, normal, high.\n\nUses Yang-Zhang (most efficient estimator).\n\nReturns:\n    Series with regime labels",
    "python_code": "def volatility_regime(\n        self,\n        open_: pd.Series,\n        high: pd.Series,\n        low: pd.Series,\n        close: pd.Series,\n        window: int = 20,\n        low_percentile: float = 25,\n        high_percentile: float = 75\n    ) -> pd.Series:\n        \"\"\"\n        Classify volatility regime: low, normal, high.\n\n        Uses Yang-Zhang (most efficient estimator).\n\n        Returns:\n            Series with regime labels\n        \"\"\"\n        vol = self.yang_zhang(open_, high, low, close, window)\n\n        low_thresh = vol.rolling(252, min_periods=50).quantile(low_percentile / 100)\n        high_thresh = vol.rolling(252, min_periods=50).quantile(high_percentile / 100)\n\n        regime = pd.Series('normal', index=vol.index)\n        regime[vol < low_thresh] = 'low'\n        regime[vol > high_thresh] = 'high'\n\n        return regime",
    "source_file": "core\\_experimental\\range_volatility.py",
    "academic_reference": "Parkinson (1980) 'Extreme Value Variance' J. Business",
    "class_name": "RangeVolatility"
  },
  {
    "name": "volatility_breakout",
    "category": "volatility",
    "formula": "True = breakout) | vol > threshold",
    "explanation": "Detect volatility breakouts (spikes above threshold).\n\nReturns:\n    Boolean series (True = breakout)",
    "python_code": "def volatility_breakout(\n        self,\n        open_: pd.Series,\n        high: pd.Series,\n        low: pd.Series,\n        close: pd.Series,\n        window: int = 20,\n        threshold_percentile: float = 90\n    ) -> pd.Series:\n        \"\"\"\n        Detect volatility breakouts (spikes above threshold).\n\n        Returns:\n            Boolean series (True = breakout)\n        \"\"\"\n        vol = self.yang_zhang(open_, high, low, close, window)\n        threshold = vol.rolling(252, min_periods=50).quantile(threshold_percentile / 100)\n        return vol > threshold",
    "source_file": "core\\_experimental\\range_volatility.py",
    "academic_reference": "Parkinson (1980) 'Extreme Value Variance' J. Business",
    "class_name": "RangeVolatility"
  },
  {
    "name": "estimate",
    "category": "volatility",
    "formula": "(gk + yz) / 2",
    "explanation": "GKYZ estimator: average of Garman-Klass and Yang-Zhang.",
    "python_code": "def estimate(\n        self,\n        open_: pd.Series,\n        high: pd.Series,\n        low: pd.Series,\n        close: pd.Series,\n        window: int = 20\n    ) -> pd.Series:\n        \"\"\"\n        GKYZ estimator: average of Garman-Klass and Yang-Zhang.\n        \"\"\"\n        gk = self.rv.garman_klass(open_, high, low, close, window)\n        yz = self.rv.yang_zhang(open_, high, low, close, window)\n        return (gk + yz) / 2",
    "source_file": "core\\_experimental\\range_volatility.py",
    "academic_reference": "Parkinson (1980) 'Extreme Value Variance' J. Business",
    "class_name": "GarmanKlassYangZhang"
  },
  {
    "name": "create_range_volatility",
    "category": "volatility",
    "formula": "RangeVolatility(annualize, trading_periods)",
    "explanation": "Factory function to create range volatility calculator.",
    "python_code": "def create_range_volatility(\n    annualize: bool = True,\n    trading_periods: int = 252\n) -> RangeVolatility:\n    \"\"\"Factory function to create range volatility calculator.\"\"\"\n    return RangeVolatility(annualize, trading_periods)",
    "source_file": "core\\_experimental\\range_volatility.py",
    "academic_reference": "Parkinson (1980) 'Extreme Value Variance' J. Business",
    "class_name": null
  },
  {
    "name": "parkinson_vol",
    "category": "volatility",
    "formula": "RangeVolatility().parkinson(high, low, window)",
    "explanation": "Quick Parkinson volatility.",
    "python_code": "def parkinson_vol(high: pd.Series, low: pd.Series, window: int = 20) -> pd.Series:\n    \"\"\"Quick Parkinson volatility.\"\"\"\n    return RangeVolatility().parkinson(high, low, window)",
    "source_file": "core\\_experimental\\range_volatility.py",
    "academic_reference": "Parkinson (1980) 'Extreme Value Variance' J. Business",
    "class_name": null
  },
  {
    "name": "garman_klass_vol",
    "category": "volatility",
    "formula": "RangeVolatility().garman_klass(open_, high, low, close, window)",
    "explanation": "Quick Garman-Klass volatility.",
    "python_code": "def garman_klass_vol(\n    open_: pd.Series, high: pd.Series, low: pd.Series, close: pd.Series, window: int = 20\n) -> pd.Series:\n    \"\"\"Quick Garman-Klass volatility.\"\"\"\n    return RangeVolatility().garman_klass(open_, high, low, close, window)",
    "source_file": "core\\_experimental\\range_volatility.py",
    "academic_reference": "Parkinson (1980) 'Extreme Value Variance' J. Business",
    "class_name": null
  },
  {
    "name": "rogers_satchell_vol",
    "category": "volatility",
    "formula": "RangeVolatility().rogers_satchell(open_, high, low, close, window)",
    "explanation": "Quick Rogers-Satchell volatility.",
    "python_code": "def rogers_satchell_vol(\n    open_: pd.Series, high: pd.Series, low: pd.Series, close: pd.Series, window: int = 20\n) -> pd.Series:\n    \"\"\"Quick Rogers-Satchell volatility.\"\"\"\n    return RangeVolatility().rogers_satchell(open_, high, low, close, window)",
    "source_file": "core\\_experimental\\range_volatility.py",
    "academic_reference": "Parkinson (1980) 'Extreme Value Variance' J. Business",
    "class_name": null
  },
  {
    "name": "yang_zhang_vol",
    "category": "volatility",
    "formula": "RangeVolatility().yang_zhang(open_, high, low, close, window)",
    "explanation": "Quick Yang-Zhang volatility.",
    "python_code": "def yang_zhang_vol(\n    open_: pd.Series, high: pd.Series, low: pd.Series, close: pd.Series, window: int = 20\n) -> pd.Series:\n    \"\"\"Quick Yang-Zhang volatility.\"\"\"\n    return RangeVolatility().yang_zhang(open_, high, low, close, window)",
    "source_file": "core\\_experimental\\range_volatility.py",
    "academic_reference": "Parkinson (1980) 'Extreme Value Variance' J. Business",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "regime",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, n_states: int = 3):\n        self.n_states = n_states\n\n        # Transition matrix (initialized to favor staying in same state)\n        self.A = np.array([\n            [0.95, 0.04, 0.01],  # Low vol stays low\n            [0.03, 0.94, 0.03],  # Normal stays normal\n            [0.01, 0.04, 0.95]   # High vol stays high\n        ])\n\n        # Emission parameters (mean, std for each state)\n        # Low vol: small returns, low variance\n        # Normal: medium returns, medium variance\n        # High vol: larger returns, high variance\n        self.emission_means = np.array([0.0, 0.0, 0.0])\n        self.emission_stds = np.array([0.0001, 0.0003, 0.001])\n\n        # Initial state probabilities\n        self.pi = np.array([0.3, 0.5, 0.2])\n\n        self.fitted = False",
    "source_file": "core\\_experimental\\regime_features.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "SimpleHMMRegime"
  },
  {
    "name": "emission_prob",
    "category": "regime",
    "formula": "norm.pdf(x, mean, std)",
    "explanation": "Probability of observing x given state.",
    "python_code": "def emission_prob(self, x: float, state: int) -> float:\n        \"\"\"Probability of observing x given state.\"\"\"\n        mean = self.emission_means[state]\n        std = self.emission_stds[state]\n        return norm.pdf(x, mean, std)",
    "source_file": "core\\_experimental\\regime_features.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "SimpleHMMRegime"
  },
  {
    "name": "fit",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Fit HMM using Baum-Welch algorithm.\n\nThis is the key algorithm Renaissance uses - same as speech recognition.",
    "python_code": "def fit(self, returns: np.ndarray, max_iter: int = 100, tol: float = 1e-6):\n        \"\"\"\n        Fit HMM using Baum-Welch algorithm.\n\n        This is the key algorithm Renaissance uses - same as speech recognition.\n        \"\"\"\n        n = len(returns)\n\n        # Initialize emission parameters from data percentiles\n        vol = pd.Series(returns).rolling(20).std().values\n        vol = vol[~np.isnan(vol)]\n\n        if len(vol) > 0:\n            p33 = np.percentile(vol, 33)\n            p67 = np.percentile(vol, 67)\n\n            self.emission_stds = np.array([\n                p33 * 0.8,  # Low vol\n                np.median(vol),  # Normal vol\n                p67 * 1.5  # High vol\n            ])\n\n        # EM iterations (Baum-Welch)\n        prev_log_lik = -np.inf\n\n        for iteration in range(max_iter):\n            # E-step: Forward-Backward\n            alpha, beta, log_lik = self._forward_backward(returns)\n\n            # Check convergence\n            if abs(log_lik - prev_log_lik) < tol:\n                break\n            prev_log_lik = log_lik\n\n            # M-step: Update parameters\n            # Compute posterior probabilities\n            gamma = alpha * beta\n            gamma = gamma / gamma.sum(axis=1, keepdims=True)\n\n            # Update transition matrix\n            xi = np.zeros((n - 1, self.n_states, self.n_states))\n            for t in range(n - 1):\n                for i in range(self.n_states):\n                    for j in range(self.n_states):\n                        xi[t, i, j] = (alpha[t, i] * self.A[i, j] *\n                                      self.emission_prob(returns[t + 1], j) * beta[t + 1, j])\n                xi[t] /= xi[t].sum() + 1e-10\n\n            for i in range(self.n_states):\n                denom = gamma[:-1, i].sum() + 1e-10\n                for j in range(self.n_states):\n                    self.A[i, j] = xi[:, i, j].sum() / denom\n\n            # Update emission parameters\n            for j in range(self.n_states):\n                denom = gamma[:, j].sum() + 1e-10\n                self.emission_means[j] = (gamma[:, j] * returns).sum() / denom\n\n                centered = returns - self.emission_means[j]\n                self.emission_stds[j] = np.sqrt((gamma[:, j] * centered**2).sum() / denom)\n                self.emission_stds[j] = max(self.emission_stds[j], 1e-6)\n\n        self.fitted = True",
    "source_file": "core\\_experimental\\regime_features.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "SimpleHMMRegime"
  },
  {
    "name": "_forward_backward",
    "category": "regime",
    "formula": "alpha, beta, log_lik",
    "explanation": "Forward-backward algorithm.",
    "python_code": "def _forward_backward(self, returns: np.ndarray) -> Tuple[np.ndarray, np.ndarray, float]:\n        \"\"\"Forward-backward algorithm.\"\"\"\n        n = len(returns)\n\n        # Forward pass\n        alpha = np.zeros((n, self.n_states))\n        alpha[0] = self.pi * np.array([self.emission_prob(returns[0], j) for j in range(self.n_states)])\n        alpha[0] /= alpha[0].sum() + 1e-10\n\n        log_lik = 0\n        for t in range(1, n):\n            for j in range(self.n_states):\n                alpha[t, j] = (alpha[t - 1] @ self.A[:, j]) * self.emission_prob(returns[t], j)\n\n            scale = alpha[t].sum() + 1e-10\n            alpha[t] /= scale\n            log_lik += np.log(scale)\n\n        # Backward pass\n        beta = np.zeros((n, self.n_states))\n        beta[-1] = 1\n\n        for t in range(n - 2, -1, -1):\n            for i in range(self.n_states):\n                for j in range(self.n_states):\n                    beta[t, i] += self.A[i, j] * self.emission_prob(returns[t + 1], j) * beta[t + 1, j]\n\n            beta[t] /= beta[t].sum() + 1e-10\n\n        return alpha, beta, log_lik",
    "source_file": "core\\_experimental\\regime_features.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "SimpleHMMRegime"
  },
  {
    "name": "predict_regime",
    "category": "machine_learning",
    "formula": "states",
    "explanation": "Predict most likely regime sequence (Viterbi).",
    "python_code": "def predict_regime(self, returns: np.ndarray) -> np.ndarray:\n        \"\"\"Predict most likely regime sequence (Viterbi).\"\"\"\n        n = len(returns)\n\n        # Viterbi algorithm\n        delta = np.zeros((n, self.n_states))\n        psi = np.zeros((n, self.n_states), dtype=int)\n\n        # Initialize\n        for j in range(self.n_states):\n            delta[0, j] = np.log(self.pi[j] + 1e-10) + np.log(self.emission_prob(returns[0], j) + 1e-10)\n\n        # Forward pass\n        for t in range(1, n):\n            for j in range(self.n_states):\n                probs = delta[t - 1] + np.log(self.A[:, j] + 1e-10)\n                psi[t, j] = np.argmax(probs)\n                delta[t, j] = probs[psi[t, j]] + np.log(self.emission_prob(returns[t], j) + 1e-10)\n\n        # Backtrack\n        states = np.zeros(n, dtype=int)\n        states[-1] = np.argmax(delta[-1])\n\n        for t in range(n - 2, -1, -1):\n            states[t] = psi[t + 1, states[t + 1]]\n\n        return states",
    "source_file": "core\\_experimental\\regime_features.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "SimpleHMMRegime"
  },
  {
    "name": "get_regime_probabilities",
    "category": "regime",
    "formula": "gamma",
    "explanation": "Get probability of each regime at each time.",
    "python_code": "def get_regime_probabilities(self, returns: np.ndarray) -> np.ndarray:\n        \"\"\"Get probability of each regime at each time.\"\"\"\n        alpha, beta, _ = self._forward_backward(returns)\n        gamma = alpha * beta\n        gamma = gamma / gamma.sum(axis=1, keepdims=True)\n        return gamma",
    "source_file": "core\\_experimental\\regime_features.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "SimpleHMMRegime"
  },
  {
    "name": "transform",
    "category": "statistical",
    "formula": "result",
    "explanation": "Transform features to regime-normalized values.",
    "python_code": "def transform(self, features: pd.DataFrame, regimes: np.ndarray) -> pd.DataFrame:\n        \"\"\"\n        Transform features to regime-normalized values.\n        \"\"\"\n        result = features.copy()\n\n        for regime in np.unique(regimes):\n            mask = regimes == regime\n\n            if regime in self.regime_stats:\n                for col in features.columns:\n                    if col in self.regime_stats[regime]:\n                        mean, std = self.regime_stats[regime][col]\n                        result.loc[mask, col] = (features.loc[mask, col] - mean) / std\n\n        return result",
    "source_file": "core\\_experimental\\regime_features.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "RegimeFeatureScaler"
  },
  {
    "name": "get_position_multiplier",
    "category": "regime",
    "formula": "",
    "explanation": "Get position size multiplier for regime.",
    "python_code": "def get_position_multiplier(self, regime: int) -> float:\n        \"\"\"Get position size multiplier for regime.\"\"\"\n        return self.config.position_mult.get(regime, 1.0)",
    "source_file": "core\\_experimental\\regime_features.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "RegimeAwarePositionSizer"
  },
  {
    "name": "get_stop_multiplier",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Get stop loss multiplier for regime.",
    "python_code": "def get_stop_multiplier(self, regime: int) -> float:\n        \"\"\"Get stop loss multiplier for regime.\"\"\"\n        return self.config.stop_mult.get(regime, 1.0)",
    "source_file": "core\\_experimental\\regime_features.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "RegimeAwarePositionSizer"
  },
  {
    "name": "regime_adjusted_kelly",
    "category": "risk",
    "formula": "Kelly = p - (1-p)/b where p = win_prob, b = win_loss_ratio | 0.0 | kelly * base_fraction * regime_mult",
    "explanation": "Compute regime-adjusted Kelly fraction.\n\nKelly = p - (1-p)/b where p = win_prob, b = win_loss_ratio\n\nThen multiply by regime adjustment and base fraction.",
    "python_code": "def regime_adjusted_kelly(self,\n                              win_prob: float,\n                              win_loss_ratio: float,\n                              regime: int,\n                              base_fraction: float = 0.25) -> float:\n        \"\"\"\n        Compute regime-adjusted Kelly fraction.\n\n        Kelly = p - (1-p)/b where p = win_prob, b = win_loss_ratio\n\n        Then multiply by regime adjustment and base fraction.\n        \"\"\"\n        if win_prob <= 0 or win_loss_ratio <= 0:\n            return 0.0\n\n        kelly = win_prob - (1 - win_prob) / win_loss_ratio\n        kelly = max(0, kelly)\n\n        regime_mult = self.get_position_multiplier(regime)\n\n        return kelly * base_fraction * regime_mult",
    "source_file": "core\\_experimental\\regime_features.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "RegimeAwarePositionSizer"
  },
  {
    "name": "detect_regime",
    "category": "regime",
    "formula": "RegimeState(",
    "explanation": "Detect current regime.",
    "python_code": "def detect_regime(self, returns: np.ndarray) -> RegimeState:\n        \"\"\"Detect current regime.\"\"\"\n        if not self.fitted:\n            # Fallback to volatility-based detection\n            vol = np.std(returns[-20:]) if len(returns) >= 20 else np.std(returns)\n            if vol < 0.0002:\n                regime = 0\n            elif vol > 0.0006:\n                regime = 2\n            else:\n                regime = 1\n\n            probs = np.zeros(3)\n            probs[regime] = 1.0\n        else:\n            probs = self.hmm.get_regime_probabilities(returns)[-1]\n            regime = np.argmax(probs)\n\n        # Track regime duration\n        if regime == self.current_regime:\n            self.regime_duration += 1\n        else:\n            self.current_regime = regime\n            self.regime_duration = 1\n\n        # Compute transition probability\n        trans_prob = 1 - self.hmm.A[regime, regime] if self.fitted else 0.05\n\n        regime_names = {0: 'low', 1: 'normal', 2: 'high'}\n\n        return RegimeState(\n            regime=regime,\n            probability=probs,\n            duration=self.regime_duration,\n            transition_prob=trans_prob,\n            regime_name=regime_names.get(regime, 'unknown')\n        )",
    "source_file": "core\\_experimental\\regime_features.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "RegimeDependentFeatureEngine"
  },
  {
    "name": "compute_features",
    "category": "regime",
    "formula": "features",
    "explanation": "Compute regime-dependent features.\n\nReturns DataFrame with:\n- Regime indicators\n- Regime probabilities\n- Transition probabilities\n- Regime-scaled base features (if provided)\n- Strategy weights\n- Position multipliers",
    "python_code": "def compute_features(self, returns: pd.Series,\n                        base_features: pd.DataFrame = None) -> pd.DataFrame:\n        \"\"\"\n        Compute regime-dependent features.\n\n        Returns DataFrame with:\n        - Regime indicators\n        - Regime probabilities\n        - Transition probabilities\n        - Regime-scaled base features (if provided)\n        - Strategy weights\n        - Position multipliers\n        \"\"\"\n        returns_arr = returns.values\n        n = len(returns_arr)\n\n        # Fit if not already\n        if not self.fitted and n > 50:\n            self.fit(returns_arr)\n\n        features = pd.DataFrame(index=returns.index)\n\n        # Regime detection\n        if self.fitted:\n            regimes = self.hmm.predict_regime(returns_arr)\n            regime_probs = self.hmm.get_regime_probabilities(returns_arr)\n        else:\n            # Fallback\n            rolling_vol = returns.rolling(20).std()\n            vol_33 = rolling_vol.quantile(0.33)\n            vol_67 = rolling_vol.quantile(0.67)\n\n            regimes = np.where(\n                rolling_vol < vol_33, 0,\n                np.where(rolling_vol > vol_67, 2, 1)\n            )\n            regimes = np.nan_to_num(regimes, nan=1).astype(int)\n\n            regime_probs = np.zeros((n, 3))\n            for t, r in enumerate(regimes):\n                regime_probs[t, r] = 1.0\n\n        features['regime'] = regimes\n        features['regime_prob_low'] = regime_probs[:, 0]\n        features['regime_prob_normal'] = regime_probs[:, 1]\n        features['regime_prob_high'] = regime_probs[:, 2]\n\n        # Regime duration\n        duration = np.ones(n)\n        for t in range(1, n):\n            if regimes[t] == regimes[t - 1]:\n                duration[t] = duration[t - 1] + 1\n\n        features['regime_duration'] = duration\n\n        # Transition indicators\n        transitions = np.zeros(n)\n        transitions[1:] = (regimes[1:] != regimes[:-1]).astype(int)\n        features['regime_transition'] = transitions\n\n        # Position and stop multipliers\n        features['position_mult'] = [self.config.position_mult.get(r, 1.0) for r in regimes]\n        features['stop_mult'] = [self.config.stop_mult.get(r, 1.0) for r in regimes]\n\n        # Strategy weights\n        for strategy in ['mean_reversion', 'momentum', 'breakout']:\n            features[f'weight_{strategy}'] = [\n                self.config.strategy_weights.get(r, {}).get(strategy, 0.33)\n                for r in regimes\n            ]\n\n        # Regime-scaled base features\n        if base_features is not None and self.fitted:\n            scaled = self.scaler.transform(base_features, regimes)\n            for col in scaled.columns:\n                features[f'{col}_regime_scaled'] = scaled[col]\n\n        # Can trade indicator (avoid transitions)\n        features['can_trade'] = (duration >= self.config.min_duration).astype(int)\n\n        return features",
    "source_file": "core\\_experimental\\regime_features.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": "RegimeDependentFeatureEngine"
  },
  {
    "name": "compute_regime_features",
    "category": "deep_learning",
    "formula": "engine.compute_features(returns, base_features)",
    "explanation": "Convenience function for regime feature computation.",
    "python_code": "def compute_regime_features(returns: pd.Series,\n                           base_features: pd.DataFrame = None) -> pd.DataFrame:\n    \"\"\"\n    Convenience function for regime feature computation.\n    \"\"\"\n    engine = RegimeDependentFeatureEngine()\n    return engine.compute_features(returns, base_features)",
    "source_file": "core\\_experimental\\regime_features.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": null
  },
  {
    "name": "get_regime_adjusted_position",
    "category": "regime",
    "formula": "position",
    "explanation": "Get regime-adjusted position size.\n\nArgs:\n    signal: Trading signal (-1 to 1)\n    confidence: Model confidence (0 to 1)\n    regime: Current regime (0=low, 1=normal, 2=high)\n    base_size: Base position size as fraction of capital\n\nReturns:\n    Adjusted position size",
    "python_code": "def get_regime_adjusted_position(signal: float,\n                                confidence: float,\n                                regime: int,\n                                base_size: float = 0.01) -> float:\n    \"\"\"\n    Get regime-adjusted position size.\n\n    Args:\n        signal: Trading signal (-1 to 1)\n        confidence: Model confidence (0 to 1)\n        regime: Current regime (0=low, 1=normal, 2=high)\n        base_size: Base position size as fraction of capital\n\n    Returns:\n        Adjusted position size\n    \"\"\"\n    config = RegimeConfig()\n\n    regime_mult = config.position_mult.get(regime, 1.0)\n    position = signal * confidence * base_size * regime_mult\n\n    return position",
    "source_file": "core\\_experimental\\regime_features.py",
    "academic_reference": "Kelly (1956) 'Information Rate' Bell System Tech J.",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "risk",
    "formula": "",
    "explanation": "Initialize the Renaissance HFT Engine.\n\nArgs:\n    institutional_dir: Path to institutional models (HMM, Kalman)\n    ensemble_dir: Path to ensemble models (XGBoost, LightGBM, CatBoost)\n    capital: Trading capital\n    max_drawdown: Maximum allowed drawdown\n    kelly_fraction: Fraction of Kelly to use (default 25%)",
    "python_code": "def __init__(self,\n                 institutional_dir: Path = None,\n                 ensemble_dir: Path = None,\n                 capital: float = 100000,\n                 max_drawdown: float = 0.05,\n                 kelly_fraction: float = 0.25):\n        \"\"\"\n        Initialize the Renaissance HFT Engine.\n\n        Args:\n            institutional_dir: Path to institutional models (HMM, Kalman)\n            ensemble_dir: Path to ensemble models (XGBoost, LightGBM, CatBoost)\n            capital: Trading capital\n            max_drawdown: Maximum allowed drawdown\n            kelly_fraction: Fraction of Kelly to use (default 25%)\n        \"\"\"\n        self.institutional_dir = institutional_dir or Path(\"models/institutional\")\n        self.ensemble_dir = ensemble_dir or Path(\"models/hft_ensemble\")\n        self.capital = capital\n        self.max_drawdown = max_drawdown\n        self.kelly_fraction = kelly_fraction\n\n        # Initialize components\n        self._init_institutional()\n        self._init_order_flow()\n        self._init_ensemble()\n        self._init_filter()\n        self._init_kelly()\n\n        # State tracking\n        self.current_drawdown = 0.0\n        self.daily_trades = 0\n        self.max_daily_trades = 100\n\n        logger.info(\"Renaissance HFT Engine initialized\")\n        self._log_status()",
    "source_file": "core\\_experimental\\renaissance_hft_engine.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "RenaissanceHFTEngine"
  },
  {
    "name": "_init_institutional",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Initialize institutional predictor (HMM + Kalman).",
    "python_code": "def _init_institutional(self):\n        \"\"\"Initialize institutional predictor (HMM + Kalman).\"\"\"\n        if HAS_INSTITUTIONAL:\n            self.institutional = InstitutionalPredictor(self.institutional_dir)\n            loaded = self.institutional.load_models()\n            if loaded:\n                logger.info(\"Institutional predictor loaded (HMM + Kalman)\")\n            else:\n                logger.warning(\"Institutional models not found - using fallback\")\n        else:\n            self.institutional = None\n            logger.warning(\"Institutional predictor not available\")",
    "source_file": "core\\_experimental\\renaissance_hft_engine.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "RenaissanceHFTEngine"
  },
  {
    "name": "_init_order_flow",
    "category": "microstructure",
    "formula": "",
    "explanation": "Initialize order flow feature generator.",
    "python_code": "def _init_order_flow(self):\n        \"\"\"Initialize order flow feature generator.\"\"\"\n        if HAS_ORDER_FLOW:\n            self.order_flow = OrderFlowFeatures(lookback=100)\n            logger.info(\"Order flow features initialized\")\n        else:\n            self.order_flow = None\n            logger.warning(\"Order flow features not available\")",
    "source_file": "core\\_experimental\\renaissance_hft_engine.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "RenaissanceHFTEngine"
  },
  {
    "name": "_init_ensemble",
    "category": "machine_learning",
    "formula": "# Load models for each symbol",
    "explanation": "Load trained ensemble models.",
    "python_code": "def _init_ensemble(self):\n        \"\"\"Load trained ensemble models.\"\"\"\n        self.ensemble_models = {}\n\n        if not self.ensemble_dir.exists():\n            logger.warning(f\"Ensemble directory not found: {self.ensemble_dir}\")\n            return\n\n        # Load models for each symbol\n        for model_file in self.ensemble_dir.glob(\"*_models.pkl\"):\n            symbol = model_file.stem.replace(\"_models\", \"\")\n            try:\n                with open(model_file, 'rb') as f:\n                    self.ensemble_models[symbol] = pickle.load(f)\n                logger.info(f\"Loaded ensemble for {symbol}\")\n            except Exception as e:\n                logger.warning(f\"Failed to load {model_file}: {e}\")\n\n        if self.ensemble_models:\n            logger.info(f\"Ensemble models loaded for: {list(self.ensemble_models.keys())}\")\n        else:\n            logger.warning(\"No ensemble models loaded\")",
    "source_file": "core\\_experimental\\renaissance_hft_engine.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "RenaissanceHFTEngine"
  },
  {
    "name": "_init_filter",
    "category": "filtering",
    "formula": "",
    "explanation": "Initialize ultra-selective filter.",
    "python_code": "def _init_filter(self):\n        \"\"\"Initialize ultra-selective filter.\"\"\"\n        if HAS_FILTER:\n            self.filter = UltraSelectiveFilter(\n                confidence_percentile=90,\n                require_unanimous=True,\n                favorable_regimes=[0, 1],  # Low vol + Normal\n                require_ofi_confirm=True,\n                base_accuracy=0.59\n            )\n            logger.info(\"Ultra-selective filter initialized (4-layer)\")\n        else:\n            self.filter = None\n            logger.warning(\"Ultra-selective filter not available\")",
    "source_file": "core\\_experimental\\renaissance_hft_engine.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "RenaissanceHFTEngine"
  },
  {
    "name": "_init_kelly",
    "category": "risk",
    "formula": "",
    "explanation": "Initialize Kelly criterion position sizing.",
    "python_code": "def _init_kelly(self):\n        \"\"\"Initialize Kelly criterion position sizing.\"\"\"\n        if HAS_KELLY:\n            self.kelly = KellyCriterion()\n            logger.info(\"Kelly criterion initialized\")\n        else:\n            self.kelly = None\n            logger.warning(\"Kelly criterion not available\")",
    "source_file": "core\\_experimental\\renaissance_hft_engine.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "RenaissanceHFTEngine"
  },
  {
    "name": "_log_status",
    "category": "quantitative",
    "formula": "",
    "explanation": "Log component status.",
    "python_code": "def _log_status(self):\n        \"\"\"Log component status.\"\"\"\n        components = [\n            ('Institutional (HMM+Kalman)', self.institutional is not None),\n            ('Order Flow (OFI)', self.order_flow is not None),\n            ('Ensemble Models', bool(self.ensemble_models)),\n            ('Ultra-Selective Filter', self.filter is not None),\n            ('Kelly Criterion', self.kelly is not None),\n        ]\n\n        logger.info(\"Component Status:\")\n        for name, ready in components:\n            status = \"READY\" if ready else \"NOT AVAILABLE\"\n            logger.info(f\"  {name}: {status}\")",
    "source_file": "core\\_experimental\\renaissance_hft_engine.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "RenaissanceHFTEngine"
  },
  {
    "name": "update_tick",
    "category": "quantitative",
    "formula": "",
    "explanation": "Update with new tick data.\n\nArgs:\n    symbol: Currency pair\n    bid: Best bid price\n    ask: Best ask price\n    bid_size: Quantity at bid\n    ask_size: Quantity at ask\n    timestamp: Tick timestamp",
    "python_code": "def update_tick(self, symbol: str, bid: float, ask: float,\n                   bid_size: float = 1.0, ask_size: float = 1.0,\n                   timestamp: datetime = None):\n        \"\"\"\n        Update with new tick data.\n\n        Args:\n            symbol: Currency pair\n            bid: Best bid price\n            ask: Best ask price\n            bid_size: Quantity at bid\n            ask_size: Quantity at ask\n            timestamp: Tick timestamp\n        \"\"\"\n        if timestamp is None:\n            timestamp = datetime.now()\n\n        # Update order flow\n        if self.order_flow:\n            self.order_flow.update_book(bid, ask, bid_size, ask_size, timestamp)",
    "source_file": "core\\_experimental\\renaissance_hft_engine.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "RenaissanceHFTEngine"
  },
  {
    "name": "update_trade",
    "category": "quantitative",
    "formula": "",
    "explanation": "Update with trade data.\n\nArgs:\n    price: Trade price\n    size: Trade size\n    side: 'buy' or 'sell'\n    timestamp: Trade timestamp",
    "python_code": "def update_trade(self, price: float, size: float, side: str,\n                    timestamp: datetime = None):\n        \"\"\"\n        Update with trade data.\n\n        Args:\n            price: Trade price\n            size: Trade size\n            side: 'buy' or 'sell'\n            timestamp: Trade timestamp\n        \"\"\"\n        if timestamp is None:\n            timestamp = datetime.now()\n\n        if self.order_flow:\n            self.order_flow.update_trade(price, size, side, timestamp)",
    "source_file": "core\\_experimental\\renaissance_hft_engine.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "RenaissanceHFTEngine"
  },
  {
    "name": "get_signal",
    "category": "machine_learning",
    "formula": "TradeSignal( | no-trade | TradeSignal(",
    "explanation": "Generate trading signal using all components.\n\nArgs:\n    symbol: Currency pair (e.g., 'EURUSD')\n    features: Feature array for model prediction\n    data: Historical data DataFrame (for regime detection)\n\nReturns:\n    TradeSignal with complete analysis",
    "python_code": "def get_signal(self, symbol: str, features: np.ndarray,\n                  data: pd.DataFrame = None) -> TradeSignal:\n        \"\"\"\n        Generate trading signal using all components.\n\n        Args:\n            symbol: Currency pair (e.g., 'EURUSD')\n            features: Feature array for model prediction\n            data: Historical data DataFrame (for regime detection)\n\n        Returns:\n            TradeSignal with complete analysis\n        \"\"\"\n        timestamp = datetime.now()\n\n        # Check daily trade limit\n        if self.daily_trades >= self.max_daily_trades:\n            return TradeSignal(\n                should_trade=False, direction=0, symbol=symbol,\n                size=0, confidence=0, theoretical_accuracy=0,\n                regime=1, regime_action='normal', ofi=0,\n                kalman_deviation=0, filters_passed=[],\n                reason=\"Daily trade limit reached\", timestamp=timestamp\n            )\n\n        # ===========================================\n        # Step 1: Get Regime from HMM\n        # ===========================================\n        regime = 1  # Default: normal\n        regime_action = 'normal'\n        kalman_deviation = 0.0\n\n        if self.institutional and data is not None:\n            try:\n                # Prepare returns and volatility\n                if 'mid' in data.columns:\n                    mid = data['mid'].values\n                elif 'bid' in data.columns and 'ask' in data.columns:\n                    mid = (data['bid'].values + data['ask'].values) / 2\n                else:\n                    mid = data.iloc[:, 0].values\n\n                returns = np.diff(mid) / mid[:-1]\n                volatility = pd.Series(returns).rolling(20).std().values\n\n                # Get regime\n                regime_info = self.institutional.detect_regime(symbol, returns, volatility)\n                regime = regime_info.get('state', 1)\n                regime_action = regime_info.get('action', 'normal')\n\n                # Get Kalman estimate\n                kalman_info = self.institutional.get_kalman_estimate(symbol, mid[-1])\n                kalman_deviation = kalman_info.get('deviation', 0.0)\n\n            except Exception as e:\n                logger.warning(f\"Regime detection failed: {e}\")\n\n        # ===========================================\n        # Step 2: Get Order Flow Imbalance\n        # ===========================================\n        ofi = 0.0\n        if self.order_flow:\n            try:\n                ofi = self.order_flow.get_ofi()\n            except Exception as e:\n                logger.warning(f\"OFI calculation failed: {e}\")\n\n        # ===========================================\n        # Step 3: Get Model Predictions\n        # ===========================================\n        model_predictions = {}\n\n        if symbol in self.ensemble_models:\n            try:\n                models_data = self.ensemble_models[symbol]\n\n                # Handle different model storage formats\n                if is",
    "source_file": "core\\_experimental\\renaissance_hft_engine.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "RenaissanceHFTEngine"
  },
  {
    "name": "update_state",
    "category": "microstructure",
    "formula": "",
    "explanation": "Update engine state after trade.\n\nArgs:\n    pnl: Profit/loss from trade",
    "python_code": "def update_state(self, pnl: float):\n        \"\"\"\n        Update engine state after trade.\n\n        Args:\n            pnl: Profit/loss from trade\n        \"\"\"\n        self.daily_trades += 1\n\n        # Update drawdown\n        if pnl < 0:\n            self.current_drawdown += abs(pnl) / self.capital",
    "source_file": "core\\_experimental\\renaissance_hft_engine.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "RenaissanceHFTEngine"
  },
  {
    "name": "reset_daily",
    "category": "quantitative",
    "formula": "",
    "explanation": "Reset daily counters.",
    "python_code": "def reset_daily(self):\n        \"\"\"Reset daily counters.\"\"\"\n        self.daily_trades = 0",
    "source_file": "core\\_experimental\\renaissance_hft_engine.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "RenaissanceHFTEngine"
  },
  {
    "name": "get_status",
    "category": "quantitative",
    "formula": "status",
    "explanation": "Get engine status.",
    "python_code": "def get_status(self) -> Dict:\n        \"\"\"Get engine status.\"\"\"\n        status = {\n            'components': {\n                'institutional': self.institutional is not None,\n                'order_flow': self.order_flow is not None,\n                'ensemble_models': list(self.ensemble_models.keys()),\n                'filter': self.filter is not None,\n                'kelly': self.kelly is not None,\n            },\n            'state': {\n                'capital': self.capital,\n                'current_drawdown': self.current_drawdown,\n                'daily_trades': self.daily_trades,\n                'max_daily_trades': self.max_daily_trades,\n            }\n        }\n\n        if self.filter:\n            status['filter_stats'] = self.filter.get_stats()\n\n        return status",
    "source_file": "core\\_experimental\\renaissance_hft_engine.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": "RenaissanceHFTEngine"
  },
  {
    "name": "create_renaissance_engine",
    "category": "reinforcement_learning",
    "formula": "RenaissanceHFTEngine(",
    "explanation": "Factory function to create Renaissance HFT Engine.\n\nArgs:\n    models_dir: Base directory for models\n\nReturns:\n    Configured RenaissanceHFTEngine",
    "python_code": "def create_renaissance_engine(models_dir: Path = None) -> RenaissanceHFTEngine:\n    \"\"\"\n    Factory function to create Renaissance HFT Engine.\n\n    Args:\n        models_dir: Base directory for models\n\n    Returns:\n        Configured RenaissanceHFTEngine\n    \"\"\"\n    if models_dir is None:\n        models_dir = Path(\"models\")\n\n    return RenaissanceHFTEngine(\n        institutional_dir=models_dir / \"institutional\",\n        ensemble_dir=models_dir / \"hft_ensemble\",\n        capital=100000,\n        max_drawdown=0.05,\n        kelly_fraction=0.25\n    )",
    "source_file": "core\\_experimental\\renaissance_hft_engine.py",
    "academic_reference": "Easley, Lopez de Prado, O'Hara (2012) 'Flow Toxicity' RFS",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "quantitative",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, model_name: str = \"ProsusAI/finbert\"):\n        self.model_name = model_name\n        self.model = None\n        self.tokenizer = None\n        self.is_loaded = False\n\n        if HAS_TRANSFORMERS:\n            self._load_model()",
    "source_file": "core\\_experimental\\sentiment_finbert.py",
    "academic_reference": null,
    "class_name": "FinBERTSentiment"
  },
  {
    "name": "_load_model",
    "category": "quantitative",
    "formula": "",
    "explanation": "Load FinBERT model.",
    "python_code": "def _load_model(self):\n        \"\"\"Load FinBERT model.\"\"\"\n        try:\n            logger.info(f\"Loading FinBERT model: {self.model_name}\")\n            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n            self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)\n            self.model.eval()\n            self.is_loaded = True\n            logger.info(\"FinBERT model loaded successfully\")\n        except Exception as e:\n            logger.warning(f\"Failed to load FinBERT: {e}\")\n            self.is_loaded = False",
    "source_file": "core\\_experimental\\sentiment_finbert.py",
    "academic_reference": null,
    "class_name": "FinBERTSentiment"
  },
  {
    "name": "analyze",
    "category": "quantitative",
    "formula": "SentimentResult(",
    "explanation": "Analyze sentiment of text.\n\nArgs:\n    text: Financial text (headline, news, tweet)\n\nReturns:\n    SentimentResult with probabilities",
    "python_code": "def analyze(self, text: str) -> SentimentResult:\n        \"\"\"\n        Analyze sentiment of text.\n\n        Args:\n            text: Financial text (headline, news, tweet)\n\n        Returns:\n            SentimentResult with probabilities\n        \"\"\"\n        if not self.is_loaded:\n            return self._keyword_fallback(text)\n\n        try:\n            inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n\n            with torch.no_grad():\n                outputs = self.model(**inputs)\n                probs = torch.softmax(outputs.logits, dim=1)[0].numpy()\n\n            # FinBERT labels: [negative, neutral, positive]\n            bearish_prob = probs[0]\n            neutral_prob = probs[1]\n            bullish_prob = probs[2]\n\n            # Determine sentiment\n            max_idx = np.argmax(probs)\n            sentiments = ['bearish', 'neutral', 'bullish']\n            sentiment = sentiments[max_idx]\n            score = probs[max_idx]\n\n            return SentimentResult(\n                text=text,\n                sentiment=sentiment,\n                score=score,\n                bullish_prob=bullish_prob,\n                neutral_prob=neutral_prob,\n                bearish_prob=bearish_prob\n            )\n\n        except Exception as e:\n            logger.warning(f\"FinBERT analysis failed: {e}\")\n            return self._keyword_fallback(text)",
    "source_file": "core\\_experimental\\sentiment_finbert.py",
    "academic_reference": null,
    "class_name": "FinBERTSentiment"
  },
  {
    "name": "_keyword_fallback",
    "category": "quantitative",
    "formula": "SentimentResult(text, 'neutral', 0.6, 0.2, 0.6, 0.2) | SentimentResult(text, sentiment, score, bullish_prob, neutral_prob, bearish_prob)",
    "explanation": "Keyword-based sentiment when FinBERT unavailable.",
    "python_code": "def _keyword_fallback(self, text: str) -> SentimentResult:\n        \"\"\"Keyword-based sentiment when FinBERT unavailable.\"\"\"\n        text_lower = text.lower()\n\n        # Bullish keywords\n        bullish_words = ['surge', 'rally', 'gain', 'rise', 'jump', 'soar', 'bullish',\n                        'strong', 'beat', 'exceed', 'upgrade', 'buy', 'positive',\n                        'growth', 'expand', 'optimistic', 'confident']\n\n        # Bearish keywords\n        bearish_words = ['fall', 'drop', 'decline', 'plunge', 'crash', 'bearish',\n                        'weak', 'miss', 'cut', 'downgrade', 'sell', 'negative',\n                        'contraction', 'pessimistic', 'concern', 'fear', 'risk']\n\n        bullish_count = sum(1 for w in bullish_words if w in text_lower)\n        bearish_count = sum(1 for w in bearish_words if w in text_lower)\n\n        total = bullish_count + bearish_count\n        if total == 0:\n            return SentimentResult(text, 'neutral', 0.6, 0.2, 0.6, 0.2)\n\n        bullish_prob = bullish_count / (total + 1)\n        bearish_prob = bearish_count / (total + 1)\n        neutral_prob = 1 - bullish_prob - bearish_prob\n\n        if bullish_prob > bearish_prob and bullish_prob > neutral_prob:\n            sentiment = 'bullish'\n            score = bullish_prob\n        elif bearish_prob > bullish_prob and bearish_prob > neutral_prob:\n            sentiment = 'bearish'\n            score = bearish_prob\n        else:\n            sentiment = 'neutral'\n            score = neutral_prob\n\n        return SentimentResult(text, sentiment, score, bullish_prob, neutral_prob, bearish_prob)",
    "source_file": "core\\_experimental\\sentiment_finbert.py",
    "academic_reference": null,
    "class_name": "FinBERTSentiment"
  },
  {
    "name": "analyze_batch",
    "category": "quantitative",
    "formula": "[self.analyze(text) for text in texts]",
    "explanation": "Analyze multiple texts efficiently.",
    "python_code": "def analyze_batch(self, texts: List[str]) -> List[SentimentResult]:\n        \"\"\"Analyze multiple texts efficiently.\"\"\"\n        return [self.analyze(text) for text in texts]",
    "source_file": "core\\_experimental\\sentiment_finbert.py",
    "academic_reference": null,
    "class_name": "FinBERTSentiment"
  },
  {
    "name": "analyze_for_pair",
    "category": "quantitative",
    "formula": "{",
    "explanation": "Analyze news sentiment for specific currency pair.\n\nArgs:\n    text: News text\n    pair: Currency pair (e.g., 'EURUSD')\n\nReturns:\n    Dict with signal strength (-1 to 1) and confidence",
    "python_code": "def analyze_for_pair(self, text: str, pair: str) -> Dict[str, float]:\n        \"\"\"\n        Analyze news sentiment for specific currency pair.\n\n        Args:\n            text: News text\n            pair: Currency pair (e.g., 'EURUSD')\n\n        Returns:\n            Dict with signal strength (-1 to 1) and confidence\n        \"\"\"\n        # Get base currencies\n        base = pair[:3]  # EUR in EURUSD\n        quote = pair[3:]  # USD in EURUSD\n\n        # General sentiment\n        general = self.finbert.analyze(text)\n\n        # Currency-specific adjustments\n        base_sentiment = self._currency_sentiment(text, base)\n        quote_sentiment = self._currency_sentiment(text, quote)\n\n        # Net signal: base bullish or quote bearish -> buy pair\n        net_signal = base_sentiment - quote_sentiment\n\n        # Adjust with general sentiment\n        if 'bullish' in general.sentiment:\n            net_signal += 0.2 * general.score\n        elif 'bearish' in general.sentiment:\n            net_signal -= 0.2 * general.score\n\n        # Normalize to -1 to 1\n        signal = np.clip(net_signal, -1, 1)\n\n        # Confidence based on keyword hits\n        confidence = min(1.0, (abs(base_sentiment) + abs(quote_sentiment)) / 2 + general.score * 0.3)\n\n        return {\n            'signal': signal,\n            'confidence': confidence,\n            'base_sentiment': base_sentiment,\n            'quote_sentiment': quote_sentiment,\n            'general_sentiment': general.sentiment,\n            'general_score': general.score\n        }",
    "source_file": "core\\_experimental\\sentiment_finbert.py",
    "academic_reference": null,
    "class_name": "ForexNewsSentiment"
  },
  {
    "name": "_currency_sentiment",
    "category": "quantitative",
    "formula": "0.0 | 0.0 | (bullish_hits - bearish_hits) / total",
    "explanation": "Get sentiment score for specific currency.",
    "python_code": "def _currency_sentiment(self, text: str, currency: str) -> float:\n        \"\"\"Get sentiment score for specific currency.\"\"\"\n        if currency not in self.CURRENCY_KEYWORDS:\n            return 0.0\n\n        text_lower = text.lower()\n        keywords = self.CURRENCY_KEYWORDS[currency]\n\n        bullish_hits = sum(1 for kw in keywords['bullish'] if kw in text_lower)\n        bearish_hits = sum(1 for kw in keywords['bearish'] if kw in text_lower)\n\n        total = bullish_hits + bearish_hits\n        if total == 0:\n            return 0.0\n\n        return (bullish_hits - bearish_hits) / total",
    "source_file": "core\\_experimental\\sentiment_finbert.py",
    "academic_reference": null,
    "class_name": "ForexNewsSentiment"
  },
  {
    "name": "process_news",
    "category": "quantitative",
    "formula": "signal",
    "explanation": "Process incoming news and generate signal.\n\nArgs:\n    timestamp: News timestamp\n    headline: News headline\n    pair: Currency pair to trade\n\nReturns:\n    Trading signal dict or None",
    "python_code": "def process_news(self, timestamp: datetime, headline: str,\n                    pair: str) -> Optional[Dict]:\n        \"\"\"\n        Process incoming news and generate signal.\n\n        Args:\n            timestamp: News timestamp\n            headline: News headline\n            pair: Currency pair to trade\n\n        Returns:\n            Trading signal dict or None\n        \"\"\"\n        analysis = self.sentiment_analyzer.analyze_for_pair(headline, pair)\n\n        if abs(analysis['signal']) < self.sentiment_threshold:\n            return None\n\n        signal = {\n            'timestamp': timestamp,\n            'pair': pair,\n            'headline': headline,\n            'direction': 1 if analysis['signal'] > 0 else -1,\n            'strength': abs(analysis['signal']),\n            'confidence': analysis['confidence'],\n            'exit_time': timestamp + self.hold_period\n        }\n\n        self.active_signals[f\"{pair}_{timestamp}\"] = signal\n        return signal",
    "source_file": "core\\_experimental\\sentiment_finbert.py",
    "academic_reference": null,
    "class_name": "NewsTradingSignal"
  },
  {
    "name": "get_active_signals",
    "category": "quantitative",
    "formula": "active",
    "explanation": "Get all active news signals (not yet expired).",
    "python_code": "def get_active_signals(self, current_time: datetime) -> List[Dict]:\n        \"\"\"Get all active news signals (not yet expired).\"\"\"\n        active = []\n        expired = []\n\n        for key, signal in self.active_signals.items():\n            if current_time < signal['exit_time']:\n                active.append(signal)\n            else:\n                expired.append(key)\n\n        # Clean up expired\n        for key in expired:\n            del self.active_signals[key]\n\n        return active",
    "source_file": "core\\_experimental\\sentiment_finbert.py",
    "academic_reference": null,
    "class_name": "NewsTradingSignal"
  },
  {
    "name": "compute_sentiment_features",
    "category": "quantitative",
    "formula": "pd.DataFrame(features).set_index('timestamp')",
    "explanation": "Compute sentiment features from news headlines.\n\nArgs:\n    headlines: List of (timestamp, headline) tuples\n    pair: Currency pair\n\nReturns:\n    DataFrame with sentiment features",
    "python_code": "def compute_sentiment_features(headlines: List[Tuple[datetime, str]],\n                              pair: str = 'EURUSD') -> pd.DataFrame:\n    \"\"\"\n    Compute sentiment features from news headlines.\n\n    Args:\n        headlines: List of (timestamp, headline) tuples\n        pair: Currency pair\n\n    Returns:\n        DataFrame with sentiment features\n    \"\"\"\n    analyzer = ForexNewsSentiment()\n\n    features = []\n    for timestamp, headline in headlines:\n        analysis = analyzer.analyze_for_pair(headline, pair)\n        features.append({\n            'timestamp': timestamp,\n            'sentiment_signal': analysis['signal'],\n            'sentiment_confidence': analysis['confidence'],\n            'base_sentiment': analysis['base_sentiment'],\n            'quote_sentiment': analysis['quote_sentiment']\n        })\n\n    return pd.DataFrame(features).set_index('timestamp')",
    "source_file": "core\\_experimental\\sentiment_finbert.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "quantitative",
    "formula": "",
    "explanation": "Initialize spectral analyzer.\n\nArgs:\n    sample_rate: Samples per unit time (e.g., 12 for 5-min bars/hour)",
    "python_code": "def __init__(self, sample_rate: float = 1.0):\n        \"\"\"\n        Initialize spectral analyzer.\n\n        Args:\n            sample_rate: Samples per unit time (e.g., 12 for 5-min bars/hour)\n        \"\"\"\n        self.sample_rate = sample_rate",
    "source_file": "core\\_experimental\\spectral_analysis.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "SpectralAnalyzer"
  },
  {
    "name": "fft_spectrum",
    "category": "quantitative",
    "formula": "SpectralResult(",
    "explanation": "Compute FFT power spectrum.\n\nArgs:\n    signal: Time series data\n\nReturns:\n    SpectralResult with frequencies and power",
    "python_code": "def fft_spectrum(self, signal: np.ndarray) -> SpectralResult:\n        \"\"\"\n        Compute FFT power spectrum.\n\n        Args:\n            signal: Time series data\n\n        Returns:\n            SpectralResult with frequencies and power\n        \"\"\"\n        n = len(signal)\n\n        # Detrend\n        detrended = signal - np.mean(signal)\n\n        # Apply window to reduce spectral leakage\n        window = np.hanning(n)\n        windowed = detrended * window\n\n        # FFT\n        spectrum = fft(windowed)\n        power = np.abs(spectrum)**2 / n\n\n        # Frequencies\n        freqs = fftfreq(n, d=1/self.sample_rate)\n\n        # Only positive frequencies\n        pos_mask = freqs > 0\n        freqs = freqs[pos_mask]\n        power = power[pos_mask]\n\n        # Find dominant frequency\n        dominant_idx = np.argmax(power)\n        dominant_freq = freqs[dominant_idx]\n        dominant_period = 1 / dominant_freq if dominant_freq > 0 else np.inf\n\n        # Spectral entropy (measure of randomness)\n        power_norm = power / (power.sum() + 1e-10)\n        spectral_entropy = -np.sum(power_norm * np.log(power_norm + 1e-10)) / np.log(len(power))\n\n        return SpectralResult(\n            frequencies=freqs,\n            power=power,\n            dominant_freq=dominant_freq,\n            dominant_period=dominant_period,\n            spectral_entropy=spectral_entropy\n        )",
    "source_file": "core\\_experimental\\spectral_analysis.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "SpectralAnalyzer"
  },
  {
    "name": "welch_spectrum",
    "category": "quantitative",
    "formula": "SpectralResult(",
    "explanation": "Welch's method for robust spectral estimation.\n\nUses overlapping segments for lower variance estimate.",
    "python_code": "def welch_spectrum(self, signal: np.ndarray,\n                      nperseg: int = None) -> SpectralResult:\n        \"\"\"\n        Welch's method for robust spectral estimation.\n\n        Uses overlapping segments for lower variance estimate.\n        \"\"\"\n        n = len(signal)\n        if nperseg is None:\n            nperseg = min(256, n // 4)\n\n        freqs, power = welch(signal, fs=self.sample_rate, nperseg=nperseg)\n\n        # Find dominant frequency\n        dominant_idx = np.argmax(power)\n        dominant_freq = freqs[dominant_idx]\n        dominant_period = 1 / dominant_freq if dominant_freq > 0 else np.inf\n\n        # Spectral entropy\n        power_norm = power / (power.sum() + 1e-10)\n        spectral_entropy = -np.sum(power_norm * np.log(power_norm + 1e-10)) / np.log(len(power))\n\n        return SpectralResult(\n            frequencies=freqs,\n            power=power,\n            dominant_freq=dominant_freq,\n            dominant_period=dominant_period,\n            spectral_entropy=spectral_entropy\n        )",
    "source_file": "core\\_experimental\\spectral_analysis.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "SpectralAnalyzer"
  },
  {
    "name": "band_power",
    "category": "quantitative",
    "formula": "float(band_power)",
    "explanation": "Compute power in a specific frequency band.\n\nArgs:\n    signal: Time series\n    low_freq: Lower bound of band\n    high_freq: Upper bound of band\n\nReturns:\n    Total power in band",
    "python_code": "def band_power(self, signal: np.ndarray,\n                   low_freq: float, high_freq: float) -> float:\n        \"\"\"\n        Compute power in a specific frequency band.\n\n        Args:\n            signal: Time series\n            low_freq: Lower bound of band\n            high_freq: Upper bound of band\n\n        Returns:\n            Total power in band\n        \"\"\"\n        result = self.welch_spectrum(signal)\n\n        mask = (result.frequencies >= low_freq) & (result.frequencies <= high_freq)\n        band_power = result.power[mask].sum()\n\n        return float(band_power)",
    "source_file": "core\\_experimental\\spectral_analysis.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "SpectralAnalyzer"
  },
  {
    "name": "noise_ratio",
    "category": "quantitative",
    "formula": "ratio = more signal, less noise | float(noise_power / (total_power + 1e-10))",
    "explanation": "Estimate noise ratio (high-freq power / total power).\n\nLower ratio = more signal, less noise",
    "python_code": "def noise_ratio(self, signal: np.ndarray,\n                   noise_freq_cutoff: float = 0.4) -> float:\n        \"\"\"\n        Estimate noise ratio (high-freq power / total power).\n\n        Lower ratio = more signal, less noise\n        \"\"\"\n        result = self.welch_spectrum(signal)\n\n        total_power = result.power.sum()\n        noise_mask = result.frequencies > noise_freq_cutoff\n        noise_power = result.power[noise_mask].sum()\n\n        return float(noise_power / (total_power + 1e-10))",
    "source_file": "core\\_experimental\\spectral_analysis.py",
    "academic_reference": "Cont, Kukanov, Stoikov (2014) 'Order Book Events' JFE",
    "class_name": "SpectralAnalyzer"
  },
  {
    "name": "_haar_decompose",
    "category": "quantitative",
    "formula": "cA = (s[2k] + s[2k+1]) / sqrt(2)  (approximation = low-pass) | cD = (s[2k] - s[2k+1]) / sqrt(2)  (detail = high-pass) | approx, detail",
    "explanation": "Simple Haar wavelet decomposition (one level).\n\ncA = (s[2k] + s[2k+1]) / sqrt(2)  (approximation = low-pass)\ncD = (s[2k] - s[2k+1]) / sqrt(2)  (detail = high-pass)",
    "python_code": "def _haar_decompose(self, signal: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Simple Haar wavelet decomposition (one level).\n\n        cA = (s[2k] + s[2k+1]) / sqrt(2)  (approximation = low-pass)\n        cD = (s[2k] - s[2k+1]) / sqrt(2)  (detail = high-pass)\n        \"\"\"\n        n = len(signal)\n        half = n // 2\n\n        # Ensure even length\n        if n % 2 != 0:\n            signal = np.append(signal, signal[-1])\n            half = len(signal) // 2\n\n        approx = np.zeros(half)\n        detail = np.zeros(half)\n\n        for k in range(half):\n            approx[k] = (signal[2 * k] + signal[2 * k + 1]) / np.sqrt(2)\n            detail[k] = (signal[2 * k] - signal[2 * k + 1]) / np.sqrt(2)\n\n        return approx, detail",
    "source_file": "core\\_experimental\\spectral_analysis.py",
    "academic_reference": null,
    "class_name": "WaveletDecomposer"
  },
  {
    "name": "_db4_decompose",
    "category": "quantitative",
    "formula": "approx, detail",
    "explanation": "Daubechies-4 wavelet decomposition (one level).\n\nMore sophisticated than Haar - smoother approximation.",
    "python_code": "def _db4_decompose(self, signal: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Daubechies-4 wavelet decomposition (one level).\n\n        More sophisticated than Haar - smoother approximation.\n        \"\"\"\n        # DB4 filter coefficients\n        h0 = (1 + np.sqrt(3)) / (4 * np.sqrt(2))\n        h1 = (3 + np.sqrt(3)) / (4 * np.sqrt(2))\n        h2 = (3 - np.sqrt(3)) / (4 * np.sqrt(2))\n        h3 = (1 - np.sqrt(3)) / (4 * np.sqrt(2))\n\n        # Low-pass filter (scaling)\n        lpf = np.array([h0, h1, h2, h3])\n        # High-pass filter (wavelet)\n        hpf = np.array([h3, -h2, h1, -h0])\n\n        n = len(signal)\n\n        # Pad for convolution\n        padded = np.concatenate([signal[-3:], signal, signal[:3]])\n\n        # Convolve and downsample\n        approx = np.convolve(padded, lpf, mode='valid')[::2]\n        detail = np.convolve(padded, hpf, mode='valid')[::2]\n\n        # Trim to correct length\n        half = (n + 1) // 2\n        approx = approx[:half]\n        detail = detail[:half]\n\n        return approx, detail",
    "source_file": "core\\_experimental\\spectral_analysis.py",
    "academic_reference": null,
    "class_name": "WaveletDecomposer"
  },
  {
    "name": "decompose",
    "category": "quantitative",
    "formula": "WaveletResult(",
    "explanation": "Multi-level wavelet decomposition.\n\nReturns approximation (trend) and detail coefficients at each level.",
    "python_code": "def decompose(self, signal: np.ndarray) -> WaveletResult:\n        \"\"\"\n        Multi-level wavelet decomposition.\n\n        Returns approximation (trend) and detail coefficients at each level.\n        \"\"\"\n        # Use simple implementation if pywt not available\n        approx = signal.copy()\n        details = []\n\n        for level in range(self.levels):\n            if len(approx) < 8:\n                break\n\n            if self.wavelet == 'haar':\n                approx, detail = self._haar_decompose(approx)\n            else:\n                approx, detail = self._db4_decompose(approx)\n\n            details.append(detail)\n\n        # Reconstruct trend (low-pass filtered signal)\n        trend = np.interp(\n            np.linspace(0, len(approx) - 1, len(signal)),\n            np.arange(len(approx)),\n            approx\n        )\n\n        # Noise is sum of high-frequency details\n        noise = signal - trend\n\n        return WaveletResult(\n            approximation=approx,\n            details=details,\n            levels=len(details),\n            trend=trend,\n            noise=noise\n        )",
    "source_file": "core\\_experimental\\spectral_analysis.py",
    "academic_reference": null,
    "class_name": "WaveletDecomposer"
  },
  {
    "name": "denoise",
    "category": "quantitative",
    "formula": "denoised",
    "explanation": "Wavelet denoising using soft thresholding.\n\nArgs:\n    signal: Noisy signal\n    threshold: Threshold for wavelet coefficients (auto if None)\n\nReturns:\n    Denoised signal",
    "python_code": "def denoise(self, signal: np.ndarray, threshold: float = None) -> np.ndarray:\n        \"\"\"\n        Wavelet denoising using soft thresholding.\n\n        Args:\n            signal: Noisy signal\n            threshold: Threshold for wavelet coefficients (auto if None)\n\n        Returns:\n            Denoised signal\n        \"\"\"\n        result = self.decompose(signal)\n\n        if threshold is None:\n            # Universal threshold (Donoho & Johnstone)\n            n = len(signal)\n            sigma = np.median(np.abs(result.details[0])) / 0.6745\n            threshold = sigma * np.sqrt(2 * np.log(n))\n\n        # Soft thresholding\n        denoised_details = []\n        for detail in result.details:\n            thresholded = np.sign(detail) * np.maximum(np.abs(detail) - threshold, 0)\n            denoised_details.append(thresholded)\n\n        # Reconstruct (simplified - use trend + reduced noise)\n        denoised = result.trend.copy()\n        for detail in denoised_details:\n            # Upsample and add\n            upsampled = np.interp(\n                np.linspace(0, len(detail) - 1, len(signal)),\n                np.arange(len(detail)),\n                detail\n            )\n            denoised += upsampled\n\n        return denoised",
    "source_file": "core\\_experimental\\spectral_analysis.py",
    "academic_reference": null,
    "class_name": "WaveletDecomposer"
  },
  {
    "name": "transform",
    "category": "quantitative",
    "formula": "np.exp(-x**2 / 2) * np.cos(5 * x) | coeffs, scales",
    "explanation": "Compute CWT.\n\nArgs:\n    signal: Time series\n    scales: Wavelet scales (auto if None)\n\nReturns:\n    (coefficients, scales) - 2D array of wavelet coefficients",
    "python_code": "def transform(self, signal: np.ndarray,\n                 scales: np.ndarray = None) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Compute CWT.\n\n        Args:\n            signal: Time series\n            scales: Wavelet scales (auto if None)\n\n        Returns:\n            (coefficients, scales) - 2D array of wavelet coefficients\n        \"\"\"\n        n = len(signal)\n\n        if scales is None:\n            # Default scales (covering various frequencies)\n            scales = np.arange(2, min(n // 2, 128))\n\n        # Use scipy's cwt with Morlet-like wavelet\n        def morlet(n_points, width):\n            \"\"\"Morlet wavelet approximation.\"\"\"\n            x = np.linspace(-4, 4, n_points)\n            return np.exp(-x**2 / 2) * np.cos(5 * x)\n\n        if cwt is not None:\n            coeffs = cwt(signal, morlet, scales)\n        else:\n            # Manual CWT implementation using convolution\n            coeffs = np.zeros((len(scales), n))\n            for i, scale in enumerate(scales):\n                n_points = min(int(scale * 10), n)\n                if n_points < 4:\n                    n_points = 4\n                wavelet = morlet(n_points, scale)\n                wavelet = wavelet / np.sqrt(scale)  # Normalize\n                # Convolve with padding\n                conv = np.convolve(signal, wavelet, mode='same')\n                coeffs[i] = conv\n\n        return coeffs, scales",
    "source_file": "core\\_experimental\\spectral_analysis.py",
    "academic_reference": null,
    "class_name": "ContinuousWaveletTransform"
  },
  {
    "name": "scalogram",
    "category": "quantitative",
    "formula": "{",
    "explanation": "Compute scalogram (power of CWT).\n\nReturns dict with:\n- power: 2D array (scales x time)\n- scales: scale axis\n- times: time axis\n- total_power: power at each time\n- dominant_scale: scale with max power at each time",
    "python_code": "def scalogram(self, signal: np.ndarray,\n                  scales: np.ndarray = None) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Compute scalogram (power of CWT).\n\n        Returns dict with:\n        - power: 2D array (scales x time)\n        - scales: scale axis\n        - times: time axis\n        - total_power: power at each time\n        - dominant_scale: scale with max power at each time\n        \"\"\"\n        coeffs, scales = self.transform(signal, scales)\n\n        power = np.abs(coeffs)**2\n\n        # Summary statistics\n        total_power = power.sum(axis=0)\n        dominant_scale = scales[np.argmax(power, axis=0)]\n\n        return {\n            'power': power,\n            'scales': scales,\n            'times': np.arange(len(signal)),\n            'total_power': total_power,\n            'dominant_scale': dominant_scale\n        }",
    "source_file": "core\\_experimental\\spectral_analysis.py",
    "academic_reference": null,
    "class_name": "ContinuousWaveletTransform"
  },
  {
    "name": "analytic_signal",
    "category": "quantitative",
    "formula": "hilbert(signal)",
    "explanation": "Compute analytic signal using Hilbert transform.\n\nReturns complex signal: z(t) = x(t) + i*H[x(t)]",
    "python_code": "def analytic_signal(self, signal: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute analytic signal using Hilbert transform.\n\n        Returns complex signal: z(t) = x(t) + i*H[x(t)]\n        \"\"\"\n        return hilbert(signal)",
    "source_file": "core\\_experimental\\spectral_analysis.py",
    "academic_reference": null,
    "class_name": "HilbertTransform"
  },
  {
    "name": "instantaneous_phase",
    "category": "quantitative",
    "formula": "np.angle(analytic)",
    "explanation": "Compute instantaneous phase.\n\n(t) = arctan(H[x(t)] / x(t))",
    "python_code": "def instantaneous_phase(self, signal: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute instantaneous phase.\n\n        (t) = arctan(H[x(t)] / x(t))\n        \"\"\"\n        analytic = self.analytic_signal(signal)\n        return np.angle(analytic)",
    "source_file": "core\\_experimental\\spectral_analysis.py",
    "academic_reference": null,
    "class_name": "HilbertTransform"
  },
  {
    "name": "instantaneous_frequency",
    "category": "quantitative",
    "formula": "inst_freq",
    "explanation": "Compute instantaneous frequency.\n\nf(t) = d/dt / (2)",
    "python_code": "def instantaneous_frequency(self, signal: np.ndarray,\n                               sample_rate: float = 1.0) -> np.ndarray:\n        \"\"\"\n        Compute instantaneous frequency.\n\n        f(t) = d/dt / (2)\n        \"\"\"\n        phase = self.instantaneous_phase(signal)\n\n        # Unwrap phase to avoid discontinuities\n        unwrapped = np.unwrap(phase)\n\n        # Compute derivative\n        inst_freq = np.diff(unwrapped) * sample_rate / (2 * np.pi)\n\n        # Pad to match original length\n        inst_freq = np.concatenate([inst_freq, [inst_freq[-1]]])\n\n        return inst_freq",
    "source_file": "core\\_experimental\\spectral_analysis.py",
    "academic_reference": null,
    "class_name": "HilbertTransform"
  },
  {
    "name": "envelope",
    "category": "quantitative",
    "formula": "np.abs(analytic)",
    "explanation": "Compute envelope (instantaneous amplitude).\n\nA(t) = |z(t)|",
    "python_code": "def envelope(self, signal: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Compute envelope (instantaneous amplitude).\n\n        A(t) = |z(t)|\n        \"\"\"\n        analytic = self.analytic_signal(signal)\n        return np.abs(analytic)",
    "source_file": "core\\_experimental\\spectral_analysis.py",
    "academic_reference": null,
    "class_name": "HilbertTransform"
  },
  {
    "name": "compute_features",
    "category": "statistical",
    "formula": "features",
    "explanation": "Compute all spectral features.\n\nArgs:\n    prices: Price series\n    window: Rolling window for spectral analysis\n\nReturns:\n    DataFrame with spectral features",
    "python_code": "def compute_features(self, prices: pd.Series,\n                        window: int = 100) -> pd.DataFrame:\n        \"\"\"\n        Compute all spectral features.\n\n        Args:\n            prices: Price series\n            window: Rolling window for spectral analysis\n\n        Returns:\n            DataFrame with spectral features\n        \"\"\"\n        returns = prices.pct_change().fillna(0)\n        n = len(prices)\n\n        features = pd.DataFrame(index=prices.index)\n\n        # Initialize arrays\n        dominant_period = np.zeros(n)\n        spectral_entropy = np.zeros(n)\n        noise_ratio = np.zeros(n)\n        trend = np.zeros(n)\n        inst_freq = np.zeros(n)\n        envelope = np.zeros(n)\n\n        for t in range(window, n):\n            window_prices = prices.iloc[t - window:t].values\n            window_returns = returns.iloc[t - window:t].values\n\n            # FFT features\n            spec_result = self.spectral.welch_spectrum(window_returns)\n            dominant_period[t] = spec_result.dominant_period\n            spectral_entropy[t] = spec_result.spectral_entropy\n\n            # Noise ratio\n            noise_ratio[t] = self.spectral.noise_ratio(window_returns)\n\n            # Wavelet features\n            wav_result = self.wavelet.decompose(window_prices)\n            trend[t] = wav_result.trend[-1]\n\n            # Hilbert features (on returns)\n            if len(window_returns) > 10:\n                inst_freq[t] = self.hilbert.instantaneous_frequency(window_returns)[-1]\n                envelope[t] = self.hilbert.envelope(window_returns)[-1]\n\n        features['spectral_dominant_period'] = dominant_period\n        features['spectral_entropy'] = spectral_entropy\n        features['spectral_noise_ratio'] = noise_ratio\n\n        # Wavelet features\n        features['wavelet_trend'] = trend\n        features['wavelet_deviation'] = prices.values - trend  # Price deviation from trend\n\n        # Trend slope\n        features['wavelet_trend_slope'] = pd.Series(trend).diff().fillna(0).values\n\n        # Hilbert features\n        features['hilbert_inst_freq'] = inst_freq\n        features['hilbert_envelope'] = envelope\n\n        # Derived features\n        # Cyclicality score: low entropy + high dominant period = cyclical\n        features['cyclicality'] = (1 - spectral_entropy) * np.log1p(dominant_period)\n\n        # Trend strength: low noise ratio = strong trend\n        features['trend_strength'] = 1 - noise_ratio\n\n        # Detrended price (mean reversion signal)\n        features['detrended_price'] = (prices.values - trend) / (trend + 1e-10) * 10000\n\n        return features",
    "source_file": "core\\_experimental\\spectral_analysis.py",
    "academic_reference": null,
    "class_name": "SpectralFeatureEngine"
  },
  {
    "name": "compute_spectral_features",
    "category": "deep_learning",
    "formula": "engine.compute_features(prices, window)",
    "explanation": "Convenience function for spectral feature computation.",
    "python_code": "def compute_spectral_features(prices: pd.Series,\n                             sample_rate: float = 12.0,\n                             window: int = 100) -> pd.DataFrame:\n    \"\"\"\n    Convenience function for spectral feature computation.\n    \"\"\"\n    engine = SpectralFeatureEngine(sample_rate)\n    return engine.compute_features(prices, window)",
    "source_file": "core\\_experimental\\spectral_analysis.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "detect_cycles",
    "category": "quantitative",
    "formula": "cycles | cycles[:5]",
    "explanation": "Detect significant cycles in price series.\n\nReturns list of detected cycles with:\n- period: Cycle length in samples\n- power: Relative power\n- significance: Statistical significance",
    "python_code": "def detect_cycles(prices: pd.Series,\n                 min_period: int = 5,\n                 max_period: int = 100) -> List[Dict]:\n    \"\"\"\n    Detect significant cycles in price series.\n\n    Returns list of detected cycles with:\n    - period: Cycle length in samples\n    - power: Relative power\n    - significance: Statistical significance\n    \"\"\"\n    returns = prices.pct_change().fillna(0).values\n\n    analyzer = SpectralAnalyzer()\n    result = analyzer.welch_spectrum(returns)\n\n    # Find peaks in power spectrum\n    cycles = []\n\n    # Convert frequency to period\n    periods = 1 / (result.frequencies + 1e-10)\n\n    # Filter to specified range\n    mask = (periods >= min_period) & (periods <= max_period)\n    periods = periods[mask]\n    power = result.power[mask]\n\n    if len(power) == 0:\n        return cycles\n\n    # Find local maxima\n    for i in range(1, len(power) - 1):\n        if power[i] > power[i - 1] and power[i] > power[i + 1]:\n            # Check significance (power > mean + 2*std)\n            threshold = np.mean(power) + 2 * np.std(power)\n            if power[i] > threshold:\n                cycles.append({\n                    'period': periods[i],\n                    'power': power[i] / power.max(),\n                    'significance': (power[i] - np.mean(power)) / np.std(power)\n                })\n\n    # Sort by power\n    cycles.sort(key=lambda x: x['power'], reverse=True)\n\n    return cycles[:5]",
    "source_file": "core\\_experimental\\spectral_analysis.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "morlet",
    "category": "quantitative",
    "formula": "np.exp(-x**2 / 2) * np.cos(5 * x)",
    "explanation": "Morlet wavelet approximation.",
    "python_code": "def morlet(n_points, width):\n            \"\"\"Morlet wavelet approximation.\"\"\"\n            x = np.linspace(-4, 4, n_points)\n            return np.exp(-x**2 / 2) * np.cos(5 * x)",
    "source_file": "core\\_experimental\\spectral_analysis.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "deep_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, input_size: int, hidden_size: int,\n                 output_size: int = None, dropout: float = 0.1,\n                 context_size: int = None):\n        super().__init__()\n        self.input_size = input_size\n        self.output_size = output_size or input_size\n        self.hidden_size = hidden_size\n\n        # Primary path\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n\n        # Context (optional)\n        self.context_fc = nn.Linear(context_size, hidden_size) if context_size else None\n\n        # Gating\n        self.gate = nn.Linear(hidden_size, self.output_size)\n\n        # Skip connection\n        self.skip = nn.Linear(input_size, self.output_size) if input_size != self.output_size else None\n\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(self.output_size)\n        self.elu = nn.ELU()",
    "source_file": "core\\_experimental\\temporal_fusion_transformer.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GatedResidualNetwork"
  },
  {
    "name": "forward",
    "category": "deep_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def forward(self, x: torch.Tensor, context: torch.Tensor = None) -> torch.Tensor:\n        # Primary path\n        hidden = self.fc1(x)\n        if self.context_fc is not None and context is not None:\n            hidden = hidden + self.context_fc(context)\n        hidden = self.elu(hidden)\n        hidden = self.fc2(hidden)\n        hidden = self.dropout(hidden)\n\n        # Gating\n        gate = torch.sigmoid(self.gate(hidden))\n        hidden = gate * hidden\n\n        # Skip connection\n        if self.skip is not None:\n            x = self.skip(x)\n\n        # Residual + norm\n        return self.layer_norm(x + hidden)",
    "source_file": "core\\_experimental\\temporal_fusion_transformer.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "GatedResidualNetwork"
  },
  {
    "name": "forward",
    "category": "deep_learning",
    "formula": "selected, weights",
    "explanation": "Args:\n    x: [B, L, num_vars, input_size] or [B, L, num_vars * input_size]\n\nReturns:\n    (selected_features, selection_weights)",
    "python_code": "def forward(self, x: torch.Tensor, context: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Args:\n            x: [B, L, num_vars, input_size] or [B, L, num_vars * input_size]\n\n        Returns:\n            (selected_features, selection_weights)\n        \"\"\"\n        # Process each variable\n        var_outputs = []\n        for i, grn in enumerate(self.var_grns):\n            var_input = x[:, :, i*self.hidden_size:(i+1)*self.hidden_size] if x.dim() == 3 else x[:, :, i, :]\n            var_outputs.append(grn(var_input))\n\n        var_outputs = torch.stack(var_outputs, dim=-2)  # [B, L, num_vars, hidden]\n\n        # Selection weights\n        flat = var_outputs.reshape(var_outputs.shape[0], var_outputs.shape[1], -1)\n        weights = F.softmax(self.selection_weights(flat, context), dim=-1)\n\n        # Weighted combination\n        selected = (var_outputs * weights.unsqueeze(-1)).sum(dim=-2)\n\n        return selected, weights",
    "source_file": "core\\_experimental\\temporal_fusion_transformer.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VariableSelectionNetwork"
  },
  {
    "name": "forward",
    "category": "deep_learning",
    "formula": "attn_output, avg_weights",
    "explanation": "",
    "python_code": "def forward(self, query: torch.Tensor, key: torch.Tensor,\n                value: torch.Tensor, mask: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor]:\n        B, L, _ = query.shape\n\n        # Project\n        q = self.W_q(query).view(B, L, self.n_heads, self.d_k).transpose(1, 2)\n        k = self.W_k(key).view(B, -1, self.n_heads, self.d_k).transpose(1, 2)\n        v = self.W_v(value).view(B, -1, self.n_heads, self.d_k).transpose(1, 2)\n\n        # Attention\n        scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.d_k)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        attn_output = torch.matmul(attn_weights, v)\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, self.d_model)\n        attn_output = self.W_o(attn_output)\n\n        # Average attention weights across heads for interpretability\n        avg_weights = attn_weights.mean(dim=1)\n\n        return attn_output, avg_weights",
    "source_file": "core\\_experimental\\temporal_fusion_transformer.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "InterpretableMultiHeadAttention"
  },
  {
    "name": "forward",
    "category": "machine_learning",
    "formula": "outputs",
    "explanation": "Forward pass.\n\nArgs:\n    x: [B, L, input_size] input sequence\n\nReturns:\n    Dict[horizon -> [B, num_quantiles]] predictions",
    "python_code": "def forward(self, x: torch.Tensor) -> Dict[int, torch.Tensor]:\n        \"\"\"\n        Forward pass.\n\n        Args:\n            x: [B, L, input_size] input sequence\n\n        Returns:\n            Dict[horizon -> [B, num_quantiles]] predictions\n        \"\"\"\n        # Project input\n        h = self.input_projection(x)\n\n        # LSTM encode\n        lstm_out, _ = self.encoder(h)\n        lstm_out = self.layer_norm(lstm_out)\n\n        # Self-attention\n        attn_out, attn_weights = self.attention(lstm_out, lstm_out, lstm_out)\n\n        # Post-attention GRN\n        h = self.post_attn_grn(lstm_out + attn_out)\n\n        # Use last hidden state for predictions\n        final_hidden = h[:, -1, :]\n\n        # Multi-horizon outputs\n        outputs = {}\n        for horizon in self.config.pred_horizons:\n            outputs[horizon] = self.output_heads[str(horizon)](final_hidden)\n\n        return outputs",
    "source_file": "core\\_experimental\\temporal_fusion_transformer.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TemporalFusionTransformer"
  },
  {
    "name": "predict_direction",
    "category": "machine_learning",
    "formula": "probs",
    "explanation": "Predict direction probability for each horizon.",
    "python_code": "def predict_direction(self, x: torch.Tensor) -> Dict[int, float]:\n        \"\"\"Predict direction probability for each horizon.\"\"\"\n        outputs = self.forward(x)\n\n        probs = {}\n        for horizon, quantile_preds in outputs.items():\n            # Use median (0.5 quantile) for direction\n            median_idx = len(self.config.quantiles) // 2\n            median_pred = quantile_preds[:, median_idx].item()\n\n            # Sigmoid for probability\n            probs[horizon] = 1 / (1 + np.exp(-median_pred * 10))\n\n        return probs",
    "source_file": "core\\_experimental\\temporal_fusion_transformer.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TemporalFusionTransformer"
  },
  {
    "name": "prepare_features",
    "category": "deep_learning",
    "formula": "np.column_stack(list(features.values()))",
    "explanation": "Prepare features for TFT.",
    "python_code": "def prepare_features(self, df: pd.DataFrame) -> np.ndarray:\n        \"\"\"Prepare features for TFT.\"\"\"\n        mid = df['mid'].values if 'mid' in df.columns else (df['bid'] + df['ask']).values / 2\n\n        features = {}\n\n        # Returns at multiple lags\n        for lag in [1, 5, 10, 20, 50]:\n            features[f'ret_{lag}'] = pd.Series(mid).pct_change(lag).fillna(0).values * 10000\n\n        # Volatility\n        for window in [10, 20, 50]:\n            features[f'vol_{window}'] = pd.Series(mid).pct_change().rolling(window).std().fillna(0.001).values * 10000\n\n        # Z-scores\n        for window in [20, 50]:\n            ma = pd.Series(mid).rolling(window).mean()\n            std = pd.Series(mid).rolling(window).std()\n            features[f'zscore_{window}'] = ((mid - ma) / (std + 1e-10)).fillna(0).values\n\n        # RSI\n        delta = pd.Series(mid).diff()\n        gain = delta.where(delta > 0, 0).rolling(14).mean()\n        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n        features['rsi_14'] = (100 - 100 / (1 + gain / (loss + 1e-10))).fillna(50).values\n\n        # Time features\n        if 'timestamp' in df.columns:\n            ts = pd.to_datetime(df['timestamp'])\n            features['hour_sin'] = np.sin(2 * np.pi * ts.dt.hour / 24)\n            features['hour_cos'] = np.cos(2 * np.pi * ts.dt.hour / 24)\n            features['day_sin'] = np.sin(2 * np.pi * ts.dt.dayofweek / 7)\n            features['day_cos'] = np.cos(2 * np.pi * ts.dt.dayofweek / 7)\n        else:\n            features['hour_sin'] = np.zeros(len(df))\n            features['hour_cos'] = np.zeros(len(df))\n            features['day_sin'] = np.zeros(len(df))\n            features['day_cos'] = np.zeros(len(df))\n\n        self.feature_names = list(features.keys())\n        return np.column_stack(list(features.values()))",
    "source_file": "core\\_experimental\\temporal_fusion_transformer.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TFTForex"
  },
  {
    "name": "fit",
    "category": "machine_learning",
    "formula": "X = self.prepare_features(df) | X_tensor = torch.FloatTensor(np.array(X_seq))",
    "explanation": "Train TFT model.",
    "python_code": "def fit(self, df: pd.DataFrame, epochs: int = 50, lr: float = 0.001):\n        \"\"\"Train TFT model.\"\"\"\n        if not HAS_TORCH:\n            logger.warning(\"PyTorch not available\")\n            return\n\n        X = self.prepare_features(df)\n\n        # Target: future returns\n        mid = df['mid'].values if 'mid' in df.columns else (df['bid'] + df['ask']).values / 2\n        returns = pd.Series(mid).pct_change().fillna(0).values * 10000\n\n        # Create sequences\n        X_seq, y_dict = [], {h: [] for h in self.pred_horizons}\n        max_horizon = max(self.pred_horizons)\n\n        for i in range(self.seq_len, len(X) - max_horizon):\n            X_seq.append(X[i-self.seq_len:i])\n            for h in self.pred_horizons:\n                y_dict[h].append(returns[i:i+h].sum())  # Cumulative return\n\n        X_tensor = torch.FloatTensor(np.array(X_seq))\n        y_tensors = {h: torch.FloatTensor(np.array(y_dict[h])).unsqueeze(1) for h in self.pred_horizons}\n\n        # Train\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n\n        self.model.train()\n        for epoch in range(epochs):\n            optimizer.zero_grad()\n            outputs = self.model(X_tensor)\n\n            # Quantile loss\n            loss = 0\n            for h in self.pred_horizons:\n                pred = outputs[h]\n                target = y_tensors[h]\n                for q_idx, q in enumerate(self.model.config.quantiles):\n                    errors = target - pred[:, q_idx:q_idx+1]\n                    loss += torch.max(q * errors, (q - 1) * errors).mean()\n\n            loss.backward()\n            optimizer.step()\n\n            if epoch % 10 == 0:\n                logger.info(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n\n        self.is_fitted = True\n        logger.info(\"TFT model trained\")",
    "source_file": "core\\_experimental\\temporal_fusion_transformer.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TFTForex"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "{h: (0.0, 0.0, 0.0) for h in self.pred_horizons} | results",
    "explanation": "Predict for all horizons.\n\nReturns:\n    Dict[horizon -> (lower, median, upper)] quantile predictions",
    "python_code": "def predict(self, df: pd.DataFrame) -> Dict[int, Tuple[float, float, float]]:\n        \"\"\"\n        Predict for all horizons.\n\n        Returns:\n            Dict[horizon -> (lower, median, upper)] quantile predictions\n        \"\"\"\n        if not HAS_TORCH or not self.is_fitted:\n            return {h: (0.0, 0.0, 0.0) for h in self.pred_horizons}\n\n        X = self.prepare_features(df)[-self.seq_len:]\n\n        self.model.eval()\n        with torch.no_grad():\n            X_tensor = torch.FloatTensor(X).unsqueeze(0)\n            outputs = self.model(X_tensor)\n\n        results = {}\n        for h in self.pred_horizons:\n            preds = outputs[h][0].numpy()\n            results[h] = (preds[0], preds[1], preds[2])  # lower, median, upper\n\n        return results",
    "source_file": "core\\_experimental\\temporal_fusion_transformer.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TFTForex"
  },
  {
    "name": "get_direction_signals",
    "category": "deep_learning",
    "formula": "signals",
    "explanation": "Get direction probability for each horizon.",
    "python_code": "def get_direction_signals(self, df: pd.DataFrame) -> Dict[int, float]:\n        \"\"\"Get direction probability for each horizon.\"\"\"\n        preds = self.predict(df)\n\n        signals = {}\n        for h, (lower, median, upper) in preds.items():\n            # Confidence based on prediction interval\n            interval_width = upper - lower\n            confidence = 1 / (1 + interval_width)\n\n            # Direction from median\n            direction = 1 if median > 0 else -1\n\n            signals[h] = direction * confidence\n\n        return signals",
    "source_file": "core\\_experimental\\temporal_fusion_transformer.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "TFTForex"
  },
  {
    "name": "__init__",
    "category": "quantitative",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, c_in: int, d_model: int):\n        super().__init__()\n        self.tokenConv = nn.Conv1d(\n            in_channels=c_in,\n            out_channels=d_model,\n            kernel_size=3,\n            padding=1,\n            padding_mode='circular'\n        )\n        nn.init.kaiming_normal_(self.tokenConv.weight)",
    "source_file": "core\\_experimental\\timexer_model.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": "TokenEmbedding"
  },
  {
    "name": "forward",
    "category": "quantitative",
    "formula": "x",
    "explanation": "",
    "python_code": "def forward(self, x):\n        # x: [B, L, C]\n        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n        return x",
    "source_file": "core\\_experimental\\timexer_model.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": "TokenEmbedding"
  },
  {
    "name": "forward",
    "category": "quantitative",
    "formula": "x + self.pe[:, :x.size(1)]",
    "explanation": "",
    "python_code": "def forward(self, x):\n        return x + self.pe[:, :x.size(1)]",
    "source_file": "core\\_experimental\\timexer_model.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": "PositionalEncoding"
  },
  {
    "name": "forward",
    "category": "quantitative",
    "formula": "out",
    "explanation": "Inject exogenous information into endogenous representation.\n\nArgs:\n    endo_hidden: [B, L, D] endogenous hidden states\n    exo_input: [B, L, E] exogenous variables\n\nReturns:\n    [B, L, D] enhanced hidden states",
    "python_code": "def forward(self, endo_hidden: torch.Tensor, exo_input: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Inject exogenous information into endogenous representation.\n\n        Args:\n            endo_hidden: [B, L, D] endogenous hidden states\n            exo_input: [B, L, E] exogenous variables\n\n        Returns:\n            [B, L, D] enhanced hidden states\n        \"\"\"\n        exo_embed = self.exo_embed(exo_input)  # [B, L, D]\n\n        # Cross-attention: endo queries exo\n        attn_out, _ = self.cross_attn(endo_hidden, exo_embed, exo_embed)\n\n        # Residual + norm\n        out = self.norm(endo_hidden + self.dropout(attn_out))\n        return out",
    "source_file": "core\\_experimental\\timexer_model.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": "ExogenousEncoder"
  },
  {
    "name": "prepare_endogenous",
    "category": "quantitative",
    "formula": "features",
    "explanation": "Prepare endogenous features.",
    "python_code": "def prepare_endogenous(self, df: pd.DataFrame) -> np.ndarray:\n        \"\"\"Prepare endogenous features.\"\"\"\n        mid = df['mid'].values if 'mid' in df.columns else (df['bid'] + df['ask']).values / 2\n\n        features = np.column_stack([\n            pd.Series(mid).pct_change().fillna(0).values * 10000,  # Returns (bps)\n            (df['ask'] - df['bid']).values / mid * 10000,  # Spread (bps)\n            df['volume'].values / df['volume'].rolling(20).mean().values if 'volume' in df.columns else np.ones(len(df)),\n            pd.Series(mid).pct_change().rolling(20).std().fillna(0.001).values * 10000,  # Volatility\n            pd.Series(mid).pct_change(5).fillna(0).values * 10000,  # Momentum 5\n            pd.Series(mid).pct_change(20).fillna(0).values * 10000,  # Momentum 20\n            ((mid - pd.Series(mid).rolling(20).mean()) / (pd.Series(mid).rolling(20).std() + 1e-10)).fillna(0).values  # Z-score\n        ])\n\n        return features",
    "source_file": "core\\_experimental\\timexer_model.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": "TimeXerForex"
  },
  {
    "name": "prepare_exogenous",
    "category": "quantitative",
    "formula": "np.column_stack([hour_sin, hour_cos, events, dxy_vals, vix_vals])",
    "explanation": "Prepare exogenous features.",
    "python_code": "def prepare_exogenous(self, df: pd.DataFrame,\n                         economic_events: Optional[pd.Series] = None,\n                         dxy: Optional[pd.Series] = None,\n                         vix: Optional[pd.Series] = None) -> np.ndarray:\n        \"\"\"Prepare exogenous features.\"\"\"\n        n = len(df)\n\n        # Time features\n        if 'timestamp' in df.columns:\n            ts = pd.to_datetime(df['timestamp'])\n            hour_sin = np.sin(2 * np.pi * ts.dt.hour / 24)\n            hour_cos = np.cos(2 * np.pi * ts.dt.hour / 24)\n        else:\n            hour_sin = np.zeros(n)\n            hour_cos = np.zeros(n)\n\n        # Economic events (binary)\n        events = economic_events.values if economic_events is not None else np.zeros(n)\n\n        # External indices\n        dxy_vals = dxy.pct_change().fillna(0).values * 10000 if dxy is not None else np.zeros(n)\n        vix_vals = vix.pct_change().fillna(0).values * 10000 if vix is not None else np.zeros(n)\n\n        return np.column_stack([hour_sin, hour_cos, events, dxy_vals, vix_vals])",
    "source_file": "core\\_experimental\\timexer_model.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": "TimeXerForex"
  },
  {
    "name": "fit",
    "category": "machine_learning",
    "formula": "# Prepare data",
    "explanation": "Train TimeXer model.\n\nArgs:\n    df: DataFrame with OHLCV data\n    exo_data: Dict with 'economic_events', 'dxy', 'vix'\n    epochs: Training epochs\n    lr: Learning rate",
    "python_code": "def fit(self, df: pd.DataFrame,\n            exo_data: Optional[Dict] = None,\n            epochs: int = 50, lr: float = 0.001, batch_size: int = 32):\n        \"\"\"\n        Train TimeXer model.\n\n        Args:\n            df: DataFrame with OHLCV data\n            exo_data: Dict with 'economic_events', 'dxy', 'vix'\n            epochs: Training epochs\n            lr: Learning rate\n        \"\"\"\n        if not HAS_TORCH:\n            logger.warning(\"PyTorch not available\")\n            return\n\n        # Prepare data\n        endo = self.prepare_endogenous(df)\n\n        if exo_data:\n            exo = self.prepare_exogenous(\n                df,\n                exo_data.get('economic_events'),\n                exo_data.get('dxy'),\n                exo_data.get('vix')\n            )\n        else:\n            exo = self.prepare_exogenous(df)\n\n        # Create sequences\n        X_endo, X_exo, y = [], [], []\n        for i in range(self.seq_len, len(endo) - self.pred_len):\n            X_endo.append(endo[i-self.seq_len:i])\n            X_exo.append(exo[i-self.seq_len:i])\n            y.append(endo[i:i+self.pred_len])\n\n        X_endo = torch.FloatTensor(np.array(X_endo))\n        X_exo = torch.FloatTensor(np.array(X_exo))\n        y = torch.FloatTensor(np.array(y))\n\n        # Train\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n        criterion = nn.MSELoss()\n\n        n_batches = len(X_endo) // batch_size\n\n        self.model.train()\n        for epoch in range(epochs):\n            total_loss = 0\n            for b in range(n_batches):\n                start = b * batch_size\n                end = start + batch_size\n\n                optimizer.zero_grad()\n                pred = self.model(X_endo[start:end], X_exo[start:end])\n                loss = criterion(pred, y[start:end])\n                loss.backward()\n                optimizer.step()\n                total_loss += loss.item()\n\n            if epoch % 10 == 0:\n                logger.info(f\"Epoch {epoch}, Loss: {total_loss/n_batches:.6f}\")\n\n        self.is_fitted = True\n        logger.info(\"TimeXer model trained\")",
    "source_file": "core\\_experimental\\timexer_model.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": "TimeXerForex"
  },
  {
    "name": "predict",
    "category": "machine_learning",
    "formula": "np.zeros(self.pred_len) | pred[0, :, 0].numpy()",
    "explanation": "Predict future returns.",
    "python_code": "def predict(self, df: pd.DataFrame,\n                exo_data: Optional[Dict] = None) -> np.ndarray:\n        \"\"\"Predict future returns.\"\"\"\n        if not HAS_TORCH or not self.is_fitted:\n            return np.zeros(self.pred_len)\n\n        endo = self.prepare_endogenous(df)[-self.seq_len:]\n\n        if exo_data:\n            exo = self.prepare_exogenous(\n                df.iloc[-self.seq_len:],\n                exo_data.get('economic_events'),\n                exo_data.get('dxy'),\n                exo_data.get('vix')\n            )\n        else:\n            exo = self.prepare_exogenous(df.iloc[-self.seq_len:])\n\n        self.model.eval()\n        with torch.no_grad():\n            X_endo = torch.FloatTensor(endo).unsqueeze(0)\n            X_exo = torch.FloatTensor(exo).unsqueeze(0)\n            pred = self.model(X_endo, X_exo)\n\n        return pred[0, :, 0].numpy()",
    "source_file": "core\\_experimental\\timexer_model.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": "TimeXerForex"
  },
  {
    "name": "get_direction_probability",
    "category": "machine_learning",
    "formula": "over prediction horizon.\"\"\" | = np.sum(preds) | / 10))  # Scaled",
    "explanation": "Get probability of positive return over prediction horizon.",
    "python_code": "def get_direction_probability(self, df: pd.DataFrame,\n                                  exo_data: Optional[Dict] = None) -> float:\n        \"\"\"Get probability of positive return over prediction horizon.\"\"\"\n        preds = self.predict(df, exo_data)\n        cum_return = np.sum(preds)\n\n        # Sigmoid to get probability\n        prob = 1 / (1 + np.exp(-cum_return / 10))  # Scaled\n        return prob",
    "source_file": "core\\_experimental\\timexer_model.py",
    "academic_reference": "Vaswani (2017) 'Attention Is All You Need' NeurIPS",
    "class_name": "TimeXerForex"
  },
  {
    "name": "__init__",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, config: FeatureExtractionConfig = None):\n        self.config = config or FeatureExtractionConfig()\n        self.selected_features: List[str] = []\n        self.feature_params = None",
    "source_file": "core\\_experimental\\tsfresh_auto_features.py",
    "academic_reference": null,
    "class_name": "TSFreshExtractor"
  },
  {
    "name": "_get_fc_parameters",
    "category": "reinforcement_learning",
    "formula": "MinimalFCParameters() | EfficientFCParameters() | ComprehensiveFCParameters()",
    "explanation": "Get feature calculation parameters based on mode.",
    "python_code": "def _get_fc_parameters(self):\n        \"\"\"Get feature calculation parameters based on mode.\"\"\"\n        if not HAS_TSFRESH:\n            return None\n\n        mode = self.config.extraction_mode\n\n        if mode == 'minimal':\n            return MinimalFCParameters()\n        elif mode == 'efficient':\n            return EfficientFCParameters()\n        elif mode == 'comprehensive':\n            return ComprehensiveFCParameters()\n        else:\n            return EfficientFCParameters()",
    "source_file": "core\\_experimental\\tsfresh_auto_features.py",
    "academic_reference": null,
    "class_name": "TSFreshExtractor"
  },
  {
    "name": "prepare_data",
    "category": "reinforcement_learning",
    "formula": "long_df",
    "explanation": "Prepare data in tsfresh long format.\n\nArgs:\n    df: Input DataFrame\n    id_col: Column identifying different time series (optional)\n    time_col: Time column name\n    value_cols: Columns to extract features from\n\nReturns:\n    DataFrame in tsfresh format",
    "python_code": "def prepare_data(\n        self,\n        df: pd.DataFrame,\n        id_col: str = None,\n        time_col: str = 'timestamp',\n        value_cols: Optional[List[str]] = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Prepare data in tsfresh long format.\n\n        Args:\n            df: Input DataFrame\n            id_col: Column identifying different time series (optional)\n            time_col: Time column name\n            value_cols: Columns to extract features from\n\n        Returns:\n            DataFrame in tsfresh format\n        \"\"\"\n        if value_cols is None:\n            value_cols = [c for c in df.columns if c not in [time_col, id_col, 'id']]\n\n        # Create id column if not present\n        if id_col is None:\n            df = df.copy()\n            df['id'] = 0\n            id_col = 'id'\n\n        # Melt to long format\n        long_df = df.melt(\n            id_vars=[id_col, time_col] if time_col in df.columns else [id_col],\n            value_vars=value_cols,\n            var_name='kind',\n            value_name='value'\n        )\n\n        return long_df",
    "source_file": "core\\_experimental\\tsfresh_auto_features.py",
    "academic_reference": null,
    "class_name": "TSFreshExtractor"
  },
  {
    "name": "extract",
    "category": "statistical",
    "formula": "pd.DataFrame() | selected | features",
    "explanation": "Extract features from time series data.\n\nArgs:\n    df: Input DataFrame with time series columns\n    target: Target variable for feature selection (optional)\n    value_cols: Columns to extract features from\n    use_rolling: Whether to use rolling windows\n\nReturns:\n    DataFrame with extracted features",
    "python_code": "def extract(\n        self,\n        df: pd.DataFrame,\n        target: Optional[pd.Series] = None,\n        value_cols: Optional[List[str]] = None,\n        use_rolling: bool = True\n    ) -> pd.DataFrame:\n        \"\"\"\n        Extract features from time series data.\n\n        Args:\n            df: Input DataFrame with time series columns\n            target: Target variable for feature selection (optional)\n            value_cols: Columns to extract features from\n            use_rolling: Whether to use rolling windows\n\n        Returns:\n            DataFrame with extracted features\n        \"\"\"\n        if not HAS_TSFRESH:\n            logger.warning(\"tsfresh not available, using fallback\")\n            return self._extract_fallback(df, value_cols)\n\n        if value_cols is None:\n            value_cols = [c for c in df.columns if c in\n                         ['open', 'high', 'low', 'close', 'volume', 'returns', 'spread']]\n\n        if not value_cols:\n            logger.warning(\"No value columns found\")\n            return pd.DataFrame()\n\n        logger.info(f\"Extracting features from {len(value_cols)} columns using {self.config.extraction_mode} mode\")\n\n        # Prepare data\n        df_prepared = df[value_cols].copy()\n        df_prepared = df_prepared.reset_index(drop=True)\n\n        # Add id and time columns\n        df_prepared['id'] = 0\n        df_prepared['time'] = range(len(df_prepared))\n\n        if use_rolling:\n            # Roll time series for sliding window features\n            logger.info(f\"Rolling time series with window [{self.config.min_timeshift}, {self.config.max_timeshift}]\")\n\n            try:\n                df_rolled = roll_time_series(\n                    df_prepared,\n                    column_id='id',\n                    column_sort='time',\n                    max_timeshift=self.config.max_timeshift,\n                    min_timeshift=self.config.min_timeshift\n                )\n            except Exception as e:\n                logger.warning(f\"Rolling failed: {e}, using non-rolled\")\n                df_rolled = df_prepared\n\n        else:\n            df_rolled = df_prepared\n\n        # Get feature calculation parameters\n        fc_params = self._get_fc_parameters()\n\n        # Extract features\n        logger.info(\"Extracting features...\")\n\n        try:\n            features = extract_features(\n                df_rolled,\n                column_id='id',\n                column_sort='time',\n                default_fc_parameters=fc_params,\n                n_jobs=self.config.n_jobs,\n                disable_progressbar=False,\n                chunksize=self.config.chunksize\n            )\n\n            # Impute missing values\n            impute(features)\n\n            logger.info(f\"Extracted {features.shape[1]} features\")\n\n        except Exception as e:\n            logger.error(f\"Feature extraction failed: {e}\")\n            return self._extract_fallback(df, value_cols)\n\n        # Feature selection if target provided\n        if target is not None:\n           ",
    "source_file": "core\\_experimental\\tsfresh_auto_features.py",
    "academic_reference": null,
    "class_name": "TSFreshExtractor"
  },
  {
    "name": "_extract_fallback",
    "category": "reinforcement_learning",
    "formula": "features",
    "explanation": "Fallback feature extraction without tsfresh.",
    "python_code": "def _extract_fallback(\n        self,\n        df: pd.DataFrame,\n        value_cols: Optional[List[str]] = None\n    ) -> pd.DataFrame:\n        \"\"\"Fallback feature extraction without tsfresh.\"\"\"\n        logger.info(\"Using fallback feature extraction\")\n\n        if value_cols is None:\n            value_cols = [c for c in df.columns if c in\n                         ['open', 'high', 'low', 'close', 'volume', 'returns', 'spread']]\n\n        features = pd.DataFrame(index=df.index)\n\n        for col in value_cols:\n            if col not in df.columns:\n                continue\n\n            x = df[col].values\n\n            # Basic statistics\n            for window in [5, 10, 20, 50]:\n                if len(x) > window:\n                    features[f'{col}__mean_{window}'] = pd.Series(x).rolling(window).mean().values\n                    features[f'{col}__std_{window}'] = pd.Series(x).rolling(window).std().values\n                    features[f'{col}__min_{window}'] = pd.Series(x).rolling(window).min().values\n                    features[f'{col}__max_{window}'] = pd.Series(x).rolling(window).max().values\n                    features[f'{col}__median_{window}'] = pd.Series(x).rolling(window).median().values\n\n                    # Quantiles\n                    features[f'{col}__q25_{window}'] = pd.Series(x).rolling(window).quantile(0.25).values\n                    features[f'{col}__q75_{window}'] = pd.Series(x).rolling(window).quantile(0.75).values\n\n            # Differences\n            for lag in [1, 5, 10]:\n                if len(x) > lag:\n                    features[f'{col}__diff_{lag}'] = pd.Series(x).diff(lag).values\n                    features[f'{col}__pct_change_{lag}'] = pd.Series(x).pct_change(lag).values\n\n            # Autocorrelation\n            for lag in [1, 5, 10]:\n                if len(x) > lag + 20:\n                    autocorr = pd.Series(x).autocorr(lag)\n                    features[f'{col}__autocorr_{lag}'] = autocorr\n\n            # Skewness and Kurtosis\n            for window in [20, 50]:\n                if len(x) > window:\n                    features[f'{col}__skew_{window}'] = pd.Series(x).rolling(window).skew().values\n                    features[f'{col}__kurt_{window}'] = pd.Series(x).rolling(window).kurt().values\n\n        logger.info(f\"Fallback extracted {features.shape[1]} features\")\n\n        return features",
    "source_file": "core\\_experimental\\tsfresh_auto_features.py",
    "academic_reference": null,
    "class_name": "TSFreshExtractor"
  },
  {
    "name": "transform",
    "category": "reinforcement_learning",
    "formula": "features[available] | features",
    "explanation": "Transform new data using selected features.\n\nArgs:\n    df: New data to transform\n    value_cols: Value columns\n\nReturns:\n    DataFrame with selected features",
    "python_code": "def transform(\n        self,\n        df: pd.DataFrame,\n        value_cols: Optional[List[str]] = None\n    ) -> pd.DataFrame:\n        \"\"\"\n        Transform new data using selected features.\n\n        Args:\n            df: New data to transform\n            value_cols: Value columns\n\n        Returns:\n            DataFrame with selected features\n        \"\"\"\n        features = self.extract(df, value_cols=value_cols, use_rolling=False)\n\n        if self.selected_features:\n            available = [f for f in self.selected_features if f in features.columns]\n            return features[available]\n\n        return features",
    "source_file": "core\\_experimental\\tsfresh_auto_features.py",
    "academic_reference": null,
    "class_name": "TSFreshExtractor"
  },
  {
    "name": "extract",
    "category": "reinforcement_learning",
    "formula": "features",
    "explanation": "Extract quick features from OHLCV data.",
    "python_code": "def extract(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Extract quick features from OHLCV data.\"\"\"\n        features = pd.DataFrame(index=df.index)\n\n        # Price columns\n        price_cols = [c for c in df.columns if c in ['open', 'high', 'low', 'close', 'mid', 'price']]\n\n        for col in price_cols:\n            x = df[col].values\n\n            for w in self.windows:\n                # Rolling statistics\n                s = pd.Series(x)\n                features[f'{col}_mean_{w}'] = s.rolling(w).mean().values\n                features[f'{col}_std_{w}'] = s.rolling(w).std().values\n                features[f'{col}_zscore_{w}'] = ((s - s.rolling(w).mean()) / s.rolling(w).std()).values\n\n                # Range position\n                roll_max = s.rolling(w).max()\n                roll_min = s.rolling(w).min()\n                range_ = roll_max - roll_min\n                features[f'{col}_range_pos_{w}'] = ((s - roll_min) / range_.replace(0, np.nan)).values\n\n                # Momentum\n                features[f'{col}_mom_{w}'] = (s / s.shift(w) - 1).values * 10000\n\n        # Volume features\n        if 'volume' in df.columns:\n            v = pd.Series(df['volume'].values)\n            for w in self.windows:\n                features[f'volume_mean_{w}'] = v.rolling(w).mean().values\n                features[f'volume_ratio_{w}'] = (v / v.rolling(w).mean()).values\n\n        # Returns\n        if 'close' in df.columns:\n            ret = pd.Series(df['close'].values).pct_change()\n            for w in self.windows:\n                features[f'ret_mean_{w}'] = ret.rolling(w).mean().values * 10000\n                features[f'ret_std_{w}'] = ret.rolling(w).std().values * 10000\n                features[f'ret_skew_{w}'] = ret.rolling(w).skew().values\n                features[f'ret_kurt_{w}'] = ret.rolling(w).kurt().values\n\n        # Cross-column features\n        if 'high' in df.columns and 'low' in df.columns:\n            h = df['high'].values\n            l = df['low'].values\n            features['hl_range'] = (h - l)\n            features['hl_range_pct'] = (h - l) / ((h + l) / 2) * 10000\n\n        if 'open' in df.columns and 'close' in df.columns:\n            o = df['open'].values\n            c = df['close'].values\n            features['oc_change'] = (c - o)\n            features['oc_direction'] = np.sign(c - o)\n\n        return features",
    "source_file": "core\\_experimental\\tsfresh_auto_features.py",
    "academic_reference": null,
    "class_name": "QuickFeatureExtractor"
  },
  {
    "name": "compute_all_features",
    "category": "reinforcement_learning",
    "formula": "",
    "explanation": "Compute all TSFresh features.\nInterface compatible with HFT Feature Engine.",
    "python_code": "def compute_all_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Compute all TSFresh features.\n        Interface compatible with HFT Feature Engine.\n        \"\"\"\n        return self.extractor.extract(df)",
    "source_file": "core\\_experimental\\tsfresh_auto_features.py",
    "academic_reference": null,
    "class_name": "TSFreshFactorEngine"
  },
  {
    "name": "__init__",
    "category": "microstructure",
    "formula": "",
    "explanation": "Initialize the ultra-selective filter.\n\nArgs:\n    confidence_percentile: Only trade top N% confidence signals (default 90)\n    require_unanimous: Require all models to agree (default True)\n    favorable_regimes: List of allowed regimes (default [0, 1] = low/normal vol)\n    require_ofi_confirm: Require OFI to confirm direction (default True)\n    min_ofi_threshold: Minimum absolute OFI to confirm (default 0)\n    base_accuracy: Base model accuracy for theoretical calculation",
    "python_code": "def __init__(self,\n                 confidence_percentile: float = 90.0,\n                 require_unanimous: bool = True,\n                 favorable_regimes: List[int] = None,\n                 require_ofi_confirm: bool = True,\n                 min_ofi_threshold: float = 0.0,\n                 base_accuracy: float = 0.59):\n        \"\"\"\n        Initialize the ultra-selective filter.\n\n        Args:\n            confidence_percentile: Only trade top N% confidence signals (default 90)\n            require_unanimous: Require all models to agree (default True)\n            favorable_regimes: List of allowed regimes (default [0, 1] = low/normal vol)\n            require_ofi_confirm: Require OFI to confirm direction (default True)\n            min_ofi_threshold: Minimum absolute OFI to confirm (default 0)\n            base_accuracy: Base model accuracy for theoretical calculation\n        \"\"\"\n        self.conf_percentile = confidence_percentile\n        self.unanimous = require_unanimous\n        self.favorable_regimes = favorable_regimes or [0, 1]\n        self.ofi_confirm = require_ofi_confirm\n        self.min_ofi = min_ofi_threshold\n        self.base_accuracy = base_accuracy\n\n        # Confidence history for percentile calculation\n        self._confidence_history = deque(maxlen=1000)\n        self._conf_threshold = 0.55  # Initial threshold\n\n        # Statistics\n        self._total_signals = 0\n        self._passed_signals = 0\n        self._filter_stats = {\n            'confidence': {'passed': 0, 'failed': 0},\n            'unanimous': {'passed': 0, 'failed': 0},\n            'regime': {'passed': 0, 'failed': 0},\n            'ofi': {'passed': 0, 'failed': 0},\n        }",
    "source_file": "core\\_experimental\\ultra_selective_filter.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "UltraSelectiveFilter"
  },
  {
    "name": "update_confidence_history",
    "category": "filtering",
    "formula": "",
    "explanation": "Update confidence history for dynamic threshold calculation.\n\nArgs:\n    confidence: New confidence value to add",
    "python_code": "def update_confidence_history(self, confidence: float):\n        \"\"\"\n        Update confidence history for dynamic threshold calculation.\n\n        Args:\n            confidence: New confidence value to add\n        \"\"\"\n        self._confidence_history.append(confidence)\n\n        # Recalculate threshold when we have enough data\n        if len(self._confidence_history) >= 100:\n            self._conf_threshold = np.percentile(\n                list(self._confidence_history),\n                self.conf_percentile\n            )",
    "source_file": "core\\_experimental\\ultra_selective_filter.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "UltraSelectiveFilter"
  },
  {
    "name": "theoretical_accuracy",
    "category": "filtering",
    "formula": "P(correct | all_agree) = p^n / (p^n + (1-p)^n) | (p ** n_models) / (p ** n_models + q ** n_models)",
    "explanation": "Calculate theoretical accuracy when all models agree.\n\nFormula: P(correct | all_agree) = p^n / (p^n + (1-p)^n)\n\nArgs:\n    n_models: Number of models that must agree\n\nReturns:\n    Theoretical accuracy (0-1)",
    "python_code": "def theoretical_accuracy(self, n_models: int = 3) -> float:\n        \"\"\"\n        Calculate theoretical accuracy when all models agree.\n\n        Formula: P(correct | all_agree) = p^n / (p^n + (1-p)^n)\n\n        Args:\n            n_models: Number of models that must agree\n\n        Returns:\n            Theoretical accuracy (0-1)\n        \"\"\"\n        p = self.base_accuracy\n        q = 1 - p\n        return (p ** n_models) / (p ** n_models + q ** n_models)",
    "source_file": "core\\_experimental\\ultra_selective_filter.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "UltraSelectiveFilter"
  },
  {
    "name": "evaluate",
    "category": "microstructure",
    "formula": "positive=bullish, negative=bearish) | FilterResult( | FilterResult(",
    "explanation": "Apply all 4 filter layers.\n\nArgs:\n    model_predictions: Dict with model predictions\n        {\n            'xgboost': {'direction': 1, 'probability': 0.62},\n            'lightgbm': {'direction': 1, 'probability': 0.61},\n            'catboost': {'direction': 1, 'probability': 0.60}\n        }\n    ensemble_confidence: Combined confidence score (0-1)\n    regime: HMM regime state (0=low vol, 1=normal, 2=high vol)\n    ofi: Order Flow Imbalance (positive=bullish, negative=bearish)\n\nReturns:\n    FilterResult with decision and details",
    "python_code": "def evaluate(self,\n                model_predictions: Dict[str, Dict],\n                ensemble_confidence: float,\n                regime: int,\n                ofi: float) -> FilterResult:\n        \"\"\"\n        Apply all 4 filter layers.\n\n        Args:\n            model_predictions: Dict with model predictions\n                {\n                    'xgboost': {'direction': 1, 'probability': 0.62},\n                    'lightgbm': {'direction': 1, 'probability': 0.61},\n                    'catboost': {'direction': 1, 'probability': 0.60}\n                }\n            ensemble_confidence: Combined confidence score (0-1)\n            regime: HMM regime state (0=low vol, 1=normal, 2=high vol)\n            ofi: Order Flow Imbalance (positive=bullish, negative=bearish)\n\n        Returns:\n            FilterResult with decision and details\n        \"\"\"\n        self._total_signals += 1\n        filters_passed = []\n        filters_failed = []\n\n        # Update confidence history\n        self.update_confidence_history(ensemble_confidence)\n\n        # Get consensus direction\n        directions = [p.get('direction', 0) for p in model_predictions.values()]\n        consensus_direction = directions[0] if directions else 0\n\n        # ===============================\n        # Layer 1: Confidence Threshold\n        # ===============================\n        if ensemble_confidence >= self._conf_threshold:\n            filters_passed.append('confidence')\n            self._filter_stats['confidence']['passed'] += 1\n        else:\n            filters_failed.append('confidence')\n            self._filter_stats['confidence']['failed'] += 1\n            return FilterResult(\n                should_trade=False,\n                direction=0,\n                confidence=ensemble_confidence,\n                theoretical_accuracy=0,\n                filters_passed=filters_passed,\n                filters_failed=filters_failed,\n                reason=f\"Confidence {ensemble_confidence:.3f} < threshold {self._conf_threshold:.3f}\"\n            )\n\n        # ===============================\n        # Layer 2: Unanimous Agreement\n        # ===============================\n        if self.unanimous and len(model_predictions) > 1:\n            if len(set(directions)) == 1 and directions[0] != 0:\n                filters_passed.append('unanimous')\n                self._filter_stats['unanimous']['passed'] += 1\n            else:\n                filters_failed.append('unanimous')\n                self._filter_stats['unanimous']['failed'] += 1\n                return FilterResult(\n                    should_trade=False,\n                    direction=0,\n                    confidence=ensemble_confidence,\n                    theoretical_accuracy=0,\n                    filters_passed=filters_passed,\n                    filters_failed=filters_failed,\n                    reason=f\"Models disagree: {directions}\"\n                )\n        else:\n            filters_passed.append('unanimous')\n            self._filter_stats['unani",
    "source_file": "core\\_experimental\\ultra_selective_filter.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "UltraSelectiveFilter"
  },
  {
    "name": "get_stats",
    "category": "filtering",
    "formula": "{",
    "explanation": "Get filter statistics.",
    "python_code": "def get_stats(self) -> Dict:\n        \"\"\"Get filter statistics.\"\"\"\n        pass_rate = self._passed_signals / max(1, self._total_signals)\n\n        return {\n            'total_signals': self._total_signals,\n            'passed_signals': self._passed_signals,\n            'pass_rate': pass_rate,\n            'confidence_threshold': self._conf_threshold,\n            'theoretical_accuracy': self.theoretical_accuracy(),\n            'filter_breakdown': self._filter_stats,\n        }",
    "source_file": "core\\_experimental\\ultra_selective_filter.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "UltraSelectiveFilter"
  },
  {
    "name": "reset_stats",
    "category": "filtering",
    "formula": "",
    "explanation": "Reset statistics.",
    "python_code": "def reset_stats(self):\n        \"\"\"Reset statistics.\"\"\"\n        self._total_signals = 0\n        self._passed_signals = 0\n        for key in self._filter_stats:\n            self._filter_stats[key] = {'passed': 0, 'failed': 0}",
    "source_file": "core\\_experimental\\ultra_selective_filter.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "UltraSelectiveFilter"
  },
  {
    "name": "record_outcome",
    "category": "filtering",
    "formula": "",
    "explanation": "Record trade outcome for adaptation.",
    "python_code": "def record_outcome(self, was_profitable: bool):\n        \"\"\"Record trade outcome for adaptation.\"\"\"\n        self._performance_history.append(1 if was_profitable else 0)\n\n        # Adapt if enough history\n        if len(self._performance_history) >= 50:\n            win_rate = np.mean(self._performance_history)\n\n            # If win rate drops, tighten filters\n            if win_rate < 0.60:\n                self.conf_percentile = min(95, self.conf_percentile + 2)\n                logger.info(f\"Tightening filter: confidence percentile  {self.conf_percentile}\")\n\n            # If win rate is very high, can loosen slightly\n            elif win_rate > 0.75:\n                self.conf_percentile = max(80, self.conf_percentile - 1)\n                logger.info(f\"Loosening filter: confidence percentile  {self.conf_percentile}\")",
    "source_file": "core\\_experimental\\ultra_selective_filter.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "AdaptiveFilter"
  },
  {
    "name": "__init__",
    "category": "microstructure",
    "formula": "",
    "explanation": "",
    "python_code": "def __init__(self, bucket_size: int = 50000, n_buckets: int = 50):\n        self.bucket_size = bucket_size\n        self.n_buckets = n_buckets\n\n        # State\n        self.current_bucket_volume = 0.0\n        self.current_bucket_buy = 0.0\n        self.current_bucket_sell = 0.0\n\n        # History of completed buckets\n        self.bucket_buys = []\n        self.bucket_sells = []\n\n        # Thresholds for regime classification\n        self.low_threshold = 0.3\n        self.high_threshold = 0.7",
    "source_file": "core\\_experimental\\vpin_toxicity.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VPINCalculator"
  },
  {
    "name": "classify_trade",
    "category": "microstructure",
    "formula": "volume, 0.0  # Buy | 0.0, volume  # Sell | volume / 2, volume / 2",
    "explanation": "Classify trade as buy or sell using tick rule.\n\nReturns:\n    (buy_volume, sell_volume)",
    "python_code": "def classify_trade(self, price: float, prev_price: float,\n                      volume: float) -> Tuple[float, float]:\n        \"\"\"\n        Classify trade as buy or sell using tick rule.\n\n        Returns:\n            (buy_volume, sell_volume)\n        \"\"\"\n        if price > prev_price:\n            return volume, 0.0  # Buy\n        elif price < prev_price:\n            return 0.0, volume  # Sell\n        else:\n            # Unchanged - split 50/50\n            return volume / 2, volume / 2",
    "source_file": "core\\_experimental\\vpin_toxicity.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VPINCalculator"
  },
  {
    "name": "bulk_classify",
    "category": "microstructure",
    "formula": "V_buy = V * CDF(Z) where Z = (P - VWAP) / sigma | 0.0, 0.0 | buy_volume, sell_volume",
    "explanation": "Bulk Volume Classification (BVC) - more robust than tick rule.\n\nUses: V_buy = V * CDF(Z) where Z = (P - VWAP) / sigma",
    "python_code": "def bulk_classify(self, prices: np.ndarray, volumes: np.ndarray) -> Tuple[float, float]:\n        \"\"\"\n        Bulk Volume Classification (BVC) - more robust than tick rule.\n\n        Uses: V_buy = V * CDF(Z) where Z = (P - VWAP) / sigma\n        \"\"\"\n        if len(prices) < 2:\n            return 0.0, 0.0\n\n        # VWAP\n        vwap = np.average(prices, weights=volumes)\n\n        # Standard deviation\n        sigma = np.std(prices) + 1e-10\n\n        # Z-scores\n        z = (prices - vwap) / sigma\n\n        # CDF approximation (normal)\n        from scipy.stats import norm\n        buy_fractions = norm.cdf(z)\n\n        buy_volume = np.sum(volumes * buy_fractions)\n        sell_volume = np.sum(volumes * (1 - buy_fractions))\n\n        return buy_volume, sell_volume",
    "source_file": "core\\_experimental\\vpin_toxicity.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VPINCalculator"
  },
  {
    "name": "update",
    "category": "microstructure",
    "formula": "",
    "explanation": "Update VPIN with new trade.\n\nReturns VPINResult when a new bucket is completed.",
    "python_code": "def update(self, price: float, prev_price: float, volume: float) -> Optional[VPINResult]:\n        \"\"\"\n        Update VPIN with new trade.\n\n        Returns VPINResult when a new bucket is completed.\n        \"\"\"\n        # Classify trade\n        buy_vol, sell_vol = self.classify_trade(price, prev_price, volume)\n\n        # Add to current bucket\n        self.current_bucket_volume += volume\n        self.current_bucket_buy += buy_vol\n        self.current_bucket_sell += sell_vol\n\n        # Check if bucket is complete\n        if self.current_bucket_volume >= self.bucket_size:\n            # Store bucket\n            self.bucket_buys.append(self.current_bucket_buy)\n            self.bucket_sells.append(self.current_bucket_sell)\n\n            # Keep only last n_buckets\n            if len(self.bucket_buys) > self.n_buckets:\n                self.bucket_buys.pop(0)\n                self.bucket_sells.pop(0)\n\n            # Reset bucket\n            self.current_bucket_volume = 0.0\n            self.current_bucket_buy = 0.0\n            self.current_bucket_sell = 0.0\n\n            # Calculate VPIN\n            return self.calculate_vpin()\n\n        return None",
    "source_file": "core\\_experimental\\vpin_toxicity.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VPINCalculator"
  },
  {
    "name": "calculate_vpin",
    "category": "microstructure",
    "formula": "VPINResult(0.5, 1, 0, 0, 0) | VPINResult(0.5, 1, 0, 0, 0) | VPINResult(",
    "explanation": "Calculate current VPIN from bucket history.",
    "python_code": "def calculate_vpin(self) -> VPINResult:\n        \"\"\"Calculate current VPIN from bucket history.\"\"\"\n        if not self.bucket_buys:\n            return VPINResult(0.5, 1, 0, 0, 0)\n\n        total_buy = sum(self.bucket_buys)\n        total_sell = sum(self.bucket_sells)\n        total_volume = total_buy + total_sell\n\n        if total_volume < 1e-10:\n            return VPINResult(0.5, 1, 0, 0, 0)\n\n        # VPIN = average absolute imbalance\n        vpin = 0.0\n        for buy, sell in zip(self.bucket_buys, self.bucket_sells):\n            bucket_total = buy + sell\n            if bucket_total > 0:\n                vpin += abs(buy - sell) / bucket_total\n        vpin /= len(self.bucket_buys)\n\n        # Imbalance ratio\n        imbalance = (total_buy - total_sell) / total_volume\n\n        # Regime\n        if vpin < self.low_threshold:\n            regime = 0  # Low toxicity\n        elif vpin > self.high_threshold:\n            regime = 2  # High toxicity\n        else:\n            regime = 1  # Normal\n\n        return VPINResult(\n            vpin=vpin,\n            toxicity_regime=regime,\n            buy_volume=total_buy,\n            sell_volume=total_sell,\n            imbalance=imbalance\n        )",
    "source_file": "core\\_experimental\\vpin_toxicity.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VPINCalculator"
  },
  {
    "name": "get_current_vpin",
    "category": "microstructure",
    "formula": "result.vpin",
    "explanation": "Get current VPIN estimate.",
    "python_code": "def get_current_vpin(self) -> float:\n        \"\"\"Get current VPIN estimate.\"\"\"\n        result = self.calculate_vpin()\n        return result.vpin",
    "source_file": "core\\_experimental\\vpin_toxicity.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VPINCalculator"
  },
  {
    "name": "compute_features",
    "category": "microstructure",
    "formula": "result",
    "explanation": "Compute VPIN features from tick data.\n\nArgs:\n    df: DataFrame with columns [timestamp, bid, ask, volume] or [price, volume]\n\nReturns:\n    DataFrame with VPIN features",
    "python_code": "def compute_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Compute VPIN features from tick data.\n\n        Args:\n            df: DataFrame with columns [timestamp, bid, ask, volume] or [price, volume]\n\n        Returns:\n            DataFrame with VPIN features\n        \"\"\"\n        if 'mid' in df.columns:\n            prices = df['mid'].values\n        elif 'bid' in df.columns and 'ask' in df.columns:\n            prices = (df['bid'].values + df['ask'].values) / 2\n        else:\n            prices = df['close'].values if 'close' in df.columns else df['price'].values\n\n        volumes = df['volume'].values if 'volume' in df.columns else np.ones(len(prices))\n\n        n = len(prices)\n\n        # Initialize output arrays\n        vpin_values = {size: np.zeros(n) for size in self.calculators}\n        toxicity_regime = {size: np.zeros(n) for size in self.calculators}\n        imbalance = {size: np.zeros(n) for size in self.calculators}\n\n        # Process each tick\n        for i in range(1, n):\n            prev_price = prices[i-1]\n            price = prices[i]\n            volume = volumes[i]\n\n            for size, calc in self.calculators.items():\n                result = calc.update(price, prev_price, volume)\n                if result:\n                    vpin_values[size][i] = result.vpin\n                    toxicity_regime[size][i] = result.toxicity_regime\n                    imbalance[size][i] = result.imbalance\n                else:\n                    # Use previous value\n                    vpin_values[size][i] = vpin_values[size][i-1]\n                    toxicity_regime[size][i] = toxicity_regime[size][i-1]\n                    imbalance[size][i] = imbalance[size][i-1]\n\n        # Create output DataFrame\n        result = pd.DataFrame(index=df.index)\n\n        for size in self.calculators:\n            result[f'vpin_{size}'] = vpin_values[size]\n            result[f'toxicity_{size}'] = toxicity_regime[size]\n            result[f'vol_imbalance_{size}'] = imbalance[size]\n\n        # Aggregate features\n        result['vpin_mean'] = np.mean([vpin_values[s] for s in self.calculators], axis=0)\n        result['vpin_max'] = np.max([vpin_values[s] for s in self.calculators], axis=0)\n        result['toxicity_alert'] = (result['vpin_max'] > 0.7).astype(int)\n\n        return result",
    "source_file": "core\\_experimental\\vpin_toxicity.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": "VPINFeatures"
  },
  {
    "name": "calculate_vpin_simple",
    "category": "microstructure",
    "formula": "vpin",
    "explanation": "Simple VPIN calculation for feature engineering.\n\nArgs:\n    prices: Mid prices\n    volumes: Trade volumes\n    n_buckets: Rolling window for VPIN\n\nReturns:\n    Array of VPIN values",
    "python_code": "def calculate_vpin_simple(prices: np.ndarray, volumes: np.ndarray,\n                         n_buckets: int = 50) -> np.ndarray:\n    \"\"\"\n    Simple VPIN calculation for feature engineering.\n\n    Args:\n        prices: Mid prices\n        volumes: Trade volumes\n        n_buckets: Rolling window for VPIN\n\n    Returns:\n        Array of VPIN values\n    \"\"\"\n    n = len(prices)\n    vpin = np.zeros(n)\n\n    # Classify trades using tick rule\n    buy_volume = np.zeros(n)\n    sell_volume = np.zeros(n)\n\n    for i in range(1, n):\n        if prices[i] > prices[i-1]:\n            buy_volume[i] = volumes[i]\n        elif prices[i] < prices[i-1]:\n            sell_volume[i] = volumes[i]\n        else:\n            buy_volume[i] = volumes[i] / 2\n            sell_volume[i] = volumes[i] / 2\n\n    # Rolling VPIN\n    for i in range(n_buckets, n):\n        window_buy = np.sum(buy_volume[i-n_buckets:i])\n        window_sell = np.sum(sell_volume[i-n_buckets:i])\n        total = window_buy + window_sell\n\n        if total > 0:\n            vpin[i] = abs(window_buy - window_sell) / total\n\n    return vpin",
    "source_file": "core\\_experimental\\vpin_toxicity.py",
    "academic_reference": "Corsi (2009) 'HAR-RV Model' J. Financial Econometrics",
    "class_name": null
  },
  {
    "name": "__init__",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Initialize walk-forward CV.\n\nArgs:\n    n_splits: Number of folds\n    train_ratio: Ratio of data for training (if not expanding)\n    gap: Number of periods to skip between train/test (prevents leakage)\n    expanding: If True, training window expands; if False, rolls",
    "python_code": "def __init__(self,\n                 n_splits: int = 5,\n                 train_ratio: float = 0.8,\n                 gap: int = 0,\n                 expanding: bool = True):\n        \"\"\"\n        Initialize walk-forward CV.\n\n        Args:\n            n_splits: Number of folds\n            train_ratio: Ratio of data for training (if not expanding)\n            gap: Number of periods to skip between train/test (prevents leakage)\n            expanding: If True, training window expands; if False, rolls\n        \"\"\"\n        self.n_splits = n_splits\n        self.train_ratio = train_ratio\n        self.gap = gap\n        self.expanding = expanding",
    "source_file": "core\\_experimental\\walk_forward.py",
    "academic_reference": null,
    "class_name": "WalkForwardCV"
  },
  {
    "name": "split",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Generate train/test indices for each fold.\n\nArgs:\n    X: Features array\n    y: Labels array (optional)\n\nYields:\n    (train_indices, test_indices) for each fold",
    "python_code": "def split(self, X: np.ndarray, y: Optional[np.ndarray] = None) -> Generator[Tuple[np.ndarray, np.ndarray], None, None]:\n        \"\"\"\n        Generate train/test indices for each fold.\n\n        Args:\n            X: Features array\n            y: Labels array (optional)\n\n        Yields:\n            (train_indices, test_indices) for each fold\n        \"\"\"\n        n_samples = len(X)\n        test_size = n_samples // (self.n_splits + 1)\n\n        for i in range(self.n_splits):\n            if self.expanding:\n                # Expanding window: train on all data up to test start\n                train_end = test_size * (i + 1)\n                train_start = 0\n            else:\n                # Rolling window: fixed training size\n                train_size = int(test_size * self.train_ratio / (1 - self.train_ratio))\n                train_end = test_size * (i + 1)\n                train_start = max(0, train_end - train_size)\n\n            test_start = train_end + self.gap\n            test_end = min(test_start + test_size, n_samples)\n\n            if test_start >= n_samples:\n                break\n\n            train_idx = np.arange(train_start, train_end)\n            test_idx = np.arange(test_start, test_end)\n\n            yield train_idx, test_idx",
    "source_file": "core\\_experimental\\walk_forward.py",
    "academic_reference": null,
    "class_name": "WalkForwardCV"
  },
  {
    "name": "run_walk_forward_backtest",
    "category": "machine_learning",
    "formula": "results",
    "explanation": "Run complete walk-forward backtest.\n\nArgs:\n    model: Sklearn-compatible model with fit/predict\n    X: Features\n    y: Labels\n    cv: Cross-validation splitter\n    timestamps: Optional timestamps for reporting\n\nReturns:\n    List of WalkForwardResult for each fold",
    "python_code": "def run_walk_forward_backtest(\n    model,\n    X: np.ndarray,\n    y: np.ndarray,\n    cv: WalkForwardCV,\n    timestamps: Optional[pd.DatetimeIndex] = None\n) -> List[WalkForwardResult]:\n    \"\"\"\n    Run complete walk-forward backtest.\n\n    Args:\n        model: Sklearn-compatible model with fit/predict\n        X: Features\n        y: Labels\n        cv: Cross-validation splitter\n        timestamps: Optional timestamps for reporting\n\n    Returns:\n        List of WalkForwardResult for each fold\n    \"\"\"\n    results = []\n\n    for fold, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n        # Fit model\n        model.fit(X[train_idx], y[train_idx])\n\n        # Predict\n        train_pred = model.predict(X[train_idx])\n        test_pred = model.predict(X[test_idx])\n\n        # Score (accuracy for classification)\n        train_score = (train_pred == y[train_idx]).mean()\n        test_score = (test_pred == y[test_idx]).mean()\n\n        # Get timestamps if available\n        if timestamps is not None:\n            train_start = timestamps[train_idx[0]]\n            train_end = timestamps[train_idx[-1]]\n            test_start = timestamps[test_idx[0]]\n            test_end = timestamps[test_idx[-1]]\n        else:\n            train_start = train_end = test_start = test_end = datetime.now()\n\n        result = WalkForwardResult(\n            fold=fold,\n            train_start=train_start,\n            train_end=train_end,\n            test_start=test_start,\n            test_end=test_end,\n            train_size=len(train_idx),\n            test_size=len(test_idx),\n            train_score=train_score,\n            test_score=test_score,\n            predictions=test_pred,\n            actuals=y[test_idx]\n        )\n        results.append(result)\n\n        logger.info(f\"Fold {fold}: Train={train_score:.4f}, Test={test_score:.4f}\")\n\n    return results",
    "source_file": "core\\_experimental\\walk_forward.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "walk_forward_analysis",
    "category": "quantitative",
    "formula": "{",
    "explanation": "Analyze walk-forward results.\n\nReturns:\n    Dict with summary statistics",
    "python_code": "def walk_forward_analysis(results: List[WalkForwardResult]) -> Dict[str, float]:\n    \"\"\"\n    Analyze walk-forward results.\n\n    Returns:\n        Dict with summary statistics\n    \"\"\"\n    train_scores = [r.train_score for r in results]\n    test_scores = [r.test_score for r in results]\n\n    return {\n        'mean_train_score': np.mean(train_scores),\n        'std_train_score': np.std(train_scores),\n        'mean_test_score': np.mean(test_scores),\n        'std_test_score': np.std(test_scores),\n        'min_test_score': np.min(test_scores),\n        'max_test_score': np.max(test_scores),\n        'overfit_ratio': np.mean(train_scores) / (np.mean(test_scores) + 1e-8),\n        'n_folds': len(results),\n        'total_predictions': sum(r.test_size for r in results),\n    }",
    "source_file": "core\\_experimental\\walk_forward.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "detect_overfitting",
    "category": "machine_learning",
    "formula": "True | ratio > threshold",
    "explanation": "Detect if model is overfitting.\n\nArgs:\n    train_score: Training accuracy\n    test_score: Test accuracy\n    threshold: Max acceptable ratio (e.g., 1.15 = 15% higher train)\n\nReturns:\n    True if overfitting detected",
    "python_code": "def detect_overfitting(\n    train_score: float,\n    test_score: float,\n    threshold: float = 1.15\n) -> bool:\n    \"\"\"\n    Detect if model is overfitting.\n\n    Args:\n        train_score: Training accuracy\n        test_score: Test accuracy\n        threshold: Max acceptable ratio (e.g., 1.15 = 15% higher train)\n\n    Returns:\n        True if overfitting detected\n    \"\"\"\n    if test_score < 0.01:\n        return True\n\n    ratio = train_score / test_score\n    return ratio > threshold",
    "source_file": "core\\_experimental\\walk_forward.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "fit",
    "category": "machine_learning",
    "formula": "",
    "explanation": "Find best parameters using walk-forward CV.",
    "python_code": "def fit(self, X: np.ndarray, y: np.ndarray) -> 'WalkForwardOptimizer':\n        \"\"\"\n        Find best parameters using walk-forward CV.\n        \"\"\"\n        from itertools import product\n\n        # Generate all parameter combinations\n        param_names = list(self.param_grid.keys())\n        param_values = list(self.param_grid.values())\n\n        best_score = -np.inf\n        best_params = None\n\n        for values in product(*param_values):\n            params = dict(zip(param_names, values))\n\n            # Evaluate this parameter set\n            scores = []\n            for train_idx, test_idx in self.cv.split(X, y):\n                model = self.model_class(**params)\n                model.fit(X[train_idx], y[train_idx])\n                pred = model.predict(X[test_idx])\n                score = (pred == y[test_idx]).mean()\n                scores.append(score)\n\n            mean_score = np.mean(scores)\n\n            if mean_score > best_score:\n                best_score = mean_score\n                best_params = params\n\n        self.best_params_ = best_params\n        self.best_score_ = best_score\n\n        return self",
    "source_file": "core\\_experimental\\walk_forward.py",
    "academic_reference": null,
    "class_name": "WalkForwardOptimizer"
  },
  {
    "name": "expanding_window_cv",
    "category": "quantitative",
    "formula": "WalkForwardCV(n_splits=n_splits, gap=gap, expanding=True)",
    "explanation": "Create expanding window CV.",
    "python_code": "def expanding_window_cv(n_splits: int = 5, gap: int = 0) -> WalkForwardCV:\n    \"\"\"Create expanding window CV.\"\"\"\n    return WalkForwardCV(n_splits=n_splits, gap=gap, expanding=True)",
    "source_file": "core\\_experimental\\walk_forward.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "rolling_window_cv",
    "category": "statistical",
    "formula": "WalkForwardCV(n_splits=n_splits, train_ratio=train_ratio, gap=gap, expanding=False)",
    "explanation": "Create rolling window CV.",
    "python_code": "def rolling_window_cv(n_splits: int = 5, train_ratio: float = 0.8, gap: int = 0) -> WalkForwardCV:\n    \"\"\"Create rolling window CV.\"\"\"\n    return WalkForwardCV(n_splits=n_splits, train_ratio=train_ratio, gap=gap, expanding=False)",
    "source_file": "core\\_experimental\\walk_forward.py",
    "academic_reference": null,
    "class_name": null
  },
  {
    "name": "purged_cv",
    "category": "quantitative",
    "formula": "PurgedKFold(n_splits=n_splits, embargo_pct=embargo_pct)",
    "explanation": "Create purged K-fold CV.",
    "python_code": "def purged_cv(n_splits: int = 5, embargo_pct: float = 0.01) -> PurgedKFold:\n    \"\"\"Create purged K-fold CV.\"\"\"\n    return PurgedKFold(n_splits=n_splits, embargo_pct=embargo_pct)",
    "source_file": "core\\_experimental\\walk_forward.py",
    "academic_reference": null,
    "class_name": null
  }
]