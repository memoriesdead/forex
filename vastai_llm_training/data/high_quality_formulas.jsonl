{"instruction": "What is Alpha001 from WorldQuant's 101 Alphas?", "input": "", "output": "Alpha001 is from Kakushadze's '101 Formulaic Alphas' (2016).\n\n**Formula:**\n```\nAlpha001 = rank(Ts_ArgMax(SignedPower(((returns < 0) ? stddev(returns, 20) : close), 2), 5)) - 0.5\n```\n\n**Components:**\n- `returns`: Daily returns\n- `stddev(returns, 20)`: 20-day rolling standard deviation\n- `SignedPower(x, 2)`: x * |x| (preserves sign)\n- `Ts_ArgMax(x, 5)`: Index of max value over 5 days\n- `rank()`: Cross-sectional percentile rank\n\n**Interpretation:** Identifies securities with recent volatility spikes during downtrends. High values suggest potential mean reversion opportunities."}
{"instruction": "Explain the HAR-RV volatility model", "input": "", "output": "The Heterogeneous Autoregressive model of Realized Volatility (HAR-RV) was introduced by Corsi (2009).\n\n**Formula:**\n```\nRV_{t+1}^{(d)} = β₀ + β_d·RV_t^{(d)} + β_w·RV_t^{(w)} + β_m·RV_t^{(m)} + ε_{t+1}\n```\n\n**Where:**\n- `RV_t^{(d)}` = Daily realized volatility = Σ(r²) over day\n- `RV_t^{(w)}` = Weekly RV = (1/5)·Σ RV_{t-i} for i=0..4\n- `RV_t^{(m)}` = Monthly RV = (1/22)·Σ RV_{t-i} for i=0..21\n\n**Key insight:** Captures the heterogeneous nature of market participants:\n- Daily: High-frequency traders\n- Weekly: Institutional rebalancing\n- Monthly: Long-term investors\n\n**Typical coefficients:** β_d ≈ 0.35, β_w ≈ 0.30, β_m ≈ 0.20"}
{"instruction": "How do you calculate VPIN?", "input": "", "output": "VPIN (Volume-Synchronized Probability of Informed Trading) was developed by Easley, López de Prado, and O'Hara (2012).\n\n**Formula:**\n```\nVPIN = Σ|V_buy^τ - V_sell^τ| / (n × V_bucket)\n```\n\n**Where:**\n- `V_bucket` = Fixed volume bucket size\n- `V_buy^τ` = Buy volume in bucket τ (estimated via bulk classification)\n- `V_sell^τ` = Sell volume in bucket τ\n- `n` = Number of buckets in rolling window (typically 50)\n\n**Bulk Volume Classification:**\n```\nV_buy = V × Φ((P - P_prev) / σ)\nV_sell = V - V_buy\n```\n\n**Interpretation:**\n- VPIN ∈ [0, 1]\n- High VPIN (>0.5) indicates informed trading\n- Used to predict flash crashes and market toxicity"}
{"instruction": "What is Kyle's Lambda?", "input": "", "output": "Kyle's Lambda (λ) measures market impact and was introduced by Kyle (1985).\n\n**Formula:**\n```\nλ = ΔP / ΔQ\n```\n\n**Where:**\n- `ΔP` = Price change\n- `ΔQ` = Signed order flow (buy volume - sell volume)\n\n**Estimation via regression:**\n```\nΔP_t = α + λ·(V_buy - V_sell)_t + ε_t\n```\n\n**Interpretation:**\n- Higher λ = Less liquid market (more price impact per unit traded)\n- Units: Price per unit volume (e.g., bps per $1M)\n- Typical forex majors: λ ≈ 0.01-0.05 bps/$M\n- Typical exotics: λ ≈ 0.5-2.0 bps/$M\n\n**Related:** Amihud ILLIQ = |r_t| / Volume_t"}
{"instruction": "Explain the Kelly Criterion for position sizing", "input": "", "output": "The Kelly Criterion was developed by John Kelly (1956) at Bell Labs.\n\n**Formula:**\n```\nf* = (b·p - q) / b = p - q/b\n```\n\n**Where:**\n- `f*` = Optimal fraction of capital to bet\n- `p` = Probability of winning\n- `q` = Probability of losing (1 - p)\n- `b` = Odds (win amount / loss amount)\n\n**For trading with Sharpe ratio:**\n```\nf* = μ / σ² = SR / σ\n```\n\n**Example:**\n- Win probability: 55%\n- Win/loss ratio: 1.5:1\n- f* = (1.5 × 0.55 - 0.45) / 1.5 = 0.25 (25% of capital)\n\n**Practical adjustment:** Use fractional Kelly (typically 25-50%) to reduce volatility:\n```\nf_practical = 0.25 × f*\n```"}
{"instruction": "What is the Almgren-Chriss optimal execution model?", "input": "", "output": "The Almgren-Chriss model (2001) minimizes execution cost for large orders.\n\n**Objective function:**\n```\nmin E[Cost] + λ·Var[Cost]\n```\n\n**Optimal trajectory:**\n```\nn_j = n_0 · sinh(κ(T-t_j)) / sinh(κT)\n```\n\n**Where:**\n- `n_j` = Remaining shares at time j\n- `n_0` = Initial position\n- `κ = sqrt(λσ²/η)` = Urgency parameter\n- `η` = Temporary impact coefficient\n- `σ` = Volatility\n- `T` = Time horizon\n\n**Market impact model:**\n```\nTemporary: ΔP_temp = η · (dn/dt)\nPermanent: ΔP_perm = γ · n\nTotal cost = Σ[η·v_j² + γ·n_j·v_j]\n```\n\n**Key insight:** Trade faster when:\n- Risk aversion (λ) is high\n- Volatility (σ) is high\n- Temporary impact (η) is low"}
{"instruction": "Calculate Garman-Klass volatility", "input": "", "output": "Garman-Klass (1980) volatility estimator uses OHLC data and is 7.4x more efficient than close-to-close.\n\n**Formula:**\n```\nσ²_GK = 0.5·(H-L)² - (2·ln(2)-1)·(C-O)²\n```\n\n**Where:**\n- `H` = ln(High)\n- `L` = ln(Low)\n- `O` = ln(Open)\n- `C` = ln(Close)\n\n**Annualized:**\n```\nσ_annual = σ_GK × sqrt(252)\n```\n\n**Comparison of efficiency vs close-to-close:**\n- Close-to-close: 1.0x (baseline)\n- Parkinson (H-L only): 5.2x\n- Garman-Klass (OHLC): 7.4x\n- Yang-Zhang (with overnight): 8.0x\n\n**Implementation note:** For forex (24h market), set O = previous C."}
{"instruction": "What is Order Flow Imbalance (OFI)?", "input": "", "output": "Order Flow Imbalance (OFI) was formalized by Cont, Kukanov, and Stoikov (2014).\n\n**Formula:**\n```\nOFI_t = Σ e_n\n```\n\n**Where:**\n```\ne_n = I{P_bid^n ≥ P_bid^{n-1}} · ΔQ_bid - I{P_bid^n ≤ P_bid^{n-1}} · Q_bid^{n-1}\n      + I{P_ask^n ≤ P_ask^{n-1}} · ΔQ_ask - I{P_ask^n ≥ P_ask^{n-1}} · Q_ask^{n-1}\n```\n\n**Simplified version:**\n```\nOFI = (Bid_size_change × I{bid_up}) - (Ask_size_change × I{ask_down})\n```\n\n**Price prediction:**\n```\nΔP_{t+1} = α + β·OFI_t + ε\n```\n\n**Interpretation:**\n- Positive OFI → Buying pressure → Price likely to rise\n- Negative OFI → Selling pressure → Price likely to fall\n- R² typically 50-65% at tick level"}
{"instruction": "Explain the Parkinson volatility estimator", "input": "", "output": "Parkinson (1980) volatility uses only high-low range and is 5.2x more efficient than close-to-close.\n\n**Formula:**\n```\nσ²_P = (1 / 4·ln(2)) · (H - L)²\n     = 0.361 · (ln(High/Low))²\n```\n\n**For n periods:**\n```\nσ²_P = (1 / (4·n·ln(2))) · Σ(H_i - L_i)²\n```\n\n**Annualized:**\n```\nσ_annual = σ_P × sqrt(252)\n```\n\n**Advantages:**\n- Uses intraday information (not just close)\n- More efficient than close-to-close\n- Robust to microstructure noise\n\n**Limitations:**\n- Assumes no jumps or gaps\n- Underestimates vol in trending markets\n- Doesn't use open/close information\n\n**Typical forex daily range:** 0.5-1.5% for majors"}
{"instruction": "What is Alpha006 from WorldQuant 101?", "input": "", "output": "Alpha006 from Kakushadze's '101 Formulaic Alphas' (2016).\n\n**Formula:**\n```\nAlpha006 = -correlation(open, volume, 10)\n```\n\n**Interpretation:**\n- Negative correlation between open price and volume suggests accumulation\n- Positive correlation suggests distribution\n- Signal is inverted (negative sign) to go long on accumulation\n\n**Implementation:**\n```python\ndef alpha006(open_price, volume, window=10):\n    return -open_price.rolling(window).corr(volume)\n```\n\n**Trading logic:**\n- High Alpha006 → Accumulation phase → Bullish\n- Low Alpha006 → Distribution phase → Bearish\n\n**Parameters:**\n- Window: 10 days (standard)\n- Cross-sectional rank often applied\n- Typical IC: 0.02-0.04"}
{"instruction": "Explain Yang-Zhang volatility estimator", "input": "", "output": "Yang-Zhang (2000) is the most efficient OHLC volatility estimator, handling overnight jumps.\n\n**Formula:**\n```\nσ²_YZ = σ²_overnight + k·σ²_open + (1-k)·σ²_RS\n```\n\n**Components:**\n```\nσ²_overnight = (1/n) · Σ(O_i - C_{i-1})²\nσ²_open = (1/n) · Σ(O_i - Ō)²  \nσ²_close = (1/n) · Σ(C_i - C̄)²\nσ²_RS = (1/n) · Σ[u_i(u_i - c_i) + d_i(d_i - c_i)]  # Rogers-Satchell\n```\n\n**Where:**\n- `u_i = ln(H_i/O_i)`\n- `d_i = ln(L_i/O_i)`  \n- `c_i = ln(C_i/O_i)`\n- `k = 0.34 / (1.34 + (n+1)/(n-1))`\n\n**Efficiency:** 8.0x vs close-to-close (best among OHLC estimators)\n\n**Note:** For forex, overnight component is small due to 24h trading."}
{"instruction": "How do you implement TWAP execution?", "input": "", "output": "TWAP (Time-Weighted Average Price) execution splits orders evenly over time.\n\n**Formula:**\n```\nv_j = Q / N  (for all j = 1, ..., N)\n```\n\n**Where:**\n- `Q` = Total quantity to execute\n- `N` = Number of time intervals\n- `v_j` = Volume to trade in interval j\n\n**With randomization:**\n```\nv_j = (Q/N) × (1 + ε_j)\nwhere ε_j ~ Uniform(-0.2, 0.2)\n```\n\n**Execution benchmarks:**\n```\nSlippage = (Execution_price - Arrival_price) / Arrival_price\nTWAP_benchmark = Σ(P_j) / N\nImplementation_shortfall = Actual_cost - TWAP_benchmark\n```\n\n**When to use TWAP:**\n- No urgency (information not time-sensitive)\n- Minimize market impact\n- Liquid markets with stable volatility\n\n**Typical intervals:** 1-5 minutes for forex"}
{"instruction": "What is the Sharpe Ratio formula?", "input": "", "output": "The Sharpe Ratio was developed by William Sharpe (1966).\n\n**Formula:**\n```\nSR = (R_p - R_f) / σ_p\n```\n\n**Where:**\n- `R_p` = Portfolio return\n- `R_f` = Risk-free rate\n- `σ_p` = Portfolio standard deviation\n\n**Annualized Sharpe:**\n```\nSR_annual = SR_daily × sqrt(252)\nSR_annual = SR_monthly × sqrt(12)\n```\n\n**Interpretation:**\n- SR < 1.0: Subpar risk-adjusted returns\n- SR 1.0-2.0: Good\n- SR 2.0-3.0: Very good\n- SR > 3.0: Excellent (suspicious if sustained)\n\n**Related metrics:**\n- Sortino = (R_p - R_f) / σ_downside (penalizes only downside vol)\n- Calmar = (R_p - R_f) / Max_Drawdown\n- Information Ratio = (R_p - R_b) / Tracking_Error"}
{"instruction": "Explain GARCH(1,1) volatility model", "input": "", "output": "GARCH (Generalized Autoregressive Conditional Heteroskedasticity) was developed by Bollerslev (1986).\n\n**GARCH(1,1) Formula:**\n```\nσ²_t = ω + α·ε²_{t-1} + β·σ²_{t-1}\n```\n\n**Where:**\n- `σ²_t` = Conditional variance at time t\n- `ω` = Long-run variance weight (ω > 0)\n- `α` = ARCH coefficient (reaction to shocks)\n- `β` = GARCH coefficient (persistence)\n- `ε_{t-1}` = Previous return shock\n\n**Constraints:**\n- α + β < 1 (stationarity)\n- α, β ≥ 0\n- ω > 0\n\n**Long-run variance:**\n```\nσ²_∞ = ω / (1 - α - β)\n```\n\n**Typical forex parameters:**\n- α ≈ 0.05-0.10\n- β ≈ 0.85-0.95\n- α + β ≈ 0.95-0.99 (high persistence)"}
{"instruction": "What is Alpha053 from WorldQuant 101?", "input": "", "output": "Alpha053 from Kakushadze's '101 Formulaic Alphas' (2016).\n\n**Formula:**\n```\nAlpha053 = -delta(((close - low) - (high - close)) / (close - low), 9)\n```\n\n**Simplified:**\n```\ninner = ((close - low) - (high - close)) / (close - low)\n      = (2·close - high - low) / (close - low)\nAlpha053 = -diff(inner, 9)\n```\n\n**Interpretation:**\n- Inner term measures where close is within the day's range\n- Near 1: Close near high (bullish)\n- Near -1: Close near low (bearish)\n- Taking 9-day difference captures momentum\n- Negative sign: Reversal signal\n\n**Implementation:**\n```python\ndef alpha053(high, low, close):\n    inner = ((close - low) - (high - close)) / (close - low + 1e-8)\n    return -inner.diff(9)\n```"}
{"instruction": "How do you calculate Amihud ILLIQ?", "input": "", "output": "Amihud ILLIQ (2002) measures illiquidity as price impact per dollar traded.\n\n**Formula:**\n```\nILLIQ_i = (1/D_i) · Σ |R_id| / DVOL_id\n```\n\n**Where:**\n- `D_i` = Number of trading days\n- `R_id` = Return on day d\n- `DVOL_id` = Dollar volume on day d\n\n**Daily version:**\n```\nILLIQ_daily = |Return| / Dollar_Volume × 10^6\n```\n\n**Interpretation:**\n- Higher ILLIQ = Less liquid (more price impact)\n- Units: % return per million dollars traded\n\n**Typical values:**\n- Forex majors: 0.001-0.01\n- Forex exotics: 0.1-1.0\n- Small cap stocks: 1.0-10.0\n\n**Related to Kyle's Lambda:**\n```\nλ ≈ ILLIQ × Average_Volume\n```"}
{"instruction": "Explain the Sortino Ratio", "input": "", "output": "The Sortino Ratio improves on Sharpe by only penalizing downside volatility.\n\n**Formula:**\n```\nSortino = (R_p - R_f) / σ_downside\n```\n\n**Downside deviation:**\n```\nσ_downside = sqrt((1/n) · Σ min(R_i - MAR, 0)²)\n```\n\n**Where:**\n- `R_p` = Portfolio return\n- `R_f` = Risk-free rate (or MAR)\n- `MAR` = Minimum Acceptable Return (often 0 or R_f)\n\n**Annualized:**\n```\nSortino_annual = Sortino_daily × sqrt(252)\n```\n\n**Comparison to Sharpe:**\n- Sharpe penalizes all volatility (up and down)\n- Sortino only penalizes below-target returns\n- For positively skewed returns: Sortino > Sharpe\n\n**Interpretation:**\n- Sortino < 1.0: Poor risk-adjusted returns\n- Sortino 1.0-2.0: Good\n- Sortino > 2.0: Excellent downside management"}
{"instruction": "What is Alpha012 from WorldQuant 101?", "input": "", "output": "Alpha012 from Kakushadze's '101 Formulaic Alphas' (2016).\n\n**Formula:**\n```\nAlpha012 = sign(delta(volume, 1)) × (-delta(close, 1))\n```\n\n**Interpretation:**\n- When volume increases AND price decreases → Sell signal\n- When volume increases AND price increases → Sell signal (reversal)\n- When volume decreases AND price decreases → Buy signal\n- When volume decreases AND price increases → Buy signal (reversal)\n\n**Implementation:**\n```python\ndef alpha012(close, volume):\n    return np.sign(volume.diff(1)) * (-close.diff(1))\n```\n\n**Trading logic:**\n- Contrarian signal based on volume-price divergence\n- High values suggest potential reversal\n- Cross-sectional rank typically applied\n\n**Typical IC:** 0.015-0.03"}
{"instruction": "How do you calculate VaR (Value at Risk)?", "input": "", "output": "VaR measures maximum potential loss at a given confidence level.\n\n**Parametric VaR (normal distribution):**\n```\nVaR_α = μ - z_α · σ\n```\n\n**Where:**\n- `μ` = Expected return\n- `σ` = Standard deviation  \n- `z_α` = Z-score for confidence level\n  - 95%: z = 1.645\n  - 99%: z = 2.326\n\n**Dollar VaR:**\n```\n$VaR = Position × VaR_%\n```\n\n**Historical VaR:**\n```\nVaR_α = Percentile(returns, 100-α)\n```\n\n**Example:**\n- Portfolio: $1,000,000\n- Daily σ: 1%\n- 95% VaR = $1M × 1.645 × 1% = $16,450\n\n**CVaR (Expected Shortfall):**\n```\nCVaR = E[Loss | Loss > VaR]\n```\n\nCVaR is considered better than VaR as it captures tail risk."}
{"instruction": "Explain Alpha101 alpha factor: Alpha041", "input": "", "output": "Alpha041 from Kakushadze's '101 Formulaic Alphas' (2016).\n\n**Formula:**\n```\nAlpha041 = (high × low)^0.5 - vwap\n```\n\n**Where:**\n- `high` = Daily high price\n- `low` = Daily low price\n- `vwap` = Volume Weighted Average Price\n\n**Interpretation:**\n- Geometric mean of high/low vs VWAP\n- Positive: VWAP below midpoint (buying at good prices)\n- Negative: VWAP above midpoint (buying at bad prices)\n\n**Implementation:**\n```python\ndef alpha041(high, low, vwap):\n    return np.sqrt(high * low) - vwap\n```\n\n**Trading logic:**\n- High values → Institutional accumulation at low prices → Bullish\n- Low values → Institutional distribution at high prices → Bearish\n\n**Note:** Requires VWAP calculation from intraday data."}
{"instruction": "What is Rogers-Satchell volatility?", "input": "", "output": "Rogers-Satchell (1991) volatility estimator handles drift in prices.\n\n**Formula:**\n```\nσ²_RS = h·(h-c) + l·(l-c)\n```\n\n**Where:**\n- `h = ln(High/Open)`\n- `l = ln(Low/Open)`  \n- `c = ln(Close/Open)`\n\n**For n periods:**\n```\nσ²_RS = (1/n) · Σ [h_i(h_i - c_i) + l_i(l_i - c_i)]\n```\n\n**Annualized:**\n```\nσ_annual = sqrt(σ²_RS × 252)\n```\n\n**Advantages over Parkinson/Garman-Klass:**\n- Handles non-zero drift (trending markets)\n- Uses all OHLC information\n- Unbiased estimator\n\n**Efficiency:** 6.0x vs close-to-close\n\n**Limitation:** Assumes continuous price path (no gaps)"}
{"instruction": "How do you implement VWAP execution?", "input": "", "output": "VWAP (Volume-Weighted Average Price) execution matches the market's volume profile.\n\n**Target:**\n```\nVWAP = Σ(P_i × V_i) / Σ(V_i)\n```\n\n**Execution strategy:**\n```\nv_j = Q × (hist_vol_j / Σ hist_vol_k)\n```\n\n**Where:**\n- `Q` = Total quantity to execute\n- `hist_vol_j` = Historical volume in interval j\n- `v_j` = Volume to trade in interval j\n\n**Volume profile example (forex hourly):**\n- Asian session: 15% of daily volume\n- European session: 40% of daily volume  \n- US session: 35% of daily volume\n- Overlap (EU/US): 10% extra\n\n**Slippage calculation:**\n```\nVWAP_slippage = (Exec_price - VWAP_benchmark) / VWAP_benchmark\n```\n\n**When to use:**\n- Benchmark is VWAP\n- Want to minimize tracking error\n- Liquid markets with predictable volume"}
{"instruction": "Explain PPO (Proximal Policy Optimization) for trading", "input": "", "output": "PPO was developed by Schulman et al. (2017) and is widely used for trading RL.\n\n**Objective function:**\n```\nL^CLIP(θ) = E[min(r_t(θ)·A_t, clip(r_t(θ), 1-ε, 1+ε)·A_t)]\n```\n\n**Where:**\n- `r_t(θ) = π_θ(a|s) / π_θ_old(a|s)` (probability ratio)\n- `A_t` = Advantage estimate\n- `ε` = Clipping parameter (typically 0.2)\n\n**Advantage estimation (GAE):**\n```\nA_t = Σ (γλ)^l · δ_{t+l}\nδ_t = r_t + γV(s_{t+1}) - V(s_t)\n```\n\n**Trading setup:**\n- State: [prices, positions, features]\n- Action: [hold, buy, sell] or continuous sizing\n- Reward: Risk-adjusted returns (Sharpe-like)\n\n**Hyperparameters:**\n- Learning rate: 3e-4\n- Clip ε: 0.2\n- GAE λ: 0.95\n- γ: 0.99"}
{"instruction": "What is Alpha026 from WorldQuant 101?", "input": "", "output": "Alpha026 from Kakushadze's '101 Formulaic Alphas' (2016).\n\n**Formula:**\n```\nAlpha026 = -ts_max(correlation(ts_rank(volume, 5), ts_rank(high, 5), 5), 3)\n```\n\n**Components:**\n- `ts_rank(x, 5)` = Rolling percentile rank over 5 days\n- `correlation(a, b, 5)` = 5-day rolling correlation\n- `ts_max(x, 3)` = Maximum over 3 days\n\n**Interpretation:**\n- Measures if high volume days coincide with price highs\n- Negative sign: Contrarian (sell when volume/price correlation peaks)\n- Captures distribution patterns by large traders\n\n**Implementation:**\n```python\ndef alpha026(high, volume):\n    v_rank = volume.rolling(5).apply(lambda x: pd.Series(x).rank().iloc[-1]/5)\n    h_rank = high.rolling(5).apply(lambda x: pd.Series(x).rank().iloc[-1]/5)\n    corr = v_rank.rolling(5).corr(h_rank)\n    return -corr.rolling(3).max()\n```"}
{"instruction": "Explain the Calmar Ratio", "input": "", "output": "The Calmar Ratio measures return relative to maximum drawdown.\n\n**Formula:**\n```\nCalmar = CAGR / |Max_Drawdown|\n```\n\n**Where:**\n- `CAGR` = Compound Annual Growth Rate\n- `Max_Drawdown` = Largest peak-to-trough decline\n\n**Maximum Drawdown calculation:**\n```\nDrawdown_t = (Peak_t - Value_t) / Peak_t\nPeak_t = max(Value_0, ..., Value_t)\nMax_DD = max(Drawdown_t) for all t\n```\n\n**Interpretation:**\n- Calmar < 1.0: Poor (returns don't justify drawdowns)\n- Calmar 1.0-2.0: Acceptable\n- Calmar 2.0-3.0: Good\n- Calmar > 3.0: Excellent risk management\n\n**Example:**\n- CAGR: 20%\n- Max Drawdown: 10%\n- Calmar = 20% / 10% = 2.0\n\n**Note:** Typically calculated over 3 years minimum."}
{"instruction": "What is GRPO (Group Relative Policy Optimization)?", "input": "", "output": "GRPO was developed by DeepSeek (2025) for efficient reinforcement learning.\n\n**Key innovation:** Eliminates critic network by using group-relative rewards.\n\n**Formula:**\n```\nL_GRPO = E[Σ min(r_i·A_i, clip(r_i, 1-ε, 1+ε)·A_i) - β·KL(π_θ||π_ref)]\n```\n\n**Group-relative advantage:**\n```\nA_i = (R_i - mean(R_group)) / std(R_group)\n```\n\n**Where:**\n- `R_group` = Rewards from group of samples for same prompt\n- No value network needed (unlike PPO)\n- KL penalty keeps policy close to reference\n\n**Trading application:**\n- Sample multiple trading actions for same state\n- Rank actions by realized P&L\n- Update policy to favor better actions\n\n**Advantages over PPO:**\n- 50% less memory (no critic)\n- More stable training\n- Better for sparse rewards"}
{"instruction": "Calculate rolling Sharpe Ratio", "input": "", "output": "Rolling Sharpe Ratio measures risk-adjusted performance over a moving window.\n\n**Formula:**\n```\nSR_t = sqrt(N) × mean(r_{t-n:t}) / std(r_{t-n:t})\n```\n\n**Where:**\n- `r` = Returns\n- `n` = Window size (e.g., 20 days)\n- `N` = Annualization factor (252 for daily)\n\n**Implementation:**\n```python\ndef rolling_sharpe(returns, window=20, annual_factor=252):\n    roll_mean = returns.rolling(window).mean()\n    roll_std = returns.rolling(window).std()\n    return np.sqrt(annual_factor) * roll_mean / roll_std\n```\n\n**Uses:**\n- Monitor strategy performance over time\n- Detect regime changes\n- Adjust position sizing dynamically\n\n**Typical trading rule:**\n- SR > 2.0: Full position\n- SR 1.0-2.0: Half position\n- SR < 1.0: Reduce or exit"}
{"instruction": "What is Alpha054 from WorldQuant 101?", "input": "", "output": "Alpha054 from Kakushadze's '101 Formulaic Alphas' (2016).\n\n**Formula:**\n```\nAlpha054 = -((low - close) × open^5) / ((low - high) × close^5)\n```\n\n**Simplified:**\n```\nAlpha054 = ((close - low) / (high - low)) × (open/close)^5\n```\n\n**Interpretation:**\n- First term: Where close is in day's range (0 to 1)\n- Second term: Open/close ratio raised to 5th power\n- Captures gap behavior and intraday reversal\n\n**Implementation:**\n```python\ndef alpha054(open_price, high, low, close):\n    numerator = -(low - close) * (open_price ** 5)\n    denominator = (low - high) * (close ** 5) + 1e-8\n    return numerator / denominator\n```\n\n**Note:** The formula is sensitive to near-zero denominators when high ≈ low."}
{"instruction": "Explain Realized Volatility calculation", "input": "", "output": "Realized Volatility (RV) measures actual historical volatility from high-frequency returns.\n\n**Formula:**\n```\nRV_t = sqrt(Σ r²_i)\n```\n\n**Where:**\n- `r_i` = Intraday returns at interval i\n- Sum over all intervals in day t\n\n**With subsampling (for noise reduction):**\n```\nRV_avg = (1/K) × Σ RV^(k)\n```\n\n**Two-Scale Realized Volatility (TSRV):**\n```\nTSRV = RV_slow - (n_bar/n) × RV_fast\n```\n\n**Sampling frequency trade-off:**\n- High frequency (1-sec): More data but microstructure noise\n- Low frequency (5-min): Less noise but fewer observations\n- Optimal: ~5 minutes for most markets\n\n**Annualized RV:**\n```\nRV_annual = RV_daily × sqrt(252)\n```\n\n**Typical forex daily RV:** 0.5-1.0% for majors"}
{"instruction": "What is the Information Ratio?", "input": "", "output": "Information Ratio measures active return relative to tracking error.\n\n**Formula:**\n```\nIR = (R_p - R_b) / TE\n```\n\n**Where:**\n- `R_p` = Portfolio return\n- `R_b` = Benchmark return\n- `TE` = Tracking Error = std(R_p - R_b)\n\n**Annualized:**\n```\nIR_annual = IR_daily × sqrt(252)\n```\n\n**Relationship to IC:**\n```\nIR ≈ IC × sqrt(Breadth)\n```\n\n**Where:**\n- `IC` = Information Coefficient (correlation of signals to returns)\n- `Breadth` = Number of independent bets per year\n\n**Interpretation:**\n- IR < 0.5: Poor skill\n- IR 0.5-1.0: Good active management\n- IR > 1.0: Excellent (rare to sustain)\n\n**For quant strategies:** IR ~0.5-1.5 typical"}
{"instruction": "Calculate Maximum Drawdown", "input": "", "output": "Maximum Drawdown (MDD) measures the largest peak-to-trough decline.\n\n**Formula:**\n```\nDD_t = (HWM_t - V_t) / HWM_t\nMDD = max(DD_t) for all t\n```\n\n**Where:**\n- `HWM_t` = High Water Mark = max(V_0, ..., V_t)\n- `V_t` = Portfolio value at time t\n\n**Implementation:**\n```python\ndef max_drawdown(returns):\n    cumulative = (1 + returns).cumprod()\n    hwm = cumulative.cummax()\n    drawdown = (hwm - cumulative) / hwm\n    return drawdown.max()\n```\n\n**Duration metrics:**\n- Drawdown Duration: Time from peak to trough\n- Recovery Time: Time from trough back to peak\n- Underwater Period: Total time below HWM\n\n**Risk management:**\n- Set max DD limit (e.g., 10%)\n- Reduce position when approaching limit\n- Stop trading if limit breached"}
{"instruction": "What is the Treynor Ratio?", "input": "", "output": "The Treynor Ratio was developed by Jack Treynor (1965) and measures risk-adjusted return using systematic risk.\n\n**Formula:**\n```\nTR = (R_p - R_f) / beta_p\n```\n\n**Where:**\n- R_p = Portfolio return\n- R_f = Risk-free rate\n- beta_p = Portfolio beta (systematic risk)\n\n**Beta calculation:**\n```\nbeta = Cov(R_p, R_m) / Var(R_m)\n```\n\n**Comparison to Sharpe:**\n- Sharpe uses total risk (std dev)\n- Treynor uses systematic risk only (beta)\n- Use Treynor when portfolio is well-diversified\n\n**Implementation:**\n```python\ndef treynor_ratio(returns, benchmark_returns, rf_rate=0.02):\n    excess = returns - rf_rate/252\n    bench_excess = benchmark_returns - rf_rate/252\n    cov = np.cov(excess, bench_excess)\n    beta = cov[0,1] / cov[1,1]\n    return np.mean(excess) * 252 / beta\n```\n\n**Citation:** Treynor, J. (1965). How to Rate Management of Investment Funds."}
{"instruction": "Explain Factor Turnover and Half-Life", "input": "", "output": "Factor Turnover and Half-Life are critical metrics for alpha decay analysis used by Chinese quant firms.\n\n**Factor Turnover Formula:**\n```\nTO_t = Sum(|w_t - w_{t-1}|)\n```\n\n**Factor Half-Life Formula:**\n```\nHL = -log(2) / log(autocorr)\nautocorr = corr(IC_t, IC_{t-1})\n```\n\n**Interpretation:**\n- Half-life = Time for IC to decay to 50%\n- Short HL (< 5 days) = Fast decay, need frequent rebalancing\n- Long HL (> 20 days) = Slow decay, can hold positions longer\n\n**Implementation:**\n```python\ndef factor_turnover(weights_t, weights_prev):\n    return np.sum(np.abs(weights_t - weights_prev))\n\ndef factor_half_life(ic_series):\n    autocorr = np.corrcoef(ic_series[:-1], ic_series[1:])[0,1]\n    if autocorr <= 0 or autocorr >= 1:\n        return np.inf\n    return -np.log(2) / np.log(autocorr)\n```\n\n**Used by:** 幻方量化, 九坤投资, 长江证券\n\n**Citation:** Qlib factor analysis, 长江证券 Research (2024)"}
{"instruction": "What is Rank IC (Spearman IC)?", "input": "", "output": "Rank IC uses Spearman correlation for robust prediction quality measurement.\n\n**Formula:**\n```\nRank_IC = corr(rank(predictions), rank(actuals))\n```\n\n**ICIR (IC Information Ratio):**\n```\nICIR = mean(IC) / std(IC)\n```\n\n**Advantages over Pearson IC:**\n- More robust to outliers\n- Captures monotonic relationships\n- Better for fat-tailed distributions\n\n**Implementation:**\n```python\nfrom scipy.stats import spearmanr\n\ndef rank_ic(predictions, actuals):\n    return spearmanr(predictions, actuals)[0]\n\ndef ic_ir(ic_series):\n    return np.mean(ic_series) / np.std(ic_series)\n```\n\n**Targets:**\n- IC > 0.03: Weak but usable signal\n- IC > 0.05: Good signal\n- ICIR > 0.5: Production-ready\n\n**Used by:** 明汯投资, All major Chinese quant firms\n\n**Citation:** Qlib documentation, Microsoft Research"}
{"instruction": "Explain Lee-Mykland Jump Detection", "input": "", "output": "Lee-Mykland (2008) is a nonparametric test for detecting jumps in high-frequency data.\n\n**Test Statistic:**\n```\nL_i = |r_i| / sigma_i\nsigma_i = sqrt(BV_i)\n```\n\n**Bipower Variation:**\n```\nBV = (pi/2) * Sum(|r_t| * |r_{t-1}|) / (n-1)\n```\n\n**Critical Value:**\n```\nC_n = sqrt(2*log(n)) - (log(pi) + log(log(n))) / (2*sqrt(2*log(n)))\n```\n\n**Jump Detection:** Jump if L_i > C_n\n\n**Implementation:**\n```python\ndef lee_mykland_jump(returns, k=270):\n    n = len(returns)\n    jumps = np.zeros(n, dtype=bool)\n    for i in range(k, n):\n        window = returns[i-k:i]\n        bv = (np.pi/2) * np.sum(np.abs(window[1:]) * np.abs(window[:-1])) / (k-1)\n        sigma = np.sqrt(bv)\n        L = np.abs(returns[i]) / sigma if sigma > 0 else 0\n        c_n = np.sqrt(2*np.log(n)) - (np.log(np.pi) + np.log(np.log(n))) / (2*np.sqrt(2*np.log(n)))\n        jumps[i] = L > c_n\n    return jumps\n```\n\n**Citation:** Lee & Mykland (2008). Jumps in Financial Markets. Review of Financial Studies."}
{"instruction": "What is MAD Winsorization?", "input": "", "output": "MAD Winsorization is a robust outlier handling technique for factor preprocessing.\n\n**Formula:**\n```\nx_win = clip(x, median - n*MAD*1.4826, median + n*MAD*1.4826)\nMAD = median(|x - median(x)|)\n```\n\n**Where:**\n- 1.4826 = consistency constant for normal distribution\n- n = Number of MADs (typically 3)\n\n**Why MAD over Standard Deviation:**\n- MAD has 50% breakdown point (robust to outliers)\n- Std dev is sensitive to extreme values\n- Better for fat-tailed financial data\n\n**Implementation:**\n```python\ndef winsorize_mad(x, n_mad=3.0):\n    median = np.median(x)\n    mad = np.median(np.abs(x - median))\n    lower = median - n_mad * mad * 1.4826\n    upper = median + n_mad * mad * 1.4826\n    return np.clip(x, lower, upper)\n```\n\n**Used by:** All Chinese quant firms for factor preprocessing\n\n**Citation:** Standard factor engineering, Barra methodology"}
{"instruction": "Explain FEDformer Frequency Enhanced Attention", "input": "", "output": "FEDformer (ICML 2022) uses frequency domain attention for efficient time series forecasting.\n\n**Key Innovation:**\n- Standard attention: O(L^2) complexity\n- FEDformer: O(L) complexity via Fourier transform\n\n**Frequency Enhanced Attention:**\n```\nFEA(Q, K, V) = IFFT(FFT(Q) * FFT(K)^*) * V\n```\n\n**Key insight:** Most time series have sparse representation in Fourier basis.\n\n**Implementation:**\n```python\nclass FEA(nn.Module):\n    def __init__(self, d_model, modes=32):\n        self.modes = modes  # Keep top-k Fourier modes\n\n    def forward(self, q, k, v):\n        q_fft = torch.fft.rfft(q, dim=-2)[:, :, :self.modes]\n        k_fft = torch.fft.rfft(k, dim=-2)[:, :, :self.modes]\n        attn = q_fft * k_fft.conj()\n        return torch.fft.irfft(attn, dim=-2)\n```\n\n**Results:** 14.8% error reduction vs SOTA.\n\n**Citation:** Zhou et al. (2022). FEDformer. ICML."}
{"instruction": "What are Tick Imbalance Bars?", "input": "", "output": "Tick Imbalance Bars (TIB) trigger based on order flow imbalance, not time.\n\n**Formula:**\n```\nTI = Sum(sign(Delta_P_i) * V_i)\nTrigger new bar when |TI| > E[TI]\n```\n\n**Advantages over time bars:**\n- Synchronized with information arrival\n- More uniform statistical properties\n- Better for ML models\n\n**Implementation:**\n```python\ndef tick_imbalance_bars(prices, volumes, expected_imb):\n    bars = []\n    tick_imb = 0\n    current_start = 0\n\n    for i in range(1, len(prices)):\n        sign = np.sign(prices[i] - prices[i-1])\n        tick_imb += sign * volumes[i]\n\n        if np.abs(tick_imb) > expected_imb:\n            bars.append({\n                'start': current_start, 'end': i,\n                'open': prices[current_start],\n                'close': prices[i],\n                'imbalance': tick_imb\n            })\n            current_start = i + 1\n            tick_imb = 0\n    return bars\n```\n\n**Used by:** 九坤投资 for HFT\n\n**Citation:** Lopez de Prado (2018). Advances in Financial Machine Learning."}
{"instruction": "Explain BNS Jump Test (Barndorff-Nielsen Shephard)", "input": "", "output": "BNS Jump Test detects jumps using the difference between realized and bipower variation.\n\n**Test Statistic:**\n```\nZ = (RV - BV) / sqrt(Theta * max(RQ, BQ))\n```\n\n**Where:**\n- RV = Realized Variance = Sum(r_i^2)\n- BV = Bipower Variation = (pi/2) * Sum(|r_i| * |r_{i-1}|)\n- RQ = Realized Quarticity\n- BQ = Bipower Quarticity\n- Theta = adjustment factor\n\n**Key Insight:**\n- RV captures both continuous and jump variation\n- BV is robust to jumps (only continuous)\n- Difference RV - BV isolates jump component\n\n**Implementation:**\n```python\ndef bns_jump_test(returns):\n    n = len(returns)\n    rv = np.sum(returns**2)\n    bv = (np.pi/2) * np.sum(np.abs(returns[1:]) * np.abs(returns[:-1]))\n\n    # Realized quarticity\n    rq = (n/3) * np.sum(returns**4)\n\n    # Test statistic\n    theta = (np.pi**2/4 + np.pi - 5) * max(rq, bv**2)\n    z = (rv - bv) / np.sqrt(theta) if theta > 0 else 0\n\n    # p-value (standard normal)\n    from scipy.stats import norm\n    p_value = 1 - norm.cdf(z)\n    return z, p_value\n```\n\n**Citation:** Barndorff-Nielsen & Shephard (2006). Econometrica."}
{"instruction": "What is Factor Neutralization?", "input": "", "output": "Factor Neutralization removes unwanted exposures (industry, size) from alpha factors.\n\n**Formula:**\n```\nfactor_neutral = factor - beta_ind * r_ind - beta_size * r_size\n```\n\n**Regression-based neutralization:**\n```\nfactor = alpha + beta_1*industry + beta_2*log(mcap) + epsilon\nfactor_neutral = epsilon (residual)\n```\n\n**Types of Neutralization:**\n1. Industry neutralization: Remove sector effects\n2. Size neutralization: Remove market cap effects\n3. Beta neutralization: Remove market exposure\n\n**Implementation:**\n```python\nfrom sklearn.linear_model import LinearRegression\n\ndef neutralize_factor(factor, industry, market_cap):\n    X = np.column_stack([\n        pd.get_dummies(industry),  # Industry dummies\n        np.log(market_cap)         # Log market cap\n    ])\n    model = LinearRegression().fit(X, factor)\n    return factor - model.predict(X)\n```\n\n**Why Neutralize:**\n- Isolate pure alpha from risk factors\n- Required by Barra risk model\n- Reduces unintended bets\n\n**Used by:** All institutional quant firms\n\n**Citation:** Barra CNE5/CNE6 methodology, MSCI"}
{"instruction": "Explain IC Decay Analysis", "input": "", "output": "IC Decay measures how factor predictive power decreases over time horizons.\n\n**Formula:**\n```\nIC_tau = E[corr(signal_t, return_{t+tau})]\nfor tau = 1, 5, 10, 20, 60 days\n```\n\n**IC Decay Curve:**\n```\nIC(tau) = IC_0 * exp(-tau / lambda)\n```\nWhere lambda = decay constant\n\n**Key Metrics:**\n1. Peak IC horizon: tau where IC is maximum\n2. Half-life: tau where IC drops to 50%\n3. Decay rate: -d(IC)/d(tau)\n\n**Implementation:**\n```python\ndef ic_decay_curve(signals, returns, horizons=[1,5,10,20,60]):\n    ic_curve = {}\n    for h in horizons:\n        future_returns = returns.shift(-h)\n        ic = signals.corrwith(future_returns).mean()\n        ic_curve[h] = ic\n    return ic_curve\n\ndef fit_decay_constant(ic_curve):\n    from scipy.optimize import curve_fit\n    def exp_decay(x, ic0, lam):\n        return ic0 * np.exp(-x / lam)\n\n    x = np.array(list(ic_curve.keys()))\n    y = np.array(list(ic_curve.values()))\n    popt, _ = curve_fit(exp_decay, x, y, p0=[y[0], 20])\n    return popt[1]  # lambda (decay constant)\n```\n\n**Trading Implications:**\n- Fast decay (HL < 5d): High turnover strategy\n- Slow decay (HL > 20d): Low turnover, lower costs\n\n**Used by:** 幻方量化, 长江证券\n\n**Citation:** Qlib factor analysis framework"}
{"instruction": "What is WaveNet for Financial Time Series?", "input": "", "output": "WaveNet uses dilated causal convolutions for financial time series prediction.\n\n**Dilated Causal Convolution:**\n```\ny_t = Sum_k(w_k * x_{t - d*k})\n```\nWhere d = 2^layer (dilation rate doubles each layer)\n\n**Key Properties:**\n- Receptive field grows exponentially: RF = 2^n * kernel_size\n- Causal: No future information leakage\n- Efficient: Fewer parameters than RNNs\n\n**Gated Activation:**\n```\nz = tanh(W_f * x) * sigmoid(W_g * x)\n```\n\n**Architecture:**\n```\nInput -> [Dilated Conv (d=1) -> Gated -> Residual] ->\n      -> [Dilated Conv (d=2) -> Gated -> Residual] ->\n      -> [Dilated Conv (d=4) -> Gated -> Residual] ->\n      -> Skip connections -> Output\n```\n\n**Implementation:**\n```python\nclass WaveNetBlock(nn.Module):\n    def __init__(self, channels, kernel_size, dilation):\n        self.conv = nn.Conv1d(channels, channels*2,\n                              kernel_size, dilation=dilation,\n                              padding=(kernel_size-1)*dilation)\n\n    def forward(self, x):\n        out = self.conv(x)[:, :, :x.size(2)]  # Causal\n        gate, filt = out.chunk(2, dim=1)\n        return x + torch.tanh(filt) * torch.sigmoid(gate)\n```\n\n**Results:** 20-37% better than naive baselines for price prediction\n\n**Citation:** van den Oord et al. (2016). WaveNet.\nBorovykh et al. (2017). arXiv:1703.04691"}
{"instruction": "What is the Treynor Ratio?", "input": "", "output": "The Treynor Ratio was developed by Jack Treynor (1965) and measures risk-adjusted return using systematic risk.\n\n**Formula:**\n```\nTR = (R_p - R_f) / beta_p\n```\n\n**Where:**\n- R_p = Portfolio return\n- R_f = Risk-free rate\n- beta_p = Portfolio beta (systematic risk)\n\n**Beta calculation:**\n```\nbeta = Cov(R_p, R_m) / Var(R_m)\n```\n\n**Comparison to Sharpe:**\n- Sharpe uses total risk (std dev)\n- Treynor uses systematic risk only (beta)\n- Use Treynor when portfolio is well-diversified\n\n**Implementation:**\n```python\ndef treynor_ratio(returns, benchmark_returns, rf_rate=0.02):\n    excess = returns - rf_rate/252\n    bench_excess = benchmark_returns - rf_rate/252\n    cov = np.cov(excess, bench_excess)\n    beta = cov[0,1] / cov[1,1]\n    return np.mean(excess) * 252 / beta\n```\n\n**Citation:** Treynor, J. (1965). How to Rate Management of Investment Funds."}
{"instruction": "Explain Factor Turnover and Half-Life", "input": "", "output": "Factor Turnover and Half-Life are critical metrics for alpha decay analysis used by Chinese quant firms.\n\n**Factor Turnover Formula:**\n```\nTO_t = Sum(|w_t - w_{t-1}|)\n```\n\n**Factor Half-Life Formula:**\n```\nHL = -log(2) / log(autocorr)\nautocorr = corr(IC_t, IC_{t-1})\n```\n\n**Interpretation:**\n- Half-life = Time for IC to decay to 50%\n- Short HL (< 5 days) = Fast decay, need frequent rebalancing\n- Long HL (> 20 days) = Slow decay, can hold positions longer\n\n**Implementation:**\n```python\ndef factor_turnover(weights_t, weights_prev):\n    return np.sum(np.abs(weights_t - weights_prev))\n\ndef factor_half_life(ic_series):\n    autocorr = np.corrcoef(ic_series[:-1], ic_series[1:])[0,1]\n    if autocorr <= 0 or autocorr >= 1:\n        return np.inf\n    return -np.log(2) / np.log(autocorr)\n```\n\n**Used by:** 幻方量化, 九坤投资, 长江证券\n\n**Citation:** Qlib factor analysis, 长江证券 Research (2024)"}
{"instruction": "What is Rank IC (Spearman IC)?", "input": "", "output": "Rank IC uses Spearman correlation for robust prediction quality measurement.\n\n**Formula:**\n```\nRank_IC = corr(rank(predictions), rank(actuals))\n```\n\n**ICIR (IC Information Ratio):**\n```\nICIR = mean(IC) / std(IC)\n```\n\n**Advantages over Pearson IC:**\n- More robust to outliers\n- Captures monotonic relationships\n- Better for fat-tailed distributions\n\n**Implementation:**\n```python\nfrom scipy.stats import spearmanr\n\ndef rank_ic(predictions, actuals):\n    return spearmanr(predictions, actuals)[0]\n\ndef ic_ir(ic_series):\n    return np.mean(ic_series) / np.std(ic_series)\n```\n\n**Targets:**\n- IC > 0.03: Weak but usable signal\n- IC > 0.05: Good signal\n- ICIR > 0.5: Production-ready\n\n**Used by:** 明汯投资, All major Chinese quant firms\n\n**Citation:** Qlib documentation, Microsoft Research"}
{"instruction": "Explain Lee-Mykland Jump Detection", "input": "", "output": "Lee-Mykland (2008) is a nonparametric test for detecting jumps in high-frequency data.\n\n**Test Statistic:**\n```\nL_i = |r_i| / sigma_i\nsigma_i = sqrt(BV_i)\n```\n\n**Bipower Variation:**\n```\nBV = (pi/2) * Sum(|r_t| * |r_{t-1}|) / (n-1)\n```\n\n**Critical Value:**\n```\nC_n = sqrt(2*log(n)) - (log(pi) + log(log(n))) / (2*sqrt(2*log(n)))\n```\n\n**Jump Detection:** Jump if L_i > C_n\n\n**Implementation:**\n```python\ndef lee_mykland_jump(returns, k=270):\n    n = len(returns)\n    jumps = np.zeros(n, dtype=bool)\n    for i in range(k, n):\n        window = returns[i-k:i]\n        bv = (np.pi/2) * np.sum(np.abs(window[1:]) * np.abs(window[:-1])) / (k-1)\n        sigma = np.sqrt(bv)\n        L = np.abs(returns[i]) / sigma if sigma > 0 else 0\n        c_n = np.sqrt(2*np.log(n)) - (np.log(np.pi) + np.log(np.log(n))) / (2*np.sqrt(2*np.log(n)))\n        jumps[i] = L > c_n\n    return jumps\n```\n\n**Citation:** Lee & Mykland (2008). Jumps in Financial Markets. Review of Financial Studies."}
{"instruction": "What is MAD Winsorization?", "input": "", "output": "MAD Winsorization is a robust outlier handling technique for factor preprocessing.\n\n**Formula:**\n```\nx_win = clip(x, median - n*MAD*1.4826, median + n*MAD*1.4826)\nMAD = median(|x - median(x)|)\n```\n\n**Where:**\n- 1.4826 = consistency constant for normal distribution\n- n = Number of MADs (typically 3)\n\n**Why MAD over Standard Deviation:**\n- MAD has 50% breakdown point (robust to outliers)\n- Std dev is sensitive to extreme values\n- Better for fat-tailed financial data\n\n**Implementation:**\n```python\ndef winsorize_mad(x, n_mad=3.0):\n    median = np.median(x)\n    mad = np.median(np.abs(x - median))\n    lower = median - n_mad * mad * 1.4826\n    upper = median + n_mad * mad * 1.4826\n    return np.clip(x, lower, upper)\n```\n\n**Used by:** All Chinese quant firms for factor preprocessing\n\n**Citation:** Standard factor engineering, Barra methodology"}
{"instruction": "Explain FEDformer Frequency Enhanced Attention", "input": "", "output": "FEDformer (ICML 2022) uses frequency domain attention for efficient time series forecasting.\n\n**Key Innovation:**\n- Standard attention: O(L^2) complexity\n- FEDformer: O(L) complexity via Fourier transform\n\n**Frequency Enhanced Attention:**\n```\nFEA(Q, K, V) = IFFT(FFT(Q) * FFT(K)^*) * V\n```\n\n**Key insight:** Most time series have sparse representation in Fourier basis.\n\n**Implementation:**\n```python\nclass FEA(nn.Module):\n    def __init__(self, d_model, modes=32):\n        self.modes = modes  # Keep top-k Fourier modes\n\n    def forward(self, q, k, v):\n        q_fft = torch.fft.rfft(q, dim=-2)[:, :, :self.modes]\n        k_fft = torch.fft.rfft(k, dim=-2)[:, :, :self.modes]\n        attn = q_fft * k_fft.conj()\n        return torch.fft.irfft(attn, dim=-2)\n```\n\n**Results:** 14.8% error reduction vs SOTA.\n\n**Citation:** Zhou et al. (2022). FEDformer. ICML."}
{"instruction": "What are Tick Imbalance Bars?", "input": "", "output": "Tick Imbalance Bars (TIB) trigger based on order flow imbalance, not time.\n\n**Formula:**\n```\nTI = Sum(sign(Delta_P_i) * V_i)\nTrigger new bar when |TI| > E[TI]\n```\n\n**Advantages over time bars:**\n- Synchronized with information arrival\n- More uniform statistical properties\n- Better for ML models\n\n**Implementation:**\n```python\ndef tick_imbalance_bars(prices, volumes, expected_imb):\n    bars = []\n    tick_imb = 0\n    current_start = 0\n\n    for i in range(1, len(prices)):\n        sign = np.sign(prices[i] - prices[i-1])\n        tick_imb += sign * volumes[i]\n\n        if np.abs(tick_imb) > expected_imb:\n            bars.append({\n                'start': current_start, 'end': i,\n                'open': prices[current_start],\n                'close': prices[i],\n                'imbalance': tick_imb\n            })\n            current_start = i + 1\n            tick_imb = 0\n    return bars\n```\n\n**Used by:** 九坤投资 for HFT\n\n**Citation:** Lopez de Prado (2018). Advances in Financial Machine Learning."}
{"instruction": "Explain BNS Jump Test (Barndorff-Nielsen Shephard)", "input": "", "output": "BNS Jump Test detects jumps using the difference between realized and bipower variation.\n\n**Test Statistic:**\n```\nZ = (RV - BV) / sqrt(Theta * max(RQ, BQ))\n```\n\n**Where:**\n- RV = Realized Variance = Sum(r_i^2)\n- BV = Bipower Variation = (pi/2) * Sum(|r_i| * |r_{i-1}|)\n- RQ = Realized Quarticity\n- BQ = Bipower Quarticity\n- Theta = adjustment factor\n\n**Key Insight:**\n- RV captures both continuous and jump variation\n- BV is robust to jumps (only continuous)\n- Difference RV - BV isolates jump component\n\n**Implementation:**\n```python\ndef bns_jump_test(returns):\n    n = len(returns)\n    rv = np.sum(returns**2)\n    bv = (np.pi/2) * np.sum(np.abs(returns[1:]) * np.abs(returns[:-1]))\n\n    # Realized quarticity\n    rq = (n/3) * np.sum(returns**4)\n\n    # Test statistic\n    theta = (np.pi**2/4 + np.pi - 5) * max(rq, bv**2)\n    z = (rv - bv) / np.sqrt(theta) if theta > 0 else 0\n\n    # p-value (standard normal)\n    from scipy.stats import norm\n    p_value = 1 - norm.cdf(z)\n    return z, p_value\n```\n\n**Citation:** Barndorff-Nielsen & Shephard (2006). Econometrica."}
{"instruction": "What is Factor Neutralization?", "input": "", "output": "Factor Neutralization removes unwanted exposures (industry, size) from alpha factors.\n\n**Formula:**\n```\nfactor_neutral = factor - beta_ind * r_ind - beta_size * r_size\n```\n\n**Regression-based neutralization:**\n```\nfactor = alpha + beta_1*industry + beta_2*log(mcap) + epsilon\nfactor_neutral = epsilon (residual)\n```\n\n**Types of Neutralization:**\n1. Industry neutralization: Remove sector effects\n2. Size neutralization: Remove market cap effects\n3. Beta neutralization: Remove market exposure\n\n**Implementation:**\n```python\nfrom sklearn.linear_model import LinearRegression\n\ndef neutralize_factor(factor, industry, market_cap):\n    X = np.column_stack([\n        pd.get_dummies(industry),  # Industry dummies\n        np.log(market_cap)         # Log market cap\n    ])\n    model = LinearRegression().fit(X, factor)\n    return factor - model.predict(X)\n```\n\n**Why Neutralize:**\n- Isolate pure alpha from risk factors\n- Required by Barra risk model\n- Reduces unintended bets\n\n**Used by:** All institutional quant firms\n\n**Citation:** Barra CNE5/CNE6 methodology, MSCI"}
{"instruction": "Explain IC Decay Analysis", "input": "", "output": "IC Decay measures how factor predictive power decreases over time horizons.\n\n**Formula:**\n```\nIC_tau = E[corr(signal_t, return_{t+tau})]\nfor tau = 1, 5, 10, 20, 60 days\n```\n\n**IC Decay Curve:**\n```\nIC(tau) = IC_0 * exp(-tau / lambda)\n```\nWhere lambda = decay constant\n\n**Key Metrics:**\n1. Peak IC horizon: tau where IC is maximum\n2. Half-life: tau where IC drops to 50%\n3. Decay rate: -d(IC)/d(tau)\n\n**Implementation:**\n```python\ndef ic_decay_curve(signals, returns, horizons=[1,5,10,20,60]):\n    ic_curve = {}\n    for h in horizons:\n        future_returns = returns.shift(-h)\n        ic = signals.corrwith(future_returns).mean()\n        ic_curve[h] = ic\n    return ic_curve\n\ndef fit_decay_constant(ic_curve):\n    from scipy.optimize import curve_fit\n    def exp_decay(x, ic0, lam):\n        return ic0 * np.exp(-x / lam)\n\n    x = np.array(list(ic_curve.keys()))\n    y = np.array(list(ic_curve.values()))\n    popt, _ = curve_fit(exp_decay, x, y, p0=[y[0], 20])\n    return popt[1]  # lambda (decay constant)\n```\n\n**Trading Implications:**\n- Fast decay (HL < 5d): High turnover strategy\n- Slow decay (HL > 20d): Low turnover, lower costs\n\n**Used by:** 幻方量化, 长江证券\n\n**Citation:** Qlib factor analysis framework"}
{"instruction": "What is WaveNet for Financial Time Series?", "input": "", "output": "WaveNet uses dilated causal convolutions for financial time series prediction.\n\n**Dilated Causal Convolution:**\n```\ny_t = Sum_k(w_k * x_{t - d*k})\n```\nWhere d = 2^layer (dilation rate doubles each layer)\n\n**Key Properties:**\n- Receptive field grows exponentially: RF = 2^n * kernel_size\n- Causal: No future information leakage\n- Efficient: Fewer parameters than RNNs\n\n**Gated Activation:**\n```\nz = tanh(W_f * x) * sigmoid(W_g * x)\n```\n\n**Architecture:**\n```\nInput -> [Dilated Conv (d=1) -> Gated -> Residual] ->\n      -> [Dilated Conv (d=2) -> Gated -> Residual] ->\n      -> [Dilated Conv (d=4) -> Gated -> Residual] ->\n      -> Skip connections -> Output\n```\n\n**Implementation:**\n```python\nclass WaveNetBlock(nn.Module):\n    def __init__(self, channels, kernel_size, dilation):\n        self.conv = nn.Conv1d(channels, channels*2,\n                              kernel_size, dilation=dilation,\n                              padding=(kernel_size-1)*dilation)\n\n    def forward(self, x):\n        out = self.conv(x)[:, :, :x.size(2)]  # Causal\n        gate, filt = out.chunk(2, dim=1)\n        return x + torch.tanh(filt) * torch.sigmoid(gate)\n```\n\n**Results:** 20-37% better than naive baselines for price prediction\n\n**Citation:** van den Oord et al. (2016). WaveNet.\nBorovykh et al. (2017). arXiv:1703.04691"}
