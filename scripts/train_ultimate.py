#!/usr/bin/env python3
"""
ULTIMATE TRAINER - Maximum GPU + 1083 Features for 89%+ Accuracy
================================================================
Uses:
- MAX GPU settings (depth=16, 3000 trees, 1024 bins)
- MEGA features (1083+ features from all libraries)
- 18 certainty modules for validation

Target: 58.6% â†’ 76% ML accuracy, 89% with certainty gates

Usage:
    python scripts/train_ultimate.py                    # Train all untrained
    python scripts/train_ultimate.py --all --no-skip    # Retrain everything
    python scripts/train_ultimate.py --symbols EURUSD,GBPUSD
    python scripts/train_ultimate.py --status           # Check current status
"""

import sys
from pathlib import Path

# Add project root
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# Initialize max GPU BEFORE any imports
from core.ml.max_gpu_config import maximize_system, get_max_xgb_params, get_max_lgb_params, get_max_catboost_params

import argparse
import json
import pickle
import time
import gc
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import numpy as np
import pandas as pd

import xgboost as xgb
import lightgbm as lgb
import catboost as cb
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score

from core.features.mega_generator import MegaFeatureGenerator


# =============================================================================
# PATHS
# =============================================================================
TRAINING_DIR = PROJECT_ROOT / 'training_package'
OUTPUT_DIR = PROJECT_ROOT / 'models' / 'production'
FEATURES_CACHE = TRAINING_DIR / 'features_cache'
CHECKPOINT_DIR = OUTPUT_DIR / 'checkpoints'


def get_available_pairs() -> List[str]:
    """Get list of pairs with training data."""
    pairs = []
    for d in TRAINING_DIR.iterdir():
        if d.is_dir() and (d / 'train.parquet').exists():
            if d.name not in ['features_cache', 'models', 'catboost_info']:
                pairs.append(d.name)
    return sorted(pairs)


def save_checkpoint(pair: str, target: str, models: Dict, results: Dict, feature_names: List[str]):
    """Save checkpoint after each target is trained."""
    CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)
    checkpoint_path = CHECKPOINT_DIR / f"{pair}_{target}_checkpoint.pkl"

    checkpoint_data = {
        'pair': pair,
        'target': target,
        'models': models,
        'results': results,
        'feature_names': feature_names,
        'timestamp': datetime.now().isoformat()
    }

    with open(checkpoint_path, 'wb') as f:
        pickle.dump(checkpoint_data, f)

    print(f"[CHECKPOINT] Saved {pair} {target} checkpoint")


def load_checkpoint(pair: str) -> Optional[Dict]:
    """Load any existing checkpoints for a pair."""
    checkpoints = {}

    for target in ['target_direction_1', 'target_direction_5', 'target_direction_10']:
        checkpoint_path = CHECKPOINT_DIR / f"{pair}_{target}_checkpoint.pkl"
        if checkpoint_path.exists():
            try:
                with open(checkpoint_path, 'rb') as f:
                    checkpoints[target] = pickle.load(f)
                print(f"[CHECKPOINT] Loaded {pair} {target} from checkpoint")
            except Exception as e:
                print(f"[CHECKPOINT] Failed to load {pair} {target}: {e}")

    return checkpoints if checkpoints else None


def clear_checkpoints(pair: str):
    """Clear all checkpoints for a pair after successful completion."""
    for target in ['target_direction_1', 'target_direction_5', 'target_direction_10']:
        checkpoint_path = CHECKPOINT_DIR / f"{pair}_{target}_checkpoint.pkl"
        if checkpoint_path.exists():
            checkpoint_path.unlink()
    print(f"[CHECKPOINT] Cleared checkpoints for {pair}")


def get_trained_pairs() -> Dict[str, Dict]:
    """Get pairs that already have results."""
    trained = {}
    for f in OUTPUT_DIR.glob('*_results.json'):
        pair = f.stem.replace('_results', '')
        with open(f) as fp:
            trained[pair] = json.load(fp)
    return trained


def print_status():
    """Print current training status."""
    available = get_available_pairs()
    trained = get_trained_pairs()

    print("\n" + "=" * 70)
    print("TRAINING STATUS")
    print("=" * 70)
    print(f"\nTotal available pairs: {len(available)}")
    print(f"Already trained: {len(trained)}")
    print(f"Remaining: {len(available) - len(trained)}")

    # Accuracy distribution
    if trained:
        accuracies = []
        for pair, results in trained.items():
            for target, metrics in results.items():
                if target.startswith('target_direction'):
                    acc = metrics.get('accuracy', 0)
                    if acc > 0:
                        accuracies.append((pair, target, acc))

        if accuracies:
            accuracies.sort(key=lambda x: -x[2])
            avg_acc = np.mean([a[2] for a in accuracies])

            print(f"\n--- Accuracy Distribution ---")
            print(f"Average accuracy: {avg_acc:.1%}")
            print(f"At 70%+: {sum(1 for _,_,a in accuracies if a >= 0.70)}")
            print(f"At 65%+: {sum(1 for _,_,a in accuracies if a >= 0.65)}")
            print(f"At 60%+: {sum(1 for _,_,a in accuracies if a >= 0.60)}")

            print(f"\n--- Top 10 Performers ---")
            for pair, target, acc in accuracies[:10]:
                horizon = target.replace('target_direction_', '')
                print(f"  {pair} ({horizon}-tick): {acc:.1%}")

            print(f"\n--- Bottom 5 ---")
            for pair, target, acc in accuracies[-5:]:
                horizon = target.replace('target_direction_', '')
                print(f"  {pair} ({horizon}-tick): {acc:.1%}")

    # Check feature counts
    print(f"\n--- Feature Counts ---")
    sample_results = list(trained.values())[:3] if trained else []
    for res in sample_results:
        meta = res.get('_meta', {})
        feat_count = meta.get('feature_count', 0)
        print(f"  Feature count: {feat_count}")

    print("\n" + "=" * 70)


def generate_mega_features(pair: str, max_samples: int = 50000) -> Tuple[
    np.ndarray, np.ndarray, np.ndarray,
    Dict[str, np.ndarray], Dict[str, np.ndarray], Dict[str, np.ndarray],
    List[str], List[str]
]:
    """Generate mega features for a pair."""
    pair_dir = TRAINING_DIR / pair

    print(f"[{pair}] Loading data...")
    train = pd.read_parquet(pair_dir / 'train.parquet')
    val = pd.read_parquet(pair_dir / 'val.parquet')
    test = pd.read_parquet(pair_dir / 'test.parquet')

    # Ensure close column
    for df in [train, val, test]:
        if 'close' not in df.columns and 'mid' in df.columns:
            df['close'] = df['mid']

    print(f"[{pair}] Generating MEGA features (1083+)...")

    # Initialize MEGA generator with ALL features enabled
    generator = MegaFeatureGenerator(
        enable_alpha101=True,        # 62
        enable_alpha158=True,        # 179
        enable_alpha360=True,        # 276 (lookback=46)
        enable_barra=True,           # 46
        enable_alpha191=True,        # 191
        enable_chinese_hft=True,     # 50
        enable_chinese_additions=True,  # 25
        enable_chinese_gold=True,    # 35
        enable_us_academic=True,     # 50
        enable_mlfinlab=True,        # 17
        enable_timeseries=True,      # 41
        enable_renaissance=True,     # 51
        enable_india=True,           # 25
        enable_japan=True,           # 30
        enable_europe=True,          # 15
        enable_emerging=True,        # 20
        enable_universal_math=True,  # 30
        enable_rl=True,              # 35
        enable_moe=True,             # 20
        enable_gnn=True,             # 15
        enable_korea=True,           # 20
        enable_asia_fx=True,         # 15
        enable_marl=True,            # 15
        alpha360_lookback=46,        # 46 * 6 = 276 features
        verbose=True
    )

    # Generate features for all splits
    train_features = generator.generate_all(train)
    val_features = generator.generate_all(val)
    test_features = generator.generate_all(test)

    # Get numeric columns only
    numeric_cols = train_features.select_dtypes(include=[np.number]).columns.tolist()

    X_train = train_features[numeric_cols].values
    X_val = val_features[numeric_cols].values
    X_test = test_features[numeric_cols].values

    # Get target columns
    target_cols = [c for c in train.columns if c.startswith('target_direction')]

    y_train = {t: train[t].values for t in target_cols}
    y_val = {t: val[t].values for t in target_cols}
    y_test = {t: test[t].values for t in target_cols}

    # Subsample if needed
    if len(X_train) > max_samples:
        idx = np.random.choice(len(X_train), max_samples, replace=False)
        X_train = X_train[idx]
        y_train = {t: y_train[t][idx] for t in target_cols}

    print(f"[{pair}] Features: {len(numeric_cols)}, Samples: {len(X_train)}")

    # Clean up
    del train, val, test, train_features, val_features, test_features
    gc.collect()

    return X_train, X_val, X_test, y_train, y_val, y_test, numeric_cols, target_cols


def train_pair(
    pair: str,
    X_train: np.ndarray,
    X_val: np.ndarray,
    X_test: np.ndarray,
    y_train: Dict[str, np.ndarray],
    y_val: Dict[str, np.ndarray],
    y_test: Dict[str, np.ndarray],
    feature_names: List[str],
    target_cols: List[str],
    verbose: bool = True,
    existing_checkpoints: Optional[Dict] = None
) -> Tuple[Dict, Dict]:
    """Train XGB + LGB + CB with MAX GPU settings and checkpointing."""

    # Get MAX GPU params
    xgb_params = get_max_xgb_params()
    lgb_params = get_max_lgb_params()
    cb_params = get_max_catboost_params()

    if verbose:
        print(f"\n[{pair}] Training with MAX GPU settings:")
        print(f"  XGBoost: depth={xgb_params['max_depth']}, n_estimators={xgb_params['n_estimators']}")
        print(f"  LightGBM: depth={lgb_params['max_depth']}, num_leaves={lgb_params['num_leaves']}")
        print(f"  CatBoost: depth={cb_params['depth']}, iterations={cb_params['iterations']}")

    results = {}
    models = {}

    # Load from checkpoints if available
    if existing_checkpoints:
        for target, checkpoint in existing_checkpoints.items():
            results[target] = checkpoint['results'].get(target, {})
            models[target] = checkpoint['models'].get(target, {})
            print(f"[{pair}] Restored {target} from checkpoint")

    # Train on top 3 target horizons
    for target in target_cols[:3]:
        # Skip if already have from checkpoint
        if target in results and results[target]:
            print(f"[{pair}] Skipping {target} (from checkpoint)")
            continue
        if verbose:
            print(f"\n[{pair}] Training {target}...")

        y_tr = y_train[target]
        y_v = y_val[target]
        y_te = y_test[target]

        start = time.time()

        # ============ XGBoost ============
        dtrain = xgb.DMatrix(X_train, label=y_tr)
        dval = xgb.DMatrix(X_val, label=y_v)

        xgb_model = xgb.train(
            xgb_params,
            dtrain,
            num_boost_round=xgb_params['n_estimators'],
            evals=[(dval, 'val')],
            early_stopping_rounds=xgb_params['early_stopping_rounds'],
            verbose_eval=100 if verbose else False
        )

        xgb_time = time.time() - start
        if verbose:
            print(f"  XGBoost: {xgb_time:.1f}s, best_iter={xgb_model.best_iteration}")

        # ============ LightGBM ============
        start = time.time()

        train_data = lgb.Dataset(X_train, label=y_tr)
        val_data = lgb.Dataset(X_val, label=y_v, reference=train_data)

        lgb_model = lgb.train(
            lgb_params,
            train_data,
            num_boost_round=lgb_params['n_estimators'],
            valid_sets=[val_data],
            callbacks=[lgb.early_stopping(lgb_params['early_stopping_rounds'], verbose=verbose)]
        )

        lgb_time = time.time() - start
        if verbose:
            print(f"  LightGBM: {lgb_time:.1f}s, best_iter={lgb_model.best_iteration}")

        # ============ CatBoost ============
        start = time.time()

        cb_model = cb.CatBoostClassifier(**cb_params)
        cb_model.fit(X_train, y_tr, eval_set=(X_val, y_v), verbose=100 if verbose else False)

        cb_time = time.time() - start
        if verbose:
            print(f"  CatBoost: {cb_time:.1f}s, best_iter={cb_model.best_iteration_}")

        # ============ Ensemble Predictions ============
        dtest = xgb.DMatrix(X_test)
        xgb_pred = xgb_model.predict(dtest)
        lgb_pred = lgb_model.predict(X_test)
        cb_pred = cb_model.predict_proba(X_test)[:, 1]

        ensemble_pred = (xgb_pred + lgb_pred + cb_pred) / 3
        ensemble_class = (ensemble_pred > 0.5).astype(int)

        # ============ Metrics ============
        acc = accuracy_score(y_te, ensemble_class)
        auc = roc_auc_score(y_te, ensemble_pred)
        f1 = f1_score(y_te, ensemble_class)

        total_time = xgb_time + lgb_time + cb_time

        print(f"[{pair}] {target}: ACC={acc:.4f}, AUC={auc:.4f}, F1={f1:.4f} ({total_time:.1f}s)")

        results[target] = {
            'accuracy': float(acc),
            'auc': float(auc),
            'f1': float(f1),
            'training_time': total_time,
            'xgb_best_iter': int(xgb_model.best_iteration),
            'lgb_best_iter': int(lgb_model.best_iteration),
            'cb_best_iter': int(cb_model.best_iteration_),
        }

        models[target] = {
            'xgb': xgb_model,
            'lgb': lgb_model,
            'cb': cb_model,
        }

        # CHECKPOINT: Save after each target completes
        save_checkpoint(pair, target, models, results, feature_names)

        # Clean up intermediate data
        del dtrain, dval, dtest
        gc.collect()

    return results, models


def save_results(pair: str, results: Dict, models: Dict, feature_names: List[str]):
    """Save models and results."""
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    # Add metadata
    results['_meta'] = {
        'feature_mode': 'mega_ultimate',
        'feature_count': len(feature_names),
        'timestamp': datetime.now().isoformat(),
        'gpu_config': 'max'
    }

    # Save results
    results_path = OUTPUT_DIR / f"{pair}_results.json"
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)

    # Save models
    models_path = OUTPUT_DIR / f"{pair}_models.pkl"
    with open(models_path, 'wb') as f:
        pickle.dump(models, f)

    print(f"[{pair}] Saved to {OUTPUT_DIR}")


def train_single_pair(pair: str, verbose: bool = True) -> bool:
    """Train a single pair with ultimate settings and checkpointing."""
    try:
        # Check for existing checkpoints
        existing_checkpoints = load_checkpoint(pair)
        if existing_checkpoints:
            completed_targets = list(existing_checkpoints.keys())
            print(f"[{pair}] Found checkpoints for: {completed_targets}")

        # Generate features
        X_train, X_val, X_test, y_train, y_val, y_test, feature_names, target_cols = \
            generate_mega_features(pair)

        # Train models (with checkpoint support)
        results, models = train_pair(
            pair, X_train, X_val, X_test,
            y_train, y_val, y_test,
            feature_names, target_cols,
            verbose=verbose,
            existing_checkpoints=existing_checkpoints
        )

        # Save final results
        save_results(pair, results, models, feature_names)

        # Clear checkpoints after successful completion
        clear_checkpoints(pair)

        # Clean up
        del X_train, X_val, X_test, y_train, y_val, y_test, models
        gc.collect()

        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except:
            pass

        return True

    except Exception as e:
        print(f"[{pair}] ERROR: {e}")
        import traceback
        traceback.print_exc()
        print(f"[{pair}] Checkpoints preserved - can resume later")
        return False


def main():
    parser = argparse.ArgumentParser(description='Ultimate trainer with MAX GPU settings')
    parser.add_argument('--symbols', type=str, help='Comma-separated symbols to train')
    parser.add_argument('--all', action='store_true', help='Train all available pairs')
    parser.add_argument('--no-skip', action='store_true', help='Retrain even if already done')
    parser.add_argument('--status', action='store_true', help='Show training status')
    parser.add_argument('--verbose', action='store_true', default=True)

    args = parser.parse_args()

    if args.status:
        print_status()
        return

    # Maximize system for training
    print("\n" + "=" * 70)
    print("ULTIMATE TRAINER - MAX GPU + 1083 FEATURES")
    print("=" * 70)

    maximize_system()

    # Determine pairs to train
    if args.symbols:
        pairs = [s.strip().upper() for s in args.symbols.split(',')]
    elif args.all:
        pairs = get_available_pairs()
    else:
        # Train untrained pairs
        available = get_available_pairs()
        trained = get_trained_pairs()
        pairs = [p for p in available if p not in trained]

    if not args.no_skip:
        trained = get_trained_pairs()
        pairs = [p for p in pairs if p not in trained]

    if not pairs:
        print("\nNo pairs to train! Use --all --no-skip to retrain everything.")
        print_status()
        return

    print(f"\nTraining {len(pairs)} pairs:")
    for i, p in enumerate(pairs):
        print(f"  [{i+1}] {p}")

    # Train each pair
    start_time = time.time()
    success = 0
    failed = []

    for i, pair in enumerate(pairs):
        print(f"\n{'='*70}")
        print(f"TRAINING {pair} ({i+1}/{len(pairs)})")
        print(f"{'='*70}")

        if train_single_pair(pair, verbose=args.verbose):
            success += 1
        else:
            failed.append(pair)

        # Progress update
        elapsed = time.time() - start_time
        rate = (i + 1) / (elapsed / 60)
        remaining = (len(pairs) - i - 1) / max(rate, 0.1)
        print(f"\n[Progress] {i+1}/{len(pairs)} ({rate:.1f} pairs/min, ~{remaining:.0f}min remaining)")

    # Summary
    total_time = time.time() - start_time
    print("\n" + "=" * 70)
    print("TRAINING COMPLETE")
    print("=" * 70)
    print(f"Total time: {total_time/60:.1f} minutes")
    print(f"Success: {success}/{len(pairs)}")
    if failed:
        print(f"Failed: {failed}")

    # Final status
    print_status()


if __name__ == '__main__':
    main()
